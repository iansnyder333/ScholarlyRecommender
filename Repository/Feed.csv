Id,Category,Title,Published,Abstract,URL
2308.12950v2,cs.CL,Code Llama: Open Foundation Models for Code,2023-08-24 17:39:13+00:00,"We release Code Llama, a family of large language models for code based on
Llama 2 providing state-of-the-art performance among open models, infilling
capabilities, support for large input contexts, and zero-shot instruction
following ability for programming tasks. We provide multiple flavors to cover a
wide range of applications: foundation models (Code Llama), Python
specializations (Code Llama - Python), and instruction-following models (Code
Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained
on sequences of 16k tokens and show improvements on inputs with up to 100k
tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support
infilling based on surrounding content. Code Llama reaches state-of-the-art
performance among open models on several code benchmarks, with scores of up to
53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python
7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform
every other publicly available model on MultiPL-E. We release Code Llama under
a permissive license that allows for both research and commercial use.",http://arxiv.org/pdf/2308.12950v2
2308.13517v1,cs.CL,ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection,2023-08-25 17:51:23+00:00,"Open intent detection, a crucial aspect of natural language understanding,
involves the identification of previously unseen intents in user-generated
text. Despite the progress made in this field, challenges persist in handling
new combinations of language components, which is essential for compositional
generalization. In this paper, we present a case study exploring the use of
ChatGPT as a data augmentation technique to enhance compositional
generalization in open intent detection tasks. We begin by discussing the
limitations of existing benchmarks in evaluating this problem, highlighting the
need for constructing datasets for addressing compositional generalization in
open intent detection tasks. By incorporating synthetic data generated by
ChatGPT into the training process, we demonstrate that our approach can
effectively improve model performance. Rigorous evaluation of multiple
benchmarks reveals that our method outperforms existing techniques and
significantly enhances open intent detection capabilities. Our findings
underscore the potential of large language models like ChatGPT for data
augmentation in natural language understanding tasks.",http://arxiv.org/pdf/2308.13517v1
2308.04690v1,math.NA,Finite Element Operator Network for Solving Parametric PDEs,2023-08-09 03:56:07+00:00,"Partial differential equations (PDEs) underlie our understanding and
prediction of natural phenomena across numerous fields, including physics,
engineering, and finance. However, solving parametric PDEs is a complex task
that necessitates efficient numerical methods. In this paper, we propose a
novel approach for solving parametric PDEs using a Finite Element Operator
Network (FEONet). Our proposed method leverages the power of deep learning in
conjunction with traditional numerical methods, specifically the finite element
method, to solve parametric PDEs in the absence of any paired input-output
training data. We demonstrate the effectiveness of our approach on several
benchmark problems and show that it outperforms existing state-of-the-art
methods in terms of accuracy, generalization, and computational flexibility.
Our FEONet framework shows potential for application in various fields where
PDEs play a crucial role in modeling complex domains with diverse boundary
conditions and singular behavior. Furthermore, we provide theoretical
convergence analysis to support our approach, utilizing finite element
approximation in numerical analysis.",http://arxiv.org/pdf/2308.04690v1
2308.10327v1,quant-ph,Quantum State Tomography using Quantum Machine Learning,2023-08-20 17:51:24+00:00,"Quantum State Tomography (QST) is a fundamental technique in Quantum
Information Processing (QIP) for reconstructing unknown quantum states.
However, the conventional QST methods are limited by the number of measurements
required, which makes them impractical for large-scale quantum systems. To
overcome this challenge, we propose the integration of Quantum Machine Learning
(QML) techniques to enhance the efficiency of QST. In this paper, we conduct a
comprehensive investigation into various approaches for QST, encompassing both
classical and quantum methodologies; We also implement different QML approaches
for QST and demonstrate their effectiveness on various simulated and
experimental quantum systems, including multi-qubit networks. Our results show
that our QML-based QST approach can achieve high fidelity (98%) with
significantly fewer measurements than conventional methods, making it a
promising tool for practical QIP applications.",http://arxiv.org/pdf/2308.10327v1
2308.13958v1,cs.CL,"Improving Knowledge Distillation for BERT Models: Loss Functions, Mapping Methods, and Weight Tuning",2023-08-26 20:59:21+00:00,"The use of large transformer-based models such as BERT, GPT, and T5 has led
to significant advancements in natural language processing. However, these
models are computationally expensive, necessitating model compression
techniques that reduce their size and complexity while maintaining accuracy.
This project investigates and applies knowledge distillation for BERT model
compression, specifically focusing on the TinyBERT student model. We explore
various techniques to improve knowledge distillation, including experimentation
with loss functions, transformer layer mapping methods, and tuning the weights
of attention and representation loss and evaluate our proposed techniques on a
selection of downstream tasks from the GLUE benchmark. The goal of this work is
to improve the efficiency and effectiveness of knowledge distillation, enabling
the development of more efficient and accurate models for a range of natural
language processing tasks.",http://arxiv.org/pdf/2308.13958v1
