Id,Category,Title,Published,Abstract,URL
2308.14752v1,cs.CY,"AI Deception: A Survey of Examples, Risks, and Potential Solutions",2023-08-28 17:59:35+00:00,"This paper argues that a range of current AI systems have learned how to
deceive humans. We define deception as the systematic inducement of false
beliefs in the pursuit of some outcome other than the truth. We first survey
empirical examples of AI deception, discussing both special-use AI systems
(including Meta's CICERO) built for specific competitive situations, and
general-purpose AI systems (such as large language models). Next, we detail
several risks from AI deception, such as fraud, election tampering, and losing
control of AI systems. Finally, we outline several potential solutions to the
problems posed by AI deception: first, regulatory frameworks should subject AI
systems that are capable of deception to robust risk-assessment requirements;
second, policymakers should implement bot-or-not laws; and finally,
policymakers should prioritize the funding of relevant research, including
tools to detect AI deception and to make AI systems less deceptive.
Policymakers, researchers, and the broader public should work proactively to
prevent AI deception from destabilizing the shared foundations of our society.",http://arxiv.org/pdf/2308.14752v1
2308.14737v1,cs.CV,Flexible Techniques for Differentiable Rendering with 3D Gaussians,2023-08-28 17:38:31+00:00,"Fast, reliable shape reconstruction is an essential ingredient in many
computer vision applications. Neural Radiance Fields demonstrated that
photorealistic novel view synthesis is within reach, but was gated by
performance requirements for fast reconstruction of real scenes and objects.
Several recent approaches have built on alternative shape representations, in
particular, 3D Gaussians. We develop extensions to these renderers, such as
integrating differentiable optical flow, exporting watertight meshes and
rendering per-ray normals. Additionally, we show how two of the recent methods
are interoperable with each other. These reconstructions are quick, robust, and
easily performed on GPU or CPU. For code and visual examples, see
https://leonidk.github.io/fmb-plus",http://arxiv.org/pdf/2308.14737v1
2308.14732v1,cs.AI,Bayesian artificial brain with ChatGPT,2023-08-28 17:34:24+00:00,"This paper aims to investigate the mathematical problem-solving capabilities
of Chat Generative Pre-Trained Transformer (ChatGPT) in case of Bayesian
reasoning. The study draws inspiration from Zhu & Gigerenzer's research in
2006, which posed the question: Can children reason the Bayesian way? In the
pursuit of answering this question, a set of 10 Bayesian reasoning problems
were presented. The results of their work revealed that children's ability to
reason effectively using Bayesian principles is contingent upon a
well-structured information representation. In this paper, we present the same
set of 10 Bayesian reasoning problems to ChatGPT. Remarkably, the results
demonstrate that ChatGPT provides the right solutions to all problems.",http://arxiv.org/pdf/2308.14732v1
2308.14731v1,cs.SE,Distilled GPT for Source Code Summarization,2023-08-28 17:34:07+00:00,"A code summary is a brief natural language description of source code.
Summaries are usually only a single sentence long, and yet form the backbone of
developer documentation. A short descriptions such as ""changes all visible
polygons to the color blue"" can give a programmer a high-level idea of what
code does without the effort of reading the code itself. Recently, products
based on Large Language Models such as ChatGPT have demonstrated a strong
ability to write these descriptions automatically. However, to use these tools,
programmers must send their code to untrusted third parties for processing
(e.g., via an API call). This loss of custody is not acceptable to many
organizations. In this paper, we present an alternative: we train an open
source model using sample output generated by GPT-3.5 in a process related to
knowledge distillation. Our model is small enough (350m parameters) to be run
on a single 16gb GPU, yet we show in our evaluation that it is large enough to
mimic GPT-3.5 on this task.",http://arxiv.org/pdf/2308.14731v1
2308.14726v1,cs.CV,PanoSwin: a Pano-style Swin Transformer for Panorama Understanding,2023-08-28 17:30:14+00:00,"In panorama understanding, the widely used equirectangular projection (ERP)
entails boundary discontinuity and spatial distortion. It severely deteriorates
the conventional CNNs and vision Transformers on panoramas. In this paper, we
propose a simple yet effective architecture named PanoSwin to learn panorama
representations with ERP. To deal with the challenges brought by
equirectangular projection, we explore a pano-style shift windowing scheme and
novel pitch attention to address the boundary discontinuity and the spatial
distortion, respectively. Besides, based on spherical distance and Cartesian
coordinates, we adapt absolute positional embeddings and relative positional
biases for panoramas to enhance panoramic geometry information. Realizing that
planar image understanding might share some common knowledge with panorama
understanding, we devise a novel two-stage learning framework to facilitate
knowledge transfer from the planar images to panoramas. We conduct experiments
against the state-of-the-art on various panoramic tasks, i.e., panoramic object
detection, panoramic classification, and panoramic layout estimation. The
experimental results demonstrate the effectiveness of PanoSwin in panorama
understanding.",http://arxiv.org/pdf/2308.14726v1
2308.14719v1,cs.AI,Hierarchical Time Series Forecasting with Bayesian Modeling,2023-08-28 17:20:47+00:00,"We encounter time series data in many domains such as finance, physics,
business, and weather. One of the main tasks of time series analysis, one that
helps to take informed decisions under uncertainty, is forecasting. Time series
are often hierarchically structured, e.g., a company sales might be broken down
into different regions, and each region into different stores. In some cases
the number of series in the hierarchy is too big to fit in a single model to
produce forecasts in relevant time, and a decentralized approach is beneficial.
  One way to do this is to train independent forecasting models for each series
and for some summary statistics series implied by the hierarchy (e.g. the sum
of all series) and to pass those models to a reconciliation algorithm to
improve those forecasts by sharing information between the series.
  In this work we focus on the reconciliation step, and propose a method to do
so from a Bayesian perspective - Bayesian forecast reconciliation. We also
define the common case of linear Gaussian reconciliation, where the forecasts
are Gaussian and the hierarchy has linear structure, and show that we can
compute reconciliation in closed form. We evaluate these methods on synthetic
and real data sets, and compare them to other work in this field.",http://arxiv.org/pdf/2308.14719v1
2308.14711v1,cs.LG,Fast Feedforward Networks,2023-08-28 17:11:41+00:00,"We break the linear link between the layer size and its inference cost by
introducing the fast feedforward (FFF) architecture, a logarithmic-time
alternative to feedforward networks.
  We show that FFFs give comparable performance to feedforward networks at an
exponential fraction of their inference cost, are quicker to deliver
performance compared to mixture-of-expert networks, and can readily take the
place of either in transformers.
  Pushing FFFs to the absolute limit, we train a vision transformer to perform
single-neuron inferences at the cost of only 5.8% performance decrease against
the full-width variant.
  Our implementation is available as a Python package; just use ""pip install
fastfeedforward"".",http://arxiv.org/pdf/2308.14711v1
2308.14710v1,cs.CV,VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation,2023-08-28 17:10:12+00:00,"Existing approaches to unsupervised video instance segmentation typically
rely on motion estimates and experience difficulties tracking small or
divergent motions. We present VideoCutLER, a simple method for unsupervised
multi-instance video segmentation without using motion-based learning signals
like optical flow or training on natural videos. Our key insight is that using
high-quality pseudo masks and a simple video synthesis method for model
training is surprisingly sufficient to enable the resulting video model to
effectively segment and track multiple instances across video frames. We show
the first competitive unsupervised learning results on the challenging
YouTubeVIS-2019 benchmark, achieving 50.7% APvideo^50 , surpassing the previous
state-of-the-art by a large margin. VideoCutLER can also serve as a strong
pretrained model for supervised video instance segmentation tasks, exceeding
DINO by 15.9% on YouTubeVIS-2019 in terms of APvideo.",http://arxiv.org/pdf/2308.14710v1
2308.14622v1,cs.IR,TRIVEA: Transparent Ranking Interpretation using Visual Explanation of Black-Box Algorithmic Rankers,2023-08-28 16:58:44+00:00,"Ranking schemes drive many real-world decisions, like, where to study, whom
to hire, what to buy, etc. Many of these decisions often come with high
consequences. For example, a university can be deemed less prestigious if not
featured in a top-k list, and consumers might not even explore products that do
not get recommended to buyers. At the heart of most of these decisions are
opaque ranking schemes, which dictate the ordering of data entities, but their
internal logic is inaccessible or proprietary. Drawing inferences about the
ranking differences is like a guessing game to the stakeholders, like, the
rankees (i.e., the entities who are ranked, like product companies) and the
decision-makers (i.e., who use the rankings, like buyers). In this paper, we
aim to enable transparency in ranking interpretation by using algorithmic
rankers that learn from available data and by enabling human reasoning about
the learned ranking differences using explainable AI (XAI) methods. To realize
this aim, we leverage the exploration-explanation paradigm of human-data
interaction to let human stakeholders explore subsets and groupings of complex
multi-attribute ranking data using visual explanations of model fit and
attribute influence on rankings. We realize this explanation paradigm for
transparent ranking interpretation in TRIVEA, a visual analytic system that is
fueled by: i) visualizations of model fit derived from algorithmic rankers that
learn the associations between attributes and rankings from available data and
ii) visual explanations derived from XAI methods that help abstract important
patterns, like, the relative influence of attributes in different ranking
ranges. Using TRIVEA, end users not trained in data science have the agency to
transparently reason about the global and local behavior of the rankings
without the need to open black-box ranking models and develop confidence in the
resulting attribute-based inferences. We demonstrate the efficacy of TRIVEA
using multiple usage scenarios and subjective feedback from researchers with
diverse domain expertise. Keywords: Visual Analytics, Learning-to-Rank,
Explainable ML, Ranking",http://arxiv.org/pdf/2308.14622v1
2308.14697v1,cs.HC,Assessing Trust in Construction AI-Powered Collaborative Robots using Structural Equation Modeling,2023-08-28 16:39:22+00:00,"This study aimed to investigate the key technical and psychological factors
that impact the architecture, engineering, and construction (AEC)
professionals' trust in collaborative robots (cobots) powered by artificial
intelligence (AI). The study employed a nationwide survey of 600 AEC industry
practitioners to gather in-depth responses and valuable insights into the
future opportunities for promoting the adoption, cultivation, and training of a
skilled workforce to leverage this technology effectively. A Structural
Equation Modeling (SEM) analysis revealed that safety and reliability are
significant factors for the adoption of AI-powered cobots in construction. Fear
of being replaced resulting from the use of cobots can have a substantial
effect on the mental health of the affected workers. A lower error rate in jobs
involving cobots, safety measurements, and security of data collected by cobots
from jobsites significantly impact reliability, while the transparency of
cobots' inner workings can benefit accuracy, robustness, security, privacy, and
communication, and results in higher levels of automation, all of which
demonstrated as contributors to trust. The study's findings provide critical
insights into the perceptions and experiences of AEC professionals towards
adoption of cobots in construction and help project teams determine the
adoption approach that aligns with the company's goals workers' welfare.",http://arxiv.org/pdf/2308.14697v1
2308.14687v1,cs.SE,MELT: Mining Effective Lightweight Transformations from Pull Requests,2023-08-28 16:21:52+00:00,"Software developers often struggle to update APIs, leading to manual,
time-consuming, and error-prone processes. We introduce MELT, a new approach
that generates lightweight API migration rules directly from pull requests in
popular library repositories. Our key insight is that pull requests merged into
open-source libraries are a rich source of information sufficient to mine API
migration rules. By leveraging code examples mined from the library source and
automatically generated code examples based on the pull requests, we infer
transformation rules in \comby, a language for structural code search and
replace. Since inferred rules from single code examples may be too specific, we
propose a generalization procedure to make the rules more applicable to client
projects. MELT rules are syntax-driven, interpretable, and easily adaptable.
Moreover, unlike previous work, our approach enables rule inference to
seamlessly integrate into the library workflow, removing the need to wait for
client code migrations. We evaluated MELT on pull requests from four popular
libraries, successfully mining 461 migration rules from code examples in pull
requests and 114 rules from auto-generated code examples. Our generalization
procedure increases the number of matches for mined rules by 9x. We applied
these rules to client projects and ran their tests, which led to an overall
decrease in the number of warnings and fixing some test cases demonstrating
MELT's effectiveness in real-world scenarios.",http://arxiv.org/pdf/2308.14687v1
2308.14683v1,cs.CL,Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts,2023-08-28 16:18:50+00:00,"Detecting online sexual predatory behaviours and abusive language on social
media platforms has become a critical area of research due to the growing
concerns about online safety, especially for vulnerable populations such as
children and adolescents. Researchers have been exploring various techniques
and approaches to develop effective detection systems that can identify and
mitigate these risks. Recent development of large language models (LLMs) has
opened a new opportunity to address this problem more effectively. This paper
proposes an approach to detection of online sexual predatory chats and abusive
language using the open-source pretrained Llama 2 7B-parameter model, recently
released by Meta GenAI. We fine-tune the LLM using datasets with different
sizes, imbalance degrees, and languages (i.e., English, Roman Urdu and Urdu).
Based on the power of LLMs, our approach is generic and automated without a
manual search for a synergy between feature extraction and classifier design
steps like conventional methods in this domain. Experimental results show a
strong performance of the proposed approach, which performs proficiently and
consistently across three distinct datasets with five sets of experiments. This
study's outcomes indicate that the proposed method can be implemented in
real-world applications (even with non-English languages) for flagging sexual
predators, offensive or toxic content, hate speech, and discriminatory language
in online discussions and comments to maintain respectful internet or digital
communities. Furthermore, it can be employed for solving text classification
problems with other potential applications such as sentiment analysis, spam and
phishing detection, sorting legal documents, fake news detection, language
identification, user intent recognition, text-based product categorization,
medical record analysis, and resume screening.",http://arxiv.org/pdf/2308.14683v1
2308.14669v1,cs.CL,ANER: Arabic and Arabizi Named Entity Recognition using Transformer-Based Approach,2023-08-28 15:54:48+00:00,"One of the main tasks of Natural Language Processing (NLP), is Named Entity
Recognition (NER). It is used in many applications and also can be used as an
intermediate step for other tasks. We present ANER, a web-based named entity
recognizer for the Arabic, and Arabizi languages. The model is built upon BERT,
which is a transformer-based encoder. It can recognize 50 different entity
classes, covering various fields. We trained our model on the WikiFANE\_Gold
dataset which consists of Wikipedia articles. We achieved an F1 score of
88.7\%, which beats CAMeL Tools' F1 score of 83\% on the ANERcorp dataset,
which has only 4 classes. We also got an F1 score of 77.7\% on the
NewsFANE\_Gold dataset which contains out-of-domain data from News articles.
The system is deployed on a user-friendly web interface that accepts users'
inputs in Arabic, or Arabizi. It allows users to explore the entities in the
text by highlighting them. It can also direct users to get information about
entities through Wikipedia directly. We added the ability to do NER using our
model, or CAMeL Tools' model through our website. ANER is publicly accessible
at \url{http://www.aner.online}. We also deployed our model on HuggingFace at
https://huggingface.co/boda/ANER, to allow developers to test and use it.",http://arxiv.org/pdf/2308.14669v1
2308.14667v1,cs.CV,Neural Network-Based Histologic Remission Prediction In Ulcerative Colitis,2023-08-28 15:54:14+00:00,"BACKGROUND & AIMS: Histological remission (HR) is advocated and considered as
a new therapeutic target in ulcerative colitis (UC). Diagnosis of histologic
remission currently relies on biopsy; during this process, patients are at risk
for bleeding, infection, and post-biopsy fibrosis. In addition, histologic
response scoring is complex and time-consuming, and there is heterogeneity
among pathologists. Endocytoscopy (EC) is a novel ultra-high magnification
endoscopic technique that can provide excellent in vivo assessment of glands.
Based on the EC technique, we propose a neural network model that can assess
histological disease activity in UC using EC images to address the above
issues. The experiment results demonstrate that the proposed method can assist
patients in precise treatment and prognostic assessment.
  METHODS: We construct a neural network model for UC evaluation. A total of
5105 images of 154 intestinal segments from 87 patients undergoing EC treatment
at a center in China between March 2022 and March 2023 are scored according to
the Geboes score. Subsequently, 103 intestinal segments are used as the
training set, 16 intestinal segments are used as the validation set for neural
network training, and the remaining 35 intestinal segments are used as the test
set to measure the model performance together with the validation set.
  RESULTS: By treating HR as a negative category and histologic activity as a
positive category, the proposed neural network model can achieve an accuracy of
0.9, a specificity of 0.95, a sensitivity of 0.75, and an area under the curve
(AUC) of 0.81.
  CONCLUSION: We develop a specific neural network model that can distinguish
histologic remission/activity in EC images of UC, which helps to accelerate
clinical histological diagnosis.
  keywords: ulcerative colitis; Endocytoscopy; Geboes score; neural network.",http://arxiv.org/pdf/2308.14667v1
2308.14657v1,cs.AI,DeepHealthNet: Adolescent Obesity Prediction System Based on a Deep Learning Framework,2023-08-28 15:40:31+00:00,"Childhood and adolescent obesity rates are a global concern because obesity
is associated with chronic diseases and long-term health risks. Artificial
intelligence technology has emerged as a promising solution to accurately
predict obesity rates and provide personalized feedback to adolescents. This
study emphasizes the importance of early identification and prevention of
obesity-related health issues. Factors such as height, weight, waist
circumference, calorie intake, physical activity levels, and other relevant
health information need to be considered for developing robust algorithms for
obesity rate prediction and delivering personalized feedback. Hence, by
collecting health datasets from 321 adolescents, we proposed an adolescent
obesity prediction system that provides personalized predictions and assists
individuals in making informed health decisions. Our proposed deep learning
framework, DeepHealthNet, effectively trains the model using data augmentation
techniques, even when daily health data are limited, resulting in improved
prediction accuracy (acc: 0.8842). Additionally, the study revealed variations
in the prediction of the obesity rate between boys (acc: 0.9320) and girls
(acc: 0.9163), allowing the identification of disparities and the determination
of the optimal time to provide feedback. The proposed system shows significant
potential in effectively addressing childhood and adolescent obesity.",http://arxiv.org/pdf/2308.14657v1
2308.14652v1,cs.AI,Learning Visual Tracking and Reaching with Deep Reinforcement Learning on a UR10e Robotic Arm,2023-08-28 15:34:43+00:00,"As technology progresses, industrial and scientific robots are increasingly
being used in diverse settings. In many cases, however, programming the robot
to perform such tasks is technically complex and costly. To maximize the
utility of robots in industrial and scientific settings, they require the
ability to quickly shift from one task to another. Reinforcement learning
algorithms provide the potential to enable robots to learn optimal solutions to
complete new tasks without directly reprogramming them. The current
state-of-the-art in reinforcement learning, however, generally relies on fast
simulations and parallelization to achieve optimal performance. These are often
not possible in robotics applications. Thus, a significant amount of research
is required to facilitate the efficient and safe, training and deployment of
industrial and scientific reinforcement learning robots. This technical report
outlines our initial research into the application of deep reinforcement
learning on an industrial UR10e robot. The report describes the reinforcement
learning environments created to facilitate policy learning with the UR10e, a
robotic arm from Universal Robots, and presents our initial results in training
deep Q-learning and proximal policy optimization agents on the developed
reinforcement learning environments. Our results show that proximal policy
optimization learns a better, more stable policy with less data than deep
Q-learning. The corresponding code for this work is available at
\url{https://github.com/cbellinger27/bendRL_reacher_tracker}",http://arxiv.org/pdf/2308.14652v1
2308.14634v1,cs.CL,Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance,2023-08-28 15:04:16+00:00,"We propose the use of conversational GPT models for easy and quick few-shot
text classification in the financial domain using the Banking77 dataset. Our
approach involves in-context learning with GPT-3.5 and GPT-4, which minimizes
the technical expertise required and eliminates the need for expensive GPU
computing while yielding quick and accurate results. Additionally, we fine-tune
other pre-trained, masked language models with SetFit, a recent contrastive
learning technique, to achieve state-of-the-art results both in full-data and
few-shot settings. Our findings show that querying GPT-3.5 and GPT-4 can
outperform fine-tuned, non-generative models even with fewer examples. However,
subscription fees associated with these solutions may be considered costly for
small organizations. Lastly, we find that generative models perform better on
the given task when shown representative samples selected by a human expert
rather than when shown random ones. We conclude that a) our proposed methods
offer a practical solution for few-shot tasks in datasets with limited label
availability, and b) our state-of-the-art results can inspire future work in
the area.",http://arxiv.org/pdf/2308.14634v1
2308.14610v1,astro-ph.IM,A Transformer-Conditioned Neural Fields Pipeline with Polar Coordinate Representation for Astronomical Radio Interferometric Data Reconstruction,2023-08-28 14:26:15+00:00,"In radio astronomy, visibility data, which are measurements of wave signals
from radio telescopes, are transformed into images for observation of distant
celestial objects. However, these resultant images usually contain both real
sources and artifacts, due to signal sparsity and other factors. One way to
obtain cleaner images is to reconstruct samples into dense forms before
imaging. Unfortunately, existing visibility reconstruction methods may miss
some components of the frequency data, so blurred object edges and persistent
artifacts remain in the images. Furthermore, the computation overhead is high
on irregular visibility samples due to the data skew. To address these
problems, we propose PolarRec, a reconstruction method for interferometric
visibility data, which consists of a transformer-conditioned neural fields
pipeline with a polar coordinate representation. This representation matches
the way in which telescopes observe a celestial area as the Earth rotates. We
further propose Radial Frequency Loss function, using radial coordinates in the
polar coordinate system to correlate with the frequency information, to help
reconstruct complete visibility. We also group visibility sample points by
angular coordinates in the polar coordinate system, and use groups as the
granularity for subsequent encoding with a Transformer encoder. Consequently,
our method can capture the inherent characteristics of visibility data
effectively and efficiently. Our experiments demonstrate that PolarRec markedly
improves imaging results by faithfully reconstructing all frequency components
in the visibility domain while significantly reducing the computation cost.",http://arxiv.org/pdf/2308.14610v1
2308.14595v1,cs.CV,Neural Network Training Strategy to Enhance Anomaly Detection Performance: A Perspective on Reconstruction Loss Amplification,2023-08-28 14:06:36+00:00,"Unsupervised anomaly detection (UAD) is a widely adopted approach in industry
due to rare anomaly occurrences and data imbalance. A desirable characteristic
of an UAD model is contained generalization ability which excels in the
reconstruction of seen normal patterns but struggles with unseen anomalies.
Recent studies have pursued to contain the generalization capability of their
UAD models in reconstruction from different perspectives, such as design of
neural network (NN) structure and training strategy. In contrast, we note that
containing of generalization ability in reconstruction can also be obtained
simply from steep-shaped loss landscape. Motivated by this, we propose a loss
landscape sharpening method by amplifying the reconstruction loss, dubbed Loss
AMPlification (LAMP). LAMP deforms the loss landscape into a steep shape so the
reconstruction error on unseen anomalies becomes greater. Accordingly, the
anomaly detection performance is improved without any change of the NN
architecture. Our findings suggest that LAMP can be easily applied to any
reconstruction error metrics in UAD settings where the reconstruction model is
trained with anomaly-free samples only.",http://arxiv.org/pdf/2308.14595v1
2308.14550v1,cs.AI,ReMAV: Reward Modeling of Autonomous Vehicles for Finding Likely Failure Events,2023-08-28 13:09:00+00:00,"Autonomous vehicles are advanced driving systems that are well known for
being vulnerable to various adversarial attacks, compromising the vehicle's
safety, and posing danger to other road users. Rather than actively training
complex adversaries by interacting with the environment, there is a need to
first intelligently find and reduce the search space to only those states where
autonomous vehicles are found less confident. In this paper, we propose a
blackbox testing framework ReMAV using offline trajectories first to analyze
the existing behavior of autonomous vehicles and determine appropriate
thresholds for finding the probability of failure events. Our reward modeling
technique helps in creating a behavior representation that allows us to
highlight regions of likely uncertain behavior even when the baseline
autonomous vehicle is performing well. This approach allows for more efficient
testing without the need for computational and inefficient active adversarial
learning techniques. We perform our experiments in a high-fidelity urban
driving environment using three different driving scenarios containing single
and multi-agent interactions. Our experiment shows 35%, 23%, 48%, and 50%
increase in occurrences of vehicle collision, road objects collision,
pedestrian collision, and offroad steering events respectively by the
autonomous vehicle under test, demonstrating a significant increase in failure
events. We also perform a comparative analysis with prior testing frameworks
and show that they underperform in terms of training-testing efficiency,
finding total infractions, and simulation steps to identify the first failure
compared to our approach. The results show that the proposed framework can be
used to understand existing weaknesses of the autonomous vehicles under test in
order to only attack those regions, starting with the simplistic perturbation
models.",http://arxiv.org/pdf/2308.14550v1
2308.14536v1,cs.CL,Spoken Language Intelligence of Large Language Models for Language Learning,2023-08-28 12:47:41+00:00,"People have long hoped for a conversational system that can assist in
real-life situations, and recent progress on large language models (LLMs) is
bringing this idea closer to reality. While LLMs are often impressive in
performance, their efficacy in real-world scenarios that demand expert
knowledge remains unclear. LLMs are believed to hold the most potential and
value in education, especially in the development of Artificial intelligence
(AI) based virtual teachers capable of facilitating language learning. Our
focus is centered on evaluating the efficacy of LLMs in the realm of education,
specifically in the areas of spoken language learning which encompass
phonetics, phonology, and second language acquisition. We introduce a new
multiple-choice question dataset to evaluate the effectiveness of LLMs in the
aforementioned scenarios, including understanding and application of spoken
language knowledge. In addition, we investigate the influence of various
prompting techniques such as zero- and few-shot method (prepending the question
with question-answer exemplars), chain-of-thought (CoT, think step-by-step),
in-domain exampler and external tools (Google, Wikipedia). We conducted
large-scale evaluation on popular LLMs (20 distinct models) using these
methods. We achieved significant performance improvements compared to the
zero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% ->
63.1%; LLaMA2-70B-Chat, 42.2% -> 48.6%). We found that models of different
sizes have good understanding of concepts in phonetics, phonology, and second
language acquisition, but show limitations in reasoning for real-world
problems. Additionally, we also explore preliminary findings on conversational
communication.",http://arxiv.org/pdf/2308.14536v1
2308.14523v1,cs.NI,Deep Reinforcement Learning for Uplink Scheduling in NOMA-URLLC Networks,2023-08-28 12:18:02+00:00,"This article addresses the problem of Ultra Reliable Low Latency
Communications (URLLC) in wireless networks, a framework with particularly
stringent constraints imposed by many Internet of Things (IoT) applications
from diverse sectors. We propose a novel Deep Reinforcement Learning (DRL)
scheduling algorithm, named NOMA-PPO, to solve the Non-Orthogonal Multiple
Access (NOMA) uplink URLLC scheduling problem involving strict deadlines. The
challenge of addressing uplink URLLC requirements in NOMA systems is related to
the combinatorial complexity of the action space due to the possibility to
schedule multiple devices, and to the partial observability constraint that we
impose to our algorithm in order to meet the IoT communication constraints and
be scalable. Our approach involves 1) formulating the NOMA-URLLC problem as a
Partially Observable Markov Decision Process (POMDP) and the introduction of an
agent state, serving as a sufficient statistic of past observations and
actions, enabling a transformation of the POMDP into a Markov Decision Process
(MDP); 2) adapting the Proximal Policy Optimization (PPO) algorithm to handle
the combinatorial action space; 3) incorporating prior knowledge into the
learning agent with the introduction of a Bayesian policy. Numerical results
reveal that not only does our approach outperform traditional multiple access
protocols and DRL benchmarks on 3GPP scenarios, but also proves to be robust
under various channel and traffic configurations, efficiently exploiting
inherent time correlations.",http://arxiv.org/pdf/2308.14523v1
2308.14522v1,cs.LG,Large Graph Models: A Perspective,2023-08-28 12:17:51+00:00,"Large models have emerged as the most recent groundbreaking achievements in
artificial intelligence, and particularly machine learning. However, when it
comes to graphs, large models have not achieved the same level of success as in
other fields, such as natural language processing and computer vision. In order
to promote applying large models for graphs forward, we present a perspective
paper to discuss the challenges and opportunities associated with developing
large graph models. First, we discuss the desired characteristics of large
graph models. Then, we present detailed discussions from three key
perspectives: representation basis, graph data, and graph models. In each
category, we provide a brief overview of recent advances and highlight the
remaining challenges together with our visions. Finally, we discuss valuable
applications of large graph models. We believe this perspective paper is able
to encourage further investigations into large graph models, ultimately pushing
us one step closer towards artificial general intelligence (AGI).",http://arxiv.org/pdf/2308.14522v1
2308.14521v1,cs.AI,Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles,2023-08-28 12:13:36+00:00,"Computational agents support humans in many areas of life and are therefore
found in heterogeneous contexts. This means that agents operate in rapidly
changing environments and can be confronted with huge state and action spaces.
In order to perform services and carry out activities in a goal-oriented
manner, agents require prior knowledge and therefore have to develop and pursue
context-dependent policies. The problem is that prescribing policies in advance
is limited and inflexible, especially in dynamically changing environments.
Moreover, the context of an agent determines its choice of actions. Since the
environments in which agents operate can be stochastic and complex in terms of
the number of states and feasible actions, activities are usually modelled in a
simplified way by Markov decision processes so that agents with reinforcement
learning are able to learn policies that help to capture the context and act
accordingly to optimally perform activities. However, training policies for all
possible contexts using reinforcement learning is time-consuming. A requirement
and challenge for agents is to learn strategies quickly and respond immediately
in cross-context environments and applications. In this work, we propose a
novel simulation-based approach that enables a) the representation of
heterogeneous contexts through knowledge graphs and entity embeddings and b)
the context-aware composition of policies on demand by ensembles of agents
running in parallel. The evaluation we performed on the ""Virtual Home"" dataset
indicates that agents that need to seamlessly switch between different
contexts, can request on-the-fly composed policies that lead to the successful
completion of context-appropriate activities without having to learn these
policies in lengthy training steps and episodes, in contrast to agents that
apply reinforcement learning.",http://arxiv.org/pdf/2308.14521v1
2308.14475v1,cs.AI,Interactive Multi Interest Process Pattern Discovery,2023-08-28 10:26:37+00:00,"Process pattern discovery methods (PPDMs) aim at identifying patterns of
interest to users. Existing PPDMs typically are unsupervised and focus on a
single dimension of interest, such as discovering frequent patterns. We present
an interactive multi interest driven framework for process pattern discovery
aimed at identifying patterns that are optimal according to a multi-dimensional
analysis goal. The proposed approach is iterative and interactive, thus taking
experts knowledge into account during the discovery process. The paper focuses
on a concrete analysis goal, i.e., deriving process patterns that affect the
process outcome. We evaluate the approach on real world event logs in both
interactive and fully automated settings. The approach extracted meaningful
patterns validated by expert knowledge in the interactive setting. Patterns
extracted in the automated settings consistently led to prediction performance
comparable to or better than patterns derived considering single interest
dimensions without requiring user defined thresholds.",http://arxiv.org/pdf/2308.14475v1
2308.14474v1,cs.AI,"Causality-Based Feature Importance Quantifying Methods:PN-FI, PS-FI and PNS-FI",2023-08-28 10:24:51+00:00,"In current ML field models are getting larger and more complex, data we use
are also getting larger in quantity and higher in dimension, so in order to
train better models, save training time and computational resources, a good
Feature Selection (FS) method in preprocessing stage is necessary. Feature
importance (FI) is of great importance since it is the basis of feature
selection.
  This paper creatively introduces the calculation of PNS(the probability of
Necessity and Sufficiency) in Causality into quantifying feature importance and
creates new FI measuring methods: PN-FI, which means how much importance a
feature has in image recognition tasks, PS_FI that means how much importance a
feature has in image generating tasks, and PNS_FI which measures both.
  The main body of this paper is three RCTs, with whose results we show how
PS_FI, PN_FI and PNS_FI of three features: dog nose, dog eyes and dog mouth are
calculated. The FI values are intervals with tight upper and lower bounds.",http://arxiv.org/pdf/2308.14474v1
2308.14448v1,cs.CV,ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment,2023-08-28 09:35:13+00:00,"The objective of stylized speech-driven facial animation is to create
animations that encapsulate specific emotional expressions. Existing methods
often depend on pre-established emotional labels or facial expression
templates, which may limit the necessary flexibility for accurately conveying
user intent. In this research, we introduce a technique that enables the
control of arbitrary styles by leveraging natural language as emotion prompts.
This technique presents benefits in terms of both flexibility and
user-friendliness. To realize this objective, we initially construct a
Text-Expression Alignment Dataset (TEAD), wherein each facial expression is
paired with several prompt-like descriptions.We propose an innovative automatic
annotation method, supported by Large Language Models (LLMs), to expedite the
dataset construction, thereby eliminating the substantial expense of manual
annotation. Following this, we utilize TEAD to train a CLIP-based model, termed
ExpCLIP, which encodes text and facial expressions into semantically aligned
style embeddings. The embeddings are subsequently integrated into the facial
animation generator to yield expressive and controllable facial animations.
Given the limited diversity of facial emotions in existing speech-driven facial
animation training data, we further introduce an effective Expression Prompt
Augmentation (EPA) mechanism to enable the animation generator to support
unprecedented richness in style control. Comprehensive experiments illustrate
that our method accomplishes expressive facial animation generation and offers
enhanced flexibility in effectively conveying the desired style.",http://arxiv.org/pdf/2308.14448v1
2308.14437v1,cs.CV,Data-iterative Optimization Score Model for Stable Ultra-Sparse-View CT Reconstruction,2023-08-28 09:23:18+00:00,"Score-based generative models (SGMs) have gained prominence in sparse-view CT
reconstruction for their precise sampling of complex distributions. In
SGM-based reconstruction, data consistency in the score-based diffusion model
ensures close adherence of generated samples to observed data distribution,
crucial for improving image quality. Shortcomings in data consistency
characterization manifest in three aspects. Firstly, data from the optimization
process can lead to artifacts in reconstructed images. Secondly, it often
neglects that the generation model and original data constraints are
independently completed, fragmenting unity. Thirdly, it predominantly focuses
on constraining intermediate results in the inverse sampling process, rather
than ideal real images. Thus, we propose an iterative optimization data scoring
model. This paper introduces the data-iterative optimization score-based model
(DOSM), integrating innovative data consistency into the Stochastic
Differential Equation, a valuable constraint for ultra-sparse-view CT
reconstruction. The novelty of this data consistency element lies in its sole
reliance on original measurement data to confine generation outcomes,
effectively balancing measurement data and generative model constraints.
Additionally, we pioneer an inference strategy that traces back from current
iteration results to ideal truth, enhancing reconstruction stability. We
leverage conventional iteration techniques to optimize DOSM updates.
Quantitative and qualitative results from 23 views of numerical and clinical
cardiac datasets demonstrate DOSM's superiority over other methods. Remarkably,
even with 10 views, our method achieves excellent performance.",http://arxiv.org/pdf/2308.14437v1
2308.14434v1,cs.CR,Using ChatGPT as a Static Application Security Testing Tool,2023-08-28 09:21:37+00:00,"In recent years, artificial intelligence has had a conspicuous growth in
almost every aspect of life. One of the most applicable areas is security code
review, in which a lot of AI-based tools and approaches have been proposed.
Recently, ChatGPT has caught a huge amount of attention with its remarkable
performance in following instructions and providing a detailed response.
Regarding the similarities between natural language and code, in this paper, we
study the feasibility of using ChatGPT for vulnerability detection in Python
source code. Toward this goal, we feed an appropriate prompt along with
vulnerable data to ChatGPT and compare its results on two datasets with the
results of three widely used Static Application Security Testing tools (Bandit,
Semgrep and SonarQube). We implement different kinds of experiments with
ChatGPT and the results indicate that ChatGPT reduces the false positive and
false negative rates and has the potential to be used for Python source code
vulnerability detection.",http://arxiv.org/pdf/2308.14434v1
2308.14429v1,cs.CL,Biomedical Entity Linking with Triple-aware Pre-Training,2023-08-28 09:06:28+00:00,"Linking biomedical entities is an essential aspect in biomedical natural
language processing tasks, such as text mining and question answering. However,
a difficulty of linking the biomedical entities using current large language
models (LLM) trained on a general corpus is that biomedical entities are
scarcely distributed in texts and therefore have been rarely seen during
training by the LLM. At the same time, those LLMs are not aware of high level
semantic connection between different biomedical entities, which are useful in
identifying similar concepts in different textual contexts. To cope with
aforementioned problems, some recent works focused on injecting knowledge graph
information into LLMs. However, former methods either ignore the relational
knowledge of the entities or lead to catastrophic forgetting. Therefore, we
propose a novel framework to pre-train the powerful generative LLM by a corpus
synthesized from a KG. In the evaluations we are unable to confirm the benefit
of including synonym, description or relational information.",http://arxiv.org/pdf/2308.14429v1
2308.14424v1,cs.LO,Shielded Reinforcement Learning for Hybrid Systems,2023-08-28 09:04:52+00:00,"Safe and optimal controller synthesis for switched-controlled hybrid systems,
which combine differential equations and discrete changes of the system's
state, is known to be intricately hard. Reinforcement learning has been
leveraged to construct near-optimal controllers, but their behavior is not
guaranteed to be safe, even when it is encouraged by reward engineering. One
way of imposing safety to a learned controller is to use a shield, which is
correct by design. However, obtaining a shield for non-linear and hybrid
environments is itself intractable. In this paper, we propose the construction
of a shield using the so-called barbaric method, where an approximate finite
representation of an underlying partition-based two-player safety game is
extracted via systematically picked samples of the true transition function.
While hard safety guarantees are out of reach, we experimentally demonstrate
strong statistical safety guarantees with a prototype implementation and UPPAAL
STRATEGO. Furthermore, we study the impact of the synthesized shield when
applied as either a pre-shield (applied before learning a controller) or a
post-shield (only applied after learning a controller). We experimentally
demonstrate superiority of the pre-shielding approach. We apply our technique
on a range of case studies, including two industrial examples, and further
study post-optimization of the post-shielding approach.",http://arxiv.org/pdf/2308.14424v1
2308.14401v1,cs.SE,CodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models,2023-08-28 08:36:53+00:00,"Code datasets are of immense value for training neural-network-based code
completion models, where companies or organizations have made substantial
investments to establish and process these datasets. Unluckily, these datasets,
either built for proprietary or public usage, face the high risk of
unauthorized exploits, resulting from data leakages, license violations, etc.
Even worse, the ``black-box'' nature of neural models sets a high barrier for
externals to audit their training datasets, which further connives these
unauthorized usages. Currently, watermarking methods have been proposed to
prohibit inappropriate usage of image and natural language datasets. However,
due to domain specificity, they are not directly applicable to code datasets,
leaving the copyright protection of this emerging and important field of code
data still exposed to threats. To fill this gap, we propose a method, named
CodeMark, to embed user-defined imperceptible watermarks into code datasets to
trace their usage in training neural code completion models. CodeMark is based
on adaptive semantic-preserving transformations, which preserve the exact
functionality of the code data and keep the changes covert against
rule-breakers. We implement CodeMark in a toolkit and conduct an extensive
evaluation of code completion models. CodeMark is validated to fulfill all
desired properties of practical watermarks, including harmlessness to model
accuracy, verifiability, robustness, and imperceptibility.",http://arxiv.org/pdf/2308.14401v1
2308.14390v1,cs.AI,ASCAPE: An open AI ecosystem to support the quality of life of cancer patients,2023-08-28 08:14:12+00:00,"The latest cancer statistics indicate a decrease in cancer-related mortality.
However, due to the growing and ageing population, the absolute number of
people living with cancer is set to keep increasing. This paper presents
ASCAPE, an open AI infrastructure that takes advantage of the recent advances
in Artificial Intelligence (AI) and Machine Learning (ML) to support cancer
patients quality of life (QoL). With ASCAPE health stakeholders (e.g.
hospitals) can locally process their private medical data and then share the
produced knowledge (ML models) through the open AI infrastructure.",http://arxiv.org/pdf/2308.14390v1
2308.14376v1,cs.LG,Are Existing Out-Of-Distribution Techniques Suitable for Network Intrusion Detection?,2023-08-28 07:49:01+00:00,"Machine learning (ML) has become increasingly popular in network intrusion
detection. However, ML-based solutions always respond regardless of whether the
input data reflects known patterns, a common issue across safety-critical
applications. While several proposals exist for detecting Out-Of-Distribution
(OOD) in other fields, it remains unclear whether these approaches can
effectively identify new forms of intrusions for network security. New attacks,
not necessarily affecting overall distributions, are not guaranteed to be
clearly OOD as instead, images depicting new classes are in computer vision. In
this work, we investigate whether existing OOD detectors from other fields
allow the identification of unknown malicious traffic. We also explore whether
more discriminative and semantically richer embedding spaces within models,
such as those created with contrastive learning and multi-class tasks, benefit
detection. Our investigation covers a set of six OOD techniques that employ
different detection strategies. These techniques are applied to models trained
in various ways and subsequently exposed to unknown malicious traffic from the
same and different datasets (network environments). Our findings suggest that
existing detectors can identify a consistent portion of new malicious traffic,
and that improved embedding spaces enhance detection. We also demonstrate that
simple combinations of certain detectors can identify almost 100% of malicious
traffic in our tested scenarios.",http://arxiv.org/pdf/2308.14376v1
2308.14370v1,cs.AI,Model-based learning for location-to-channel mapping,2023-08-28 07:39:53+00:00,"Modern communication systems rely on accurate channel estimation to achieve
efficient and reliable transmission of information. As the communication
channel response is highly related to the user's location, one can use a neural
network to map the user's spatial coordinates to the channel coefficients.
However, these latter are rapidly varying as a function of the location, on the
order of the wavelength. Classical neural architectures being biased towards
learning low frequency functions (spectral bias), such mapping is therefore
notably difficult to learn. In order to overcome this limitation, this paper
presents a frugal, model-based network that separates the low frequency from
the high frequency components of the target mapping function. This yields an
hypernetwork architecture where the neural network only learns low frequency
sparse coefficients in a dictionary of high frequency components. Simulation
results show that the proposed neural network outperforms standard approaches
on realistic synthetic data.",http://arxiv.org/pdf/2308.14370v1
2308.14364v1,cs.LG,Target-independent XLA optimization using Reinforcement Learning,2023-08-28 07:23:03+00:00,"An important challenge in Machine Learning compilers like XLA is multi-pass
optimization and analysis. There has been recent interest chiefly in XLA
target-dependent optimization on the graph-level, subgraph-level, and
kernel-level phases. We specifically focus on target-independent optimization
XLA HLO pass ordering: our approach aims at finding the optimal sequence of
compiler optimization passes, which is decoupled from target-dependent
optimization. However, there is little domain specific study in pass ordering
for XLA HLO. To this end, we propose introducing deep Reinforcement Learning
(RL) based search for optimal XLA HLO pass ordering. We also propose
enhancements to the deep RL algorithms to further improve optimal search
performance and open the research direction for domain-specific guidance for
RL. We create an XLA Gym experimentation framework as a tool to enable RL
algorithms to interact with the compiler for passing optimizations and thereby
train agents. Overall, in our experimentation we observe an average of $13.3\%$
improvement in operation count reduction on a benchmark of GPT-2 training
graphs and $10.4\%$ improvement on a diverse benchmark including GPT-2, BERT,
and ResNet graphs using the proposed approach over the compiler's default phase
ordering.",http://arxiv.org/pdf/2308.14364v1
2308.14363v1,cs.AI,Rethinking Mobile AI Ecosystem in the LLM Era,2023-08-28 07:21:26+00:00,"In today's landscape, smartphones have evolved into hubs for hosting a
multitude of deep learning models aimed at local execution. A key realization
driving this work is the notable fragmentation among these models,
characterized by varied architectures, operators, and implementations. This
fragmentation imposes a significant burden on the comprehensive optimization of
hardware, system settings, and algorithms.
  Buoyed by the recent strides in large foundation models, this work introduces
a pioneering paradigm for mobile AI: a collaborative management approach
between the mobile OS and hardware, overseeing a foundational model capable of
serving a broad spectrum of mobile AI tasks, if not all. This foundational
model resides within the NPU and remains impervious to app or OS revisions,
akin to firmware. Concurrently, each app contributes a concise, offline
fine-tuned ""adapter"" tailored to distinct downstream tasks. From this concept
emerges a concrete instantiation known as \sys. It amalgamates a curated
selection of publicly available Large Language Models (LLMs) and facilitates
dynamic data flow. This concept's viability is substantiated through the
creation of an exhaustive benchmark encompassing 38 mobile AI tasks spanning 50
datasets, including domains such as Computer Vision (CV), Natural Language
Processing (NLP), audio, sensing, and multimodal inputs. Spanning this
benchmark, \sys unveils its impressive performance. It attains accuracy parity
in 85\% of tasks, demonstrates improved scalability in terms of storage and
memory, and offers satisfactory inference speed on Commercial Off-The-Shelf
(COTS) mobile devices fortified with NPU support. This stands in stark contrast
to task-specific models tailored for individual applications.",http://arxiv.org/pdf/2308.14363v1
2308.14360v1,cs.SD,InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models,2023-08-28 07:11:42+00:00,"Music editing primarily entails the modification of instrument tracks or
remixing in the whole, which offers a novel reinterpretation of the original
piece through a series of operations. These music processing methods hold
immense potential across various applications but demand substantial expertise.
Prior methodologies, although effective for image and audio modifications,
falter when directly applied to music. This is attributed to music's
distinctive data nature, where such methods can inadvertently compromise the
intrinsic harmony and coherence of music. In this paper, we develop InstructME,
an Instruction guided Music Editing and remixing framework based on latent
diffusion models. Our framework fortifies the U-Net with multi-scale
aggregation in order to maintain consistency before and after editing. In
addition, we introduce chord progression matrix as condition information and
incorporate it in the semantic space to improve melodic harmony while editing.
For accommodating extended musical pieces, InstructME employs a chunk
transformer, enabling it to discern long-term temporal dependencies within
music sequences. We tested InstructME in instrument-editing, remixing, and
multi-round editing. Both subjective and objective evaluations indicate that
our proposed method significantly surpasses preceding systems in music quality,
text relevance and harmony. Demo samples are available at
https://musicedit.github.io/",http://arxiv.org/pdf/2308.14360v1
2308.14359v1,cs.AI,Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks,2023-08-28 07:11:27+00:00,"Human emotion understanding is pivotal in making conversational technology
mainstream. We view speech emotion understanding as a perception task which is
a more realistic setting. With varying contexts (languages, demographics, etc.)
different share of people perceive the same speech segment as a non-unanimous
emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics
ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset
of multilingual speakers and multi-label regression target of 'emotion share'
or perception of that emotion. We demonstrate that the training scheme of
different foundation models dictates their effectiveness for tasks beyond
speech recognition, especially for non-semantic speech tasks like emotion
  understanding. This is a very complex task due to multilingual speakers,
variability in the target labels, and inherent imbalance in the regression
dataset. Our results show that HuBERT-Large with a self-attention-based
light-weight sequence model provides 4.6% improvement over the reported
baseline.",http://arxiv.org/pdf/2308.14359v1
2308.14352v1,cs.LG,EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models,2023-08-28 06:56:08+00:00,"Large Language Models (LLMs) such as GPTs and LLaMa have ushered in a
revolution in machine intelligence, owing to their exceptional capabilities in
a wide range of machine learning tasks. However, the transition of LLMs from
data centers to edge devices presents a set of challenges and opportunities.
While this shift can enhance privacy and availability, it is hampered by the
enormous parameter sizes of these models, leading to impractical runtime costs.
In light of these considerations, we introduce EdgeMoE, the first on-device
inference engine tailored for mixture-of-expert (MoE) LLMs, a popular variant
of sparse LLMs that exhibit nearly constant computational complexity as their
parameter size scales. EdgeMoE achieves both memory and computational
efficiency by strategically partitioning the model across the storage
hierarchy. Specifically, non-expert weights are stored in the device's memory,
while expert weights are kept in external storage and are fetched into memory
only when they are activated. This design is underpinned by a crucial insight
that expert weights, though voluminous, are infrequently accessed due to sparse
activation patterns. To further mitigate the overhead associated with expert
I/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wise
bitwidth adaptation: This method reduces the size of expert weights with an
acceptable level of accuracy loss. (2) Expert management: It predicts the
experts that will be activated in advance and preloads them into the
compute-I/O pipeline, thus further optimizing the process. In empirical
evaluations conducted on well-established MoE LLMs and various edge devices,
EdgeMoE demonstrates substantial memory savings and performance improvements
when compared to competitive baseline solutions.",http://arxiv.org/pdf/2308.14352v1
2308.14346v1,cs.CL,DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation,2023-08-28 06:41:49+00:00,"We propose DISC-MedLLM, a comprehensive solution that leverages Large
Language Models (LLMs) to provide accurate and truthful medical response in
end-to-end conversational healthcare services. To construct high-quality
Supervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing
medical knowledge-graphs, reconstructing real-world dialogues, and
incorporating human-guided preference rephrasing. These datasets are
instrumental in training DISC-MedLLM, surpassing existing medical LLMs in both
single-turn and multi-turn consultation scenarios. Extensive experimental
results demonstrate the effectiveness of the proposed model in bridging the gap
between general language models and real-world medical consultation.
Additionally, we release the constructed dataset and model weights to further
contribute to research and development. Further details and resources can be
found at https://github.com/FudanDISC/DISC-MedLLM",http://arxiv.org/pdf/2308.14346v1
2308.14338v1,cs.LG,Fair Few-shot Learning with Auxiliary Sets,2023-08-28 06:31:37+00:00,"Recently, there has been a growing interest in developing machine learning
(ML) models that can promote fairness, i.e., eliminating biased predictions
towards certain populations (e.g., individuals from a specific demographic
group). Most existing works learn such models based on well-designed fairness
constraints in optimization. Nevertheless, in many practical ML tasks, only
very few labeled data samples can be collected, which can lead to inferior
fairness performance. This is because existing fairness constraints are
designed to restrict the prediction disparity among different sensitive groups,
but with few samples, it becomes difficult to accurately measure the disparity,
thus rendering ineffective fairness optimization. In this paper, we define the
fairness-aware learning task with limited training samples as the \emph{fair
few-shot learning} problem. To deal with this problem, we devise a novel
framework that accumulates fairness-aware knowledge across different
meta-training tasks and then generalizes the learned knowledge to meta-test
tasks. To compensate for insufficient training samples, we propose an essential
strategy to select and leverage an auxiliary set for each meta-test task. These
auxiliary sets contain several labeled training samples that can enhance the
model performance regarding fairness in meta-test tasks, thereby allowing for
the transfer of learned useful fairness-oriented knowledge to meta-test tasks.
Furthermore, we conduct extensive experiments on three real-world datasets to
validate the superiority of our framework against the state-of-the-art
baselines.",http://arxiv.org/pdf/2308.14338v1
2308.14337v1,cs.AI,Cognitive Effects in Large Language Models,2023-08-28 06:30:33+00:00,"Large Language Models (LLMs) such as ChatGPT have received enormous attention
over the past year and are now used by hundreds of millions of people every
day. The rapid adoption of this technology naturally raises questions about the
possible biases such models might exhibit. In this work, we tested one of these
models (GPT-3) on a range of cognitive effects, which are systematic patterns
that are usually found in human cognitive tasks. We found that LLMs are indeed
prone to several human cognitive effects. Specifically, we show that the
priming, distance, SNARC, and size congruity effects were presented with GPT-3,
while the anchoring effect is absent. We describe our methodology, and
specifically the way we converted real-world experiments to text-based
experiments. Finally, we speculate on the possible reasons why GPT-3 exhibits
these effects and discuss whether they are imitated or reinvented.",http://arxiv.org/pdf/2308.14337v1
2308.14333v1,cs.LG,DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing,2023-08-28 06:22:43+00:00,"Diffusion models have been leveraged to perform adversarial purification and
thus provide both empirical and certified robustness for a standard model. On
the other hand, different robustly trained smoothed models have been studied to
improve the certified robustness. Thus, it raises a natural question: Can
diffusion model be used to achieve improved certified robustness on those
robustly trained smoothed models? In this work, we first theoretically show
that recovered instances by diffusion models are in the bounded neighborhood of
the original instance with high probability; and the ""one-shot"" denoising
diffusion probabilistic models (DDPM) can approximate the mean of the generated
distribution of a continuous-time diffusion model, which approximates the
original instance under mild conditions. Inspired by our analysis, we propose a
certifiably robust pipeline DiffSmooth, which first performs adversarial
purification via diffusion models and then maps the purified instances to a
common region via a simple yet effective local smoothing strategy. We conduct
extensive experiments on different datasets and show that DiffSmooth achieves
SOTA-certified robustness compared with eight baselines. For instance,
DiffSmooth improves the SOTA-certified accuracy from $36.0\%$ to $53.0\%$ under
$\ell_2$ radius $1.5$ on ImageNet. The code is available at
[https://github.com/javyduck/DiffSmooth].",http://arxiv.org/pdf/2308.14333v1
2308.14329v1,cs.RO,End-to-End Driving via Self-Supervised Imitation Learning Using Camera and LiDAR Data,2023-08-28 06:17:15+00:00,"In autonomous driving, the end-to-end (E2E) driving approach that predicts
vehicle control signals directly from sensor data is rapidly gaining attention.
To learn a safe E2E driving system, one needs an extensive amount of driving
data and human intervention. Vehicle control data is constructed by many hours
of human driving, and it is challenging to construct large vehicle control
datasets. Often, publicly available driving datasets are collected with limited
driving scenes, and collecting vehicle control data is only available by
vehicle manufacturers. To address these challenges, this paper proposes the
first self-supervised learning framework, self-supervised imitation learning
(SSIL), that can learn E2E driving networks without using driving command data.
To construct pseudo steering angle data, proposed SSIL predicts a pseudo target
from the vehicle's poses at the current and previous time points that are
estimated with light detection and ranging sensors. Our numerical experiments
demonstrate that the proposed SSIL framework achieves comparable E2E driving
accuracy with the supervised learning counterpart. In addition, our qualitative
analyses using a conventional visual explanation tool show that trained NNs by
proposed SSIL and the supervision counterpart attend similar objects in making
predictions.",http://arxiv.org/pdf/2308.14329v1
2308.14328v1,cs.LG,Reinforcement Learning for Generative AI: A Survey,2023-08-28 06:15:14+00:00,"Deep Generative AI has been a long-standing essential topic in the machine
learning community, which can impact a number of application areas like text
generation and computer vision. The major paradigm to train a generative model
is maximum likelihood estimation, which pushes the learner to capture and
approximate the target data distribution by decreasing the divergence between
the model distribution and the target distribution. This formulation
successfully establishes the objective of generative tasks, while it is
incapable of satisfying all the requirements that a user might expect from a
generative model. Reinforcement learning, serving as a competitive option to
inject new training signals by creating new objectives that exploit novel
signals, has demonstrated its power and flexibility to incorporate human
inductive bias from multiple angles, such as adversarial learning,
hand-designed rules and learned reward model to build a performant model.
Thereby, reinforcement learning has become a trending research field and has
stretched the limits of generative AI in both model design and application. It
is reasonable to summarize and conclude advances in recent years with a
comprehensive review. Although there are surveys in different application areas
recently, this survey aims to shed light on a high-level review that spans a
range of application areas. We provide a rigorous taxonomy in this area and
make sufficient coverage on various models and applications. Notably, we also
surveyed the fast-developing large language model area. We conclude this survey
by showing the potential directions that might tackle the limit of current
models and expand the frontiers for generative AI.",http://arxiv.org/pdf/2308.14328v1
2308.14326v1,cs.AI,Towards solving ontological dissonance using network graphs,2023-08-28 06:10:26+00:00,"Data Spaces are an emerging concept for the trusted implementation of
data-based applications and business models, offering a high degree of
flexibility and sovereignty to all stakeholders. As Data Spaces are currently
emerging in different domains such as mobility, health or food, semantic
interfaces need to be identified and implemented to ensure the technical
interoperability of these Data Spaces. This paper consolidates data models from
13 different domains and analyzes the ontological dissonance of these domains.
Using a network graph, central data models and ontology attributes are
identified, while the semantic heterogeneity of these domains is described
qualitatively. The research outlook describes how these results help to connect
different Data Spaces across domains.",http://arxiv.org/pdf/2308.14326v1
2308.14321v1,cs.CL,Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction,2023-08-28 06:05:18+00:00,"Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.",http://arxiv.org/pdf/2308.14321v1
2308.14311v1,cs.AI,Spread Control Method on Unknown Networks Based on Hierarchical Reinforcement Learning,2023-08-28 05:29:49+00:00,"The spread of infectious diseases, rumors, and harmful speech in networks can
result in substantial losses, underscoring the significance of studying how to
suppress such hazardous events. However, previous studies often assume full
knowledge of the network structure, which is often not the case in real-world
scenarios. In this paper, we address the challenge of controlling the
propagation of hazardous events by removing nodes when the network structure is
unknown. To tackle this problem, we propose a hierarchical reinforcement
learning method that drastically reduces the action space, making the problem
feasible to solve. Simulation experiments demonstrate the superiority of our
method over the baseline methods. Remarkably, even though the baseline methods
possess extensive knowledge of the network structure, while our method has no
prior information about it, our approach still achieves better results.",http://arxiv.org/pdf/2308.14311v1
2308.14308v1,cs.LG,Policy Diversity for Cooperative Agents,2023-08-28 05:23:16+00:00,"Standard cooperative multi-agent reinforcement learning (MARL) methods aim to
find the optimal team cooperative policy to complete a task. However there may
exist multiple different ways of cooperating, which usually are very needed by
domain experts. Therefore, identifying a set of significantly different
policies can alleviate the task complexity for them. Unfortunately, there is a
general lack of effective policy diversity approaches specifically designed for
the multi-agent domain. In this work, we propose a method called
Moment-Matching Policy Diversity to alleviate this problem. This method can
generate different team policies to varying degrees by formalizing the
difference between team policies as the difference in actions of selected
agents in different policies. Theoretically, we show that our method is a
simple way to implement a constrained optimization problem that regularizes the
difference between two trajectory distributions by using the maximum mean
discrepancy. The effectiveness of our approach is demonstrated on a challenging
team-based shooter.",http://arxiv.org/pdf/2308.14308v1
2308.14306v1,cs.CL,Evaluating the Robustness to Instructions of Large Language Models,2023-08-28 04:57:07+00:00,"Recently, Instruction fine-tuning has risen to prominence as a potential
method for enhancing the zero-shot capabilities of Large Language Models (LLMs)
on novel tasks. This technique has shown an exceptional ability to boost the
performance of moderately sized LLMs, sometimes even reaching performance
levels comparable to those of much larger model variants. The focus is on the
robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an
exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional
Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction
datasets as case studies. We carried out a comprehensive evaluation of these
instruction-following LLMs which have been tuned based on open-domain
instructions and task-oriented instructions. The main discussion is their
performance and robustness towards instructions. We have observed that in most
cases, the model's performance in dealing with unfamiliar instructions tends to
worsen significantly, and the robustness of the model for RE instructions
deteriorates compared to QA. Further, we discovered that up until a certain
parameter size threshold (3B), the performance of the FLAN-T5 model improves as
the parameter count increases. The robustness of different scales of FLAN-T5
models to RE instruction is worse than the robustness to QA instruction.",http://arxiv.org/pdf/2308.14306v1
2308.14301v1,cs.AI,Artificial Intelligence in Career Counseling: A Test Case with ResumAI,2023-08-28 04:35:20+00:00,"The rise of artificial intelligence (AI) has led to various means of
integration of AI aimed to provide efficiency in tasks, one of which is career
counseling. A key part of getting a job is having a solid resume that passes
through the first round of programs and recruiters. It is difficult to find
good resources or schedule an appointment with a career counselor to help with
editing a resume for a specific role. With the rise of ChatGPT, Bard, and
several other AI chat programs it is possible to provide specific, automated
feedback on various concerns to suggest places for improvement within the
context of career counseling. This paper begins with a quick literature review
on the ethical considerations and limitations of AI in career counseling. The
authors also have created their own website service, called ResumAI, to test
and review the functionality of an AI career counselor. The findings of this
study will contribute to the understanding of chat AI ResumAI reviewer programs
and sites. The implications of the findings for the field of career counseling,
AI development, and ethical practice will be discussed.",http://arxiv.org/pdf/2308.14301v1
2308.14296v1,cs.IR,RecMind: Large Language Model Powered Agent For Recommendation,2023-08-28 04:31:04+00:00,"Recent advancements in instructing Large Language Models (LLMs) to utilize
external tools and execute multi-step plans have significantly enhanced their
ability to solve intricate tasks, ranging from mathematical problems to
creative writing. Yet, there remains a notable gap in studying the capacity of
LLMs in responding to personalized queries such as a recommendation request. To
bridge this gap, we have designed an LLM-powered autonomous recommender agent,
RecMind, which is capable of providing precise personalized recommendations
through careful planning, utilizing tools for obtaining external knowledge, and
leveraging individual data. We propose a novel algorithm, Self-Inspiring, to
improve the planning ability of the LLM agent. At each intermediate planning
step, the LLM 'self-inspires' to consider all previously explored states to
plan for next step. This mechanism greatly improves the model's ability to
comprehend and utilize historical planning information for recommendation. We
evaluate RecMind's performance in various recommendation scenarios, including
rating prediction, sequential recommendation, direct recommendation,
explanation generation, and review summarization. Our experiment shows that
RecMind outperforms existing zero/few-shot LLM-based recommendation methods in
different recommendation tasks and achieves competitive performance to a recent
model P5, which requires fully pre-train for the recommendation tasks.",http://arxiv.org/pdf/2308.14296v1
2308.14295v1,cs.AI,Traffic Light Control with Reinforcement Learning,2023-08-28 04:29:49+00:00,"Traffic light control is important for reducing congestion in urban mobility
systems. This paper proposes a real-time traffic light control method using
deep Q learning. Our approach incorporates a reward function considering queue
lengths, delays, travel time, and throughput. The model dynamically decides
phase changes based on current traffic conditions. The training of the deep Q
network involves an offline stage from pre-generated data with fixed schedules
and an online stage using real-time traffic data. A deep Q network structure
with a ""phase gate"" component is used to simplify the model's learning task
under different phases. A ""memory palace"" mechanism is used to address sample
imbalance during the training process. We validate our approach using both
synthetic and real-world traffic flow data on a road intersecting in Hangzhou,
China. Results demonstrate significant performance improvements of the proposed
method in reducing vehicle waiting time (57.1% to 100%), queue lengths (40.9%
to 100%), and total travel time (16.8% to 68.0%) compared to traditional fixed
signal plans.",http://arxiv.org/pdf/2308.14295v1
2308.14284v1,cs.AI,LLM Powered Sim-to-real Transfer for Traffic Signal Control,2023-08-28 03:49:13+00:00,"Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks
aiming to provide efficient transportation and mitigate congestion waste. In
recent, promising results have been attained by Reinforcement Learning (RL)
methods through trial and error in simulators, bringing confidence in solving
cities' congestion headaches. However, there still exist performance gaps when
simulator-trained policies are deployed to the real world. This issue is mainly
introduced by the system dynamic difference between the training simulator and
the real-world environments. The Large Language Models (LLMs) are trained on
mass knowledge and proved to be equipped with astonishing inference abilities.
In this work, we leverage LLMs to understand and profile the system dynamics by
a prompt-based grounded action transformation. Accepting the cloze prompt
template, and then filling in the answer based on accessible context, the
pre-trained LLM's inference ability is exploited and applied to understand how
weather conditions, traffic states, and road types influence traffic dynamics,
being aware of this, the policies' action is taken and grounded based on
realistic dynamics, thus help the agent learn a more realistic policy. We
conduct experiments using DQN to show the effectiveness of the proposed
PromptGAT's ability in mitigating the performance gap from simulation to
reality (sim-to-real).",http://arxiv.org/pdf/2308.14284v1
2308.14280v1,cs.CL,FonMTL: Towards Multitask Learning for the Fon Language,2023-08-28 03:26:21+00:00,"The Fon language, spoken by an average 2 million of people, is a truly
low-resourced African language, with a limited online presence, and existing
datasets (just to name but a few). Multitask learning is a learning paradigm
that aims to improve the generalization capacity of a model by sharing
knowledge across different but related tasks: this could be prevalent in very
data-scarce scenarios. In this paper, we present the first explorative approach
to multitask learning, for model capabilities enhancement in Natural Language
Processing for the Fon language. Specifically, we explore the tasks of Named
Entity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage
two language model heads as encoders to build shared representations for the
inputs, and we use linear layers blocks for classification relative to each
task. Our results on the NER and POS tasks for Fon, show competitive (or
better) performances compared to several multilingual pretrained language
models finetuned on single tasks. Additionally, we perform a few ablation
studies to leverage the efficiency of two different loss combination strategies
and find out that the equal loss weighting approach works best in our case. Our
code is open-sourced at https://github.com/bonaventuredossou/multitask_fon.",http://arxiv.org/pdf/2308.14280v1
2308.14269v1,cs.AI,Utilizing Mood-Inducing Background Music in Human-Robot Interaction,2023-08-28 02:54:05+00:00,"Past research has clearly established that music can affect mood and that
mood affects emotional and cognitive processing, and thus decision-making. It
follows that if a robot interacting with a person needs to predict the person's
behavior, knowledge of the music the person is listening to when acting is a
potentially relevant feature. To date, however, there has not been any concrete
evidence that a robot can improve its human-interactive decision-making by
taking into account what the person is listening to. This research fills this
gap by reporting the results of an experiment in which human participants were
required to complete a task in the presence of an autonomous agent while
listening to background music. Specifically, the participants drove a simulated
car through an intersection while listening to music. The intersection was not
empty, as another simulated vehicle, controlled autonomously, was also crossing
the intersection in a different direction. Our results clearly indicate that
such background information can be effectively incorporated in an agent's world
representation in order to better predict people's behavior. We subsequently
analyze how knowledge of music impacted both participant behavior and the
resulting learned policy.\setcounter{footnote}{2}\footnote{An earlier version
of part of the material in this paper appeared originally in the first author's
Ph.D. Dissertation~\cite{liebman2020sequential} but it has not appeared in any
pear-reviewed conference or journal.}",http://arxiv.org/pdf/2308.14269v1
2308.14266v1,cs.CL,SalesBot 2.0: A Human-Like Intent-Guided Chit-Chat Dataset,2023-08-28 02:48:49+00:00,"In recent research on dialogue systems and corpora, there has been a
significant focus on two distinct categories: task-oriented (TOD) and
open-domain (chit-chat) dialogues. TOD systems aim to satisfy specific user
goals, such as finding a movie to watch, whereas open-domain systems primarily
focus on generating engaging conversations. A recent study by Chiu et al.
(2022) introduced SalesBot, which provides simulators and a dataset with
one-turn transition from chit-chat to task-oriented dialogues. However, the
previously generated data solely relied on BlenderBot, which raised concerns
about its long-turn naturalness and consistency during a conversation. To
address this issue, this paper aims to build SalesBot 2.0, a revised version of
the published data, by leveraging the commonsense knowledge of large language
models (LLMs) through proper prompting. The objective is to gradually bridge
the gap between chit-chat and TOD towards better naturalness and consistency.
The newly released large-scale dataset with detailed annotations exhibits
smoother transitions between topics and is more human-like in terms of
naturalness and consistency. It can serve as a valuable resource for both
academic research and commercial applications. Furthermore, our proposed
framework can be applied to generate numerous dialogues with various target
intents.",http://arxiv.org/pdf/2308.14266v1
2308.14256v1,cs.CV,FaceChain: A Playground for Identity-Preserving Portrait Generation,2023-08-28 02:20:44+00:00,"Recent advancement in personalized image generation have unveiled the
intriguing capability of pre-trained text-to-image models on learning identity
information from a collection of portrait images. However, existing solutions
can be vulnerable in producing truthful details, and usually suffer from
several defects such as (i) The generated face exhibit its own unique
characteristics, \ie facial shape and facial feature positioning may not
resemble key characteristics of the input, and (ii) The synthesized face may
contain warped, blurred or corrupted regions. In this paper, we present
FaceChain, a personalized portrait generation framework that combines a series
of customized image-generation model and a rich set of face-related perceptual
understanding models (\eg, face detection, deep face embedding extraction, and
facial attribute recognition), to tackle aforementioned challenges and to
generate truthful personalized portraits, with only a handful of portrait
images as input. Concretely, we inject several SOTA face models into the
generation procedure, achieving a more efficient label-tagging,
data-processing, and model post-processing compared to previous solutions, such
as DreamBooth ~\cite{ruiz2023dreambooth} , InstantBooth
~\cite{shi2023instantbooth} , or other LoRA-only approaches ~\cite{hu2021lora}
. Through the development of FaceChain, we have identified several potential
directions to accelerate development of Face/Human-Centric AIGC research and
application. We have designed FaceChain as a framework comprised of pluggable
components that can be easily adjusted to accommodate different styles and
personalized needs. We hope it can grow to serve the burgeoning needs from the
communities. FaceChain is open-sourced under Apache-2.0 license at
\url{https://github.com/modelscope/facechain}.",http://arxiv.org/pdf/2308.14256v1
2308.14253v1,cs.AI,The Promise and Peril of Artificial Intelligence -- Violet Teaming Offers a Balanced Path Forward,2023-08-28 02:10:38+00:00,"Artificial intelligence (AI) promises immense benefits across sectors, yet
also poses risks from dual-use potentials, biases, and unintended behaviors.
This paper reviews emerging issues with opaque and uncontrollable AI systems
and proposes an integrative framework called violet teaming to develop reliable
and responsible AI. Violet teaming combines adversarial vulnerability probing
(red teaming) with solutions for safety and security (blue teaming) while
prioritizing ethics and social benefit. It emerged from AI safety research to
manage risks proactively by design. The paper traces the evolution of red,
blue, and purple teaming toward violet teaming, and then discusses applying
violet techniques to address biosecurity risks of AI in biotechnology.
Additional sections review key perspectives across law, ethics, cybersecurity,
macrostrategy, and industry best practices essential for operationalizing
responsible AI through holistic technical and social considerations. Violet
teaming provides both philosophy and method for steering AI trajectories toward
societal good. With conscience and wisdom, the extraordinary capabilities of AI
can enrich humanity. But without adequate precaution, the risks could prove
catastrophic. Violet teaming aims to empower moral technology for the common
welfare.",http://arxiv.org/pdf/2308.14253v1
2308.14250v1,cs.LG,Rule-Based Error Detection and Correction to Operationalize Movement Trajectory Classification,2023-08-28 01:57:38+00:00,"Classification of movement trajectories has many applications in
transportation. Supervised neural models represent the current
state-of-the-art. Recent security applications require this task to be rapidly
employed in environments that may differ from the data used to train such
models for which there is little training data. We provide a neuro-symbolic
rule-based framework to conduct error correction and detection of these models
to support eventual deployment in security applications. We provide a suite of
experiments on several recent and state-of-the-art models and show an accuracy
improvement of 1.7% over the SOTA model in the case where all classes are
present in training and when 40% of classes are omitted from training, we
obtain a 5.2% improvement (zero-shot) and 23.9% (few-shot) improvement over the
SOTA model without resorting to retraining of the base model.",http://arxiv.org/pdf/2308.14250v1
2308.14242v1,cs.AI,The Cultural Psychology of Large Language Models: Is ChatGPT a Holistic or Analytic Thinker?,2023-08-28 01:05:18+00:00,"The prevalent use of Large Language Models (LLMs) has necessitated studying
their mental models, yielding noteworthy theoretical and practical
implications. Current research has demonstrated that state-of-the-art LLMs,
such as ChatGPT, exhibit certain theory of mind capabilities and possess
relatively stable Big Five and/or MBTI personality traits. In addition,
cognitive process features form an essential component of these mental models.
Research in cultural psychology indicated significant differences in the
cognitive processes of Eastern and Western people when processing information
and making judgments. While Westerners predominantly exhibit analytical
thinking that isolates things from their environment to analyze their nature
independently, Easterners often showcase holistic thinking, emphasizing
relationships and adopting a global viewpoint. In our research, we probed the
cultural cognitive traits of ChatGPT. We employed two scales that directly
measure the cognitive process: the Analysis-Holism Scale (AHS) and the Triadic
Categorization Task (TCT). Additionally, we used two scales that investigate
the value differences shaped by cultural thinking: the Dialectical Self Scale
(DSS) and the Self-construal Scale (SCS). In cognitive process tests (AHS/TCT),
ChatGPT consistently tends towards Eastern holistic thinking, but regarding
value judgments (DSS/SCS), ChatGPT does not significantly lean towards the East
or the West. We suggest that the result could be attributed to both the
training paradigm and the training data in LLM development. We discuss the
potential value of this finding for AI research and directions for future
research.",http://arxiv.org/pdf/2308.14242v1
2308.14224v1,cs.AI,Modeling Player Personality Factors from In-Game Behavior and Affective Expression,2023-08-27 22:59:08+00:00,"Developing a thorough understanding of the target audience (and/or single
individuals) is a key factor for success - which is exceptionally important and
powerful for the domain of video games that can not only benefit from informed
decision making during development, but ideally even tailor game content,
difficulty and player experience while playing. The granular assessment of
individual personality and differences across players is a particularly
difficult endeavor, given the highly variant human nature, disagreement in
psychological background models and because of the effortful data collection
that most often builds upon long, time-consuming and deterrent questionnaires.
In this work, we explore possibilities to predict a series of player
personality questionnaire metrics from recorded in-game behavior and extend
related work by explicitly adding affective dialog decisions to the game
environment which could elevate the model's accuracy. Using random forest
regression, we predicted a wide variety of personality metrics from seven
established questionnaires across 62 players over 60 minute gameplay of a
customized version of the role-playing game Fallout: New Vegas. While some
personality variables could already be identified from reasonable underlying
in-game actions and affective expressions, we did not find ways to predict
others or encountered questionable correlations that could not be justified by
theoretical background literature. Yet, building on the initial opportunities
of this explorative study, we are striving to massively enlarge our data set to
players from an ecologically valid industrial game environment and investigate
the performance of more sophisticated machine learning approaches.",http://arxiv.org/pdf/2308.14224v1
2308.14217v1,cs.DB,Generations of Knowledge Graphs: The Crazy Ideas and the Business Impact,2023-08-27 22:35:27+00:00,"Knowledge Graphs (KGs) have been used to support a wide range of
applications, from web search to personal assistant. In this paper, we describe
three generations of knowledge graphs: entity-based KGs, which have been
supporting general search and question answering (e.g., at Google and Bing);
text-rich KGs, which have been supporting search and recommendations for
products, bio-informatics, etc. (e.g., at Amazon and Alibaba); and the emerging
integration of KGs and LLMs, which we call dual neural KGs. We describe the
characteristics of each generation of KGs, the crazy ideas behind the scenes in
constructing such KGs, and the techniques developed over time to enable
industry impact. In addition, we use KGs as examples to demonstrate a recipe to
evolve research ideas from innovations to production practice, and then to the
next level of innovations, to advance both science and business.",http://arxiv.org/pdf/2308.14217v1
2308.14206v1,cs.RO,Using Knowledge Representation and Task Planning for Robot-agnostic Skills on the Example of Contact-Rich Wiping Tasks,2023-08-27 21:17:32+00:00,"The transition to agile manufacturing, Industry 4.0, and high-mix-low-volume
tasks require robot programming solutions that are flexible. However, most
deployed robot solutions are still statically programmed and use stiff position
control, which limit their usefulness. In this paper, we show how a single
robot skill that utilizes knowledge representation, task planning, and
automatic selection of skill implementations based on the input parameters can
be executed in different contexts. We demonstrate how the skill-based control
platform enables this with contact-rich wiping tasks on different robot
systems. To achieve that in this case study, our approach needs to address
different kinematics, gripper types, vendors, and fundamentally different
control interfaces. We conducted the experiments with a mobile platform that
has a Universal Robots UR5e 6 degree-of-freedom robot arm with position control
and a 7 degree-of-freedom KUKA iiwa with torque control.",http://arxiv.org/pdf/2308.14206v1
2308.14199v1,cs.AI,Symbolic and Language Agnostic Large Language Models,2023-08-27 20:24:33+00:00,"We argue that the relative success of large language models (LLMs) is not a
reflection on the symbolic vs. subsymbolic debate but a reflection on employing
an appropriate strategy of bottom-up reverse engineering of language at scale.
However, due to the subsymbolic nature of these models whatever knowledge these
systems acquire about language will always be buried in millions of
microfeatures (weights) none of which is meaningful on its own. Moreover, and
due to their stochastic nature, these models will often fail in capturing
various inferential aspects that are prevalent in natural language. What we
suggest here is employing the successful bottom-up strategy in a symbolic
setting, producing symbolic, language agnostic and ontologically grounded large
language models.",http://arxiv.org/pdf/2308.14199v1
2308.14190v1,eess.IV,Score-Based Generative Models for PET Image Reconstruction,2023-08-27 19:43:43+00:00,"Score-based generative models have demonstrated highly promising results for
medical image reconstruction tasks in magnetic resonance imaging or computed
tomography. However, their application to Positron Emission Tomography (PET) is
still largely unexplored. PET image reconstruction involves a variety of
challenges, including Poisson noise with high variance and a wide dynamic
range. To address these challenges, we propose several PET-specific adaptations
of score-based generative models. The proposed framework is developed for both
2D and 3D PET. In addition, we provide an extension to guided reconstruction
using magnetic resonance images. We validate the approach through extensive 2D
and 3D $\textit{in-silico}$ experiments with a model trained on
patient-realistic data without lesions, and evaluate on data without lesions as
well as out-of-distribution data with lesions. This demonstrates the proposed
method's robustness and significant potential for improved PET reconstruction.",http://arxiv.org/pdf/2308.14190v1
2308.14186v1,cs.CL,Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations,2023-08-27 19:22:12+00:00,"The language ability of Large Language Models (LLMs) is often unbalanced
towards English because of the imbalance in the distribution of the
pre-training data. This disparity is demanded in further fine-tuning and
affecting the cross-lingual abilities of LLMs. In this paper, we propose to
empower Instructiontuned LLMs (It-LLMs) in languages other than English by
building semantic alignment between them. Hence, we propose CrossAlpaca, an
It-LLM with cross-lingual instruction-following and Translation-following
demonstrations to improve semantic alignment between languages. We validate our
approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA
and adapted versions of MMLU and BBH. Our models, tested over six different
languages, outperform the It-LLMs tuned on monolingual data. The final results
show that instruction tuning on non-English data is not enough and that
semantic alignment can be further improved by Translation-following
demonstrations.",http://arxiv.org/pdf/2308.14186v1
2308.14181v1,cs.LG,Topological Augmentation for Class-Imbalanced Node Classification,2023-08-27 19:01:29+00:00,"Class imbalance is prevalent in real-world node classification tasks and
often biases graph learning models toward majority classes. Most existing
studies root from a node-centric perspective and aim to address the class
imbalance in training data by node/class-wise reweighting or resampling. In
this paper, we approach the source of the class-imbalance bias from an
under-explored topology-centric perspective. Our investigation reveals that
beyond the inherently skewed training class distribution, the graph topology
also plays an important role in the formation of predictive bias: we identify
two fundamental challenges, namely ambivalent and distant message-passing, that
can exacerbate the bias by aggravating majority-class over-generalization and
minority-class misclassification. In light of these findings, we devise a
lightweight topological augmentation method ToBA to dynamically rectify the
nodes influenced by ambivalent/distant message-passing during graph learning,
so as to mitigate the class-imbalance bias. We highlight that ToBA is a
model-agnostic, efficient, and versatile solution that can be seamlessly
combined with and further boost other imbalance-handling techniques. Systematic
experiments validate the superior performance of ToBA in both promoting
imbalanced node classification and mitigating the prediction bias between
different classes.",http://arxiv.org/pdf/2308.14181v1
2308.14179v1,cs.CL,Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP,2023-08-27 18:46:47+00:00,"Mechanistic interpretability seeks to understand the neural mechanisms that
enable specific behaviors in Large Language Models (LLMs) by leveraging
causality-based methods. While these approaches have identified neural circuits
that copy spans of text, capture factual knowledge, and more, they remain
unusable for multimodal models since adapting these tools to the
vision-language domain requires considerable architectural changes. In this
work, we adapt a unimodal causal tracing tool to BLIP to enable the study of
the neural mechanisms underlying image-conditioned text generation. We
demonstrate our approach on a visual question answering dataset, highlighting
the causal relevance of later layer representations for all tokens.
Furthermore, we release our BLIP causal tracing tool as open source to enable
further experimentation in vision-language mechanistic interpretability by the
community. Our code is available at
https://github.com/vedantpalit/Towards-Vision-Language-Mechanistic-Interpretability.",http://arxiv.org/pdf/2308.14179v1
2308.14172v1,cs.LG,Hypergraph Structure Inference From Data Under Smoothness Prior,2023-08-27 18:28:58+00:00,"Hypergraphs are important for processing data with higher-order relationships
involving more than two entities. In scenarios where explicit hypergraphs are
not readily available, it is desirable to infer a meaningful hypergraph
structure from the node features to capture the intrinsic relations within the
data. However, existing methods either adopt simple pre-defined rules that fail
to precisely capture the distribution of the potential hypergraph structure, or
learn a mapping between hypergraph structures and node features but require a
large amount of labelled data, i.e., pre-existing hypergraph structures, for
training. Both restrict their applications in practical scenarios. To fill this
gap, we propose a novel smoothness prior that enables us to design a method to
infer the probability for each potential hyperedge without labelled data as
supervision. The proposed prior indicates features of nodes in a hyperedge are
highly correlated by the features of the hyperedge containing them. We use this
prior to derive the relation between the hypergraph structure and the node
features via probabilistic modelling. This allows us to develop an unsupervised
inference method to estimate the probability for each potential hyperedge via
solving an optimisation problem that has an analytical solution. Experiments on
both synthetic and real-world data demonstrate that our method can learn
meaningful hypergraph structures from data more efficiently than existing
hypergraph structure inference methods.",http://arxiv.org/pdf/2308.14172v1
2308.14165v1,cs.IR,Distributional Off-Policy Evaluation for Slate Recommendations,2023-08-27 17:58:32+00:00,"Recommendation strategies are typically evaluated by using previously logged
data, employing off-policy evaluation methods to estimate their expected
performance. However, for strategies that present users with slates of multiple
items, the resulting combinatorial action space renders many of these methods
impractical. Prior work has developed estimators that leverage the structure in
slates to estimate the expected off-policy performance, but the estimation of
the entire performance distribution remains elusive. Estimating the complete
distribution allows for a more comprehensive evaluation of recommendation
strategies, particularly along the axes of risk and fairness that employ
metrics computable from the distribution. In this paper, we propose an
estimator for the complete off-policy performance distribution for slates and
establish conditions under which the estimator is unbiased and consistent. This
builds upon prior work on off-policy evaluation for slates and off-policy
distribution estimation in reinforcement learning. We validate the efficacy of
our method empirically on synthetic data as well as on a slate recommendation
simulator constructed from real-world data (MovieLens-20M). Our results show a
significant reduction in estimation variance and improved sample efficiency
over prior work across a range of slate structures.",http://arxiv.org/pdf/2308.14165v1
2308.14163v1,cs.AI,Explaining with Attribute-based and Relational Near Misses: An Interpretable Approach to Distinguishing Facial Expressions of Pain and Disgust,2023-08-27 17:47:30+00:00,"Explaining concepts by contrasting examples is an efficient and convenient
way of giving insights into the reasons behind a classification decision. This
is of particular interest in decision-critical domains, such as medical
diagnostics. One particular challenging use case is to distinguish facial
expressions of pain and other states, such as disgust, due to high similarity
of manifestation. In this paper, we present an approach for generating
contrastive explanations to explain facial expressions of pain and disgust
shown in video sequences. We implement and compare two approaches for
contrastive explanation generation. The first approach explains a specific pain
instance in contrast to the most similar disgust instance(s) based on the
occurrence of facial expressions (attributes). The second approach takes into
account which temporal relations hold between intervals of facial expressions
within a sequence (relations). The input to our explanation generation approach
is the output of an interpretable rule-based classifier for pain and disgust.We
utilize two different similarity metrics to determine near misses and far
misses as contrasting instances. Our results show that near miss explanations
are shorter than far miss explanations, independent from the applied similarity
metric. The outcome of our evaluation indicates that pain and disgust can be
distinguished with the help of temporal relations. We currently plan
experiments to evaluate how the explanations help in teaching concepts and how
they could be enhanced by further modalities and interaction.",http://arxiv.org/pdf/2308.14163v1
2308.14161v1,cs.CV,Intergrated Segmentation and Detection Models for Dentex Challenge 2023,2023-08-27 17:44:25+00:00,"Dental panoramic x-rays are commonly used in dental diagnosing. With the
development of deep learning, auto detection of diseases from dental panoramic
x-rays can help dentists to diagnose diseases more efficiently.The Dentex
Challenge 2023 is a competition for automatic detection of abnormal teeth along
with their enumeration ids from dental panoramic x-rays. In this paper, we
propose a method integrating segmentation and detection models to detect
abnormal teeth as well as obtain their enumeration ids.Our codes are available
at https://github.com/xyzlancehe/DentexSegAndDet.",http://arxiv.org/pdf/2308.14161v1
2308.14160v1,cs.CV,A Unified Transformer-based Network for multimodal Emotion Recognition,2023-08-27 17:30:56+00:00,"The development of transformer-based models has resulted in significant
advances in addressing various vision and NLP-based research challenges.
However, the progress made in transformer-based methods has not been
effectively applied to biosensing research. This paper presents a novel Unified
Biosensor-Vision Multi-modal Transformer-based (UBVMT) method to classify
emotions in an arousal-valence space by combining a 2D representation of an
ECG/PPG signal with the face information. To achieve this goal, we first
investigate and compare the unimodal emotion recognition performance of three
image-based representations of the ECG/PPG signal. We then present our UBVMT
network which is trained to perform emotion recognition by combining the 2D
image-based representation of the ECG/PPG signal and the facial expression
features. Our unified transformer model consists of homogeneous transformer
blocks that take as an input the 2D representation of the ECG/PPG signal and
the corresponding face frame for emotion representation learning with minimal
modality-specific design. Our UBVMT model is trained by reconstructing masked
patches of video frames and 2D images of ECG/PPG signals, and contrastive
modeling to align face and ECG/PPG data. Extensive experiments on the
MAHNOB-HCI and DEAP datasets show that our Unified UBVMT-based model produces
comparable results to the state-of-the-art techniques.",http://arxiv.org/pdf/2308.14160v1
2308.14149v1,cs.CL,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",2023-08-27 16:14:19+00:00,"Generative pre-trained transformer (GPT) models have revolutionized the field
of natural language processing (NLP) with remarkable performance in various
tasks and also extend their power to multimodal domains. Despite their success,
large GPT models like GPT-4 face inherent limitations such as considerable
size, high computational requirements, complex deployment processes, and closed
development loops. These constraints restrict their widespread adoption and
raise concerns regarding their responsible development and usage. The need for
user-friendly, relatively small, and open-sourced alternative GPT models arises
from the desire to overcome these limitations while retaining high performance.
In this survey paper, we provide an examination of alternative open-sourced
models of large GPTs, focusing on user-friendly and relatively small models
that facilitate easier deployment and accessibility. Through this extensive
survey, we aim to equip researchers, practitioners, and enthusiasts with a
thorough understanding of user-friendly and relatively small open-sourced
models of large GPTs, their current state, challenges, and future research
directions, inspiring the development of more efficient, accessible, and
versatile GPT models that cater to the broader scientific community and advance
the field of general artificial intelligence. The source contents are
continuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.",http://arxiv.org/pdf/2308.14149v1
2308.14133v1,cs.CV,Cheap Lunch for Medical Image Segmentation by Fine-tuning SAM on Few Exemplars,2023-08-27 15:21:25+00:00,"The Segment Anything Model (SAM) has demonstrated remarkable capabilities of
scaled-up segmentation models, enabling zero-shot generalization across a
variety of domains. By leveraging large-scale foundational models as
pre-trained models, it is a natural progression to fine-tune SAM for specific
domains to further enhance performances. However, the adoption of foundational
models in the medical domain presents a challenge due to the difficulty and
expense of labeling sufficient data for adaptation within hospital systems. In
this paper, we introduce an efficient and practical approach for fine-tuning
SAM using a limited number of exemplars, making it suitable for such scenarios.
Our approach combines two established techniques from the literature: an
exemplar-guided synthesis module and the widely recognized Low-Rank Adaptation
(LoRA) fine-tuning strategy, serving as data-level and model-level attempts
respectively. Interestingly, our empirical findings suggest that SAM can be
effectively aligned within the medical domain even with few labeled data. We
validate our approach through experiments on brain tumor segmentation (BraTS)
and multi-organ CT segmentation (Synapse). The comprehensive results underscore
the feasibility and effectiveness of such an approach, paving the way for the
practical application of SAM in the medical domain.",http://arxiv.org/pdf/2308.14133v1
2308.14132v1,cs.CL,Detecting Language Model Attacks with Perplexity,2023-08-27 15:20:06+00:00,"A novel hack involving Large Language Models (LLMs) has emerged, leveraging
adversarial suffixes to trick models into generating perilous responses. This
method has garnered considerable attention from reputable media outlets such as
the New York Times and Wired, thereby influencing public perception regarding
the security and safety of LLMs. In this study, we advocate the utilization of
perplexity as one of the means to recognize such potential attacks. The
underlying concept behind these hacks revolves around appending an unusually
constructed string of text to a harmful query that would otherwise be blocked.
This maneuver confuses the protective mechanisms and tricks the model into
generating a forbidden response. Such scenarios could result in providing
detailed instructions to a malicious user for constructing explosives or
orchestrating a bank heist. Our investigation demonstrates the feasibility of
employing perplexity, a prevalent natural language processing metric, to detect
these adversarial tactics before generating a forbidden response. By evaluating
the perplexity of queries with and without such adversarial suffixes using an
open-source LLM, we discovered that nearly 90 percent were above a perplexity
of 1000. This contrast underscores the efficacy of perplexity for detecting
this type of exploit.",http://arxiv.org/pdf/2308.14132v1
2308.14120v1,cs.LG,Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies,2023-08-27 14:28:38+00:00,"A knowledge gap persists between Machine Learning (ML) developers (e.g., data
scientists) and practitioners (e.g., clinicians), hampering the full
utilization of ML for clinical data analysis. We investigated the potential of
the chatGPT Code Interpreter (CI), an extension of GPT-4, to bridge this gap
and perform ML analyses efficiently. Real-world clinical datasets and study
details from large trials across various medical specialties were presented to
chatGPT CI without specific guidance. ChatGPT CI autonomously developed
state-of-the-art ML models based on the original study's training data to
predict clinical outcomes such as cancer development, cancer progression,
disease complications, or biomarkers such as pathogenic gene sequences.
Strikingly, these ML models matched or outperformed their published
counterparts. We conclude that chatGPT CI offers a promising avenue to
democratize ML in medicine, making advanced analytics accessible to non-ML
experts and promoting broader applications in medical research and practice.",http://arxiv.org/pdf/2308.14120v1
2308.14111v1,eess.SY,MARL for Decentralized Electric Vehicle Charging Coordination with V2V Energy Exchange,2023-08-27 14:06:21+00:00,"Effective energy management of electric vehicle (EV) charging stations is
critical to supporting the transport sector's sustainable energy transition.
This paper addresses the EV charging coordination by considering
vehicle-to-vehicle (V2V) energy exchange as the flexibility to harness in EV
charging stations. Moreover, this paper takes into account EV user experiences,
such as charging satisfaction and fairness. We propose a Multi-Agent
Reinforcement Learning (MARL) approach to coordinate EV charging with V2V
energy exchange while considering uncertainties in the EV arrival time, energy
price, and solar energy generation. The exploration capability of MARL is
enhanced by introducing parameter noise into MARL's neural network models.
Experimental results demonstrate the superior performance and scalability of
our proposed method compared to traditional optimization baselines. The
decentralized execution of the algorithm enables it to effectively deal with
partial system faults in the charging station.",http://arxiv.org/pdf/2308.14111v1
2308.14108v1,cs.CV,Depth self-supervision for single image novel view synthesis,2023-08-27 13:50:15+00:00,"In this paper, we tackle the problem of generating a novel image from an
arbitrary viewpoint given a single frame as input. While existing methods
operating in this setup aim at predicting the target view depth map to guide
the synthesis, without explicit supervision over such a task, we jointly
optimize our framework for both novel view synthesis and depth estimation to
unleash the synergy between the two at its best. Specifically, a shared depth
decoder is trained in a self-supervised manner to predict depth maps that are
consistent across the source and target views. Our results demonstrate the
effectiveness of our approach in addressing the challenges of both tasks
allowing for higher-quality generated images, as well as more accurate depth
for the target viewpoint.",http://arxiv.org/pdf/2308.14108v1
2308.14105v1,cs.CV,Unified and Dynamic Graph for Temporal Character Grouping in Long Videos,2023-08-27 13:22:55+00:00,"Video temporal character grouping locates appearing moments of major
characters within a video according to their identities. To this end, recent
works have evolved from unsupervised clustering to graph-based supervised
clustering. However, graph methods are built upon the premise of fixed affinity
graphs, bringing many inexact connections. Besides, they extract multi-modal
features with kinds of models, which are unfriendly to deployment. In this
paper, we present a unified and dynamic graph (UniDG) framework for temporal
character grouping. This is accomplished firstly by a unified representation
network that learns representations of multiple modalities within the same
space and still preserves the modality's uniqueness simultaneously. Secondly,
we present a dynamic graph clustering where the neighbors of different
quantities are dynamically constructed for each node via a cyclic matching
strategy, leading to a more reliable affinity graph. Thirdly, a progressive
association method is introduced to exploit spatial and temporal contexts among
different modalities, allowing multi-modal clustering results to be well fused.
As current datasets only provide pre-extracted features, we evaluate our UniDG
method on a collected dataset named MTCG, which contains each character's
appearing clips of face and body and speaking voice tracks. We also evaluate
our key components on existing clustering and retrieval datasets to verify the
generalization ability. Experimental results manifest that our method can
achieve promising results and outperform several state-of-the-art approaches.",http://arxiv.org/pdf/2308.14105v1
2308.14100v1,cs.CV,Rethinking Exemplars for Continual Semantic Segmentation in Endoscopy Scenes: Entropy-based Mini-Batch Pseudo-Replay,2023-08-27 13:07:44+00:00,"Endoscopy is a widely used technique for the early detection of diseases or
robotic-assisted minimally invasive surgery (RMIS). Numerous deep learning
(DL)-based research works have been developed for automated diagnosis or
processing of endoscopic view. However, existing DL models may suffer from
catastrophic forgetting. When new target classes are introduced over time or
cross institutions, the performance of old classes may suffer severe
degradation. More seriously, data privacy and storage issues may lead to the
unavailability of old data when updating the model. Therefore, it is necessary
to develop a continual learning (CL) methodology to solve the problem of
catastrophic forgetting in endoscopic image segmentation. To tackle this, we
propose a Endoscopy Continual Semantic Segmentation (EndoCSS) framework that
does not involve the storage and privacy issues of exemplar data. The framework
includes a mini-batch pseudo-replay (MB-PR) mechanism and a self-adaptive noisy
cross-entropy (SAN-CE) loss. The MB-PR strategy circumvents privacy and storage
issues by generating pseudo-replay images through a generative model.
Meanwhile, the MB-PR strategy can also correct the model deviation to the
replay data and current training data, which is aroused by the significant
difference in the amount of current and replay images. Therefore, the model can
perform effective representation learning on both new and old tasks. SAN-CE
loss can help model fitting by adjusting the model's output logits, and also
improve the robustness of training. Extensive continual semantic segmentation
(CSS) experiments on public datasets demonstrate that our method can robustly
and effectively address the catastrophic forgetting brought by class increment
in endoscopy scenes. The results show that our framework holds excellent
potential for real-world deployment in a streaming learning manner.",http://arxiv.org/pdf/2308.14100v1
2308.14093v1,cs.LG,The inverse problem for neural networks,2023-08-27 12:35:38+00:00,"We study the problem of computing the preimage of a set under a neural
network with piecewise-affine activation functions. We recall an old result
that the preimage of a polyhedral set is again a union of polyhedral sets and
can be effectively computed. We show several applications of computing the
preimage for analysis and interpretability of neural networks.",http://arxiv.org/pdf/2308.14093v1
2308.14089v1,cs.CL,MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records,2023-08-27 12:24:39+00:00,"The ability of large language models (LLMs) to follow natural language
instructions with human-level fluency suggests many opportunities in healthcare
to reduce administrative burden and improve quality of care. However,
evaluating LLMs on realistic text generation tasks for healthcare remains
challenging. Existing question answering datasets for electronic health record
(EHR) data fail to capture the complexity of information needs and
documentation burdens experienced by clinicians. To address these challenges,
we introduce MedAlign, a benchmark dataset of 983 natural language instructions
for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes
clinician-written reference responses for 303 instructions, and provides 276
longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to
evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality
of each LLM response. We found high error rates, ranging from 35% (GPT-4) to
68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k
context lengths for GPT-4. Finally, we report correlations between clinician
rankings and automated natural language generation metrics as a way to rank
LLMs without human review. We make MedAlign available under a research data use
agreement to enable LLM evaluations on tasks aligned with clinician needs and
preferences.",http://arxiv.org/pdf/2308.14089v1
2308.14084v1,cs.CV,Practical Edge Detection via Robust Collaborative Learning,2023-08-27 12:12:27+00:00,"Edge detection, as a core component in a wide range of visionoriented tasks,
is to identify object boundaries and prominent edges in natural images. An edge
detector is desired to be both efficient and accurate for practical use. To
achieve the goal, two key issues should be concerned: 1) How to liberate deep
edge models from inefficient pre-trained backbones that are leveraged by most
existing deep learning methods, for saving the computational cost and cutting
the model size; and 2) How to mitigate the negative influence from noisy or
even wrong labels in training data, which widely exist in edge detection due to
the subjectivity and ambiguity of annotators, for the robustness and accuracy.
In this paper, we attempt to simultaneously address the above problems via
developing a collaborative learning based model, termed PEdger. The principle
behind our PEdger is that, the information learned from different training
moments and heterogeneous (recurrent and non recurrent in this work)
architectures, can be assembled to explore robust knowledge against noisy
annotations, even without the help of pre-training on extra data. Extensive
ablation studies together with quantitative and qualitative experimental
comparisons on the BSDS500 and NYUD datasets are conducted to verify the
effectiveness of our design, and demonstrate its superiority over other
competitors in terms of accuracy, speed, and model size. Codes can be found at
https://github.co/ForawardStar/PEdger.",http://arxiv.org/pdf/2308.14084v1
2308.14058v1,cs.LG,Pruning the Unlabeled Data to Improve Semi-Supervised Learning,2023-08-27 09:45:41+00:00,"In the domain of semi-supervised learning (SSL), the conventional approach
involves training a learner with a limited amount of labeled data alongside a
substantial volume of unlabeled data, both drawn from the same underlying
distribution. However, for deep learning models, this standard practice may not
yield optimal results. In this research, we propose an alternative perspective,
suggesting that distributions that are more readily separable could offer
superior benefits to the learner as compared to the original distribution. To
achieve this, we present PruneSSL, a practical technique for selectively
removing examples from the original unlabeled dataset to enhance its
separability. We present an empirical study, showing that although PruneSSL
reduces the quantity of available training data for the learner, it
significantly improves the performance of various competitive SSL algorithms,
thereby achieving state-of-the-art results across several image classification
tasks.",http://arxiv.org/pdf/2308.14058v1
2308.14050v1,cs.CV,PECon: Contrastive Pretraining to Enhance Feature Alignment between CT and EHR Data for Improved Pulmonary Embolism Diagnosis,2023-08-27 09:07:26+00:00,"Previous deep learning efforts have focused on improving the performance of
Pulmonary Embolism(PE) diagnosis from Computed Tomography (CT) scans using
Convolutional Neural Networks (CNN). However, the features from CT scans alone
are not always sufficient for the diagnosis of PE. CT scans along with
electronic heath records (EHR) can provide a better insight into the patients
condition and can lead to more accurate PE diagnosis. In this paper, we propose
Pulmonary Embolism Detection using Contrastive Learning (PECon), a supervised
contrastive pretraining strategy that employs both the patients CT scans as
well as the EHR data, aiming to enhance the alignment of feature
representations between the two modalities and leverage information to improve
the PE diagnosis. In order to achieve this, we make use of the class labels and
pull the sample features of the same class together, while pushing away those
of the other class. Results show that the proposed work outperforms the
existing techniques and achieves state-of-the-art performance on the RadFusion
dataset with an F1-score of 0.913, accuracy of 0.90 and an AUROC of 0.943.
Furthermore, we also explore the explainability of our approach in comparison
to other methods. Our code is publicly available at
https://github.com/BioMedIA-MBZUAI/PECon.",http://arxiv.org/pdf/2308.14050v1
2308.14035v1,cond-mat.mtrl-sci,Multi-plane denoising diffusion-based dimensionality expansion for 2D-to-3D reconstruction of microstructures with harmonized sampling,2023-08-27 07:57:25+00:00,"Acquiring reliable microstructure datasets is a pivotal step toward the
systematic design of materials with the aid of integrated computational
materials engineering (ICME) approaches. However, obtaining three-dimensional
(3D) microstructure datasets is often challenging due to high experimental
costs or technical limitations, while acquiring two-dimensional (2D)
micrographs is comparatively easier. To deal with this issue, this study
proposes a novel framework for 2D-to-3D reconstruction of microstructures
called Micro3Diff using diffusion-based generative models (DGMs). Specifically,
this approach solely requires pre-trained DGMs for the generation of 2D
samples, and dimensionality expansion (2D-to-3D) takes place only during the
generation process (i.e., reverse diffusion process). The proposed framework
incorporates a new concept referred to as multi-plane denoising diffusion,
which transforms noisy samples (i.e., latent variables) from different planes
into the data structure while maintaining spatial connectivity in 3D space.
Furthermore, a harmonized sampling process is developed to address possible
deviations from the reverse Markov chain of DGMs during the dimensionality
expansion. Combined, we demonstrate the feasibility of Micro3Diff in
reconstructing 3D samples with connected slices that maintain morphologically
equivalence to the original 2D images. To validate the performance of
Micro3Diff, various types of microstructures (synthetic and experimentally
observed) are reconstructed, and the quality of the generated samples is
assessed both qualitatively and quantitatively. The successful reconstruction
outcomes inspire the potential utilization of Micro3Diff in upcoming ICME
applications while achieving a breakthrough in comprehending and manipulating
the latent space of DGMs.",http://arxiv.org/pdf/2308.14035v1
2308.14034v1,cs.AI,Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum,2023-08-27 07:53:00+00:00,"Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to extending the capability of LLMs. Although some works
employ open-source LLMs for the tool learning task, most of them are trained in
a controlled environment in which LLMs only learn to execute the human-provided
tools. However, selecting proper tools from the large toolset is also a crucial
ability for the tool learning model to be applied in real-world applications.
Existing methods usually directly employ self-instruction methods to train the
model, which ignores differences in tool complexity. In this paper, we propose
the Confucius, a novel tool learning framework to train LLM to use complicated
tools in real-world scenarios, which contains two main phases: (1) We first
propose a multi-stage learning method to teach the LLM to use various tools
from an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative
Self-instruct from Introspective Feedback (ISIF) to dynamically construct the
dataset to improve the ability to use the complicated tool. Extensive
experiments conducted on both controlled and real-world settings demonstrate
the superiority of our tool learning framework in the real-world application
scenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based
baselines (e.g. GPT4Tools).",http://arxiv.org/pdf/2308.14034v1
2308.14029v1,cs.IR,Text Matching Improves Sequential Recommendation by Reducing Popularity Biases,2023-08-27 07:44:33+00:00,"This paper proposes Text mAtching based SequenTial rEcommendation model
(TASTE), which maps items and users in an embedding space and recommends items
by matching their text representations. TASTE verbalizes items and user-item
interactions using identifiers and attributes of items. To better characterize
user behaviors, TASTE additionally proposes an attention sparsity method, which
enables TASTE to model longer user-item interactions by reducing the
self-attention computations during encoding. Our experiments show that TASTE
outperforms the state-of-the-art methods on widely used sequential
recommendation datasets. TASTE alleviates the cold start problem by
representing long-tail items using full-text modeling and bringing the benefits
of pretrained language models to recommendation systems. Our further analyses
illustrate that TASTE significantly improves the recommendation accuracy by
reducing the popularity bias of previous item id based recommendation models
and returning more appropriate and text-relevant items to satisfy users. All
codes are available at https://github.com/OpenMatch/TASTE.",http://arxiv.org/pdf/2308.14029v1
2308.14017v1,cs.LG,Revolutionizing Disease Diagnosis: A Microservices-Based Architecture for Privacy-Preserving and Efficient IoT Data Analytics Using Federated Learning,2023-08-27 06:31:43+00:00,"Deep learning-based disease diagnosis applications are essential for accurate
diagnosis at various disease stages. However, using personal data exposes
traditional centralized learning systems to privacy concerns. On the other
hand, by positioning processing resources closer to the device and enabling
more effective data analyses, a distributed computing paradigm has the
potential to revolutionize disease diagnosis. Scalable architectures for data
analytics are also crucial in healthcare, where data analytics results must
have low latency and high dependability and reliability. This study proposes a
microservices-based approach for IoT data analytics systems to satisfy privacy
and performance requirements by arranging entities into fine-grained, loosely
connected, and reusable collections. Our approach relies on federated learning,
which can increase disease diagnosis accuracy while protecting data privacy.
Additionally, we employ transfer learning to obtain more efficient models.
Using more than 5800 chest X-ray images for pneumonia detection from a publicly
available dataset, we ran experiments to assess the effectiveness of our
approach. Our experiments reveal that our approach performs better in
identifying pneumonia than other cutting-edge technologies, demonstrating our
approach's promising potential detection performance.",http://arxiv.org/pdf/2308.14017v1
2308.14009v1,cs.CV,Towards Fast and Accurate Image-Text Retrieval with Self-Supervised Fine-Grained Alignment,2023-08-27 05:45:54+00:00,"Image-text retrieval requires the system to bridge the heterogenous gap
between vision and language for accurate retrieval while keeping the network
lightweight-enough for efficient retrieval. Existing trade-off solutions mainly
study from the view of incorporating cross-modal interactions with the
independent-embedding framework or leveraging stronger pretrained encoders,
which still demand time-consuming similarity measurement or heavyweight model
structure in the retrieval stage. In this work, we propose an image-text
alignment module SelfAlign on top of the independent-embedding framework, which
improves the retrieval accuracy while maintains the retrieval efficiency
without extra supervision. SelfAlign contains two collaborative sub-modules
that force image-text alignment at both concept level and context level by
self-supervised contrastive learning. It does not require cross-modal embedding
interactions during training while maintaining independent image and text
encoders during retrieval. With comparable time cost, SelfAlign consistently
boosts the accuracy of state-of-the-art non-pretraining independent-embedding
models respectively by 9.1%, 4.2% and 6.6% in terms of R@sum score on
Flickr30K, MSCOCO 1K and MS-COCO 5K datasets. The retrieval accuracy also
outperforms most existing interactive-embedding models with orders of magnitude
decrease in retrieval time. The source code is available at:
https://github.com/Zjamie813/SelfAlign.",http://arxiv.org/pdf/2308.14009v1
2308.13998v1,cs.CV,Computation-efficient Deep Learning for Computer Vision: A Survey,2023-08-27 03:55:28+00:00,"Over the past decade, deep learning models have exhibited considerable
advancements, reaching or even exceeding human-level performance in a range of
visual perception tasks. This remarkable progress has sparked interest in
applying deep networks to real-world applications, such as autonomous vehicles,
mobile devices, robotics, and edge computing. However, the challenge remains
that state-of-the-art models usually demand significant computational
resources, leading to impractical power consumption, latency, or carbon
emissions in real-world scenarios. This trade-off between effectiveness and
efficiency has catalyzed the emergence of a new research focus: computationally
efficient deep learning, which strives to achieve satisfactory performance
while minimizing the computational cost during inference. This review offers an
extensive analysis of this rapidly evolving field by examining four key areas:
1) the development of static or dynamic light-weighted backbone models for the
efficient extraction of discriminative deep representations; 2) the specialized
network architectures or algorithms tailored for specific computer vision
tasks; 3) the techniques employed for compressing deep learning models; and 4)
the strategies for deploying efficient deep networks on hardware platforms.
Additionally, we provide a systematic discussion on the critical challenges
faced in this domain, such as network architecture design, training schemes,
practical efficiency, and more realistic model compression approaches, as well
as potential future research directions.",http://arxiv.org/pdf/2308.13998v1
2308.13985v1,cs.LG,Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective,2023-08-27 02:10:22+00:00,"Linear scalarization, i.e., combining all loss functions by a weighted sum,
has been the default choice in the literature of multi-task learning (MTL)
since its inception. In recent years, there is a surge of interest in
developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a
multi-objective optimization problem. However, it remains open whether there is
a fundamental advantage of SMTOs over scalarization. In fact, heated debates
exist in the community comparing these two types of algorithms, mostly from an
empirical perspective. To approach the above question, in this paper, we
revisit scalarization from a theoretical perspective. We focus on linear MTL
models and study whether scalarization is capable of fully exploring the Pareto
front. Our findings reveal that, in contrast to recent works that claimed
empirical advantages of scalarization, scalarization is inherently incapable of
full exploration, especially for those Pareto optimal solutions that strike the
balanced trade-offs between multiple tasks. More concretely, when the model is
under-parametrized, we reveal a multi-surface structure of the feasible region
and identify necessary and sufficient conditions for full exploration. This
leads to the conclusion that scalarization is in general incapable of tracing
out the Pareto front. Our theoretical results partially answer the open
questions in Xin et al. (2021), and provide a more intuitive explanation on why
scalarization fails beyond non-convexity. We additionally perform experiments
on a real-world dataset using both scalarization and state-of-the-art SMTOs.
The experimental results not only corroborate our theoretical findings, but
also unveil the potential of SMTOs in finding balanced solutions, which cannot
be achieved by scalarization.",http://arxiv.org/pdf/2308.13985v1
2308.13979v1,cs.CV,Enhancing Bloodstain Analysis Through AI-Based Segmentation: Leveraging Segment Anything Model for Crime Scene Investigation,2023-08-27 01:11:03+00:00,"Bloodstain pattern analysis plays a crucial role in crime scene
investigations by providing valuable information through the study of unique
blood patterns. Conventional image analysis methods, like Thresholding and
Contrast, impose stringent requirements on the image background and is
labor-intensive in the context of droplet image segmentation. The Segment
Anything Model (SAM), a recently proposed method for extensive image
recognition, is yet to be adequately assessed for its accuracy and efficiency
on bloodstain image segmentation. This paper explores the application of
pre-trained SAM and fine-tuned SAM on bloodstain image segmentation with
diverse image backgrounds. Experiment results indicate that both pre-trained
and fine-tuned SAM perform the bloodstain image segmentation task with
satisfactory accuracy and efficiency, while fine-tuned SAM achieves an overall
2.2\% accuracy improvement than pre-trained SAM and 4.70\% acceleration in
terms of speed for image recognition. Analysis of factors that influence
bloodstain recognition is carried out. This research demonstrates the potential
application of SAM on bloodstain image segmentation, showcasing the
effectiveness of Artificial Intelligence application in criminology research.
We release all code and demos at
\url{https://github.com/Zdong104/Bloodstain_Analysis_Ai_Tool}",http://arxiv.org/pdf/2308.13979v1
2308.13978v1,cs.AI,Understanding the Usage of QUBO-based Hamiltonian Function in Combinatorial Optimization over Graphs: A Discussion Using Max Cut (MC) Problem,2023-08-27 00:57:01+00:00,"Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to
model various NP-hard combinatorial optimization problems in the form of binary
variables. The Hamiltonian function is often used to formulate QUBO problems
where it is used as the objective function in the context of optimization. In
this study, we investigate how reinforcement learning-based (RL) paradigms with
the presence of the Hamiltonian function can address combinatorial optimization
problems over graphs in QUBO formulations. We use Graph Neural Network (GNN) as
the message-passing architecture to convey the information among the nodes. We
have centered our discussion on QUBO formulated Max-Cut problem but the
intuitions can be extended to any QUBO supported canonical NP-Hard
combinatorial optimization problems. We mainly investigate three formulations,
Monty-Carlo Tree Search with GNN-based RL (MCTS-GNN), DQN with GNN-based RL,
and a generic GNN with attention-based RL (GRL). Our findings state that in the
RL-based paradigm, the Hamiltonian function-based optimization in QUBO
formulation brings model convergence and can be used as a generic reward
function. We also analyze and present the performance of our RL-based setups
through experimenting over graphs of different densities and compare them with
a simple GNN-based setup in the light of constraint violation, learning
stability and computation cost. As per one of our findings, all the
architectures provide a very comparable performance in sparse graphs as per the
number of constraint violation whreas MCTS-GNN gives the best performance. In
the similar criteria, the performance significantly starts to drop both for GRL
and simple GNN-based setups whereas MCTS-GNN and DQN shines. We also present
the corresponding mathematical formulations and in-depth discussion of the
observed characteristics during experimentations.",http://arxiv.org/pdf/2308.13978v1
2308.13976v1,cs.LG,Label Denoising through Cross-Model Agreement,2023-08-27 00:31:04+00:00,"Learning from corrupted labels is very common in real-world machine-learning
applications. Memorizing such noisy labels could affect the learning of the
model, leading to sub-optimal performances. In this work, we propose a novel
framework to learn robust machine-learning models from noisy labels. Through an
empirical study, we find that different models make relatively similar
predictions on clean examples, while the predictions on noisy examples vary
much more across different models. Motivated by this observation, we propose
\em denoising with cross-model agreement \em (DeCA) which aims to minimize the
KL-divergence between the true label distributions parameterized by two machine
learning models while maximizing the likelihood of data observation. We employ
the proposed DeCA on both the binary label scenario and the multiple label
scenario. For the binary label scenario, we select implicit feedback
recommendation as the downstream task and conduct experiments with four
state-of-the-art recommendation models on four datasets. For the multiple-label
scenario, the downstream application is image classification on two benchmark
datasets. Experimental results demonstrate that the proposed methods
significantly improve the model performance compared with normal training and
other denoising methods on both binary and multiple-label scenarios.",http://arxiv.org/pdf/2308.13976v1
2308.13970v1,cs.LG,FAM: fast adaptive meta-learning,2023-08-26 22:54:45+00:00,"In this work, we propose a fast adaptive federated meta-learning (FAM)
framework for collaboratively learning a single global model, which can then be
personalized locally on individual clients. Federated learning enables multiple
clients to collaborate to train a model without sharing data. Clients with
insufficient data or data diversity participate in federated learning to learn
a model with superior performance. Nonetheless, learning suffers when data
distributions diverge. There is a need to learn a global model that can be
adapted using client's specific information to create personalised models on
clients is required. MRI data suffers from this problem, wherein, one, due to
data acquisition challenges, local data at a site is sufficient for training an
accurate model and two, there is a restriction of data sharing due to privacy
concerns and three, there is a need for personalization of a learnt shared
global model on account of domain shift across client sites. The global model
is sparse and captures the common features in the MRI. This skeleton network is
grown on each client to train a personalised model by learning additional
client-specific parameters from local data. Experimental results show that the
personalization process at each client quickly converges using a limited number
of epochs. The personalized client models outperformed the locally trained
models, demonstrating the efficacy of the FAM mechanism. Additionally, the
sparse parameter set to be communicated during federated learning drastically
reduced communication overhead, which makes the scheme viable for networks with
limited resources.",http://arxiv.org/pdf/2308.13970v1
2308.13969v1,cs.CV,Fixating on Attention: Integrating Human Eye Tracking into Vision Transformers,2023-08-26 22:48:06+00:00,"Modern transformer-based models designed for computer vision have
outperformed humans across a spectrum of visual tasks. However, critical tasks,
such as medical image interpretation or autonomous driving, still require
reliance on human judgments. This work demonstrates how human visual input,
specifically fixations collected from an eye-tracking device, can be integrated
into transformer models to improve accuracy across multiple driving situations
and datasets. First, we establish the significance of fixation regions in
left-right driving decisions, as observed in both human subjects and a Vision
Transformer (ViT). By comparing the similarity between human fixation maps and
ViT attention weights, we reveal the dynamics of overlap across individual
heads and layers. This overlap is exploited for model pruning without
compromising accuracy. Thereafter, we incorporate information from the driving
scene with fixation data, employing a ""joint space-fixation"" (JSF) attention
setup. Lastly, we propose a ""fixation-attention intersection"" (FAX) loss to
train the ViT model to attend to the same regions that humans fixated on. We
find that the ViT performance is improved in accuracy and number of training
epochs when using JSF and FAX. These results hold significant implications for
human-guided artificial intelligence.",http://arxiv.org/pdf/2308.13969v1
2308.14654v1,cs.CL,Joint Multiple Intent Detection and Slot Filling with Supervised Contrastive Learning and Self-Distillation,2023-08-28 15:36:33+00:00,"Multiple intent detection and slot filling are two fundamental and crucial
tasks in spoken language understanding. Motivated by the fact that the two
tasks are closely related, joint models that can detect intents and extract
slots simultaneously are preferred to individual models that perform each task
independently. The accuracy of a joint model depends heavily on the ability of
the model to transfer information between the two tasks so that the result of
one task can correct the result of the other. In addition, since a joint model
has multiple outputs, how to train the model effectively is also challenging.
In this paper, we present a method for multiple intent detection and slot
filling by addressing these challenges. First, we propose a bidirectional joint
model that explicitly employs intent information to recognize slots and slot
features to detect intents. Second, we introduce a novel method for training
the proposed joint model using supervised contrastive learning and
self-distillation. Experimental results on two benchmark datasets MixATIS and
MixSNIPS show that our method outperforms state-of-the-art models in both
tasks. The results also demonstrate the contributions of both bidirectional
design and the training method to the accuracy improvement. Our source code is
available at https://github.com/anhtunguyen98/BiSLU",http://arxiv.org/pdf/2308.14654v1
2308.14641v1,cs.CL,Challenges of GPT-3-based Conversational Agents for Healthca,2023-08-28 15:12:34+00:00,"The potential to provide patients with faster information access while
allowing medical specialists to concentrate on critical tasks makes medical
domain dialog agents appealing. However, the integration of large-language
models (LLMs) into these agents presents certain limitations that may result in
serious consequences. This paper investigates the challenges and risks of using
GPT-3-based models for medical question-answering (MedQA). We perform several
evaluations contextualized in terms of standard medical principles. We provide
a procedure for manually designing patient queries to stress-test high-risk
limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to
respond adequately to these queries, generating erroneous medical information,
unsafe recommendations, and content that may be considered offensive.",http://arxiv.org/pdf/2308.14641v1
2308.14608v1,cs.LG,AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics,2023-08-28 14:23:04+00:00,"The introduction of ChatGPT and the subsequent improvement of Large Language
Models (LLMs) have prompted more and more individuals to turn to the use of
ChatBots, both for information and assistance with decision-making. However,
the information the user is after is often not formulated by these ChatBots
objectively enough to be provided with a definite, globally accepted answer.
  Controversial topics, such as ""religion"", ""gender identity"", ""freedom of
speech"", and ""equality"", among others, can be a source of conflict as partisan
or biased answers can reinforce preconceived notions or promote disinformation.
By exposing ChatGPT to such debatable questions, we aim to understand its level
of awareness and if existing models are subject to socio-political and/or
economic biases. We also aim to explore how AI-generated answers compare to
human ones. For exploring this, we use a dataset of a social media platform
created for the purpose of debating human-generated claims on polemic subjects
among users, dubbed Kialo.
  Our results show that while previous versions of ChatGPT have had important
issues with controversial topics, more recent versions of ChatGPT
(gpt-3.5-turbo) are no longer manifesting significant explicit biases in
several knowledge areas. In particular, it is well-moderated regarding economic
aspects. However, it still maintains degrees of implicit libertarian leaning
toward right-winged ideals which suggest the need for increased moderation from
the socio-political point of view. In terms of domain knowledge on
controversial topics, with the exception of the ""Philosophical"" category,
ChatGPT is performing well in keeping up with the collective human level of
knowledge. Finally, we see that sources of Bing AI have slightly more tendency
to the center when compared to human answers. All the analyses we make are
generalizable to other types of biases and domains.",http://arxiv.org/pdf/2308.14608v1
2308.14533v1,cs.CL,A Multi-Task Semantic Decomposition Framework with Task-specific Pre-training for Few-Shot NER,2023-08-28 12:46:21+00:00,"The objective of few-shot named entity recognition is to identify named
entities with limited labeled instances. Previous works have primarily focused
on optimizing the traditional token-wise classification framework, while
neglecting the exploration of information based on NER data characteristics. To
address this issue, we propose a Multi-Task Semantic Decomposition Framework
via Joint Task-specific Pre-training (MSDP) for few-shot NER. Drawing
inspiration from demonstration-based and contrastive learning, we introduce two
novel pre-training tasks: Demonstration-based Masked Language Modeling (MLM)
and Class Contrastive Discrimination. These tasks effectively incorporate
entity boundary information and enhance entity representation in Pre-trained
Language Models (PLMs). In the downstream main task, we introduce a multi-task
joint optimization framework with the semantic decomposing method, which
facilitates the model to integrate two different semantic information for
entity classification. Experimental results of two few-shot NER benchmarks
demonstrate that MSDP consistently outperforms strong baselines by a large
margin. Extensive analyses validate the effectiveness and generalization of
MSDP.",http://arxiv.org/pdf/2308.14533v1
2308.14508v1,cs.CL,"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",2023-08-28 11:53:40+00:00,"Although large language models (LLMs) demonstrate impressive performance for
many language tasks, most of them can only handle texts a few thousand tokens
long, limiting their applications on longer sequence inputs, such as books,
reports, and codebases. Recent works have proposed methods to improve LLMs'
long context capabilities by extending context windows and more sophisticated
memory mechanisms. However, comprehensive benchmarks tailored for evaluating
long context understanding are lacking. In this paper, we introduce LongBench,
the first bilingual, multi-task benchmark for long context understanding,
enabling a more rigorous evaluation of long context understanding. LongBench
comprises 21 datasets across 6 task categories in both English and Chinese,
with an average length of 6,711 words (English) and 13,386 characters
(Chinese). These tasks cover key long-text application areas including
single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,
and code completion. All datasets in LongBench are standardized into a unified
format, allowing for effortless automatic evaluation of LLMs. Upon
comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial
model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still
struggles on longer contexts. (2) Scaled position embedding and fine-tuning on
longer sequences lead to substantial improvement on long context understanding.
(3) Context compression technique such as retrieval brings improvement for
model with weak ability on long contexts, but the performance still lags behind
models that have strong long context understanding capability. The code and
datasets are available at https://github.com/THUDM/LongBench.",http://arxiv.org/pdf/2308.14508v1
2308.14484v1,cs.CL,Multimodal Detection of Social Spambots in Twitter using Transformers,2023-08-28 10:51:11+00:00,"Although not all bots are malicious, the vast majority of them are
responsible for spreading misinformation and manipulating the public opinion
about several issues, i.e., elections and many more. Therefore, the early
detection of social spambots is crucial. Although there have been proposed
methods for detecting bots in social media, there are still substantial
limitations. For instance, existing research initiatives still extract a large
number of features and train traditional machine learning algorithms or use
GloVe embeddings and train LSTMs. However, feature extraction is a tedious
procedure demanding domain expertise. Also, language models based on
transformers have been proved to be better than LSTMs. Other approaches create
large graphs and train graph neural networks requiring in this way many hours
for training and access to computational resources. To tackle these
limitations, this is the first study employing only the user description field
and images of three channels denoting the type and content of tweets posted by
the users. Firstly, we create digital DNA sequences, transform them to 3d
images, and apply pretrained models of the vision domain, including
EfficientNet, AlexNet, VGG16, etc. Next, we propose a multimodal approach,
where we use TwHIN-BERT for getting the textual representation of the user
description field and employ VGG16 for acquiring the visual representation for
the image modality. We propose three different fusion methods, namely
concatenation, gated multimodal unit, and crossmodal attention, for fusing the
different modalities and compare their performances. Extensive experiments
conducted on the Cresci '17 dataset demonstrate valuable advantages of our
introduced approaches over state-of-the-art ones reaching Accuracy up to
99.98%.",http://arxiv.org/pdf/2308.14484v1
2308.14482v1,cs.CL,An Empirical Study of Consistency Regularization for End-to-End Speech-to-Text Translation,2023-08-28 10:44:18+00:00,"Consistency regularization methods, such as R-Drop (Liang et al., 2021) and
CrossConST (Gao et al., 2023), have achieved impressive supervised and
zero-shot performance in the neural machine translation (NMT) field. Can we
also boost end-to-end (E2E) speech-to-text translation (ST) by leveraging
consistency regularization? In this paper, we conduct empirical studies on
intra-modal and cross-modal consistency and propose two training strategies,
SimRegCR and SimZeroCR, for E2E ST in regular and zero-shot scenarios.
Experiments on the MuST-C benchmark show that our approaches achieve
state-of-the-art (SOTA) performance in most translation directions. The
analyses prove that regularization brought by the intra-modal consistency,
instead of modality gap, is crucial for the regular E2E ST, and the cross-modal
consistency could close the modality gap and boost the zero-shot E2E ST
performance.",http://arxiv.org/pdf/2308.14482v1
2308.14436v1,cs.CL,Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA,2023-08-28 09:22:02+00:00,"Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with factual information such as entities and relations in KBs.
However, traditional Pre-trained Language Models (PLMs) are directly
pre-trained on large-scale natural language corpus, which poses challenges for
them in understanding and representing complex subgraphs in structured KBs. To
bridge the gap between texts and structured KBs, we propose a Structured
Knowledge-aware Pre-training method (SKP). In the pre-training stage, we
introduce two novel structured knowledge-aware tasks, guiding the model to
effectively learn the implicit relationship and better representations of
complex subgraphs. In downstream KBQA task, we further design an efficient
linearization strategy and an interval attention mechanism, which assist the
model to better encode complex subgraphs and shield the interference of
irrelevant subgraphs during reasoning respectively. Detailed experiments and
analyses on WebQSP verify the effectiveness of SKP, especially the significant
improvement in subgraph retrieval (+4.08% H@10).",http://arxiv.org/pdf/2308.14436v1
2308.14423v1,cs.CL,GADePo: Graph-Assisted Declarative Pooling Transformers for Document-Level Relation Extraction,2023-08-28 09:04:03+00:00,"Document-level relation extraction aims to identify relationships between
entities within a document. Current methods rely on text-based encoders and
employ various hand-coded pooling heuristics to aggregate information from
entity mentions and associated contexts. In this paper, we replace these rigid
pooling functions with explicit graph relations by leveraging the intrinsic
graph processing capabilities of the Transformer model. We propose a joint
text-graph Transformer model, and a graph-assisted declarative pooling (GADePo)
specification of the input which provides explicit and high-level instructions
for information aggregation. This allows the pooling process to be guided by
domain-specific knowledge or desired outcomes but still learned by the
Transformer, leading to more flexible and customizable pooling strategies. We
extensively evaluate our method across diverse datasets and models, and show
that our approach yields promising results that are comparable to those
achieved by the hand-coded pooling functions.",http://arxiv.org/pdf/2308.14423v1
2308.14391v1,cs.CV,FIRE: Food Image to REcipe generation,2023-08-28 08:14:20+00:00,"Food computing has emerged as a prominent multidisciplinary field of research
in recent years. An ambitious goal of food computing is to develop end-to-end
intelligent systems capable of autonomously producing recipe information for a
food image. Current image-to-recipe methods are retrieval-based and their
success depends heavily on the dataset size and diversity, as well as the
quality of learned embeddings. Meanwhile, the emergence of powerful
attention-based vision and language models presents a promising avenue for
accurate and generalizable recipe generation, which has yet to be extensively
explored. This paper proposes FIRE, a novel multimodal methodology tailored to
recipe generation in the food computing domain, which generates the food title,
ingredients, and cooking instructions based on input food images. FIRE
leverages the BLIP model to generate titles, utilizes a Vision Transformer with
a decoder for ingredient extraction, and employs the T5 model to generate
recipes incorporating titles and ingredients as inputs. We showcase two
practical applications that can benefit from integrating FIRE with large
language model prompting: recipe customization to fit recipes to user
preferences and recipe-to-code transformation to enable automated cooking
processes. Our experimental findings validate the efficacy of our proposed
approach, underscoring its potential for future advancements and widespread
adoption in food computing.",http://arxiv.org/pdf/2308.14391v1
2308.14353v1,cs.CL,"ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models",2023-08-28 06:56:44+00:00,"The unprecedented performance of large language models (LLMs) requires
comprehensive and accurate evaluation. We argue that for LLMs evaluation,
benchmarks need to be comprehensive and systematic. To this end, we propose the
ZhuJiu benchmark, which has the following strengths: (1) Multi-dimensional
ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions
covering 51 tasks. Especially, we also propose a new benchmark that focuses on
knowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration:
We use 3 different yet complementary evaluation methods to comprehensively
evaluate LLMs, which can ensure the authority and accuracy of the evaluation
results. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering
benchmark that fully assesses LLMs in Chinese, while also providing equally
robust evaluation abilities in English. (4) Avoiding potential data leakage: To
avoid data leakage, we construct evaluation data specifically for 37 tasks. We
evaluate 10 current mainstream LLMs and conduct an in-depth discussion and
analysis of their results. The ZhuJiu benchmark and open-participation
leaderboard are publicly released at http://www.zhujiu-benchmark.com/ and we
also provide a demo video at https://youtu.be/qypkJ89L1Ic.",http://arxiv.org/pdf/2308.14353v1
2308.14272v1,cs.CL,Goodhart's Law Applies to NLP's Explanation Benchmarks,2023-08-28 03:03:03+00:00,"Despite the rising popularity of saliency-based explanations, the research
community remains at an impasse, facing doubts concerning their purpose,
efficacy, and tendency to contradict each other. Seeking to unite the
community's efforts around common goals, several recent works have proposed
evaluation metrics. In this paper, we critically examine two sets of metrics:
the ERASER metrics (comprehensiveness and sufficiency) and the EVAL-X metrics,
focusing our inquiry on natural language processing. First, we show that we can
inflate a model's comprehensiveness and sufficiency scores dramatically without
altering its predictions or explanations on in-distribution test inputs. Our
strategy exploits the tendency for extracted explanations and their complements
to be ""out-of-support"" relative to each other and in-distribution inputs. Next,
we demonstrate that the EVAL-X metrics can be inflated arbitrarily by a simple
method that encodes the label, even though EVAL-X is precisely motivated to
address such exploits. Our results raise doubts about the ability of current
metrics to guide explainability research, underscoring the need for a broader
reassessment of what precisely these metrics are intended to capture.",http://arxiv.org/pdf/2308.14272v1
2308.14182v1,cs.CL,Generative AI for Business Strategy: Using Foundation Models to Create Business Strategy Tools,2023-08-27 19:03:12+00:00,"Generative models (foundation models) such as LLMs (large language models)
are having a large impact on multiple fields. In this work, we propose the use
of such models for business decision making. In particular, we combine
unstructured textual data sources (e.g., news data) with multiple foundation
models (namely, GPT4, transformer-based Named Entity Recognition (NER) models
and Entailment-based Zero-shot Classifiers (ZSC)) to derive IT (information
technology) artifacts in the form of a (sequence of) signed business networks.
We posit that such artifacts can inform business stakeholders about the state
of the market and their own positioning as well as provide quantitative
insights into improving their future outlook.",http://arxiv.org/pdf/2308.14182v1
2308.14115v1,cs.CL,Situated Natural Language Explanations,2023-08-27 14:14:28+00:00,"Natural language is among the most accessible tools for explaining decisions
to humans, and large pretrained language models (PLMs) have demonstrated
impressive abilities to generate coherent natural language explanations (NLE).
The existing NLE research perspectives do not take the audience into account.
An NLE can have high textual quality, but it might not accommodate audiences'
needs and preference. To address this limitation, we propose an alternative
perspective, situated NLE, including a situated generation framework and a
situated evaluation framework. On the generation side, we propose simple prompt
engineering methods that adapt the NLEs to situations. In human studies, the
annotators preferred the situated NLEs. On the evaluation side, we set up
automated evaluation scores in lexical, semantic, and pragmatic categories. The
scores can be used to select the most suitable prompts to generate NLEs.
Situated NLE provides a perspective to conduct further research on automatic
NLE generations.",http://arxiv.org/pdf/2308.14115v1
2308.14077v1,cs.FL,An Analysis of On-the-fly Determinization of Finite-state Automata,2023-08-27 11:51:27+00:00,"In this paper we establish an abstraction of on-the-fly determinization of
finite-state automata using transition monoids and demonstrate how it can be
applied to bound the asymptotics. We present algebraic and combinatorial
properties that are sufficient for a polynomial state complexity of the
deterministic automaton constructed on-the-fly. A special case of our findings
is that automata with many non-deterministic transitions almost always admit a
determinization of polynomial complexity. Furthermore, we extend our ideas to
weighted finite-state automata.",http://arxiv.org/pdf/2308.14077v1
2308.13961v1,cs.CL,"Translate Meanings, Not Just Words: IdiomKB's Role in Optimizing Idiomatic Translation with Language Models",2023-08-26 21:38:31+00:00,"To translate well, machine translation (MT) systems and general-purposed
language models (LMs) need a deep understanding of both source and target
languages and cultures. Therefore, idioms, with their non-compositional nature,
pose particular challenges for Transformer-based systems, as literal
translations often miss the intended meaning. Traditional methods, which
replace idioms using existing knowledge bases (KBs), often lack scale and
context awareness. Addressing these challenges, our approach prioritizes
context awareness and scalability, allowing for offline storage of idioms in a
manageable KB size. This ensures efficient serving with smaller models and
provides a more comprehensive understanding of idiomatic expressions. We
introduce a multilingual idiom KB (IdiomKB) developed using large LMs to
address this. This KB facilitates better translation by smaller models, such as
BLOOMZ (7.1B), Alpaca (7B), and InstructGPT (6.7B), by retrieving idioms'
figurative meanings. We present a novel, GPT-4-powered metric for human-aligned
evaluation, demonstrating that IdiomKB considerably boosts model performance.
Human evaluations further validate our KB's quality.",http://arxiv.org/pdf/2308.13961v1
2308.13958v1,cs.CL,"Improving Knowledge Distillation for BERT Models: Loss Functions, Mapping Methods, and Weight Tuning",2023-08-26 20:59:21+00:00,"The use of large transformer-based models such as BERT, GPT, and T5 has led
to significant advancements in natural language processing. However, these
models are computationally expensive, necessitating model compression
techniques that reduce their size and complexity while maintaining accuracy.
This project investigates and applies knowledge distillation for BERT model
compression, specifically focusing on the TinyBERT student model. We explore
various techniques to improve knowledge distillation, including experimentation
with loss functions, transformer layer mapping methods, and tuning the weights
of attention and representation loss and evaluate our proposed techniques on a
selection of downstream tasks from the GLUE benchmark. The goal of this work is
to improve the efficiency and effectiveness of knowledge distillation, enabling
the development of more efficient and accurate models for a range of natural
language processing tasks.",http://arxiv.org/pdf/2308.13958v1
2308.13916v1,cs.CL,Exploring Large Language Models for Knowledge Graph Completion,2023-08-26 16:51:17+00:00,"Knowledge graphs play a vital role in numerous artificial intelligence tasks,
yet they frequently face the issue of incompleteness. In this study, we explore
utilizing Large Language Models (LLM) for knowledge graph completion. We
consider triples in knowledge graphs as text sequences and introduce an
innovative framework called Knowledge Graph LLM (KG-LLM) to model these
triples. Our technique employs entity and relation descriptions of a triple as
prompts and utilizes the response for predictions. Experiments on various
benchmark knowledge graphs demonstrate that our method attains state-of-the-art
performance in tasks such as triple classification and relation prediction. We
also find that fine-tuning relatively smaller models (e.g., LLaMA-7B,
ChatGLM-6B) outperforms recent ChatGPT and GPT-4.",http://arxiv.org/pdf/2308.13916v1
2308.13911v1,cs.AI,A Wide Evaluation of ChatGPT on Affective Computing Tasks,2023-08-26 16:10:30+00:00,"With the rise of foundation models, a new artificial intelligence paradigm
has emerged, by simply using general purpose foundation models with prompting
to solve problems instead of training a separate machine learning model for
each problem. Such models have been shown to have emergent properties of
solving problems that they were not initially trained on. The studies for the
effectiveness of such models are still quite limited. In this work, we widely
study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13
affective computing problems, namely aspect extraction, aspect polarity
classification, opinion extraction, sentiment analysis, sentiment intensity
ranking, emotions intensity ranking, suicide tendency detection, toxicity
detection, well-being assessment, engagement measurement, personality
assessment, sarcasm detection, and subjectivity detection. We introduce a
framework to evaluate the ChatGPT models on regression-based problems, such as
intensity ranking problems, by modelling them as pairwise ranking
classification. We compare ChatGPT against more traditional NLP methods, such
as end-to-end recurrent neural networks and transformers. The results
demonstrate the emergent abilities of the ChatGPT models on a wide range of
affective computing problems, where GPT-3.5 and especially GPT-4 have shown
strong performance on many problems, particularly the ones related to
sentiment, emotions, or toxicity. The ChatGPT models fell short for problems
with implicit signals, such as engagement measurement and subjectivity
detection.",http://arxiv.org/pdf/2308.13911v1
2308.13904v1,cs.CL,LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors,2023-08-26 15:21:47+00:00,"Prompt-tuning has emerged as an attractive paradigm for deploying large-scale
language models due to its strong downstream task performance and efficient
multitask serving ability. Despite its wide adoption, we empirically show that
prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside
in the pretrained models and can affect arbitrary downstream tasks. The
state-of-the-art backdoor detection approaches cannot defend against
task-agnostic backdoors since they hardly converge in reversing the backdoor
triggers. To address this issue, we propose LMSanitator, a novel approach for
detecting and removing task-agnostic backdoors on Transformer models. Instead
of directly inversing the triggers, LMSanitator aims to inverse the predefined
attack vectors (pretrained models' output when the input is embedded with
triggers) of the task-agnostic backdoors, which achieves much better
convergence performance and backdoor detection accuracy. LMSanitator further
leverages prompt-tuning's property of freezing the pretrained model to perform
accurate and fast output monitoring and input purging during the inference
phase. Extensive experiments on multiple language models and NLP tasks
illustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves
92.8% backdoor detection accuracy on 960 models and decreases the attack
success rate to less than 1% in most scenarios.",http://arxiv.org/pdf/2308.13904v1
2308.13844v1,cs.CL,Solving Math Word Problem with Problem Type Classification,2023-08-26 10:35:16+00:00,"Math word problems (MWPs) require analyzing text descriptions and generating
mathematical equations to derive solutions. Existing works focus on solving
MWPs with two types of solvers: tree-based solver and large language model
(LLM) solver. However, these approaches always solve MWPs by a single solver,
which will bring the following problems: (1) Single type of solver is hard to
solve all types of MWPs well. (2) A single solver will result in poor
performance due to over-fitting. To address these challenges, this paper
utilizes multiple ensemble approaches to improve MWP-solving ability. Firstly,
We propose a problem type classifier that combines the strengths of the
tree-based solver and the LLM solver. This ensemble approach leverages their
respective advantages and broadens the range of MWPs that can be solved.
Furthermore, we also apply ensemble techniques to both tree-based solver and
LLM solver to improve their performance. For the tree-based solver, we propose
an ensemble learning framework based on ten-fold cross-validation and voting
mechanism. In the LLM solver, we adopt self-consistency (SC) method to improve
answer selection. Experimental results demonstrate the effectiveness of these
ensemble approaches in enhancing MWP-solving ability. The comprehensive
evaluation showcases improved performance, validating the advantages of our
proposed approach. Our code is available at this url:
https://github.com/zhouzihao501/NLPCC2023-Shared-Task3-ChineseMWP.",http://arxiv.org/pdf/2308.13844v1
2308.13782v1,cs.CL,Planning with Logical Graph-based Language Model for Instruction Generation,2023-08-26 06:28:14+00:00,"Despite the superior performance of large language models to generate natural
language texts, it is hard to generate texts with correct logic according to a
given task, due to the difficulties for neural models to capture implied rules
from free-form texts. In this paper, we propose a novel graph-based language
model, Logical-GLM, to infuse logic into language models for more valid text
generation and interpretability. Specifically, we first capture information
from natural language instructions and construct logical bayes graphs that
generally describe domains. Next, we generate logical skeletons to guide
language model training, infusing domain knowledge into language models.
Finally, we alternately optimize the searching policy of graphs and language
models until convergence. The experimental results show that Logical-GLM is
both effective and efficient compared with traditional language models, despite
using smaller-scale training data and fewer parameters. Our approach can
generate instructional texts with more correct logic owing to the internalized
domain knowledge. Moreover, the usage of logical graphs reflects the inner
mechanism of the language models, which improves the interpretability of
black-box models.",http://arxiv.org/pdf/2308.13782v1
2308.13775v1,cs.SE,EditSum: A Retrieve-and-Edit Framework for Source Code Summarization,2023-08-26 05:48:57+00:00,"Existing studies show that code summaries help developers understand and
maintain source code. Unfortunately, these summaries are often missing or
outdated in software projects. Code summarization aims to generate natural
language descriptions automatically for source code. Code summaries are highly
structured and have repetitive patterns. Besides the patternized words, a code
summary also contains important keywords, which are the key to reflecting the
functionality of the code. However, the state-of-the-art approaches perform
poorly on predicting the keywords, which leads to the generated summaries
suffering a loss in informativeness. To alleviate this problem, this paper
proposes a novel retrieve-and-edit approach named EditSum for code
summarization. Specifically, EditSum first retrieves a similar code snippet
from a pre-defined corpus and treats its summary as a prototype summary to
learn the pattern. Then, EditSum edits the prototype automatically to combine
the pattern in the prototype with the semantic information of input code. Our
motivation is that the retrieved prototype provides a good start-point for
post-generation because the summaries of similar code snippets often have the
same pattern. The post-editing process further reuses the patternized words in
the prototype and generates keywords based on the semantic information of input
code. We conduct experiments on a large-scale Java corpus and experimental
results demonstrate that EditSum outperforms the state-of-the-art approaches by
a substantial margin. The human evaluation also proves the summaries generated
by EditSum are more informative and useful. We also verify that EditSum
performs well on predicting the patternized words and keywords.",http://arxiv.org/pdf/2308.13775v1
2308.13768v1,cs.CL,Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content,2023-08-26 05:20:58+00:00,"In this paper, we tackle the emerging challenge of unintended harmful content
generation in Large Language Models (LLMs) with a novel dual-stage optimisation
technique using adversarial fine-tuning. Our two-pronged approach employs an
adversarial model, fine-tuned to generate potentially harmful prompts, and a
judge model, iteratively optimised to discern these prompts. In this
adversarial cycle, the two models seek to outperform each other in the
prompting phase, generating a dataset of rich examples which are then used for
fine-tuning. This iterative application of prompting and fine-tuning allows
continuous refinement and improved performance. The performance of our approach
is evaluated through classification accuracy on a dataset consisting of
problematic prompts not detected by GPT-4, as well as a selection of
contentious but unproblematic prompts. We show considerable increase in
classification accuracy of the judge model on this challenging dataset as it
undergoes the optimisation process. Furthermore, we show that a rudimentary
model \texttt{ada} can achieve 13\% higher accuracy on the hold-out test set
than GPT-4 after only a few rounds of this process, and that this fine-tuning
improves performance in parallel tasks such as toxic comment identification.",http://arxiv.org/pdf/2308.13768v1
2308.13760v1,cs.AI,How Can Context Help? Exploring Joint Retrieval of Passage and Personalized Context,2023-08-26 04:49:46+00:00,"The integration of external personalized context information into
document-grounded conversational systems has significant potential business
value, but has not been well-studied. Motivated by the concept of personalized
context-aware document-grounded conversational systems, we introduce the task
of context-aware passage retrieval. We also construct a dataset specifically
curated for this purpose. We describe multiple baseline systems to address this
task, and propose a novel approach, Personalized Context-Aware Search (PCAS),
that effectively harnesses contextual information during passage retrieval.
Experimental evaluations conducted on multiple popular dense retrieval systems
demonstrate that our proposed approach not only outperforms the baselines in
retrieving the most relevant passage but also excels at identifying the
pertinent context among all the available contexts. We envision that our
contributions will serve as a catalyst for inspiring future research endeavors
in this promising direction.",http://arxiv.org/pdf/2308.13760v1
2308.13754v1,cs.SE,ZC3: Zero-Shot Cross-Language Code Clone Detection,2023-08-26 03:48:10+00:00,"Developers introduce code clones to improve programming productivity. Many
existing studies have achieved impressive performance in monolingual code clone
detection. However, during software development, more and more developers write
semantically equivalent programs with different languages to support different
platforms and help developers translate projects from one language to another.
Considering that collecting cross-language parallel data, especially for
low-resource languages, is expensive and time-consuming, how designing an
effective cross-language model that does not rely on any parallel data is a
significant problem. In this paper, we propose a novel method named ZC3 for
Zero-shot Cross-language Code Clone detection. ZC3 designs the contrastive
snippet prediction to form an isomorphic representation space among different
programming languages. Based on this, ZC3 exploits domain-aware learning and
cycle consistency learning to further constrain the model to generate
representations that are aligned among different languages meanwhile are
diacritical for different types of clones. To evaluate our approach, we conduct
extensive experiments on four representative cross-language clone detection
datasets. Experimental results show that ZC3 outperforms the state-of-the-art
baselines by 67.12%, 51.39%, 14.85%, and 53.01% on the MAP score, respectively.
We further investigate the representational distribution of different languages
and discuss the effectiveness of our method.",http://arxiv.org/pdf/2308.13754v1
2308.13738v1,math.HO,On Philomatics and Psychomatics for Combining Philosophy and Psychology with Mathematics,2023-08-26 02:52:42+00:00,"We propose the concepts of philomatics and psychomatics as hybrid
combinations of philosophy and psychology with mathematics. We explain four
motivations for this combination which are fulfilling the desire of analytical
philosophy, proposing science of philosophy, justifying mathematical algorithms
by philosophy, and abstraction in both philosophy and mathematics. We enumerate
various examples for philomatics and psychomatics, some of which are explained
in more depth. The first example is the analysis of relation between the
context principle, semantic holism, and the usage theory of meaning with the
attention mechanism in mathematics. The other example is on the relations of
Plato's theory of forms in philosophy with the holographic principle in string
theory, object-oriented programming, and machine learning. Finally, the
relation between Wittgenstein's family resemblance and clustering in
mathematics is explained. This paper opens the door of research for combining
philosophy and psychology with mathematics.",http://arxiv.org/pdf/2308.13738v1
2308.13715v1,cs.CL,A Computational Evaluation Framework for Singable Lyric Translation,2023-08-26 00:27:08+00:00,"Lyric translation plays a pivotal role in amplifying the global resonance of
music, bridging cultural divides, and fostering universal connections.
Translating lyrics, unlike conventional translation tasks, requires a delicate
balance between singability and semantics. In this paper, we present a
computational framework for the quantitative evaluation of singable lyric
translation, which seamlessly integrates musical, linguistic, and cultural
dimensions of lyrics. Our comprehensive framework consists of four metrics that
measure syllable count distance, phoneme repetition similarity, musical
structure distance, and semantic similarity. To substantiate the efficacy of
our framework, we collected a singable lyrics dataset, which precisely aligns
English, Japanese, and Korean lyrics on a line-by-line and section-by-section
basis, and conducted a comparative analysis between singable and non-singable
lyrics. Our multidisciplinary approach provides insights into the key
components that underlie the art of lyric translation and establishes a solid
groundwork for the future of computational lyric translation assessment.",http://arxiv.org/pdf/2308.13715v1
2308.13710v1,cs.CL,WellXplain: Wellness Concept Extraction and Classification in Reddit Posts for Mental Health Analysis,2023-08-25 23:50:05+00:00,"During the current mental health crisis, the importance of identifying
potential indicators of mental issues from social media content has surged.
Overlooking the multifaceted nature of mental and social well-being can have
detrimental effects on one's mental state. In traditional therapy sessions,
professionals manually pinpoint the origins and outcomes of underlying mental
challenges, a process both detailed and time-intensive. We introduce an
approach to this intricate mental health analysis by framing the identification
of wellness dimensions in Reddit content as a wellness concept extraction and
categorization challenge. We've curated a unique dataset named WELLXPLAIN,
comprising 3,092 entries and totaling 72,813 words. Drawing from Halbert L.
Dunn's well-regarded wellness theory, our team formulated an annotation
framework along with guidelines. This dataset also includes human-marked
textual segments, offering clear reasoning for decisions made in the wellness
concept categorization process. Our aim in publishing this dataset and
analyzing initial benchmarks is to spearhead the creation of advanced language
models tailored for healthcare-focused concept extraction and categorization.",http://arxiv.org/pdf/2308.13710v1
2308.13696v1,cs.CL,On the Depth between Beam Search and Exhaustive Search for Text Generation,2023-08-25 22:57:53+00:00,"Beam search and exhaustive search are two extreme ends of text decoding
algorithms with respect to the search depth. Beam search is limited in both
search width and depth, whereas exhaustive search is a global search that has
no such limitations. Surprisingly, beam search is not only computationally
cheaper but also performs better than exhaustive search despite its higher
search error. Plenty of research has investigated a range of beam widths, from
small to large, and reported that a beam width that is neither too large nor
too small is desirable. However, in terms of search depth, only the two extreme
ends, beam search and exhaustive search are studied intensively. In this paper,
we examine a range of search depths between the two extremes to discover the
desirable search depth. To this end, we introduce Lookahead Beam Search (LBS),
a multi-step lookahead search that optimizes the objective considering a fixed
number of future steps. Beam search and exhaustive search are special cases of
LBS where the lookahead depth is set to $0$ and $\infty$, respectively. We
empirically evaluate the performance of LBS and find that it outperforms beam
search overall on machine translation tasks. The result suggests there is room
for improvement in beam search by searching deeper. Inspired by the analysis,
we propose Lookbehind Heuristic Beam Search, a computationally feasible search
algorithm that heuristically simulates LBS with 1-step lookahead. The empirical
results show that the proposed method outperforms vanilla beam search on
machine translation and text summarization tasks.",http://arxiv.org/pdf/2308.13696v1
2308.13687v1,cond-mat.mtrl-sci,1.5 million materials narratives generated by chatbots,2023-08-25 22:00:53+00:00,"The advent of artificial intelligence (AI) has enabled a comprehensive
exploration of materials for various applications. However, AI models often
prioritize frequently encountered materials in the scientific literature,
limiting the selection of suitable candidates based on inherent physical and
chemical properties. To address this imbalance, we have generated a dataset of
1,494,017 natural language-material paragraphs based on combined OQMD,
Materials Project, JARVIS, COD and AFLOW2 databases, which are dominated by ab
initio calculations and tend to be much more evenly distributed on the periodic
table. The generated text narratives were then polled and scored by both human
experts and ChatGPT-4, based on three rubrics: technical accuracy, language and
structure, and relevance and depth of content, showing similar scores but with
human-scored depth of content being the most lagging. The merger of
multi-modality data sources and large language model (LLM) holds immense
potential for AI frameworks to help the exploration and discovery of
solid-state materials for specific applications.",http://arxiv.org/pdf/2308.13687v1
2308.13676v1,cs.CL,Rethinking Language Models as Symbolic Knowledge Graphs,2023-08-25 21:25:08+00:00,"Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric
applications such as search, question answering and recommendation. As
contemporary language models (LMs) trained on extensive textual data have
gained prominence, researchers have extensively explored whether the parametric
knowledge within these models can match up to that present in knowledge graphs.
Various methodologies have indicated that enhancing the size of the model or
the volume of training data enhances its capacity to retrieve symbolic
knowledge, often with minimal or no human supervision. Despite these
advancements, there is a void in comprehensively evaluating whether LMs can
encompass the intricate topological and semantic attributes of KGs, attributes
crucial for reasoning processes. In this work, we provide an exhaustive
evaluation of language models of varying sizes and capabilities. We construct
nine qualitative benchmarks that encompass a spectrum of attributes including
symmetry, asymmetry, hierarchy, bidirectionality, compositionality, paths,
entity-centricity, bias and ambiguity. Additionally, we propose novel
evaluation metrics tailored for each of these attributes. Our extensive
evaluation of various LMs shows that while these models exhibit considerable
potential in recalling factual information, their ability to capture intricate
topological and semantic traits of KGs remains significantly constrained. We
note that our proposed evaluation metrics are more reliable in evaluating these
abilities than the existing metrics. Lastly, some of our benchmarks challenge
the common notion that larger LMs (e.g., GPT-4) universally outshine their
smaller counterparts (e.g., BERT).",http://arxiv.org/pdf/2308.13676v1
2308.13646v1,cs.LG,GRASP: A Rehearsal Policy for Efficient Online Continual Learning,2023-08-25 19:34:21+00:00,"Continual learning (CL) in deep neural networks (DNNs) involves incrementally
accumulating knowledge in a DNN from a growing data stream. A major challenge
in CL is that non-stationary data streams cause catastrophic forgetting of
previously learned abilities. Rehearsal is a popular and effective way to
mitigate this problem, which is storing past observations in a buffer and
mixing them with new observations during learning. This leads to a question:
Which stored samples should be selected for rehearsal? Choosing samples that
are best for learning, rather than simply selecting them at random, could lead
to significantly faster learning. For class incremental learning, prior work
has shown that a simple class balanced random selection policy outperforms more
sophisticated methods. Here, we revisit this question by exploring a new sample
selection policy called GRASP. GRASP selects the most prototypical (class
representative) samples first and then gradually selects less prototypical
(harder) examples to update the DNN. GRASP has little additional compute or
memory overhead compared to uniform selection, enabling it to scale to large
datasets. We evaluate GRASP and other policies by conducting CL experiments on
the large-scale ImageNet-1K and Places-LT image classification datasets. GRASP
outperforms all other rehearsal policies. Beyond vision, we also demonstrate
that GRASP is effective for CL on five text classification datasets.",http://arxiv.org/pdf/2308.13646v1
2308.13517v1,cs.CL,ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection,2023-08-25 17:51:23+00:00,"Open intent detection, a crucial aspect of natural language understanding,
involves the identification of previously unseen intents in user-generated
text. Despite the progress made in this field, challenges persist in handling
new combinations of language components, which is essential for compositional
generalization. In this paper, we present a case study exploring the use of
ChatGPT as a data augmentation technique to enhance compositional
generalization in open intent detection tasks. We begin by discussing the
limitations of existing benchmarks in evaluating this problem, highlighting the
need for constructing datasets for addressing compositional generalization in
open intent detection tasks. By incorporating synthetic data generated by
ChatGPT into the training process, we demonstrate that our approach can
effectively improve model performance. Rigorous evaluation of multiple
benchmarks reveals that our method outperforms existing techniques and
significantly enhances open intent detection capabilities. Our findings
underscore the potential of large language models like ChatGPT for data
augmentation in natural language understanding tasks.",http://arxiv.org/pdf/2308.13517v1
2308.13506v2,cs.CL,Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level,2023-08-25 17:31:46+00:00,"As research on machine translation moves to translating text beyond the
sentence level, it remains unclear how effective automatic evaluation metrics
are at scoring longer translations. In this work, we first propose a method for
creating paragraph-level data for training and meta-evaluating metrics from
existing sentence-level data. Then, we use these new datasets to benchmark
existing sentence-level metrics as well as train learned metrics at the
paragraph level. Interestingly, our experimental results demonstrate that using
sentence-level metrics to score entire paragraphs is equally as effective as
using a metric designed to work at the paragraph level. We speculate this
result can be attributed to properties of the task of reference-based
evaluation as well as limitations of our datasets with respect to capturing all
types of phenomena that occur in paragraph-level translations.",http://arxiv.org/pdf/2308.13506v2
2308.13590v1,cs.IR,LSTM-based QoE Evaluation for Web Microservices' Reputation Scoring,2023-08-25 17:23:12+00:00,"Sentiment analysis is the task of mining the authors' opinions about specific
entities. It allows organizations to monitor different services in real time
and act accordingly. Reputation is what is generally said or believed about
people or things. Informally, reputation combines the measure of reliability
derived from feedback, reviews, and ratings gathered from users, which reflect
their quality of experience (QoE) and can either increase or harm the
reputation of the provided services. In this study, we propose to perform
sentiment analysis on web microservices reviews to exploit the provided
information to assess and score the microservices' reputation. Our proposed
approach uses the Long Short-Term Memory (LSTM) model to perform sentiment
analysis and the Net Brand Reputation (NBR) algorithm to assess reputation
scores for microservices. This approach is tested on a set of more than 10,000
reviews related to 15 Amazon Web microservices, and the experimental results
have shown that our approach is more accurate than existing approaches, with an
accuracy and precision of 93% obtained after applying an oversampling strategy
and a resulting reputation score of the considered microservices community of
89%.",http://arxiv.org/pdf/2308.13590v1
2308.13497v1,cs.CL,Ngambay-French Neural Machine Translation (sba-Fr),2023-08-25 17:13:20+00:00,"In Africa, and the world at large, there is an increasing focus on developing
Neural Machine Translation (NMT) systems to overcome language barriers. NMT for
Low-resource language is particularly compelling as it involves learning with
limited labelled data. However, obtaining a well-aligned parallel corpus for
low-resource languages can be challenging. The disparity between the
technological advancement of a few global languages and the lack of research on
NMT for local languages in Chad is striking. End-to-end NMT trials on
low-resource Chad languages have not been attempted. Additionally, there is a
dearth of online and well-structured data gathering for research in Natural
Language Processing, unlike some African languages. However, a guided approach
for data gathering can produce bitext data for many Chadian language
translation pairs with well-known languages that have ample data. In this
project, we created the first sba-Fr Dataset, which is a corpus of
Ngambay-to-French translations, and fine-tuned three pre-trained models using
this dataset. Our experiments show that the M2M100 model outperforms other
models with high BLEU scores on both original and original+synthetic data. The
publicly available bitext dataset can be used for research purposes.",http://arxiv.org/pdf/2308.13497v1
2308.13479v1,cs.CL,Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages,2023-08-25 16:35:06+00:00,"Large language models (LLMs) are increasingly capable and prevalent, and can
be used to produce creative content. The quality of content is influenced by
the prompt used, with more specific prompts that incorporate examples generally
producing better results. On from this, it could be seen that using
instructions written for crowdsourcing tasks (that are specific and include
examples to guide workers) could prove effective LLM prompts. To explore this,
we used a previous crowdsourcing pipeline that gave examples to people to help
them generate a collectively diverse corpus of motivational messages. We then
used this same pipeline to generate messages using GPT-4, and compared the
collective diversity of messages from: (1) crowd-writers, (2) GPT-4 using the
pipeline, and (3 & 4) two baseline GPT-4 prompts. We found that the LLM prompts
using the crowdsourcing pipeline caused GPT-4 to produce more diverse messages
than the two baseline prompts. We also discuss implications from messages
generated by both human writers and LLMs.",http://arxiv.org/pdf/2308.13479v1
2308.13467v1,cs.CL,Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models,2023-08-25 16:11:08+00:00,"The Natural Language Processing(NLP) community has been using crowd sourcing
techniques to create benchmark datasets such as General Language Understanding
and Evaluation(GLUE) for training modern Language Models such as BERT. GLUE
tasks measure the reliability scores using inter annotator metrics i.e. Cohens
Kappa. However, the reliability aspect of LMs has often been overlooked. To
counter this problem, we explore a knowledge-guided LM ensembling approach that
leverages reinforcement learning to integrate knowledge from ConceptNet and
Wikipedia as knowledge graph embeddings. This approach mimics human annotators
resorting to external knowledge to compensate for information deficits in the
datasets. Across nine GLUE datasets, our research shows that ensembling
strengthens reliability and accuracy scores, outperforming state of the art.",http://arxiv.org/pdf/2308.13467v1
2308.13458v1,cs.CL,ARTIST: ARTificial Intelligence for Simplified Text,2023-08-25 16:06:06+00:00,"Complex text is a major barrier for many citizens when accessing public
information and knowledge. While often done manually, Text Simplification is a
key Natural Language Processing task that aims for reducing the linguistic
complexity of a text while preserving the original meaning. Recent advances in
Generative Artificial Intelligence (AI) have enabled automatic text
simplification both on the lexical and syntactical levels. However, as
applications often focus on English, little is understood about the
effectiveness of Generative AI techniques on low-resource languages such as
Dutch. For this reason, we carry out empirical studies to understand the
benefits and limitations of applying generative technologies for text
simplification and provide the following outcomes: 1) the design and
implementation for a configurable text simplification pipeline that
orchestrates state-of-the-art generative text simplification models, domain and
reader adaptation, and visualisation modules; 2) insights and lessons learned,
showing the strengths of automatic text simplification while exposing the
challenges in handling cultural and commonsense knowledge. These outcomes
represent a first step in the exploration of Dutch text simplification and shed
light on future endeavours both for research and practice.",http://arxiv.org/pdf/2308.13458v1
2308.13449v1,cs.CL,The Poison of Alignment,2023-08-25 15:51:15+00:00,"From the perspective of content safety issues, alignment has shown to limit
large language models' (LLMs) harmful content generation. This intentional
method of reinforcing models to not respond to certain user inputs seem to be
present in many modern open-source instruction tuning datasets such as
OpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned
model's performance affected by the presence of alignment in supervised
fine-tuning dataset. To be specific, we noticed that alignment acts as if it is
poisoning the instruction dataset. Experimentally, we demonstrate that aligned
answers significantly worsen the performance of the resulting fine-tuned
model's on various reasoning benchmarks such as Big Bench (BBH), Massive
Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning
Over Paragraphs (DROP), performing worse than the counterpart tuned without
alignment by 4-33%.",http://arxiv.org/pdf/2308.13449v1
2308.13399v1,cs.CL,EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression,2023-08-25 14:23:40+00:00,"We propose an unsupervised method to extract keywords and keyphrases from
texts based on a pre-trained language model (LM) and Shannon's information
maximization. Specifically, our method extracts phrases having the highest
conditional entropy under the LM. The resulting set of keyphrases turns out to
solve a relevant information-theoretic problem: if provided as side
information, it leads to the expected minimal binary code length in compressing
the text using the LM and an entropy encoder. Alternately, the resulting set is
an approximation via a causal LM to the set of phrases that minimize the
entropy of the text when conditioned upon it. Empirically, the method provides
results comparable to the most commonly used methods in various keyphrase
extraction benchmark challenges.",http://arxiv.org/pdf/2308.13399v1
2308.13387v1,cs.CL,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,2023-08-25 14:02:12+00:00,"With the rapid evolution of large language models (LLMs), new and
hard-to-predict harmful capabilities are emerging. This requires developers to
be able to identify risks through the evaluation of ""dangerous capabilities"" in
order to responsibly deploy LLMs. In this work, we collect the first
open-source dataset to evaluate safeguards in LLMs, and deploy safer
open-source LLMs at a low cost. Our dataset is curated and filtered to consist
only of instructions that responsible language models should not follow. We
annotate and assess the responses of six popular LLMs to these instructions.
Based on our annotation, we proceed to train several BERT-like classifiers, and
find that these small classifiers can achieve results that are comparable with
GPT-4 on automatic safety evaluation. Warning: this paper contains example data
that may be offensive, harmful, or biased.",http://arxiv.org/pdf/2308.13387v1
2308.13383v1,cs.CL,Assessing Keyness using Permutation Tests,2023-08-25 13:52:57+00:00,"We propose a resampling-based approach for assessing keyness in corpus
linguistics based on suggestions by Gries (2006, 2022). Traditional approaches
based on hypothesis tests (e.g. Likelihood Ratio) model the copora as
independent identically distributed samples of tokens. This model does not
account for the often observed uneven distribution of occurences of a word
across a corpus. When occurences of a word are concentrated in few documents,
large values of LLR and similar scores are in fact much more likely than
accounted for by the token-by-token sampling model, leading to false positives.
  We replace the token-by-token sampling model by a model where corpora are
samples of documents rather than tokens, which is much closer to the way
corpora are actually assembled. We then use a permutation approach to
approximate the distribution of a given keyness score under the null hypothesis
of equal frequencies and obtain p-values for assessing significance. We do not
need any assumption on how the tokens are organized within or across documents,
and the approach works with basically *any* keyness score. Hence, appart from
obtaining more accurate p-values for scores like LLR, we can also assess
significance for e.g. the logratio which has been proposed as a measure of
effect size.
  An efficient implementation of the proposed approach is provided in the `R`
package `keyperm` available from github.",http://arxiv.org/pdf/2308.13383v1
2308.13577v1,cs.CL,Text Style Transfer Evaluation Using Large Language Models,2023-08-25 13:07:33+00:00,"Text Style Transfer (TST) is challenging to evaluate because the quality of
the generated text manifests itself in multiple aspects, each of which is hard
to measure individually: style transfer accuracy, content preservation, and
overall fluency of the text. Human evaluation is the gold standard in TST
evaluation; however, it is expensive, and the results are difficult to
reproduce. Numerous automated metrics are employed to assess performance in
these aspects, serving as substitutes for human evaluation. However, the
correlation between many of these automated metrics and human evaluations
remains unclear, raising doubts about their effectiveness as reliable
benchmarks. Recent advancements in Large Language Models (LLMs) have
demonstrated their ability to not only match but also surpass the average human
performance across a wide range of unseen tasks. This suggests that LLMs have
the potential to serve as a viable alternative to human evaluation and other
automated metrics. We assess the performance of different LLMs on TST
evaluation by employing multiple input prompts and comparing their results. Our
findings indicate that (even zero-shot) prompting correlates strongly with
human evaluation and often surpasses the performance of (other) automated
metrics. Additionally, we propose the ensembling of prompts and show it
increases the robustness of TST evaluation.This work contributes to the ongoing
efforts in evaluating LLMs on diverse tasks, which includes a discussion of
failure cases and limitations.",http://arxiv.org/pdf/2308.13577v1
2308.13354v1,cs.SE,On the Impact of Language Selection for Training and Evaluating Programming Language Models,2023-08-25 12:57:59+00:00,"The recent advancements in Transformer-based Language Models have
demonstrated significant potential in enhancing the multilingual capabilities
of these models. The remarkable progress made in this domain not only applies
to natural language tasks but also extends to the domain of programming
languages. Despite the ability of these models to learn from multiple
languages, evaluations typically focus on particular combinations of the same
languages. In this study, we evaluate the similarity of programming languages
by analyzing their representations using a CodeBERT-based model. Our
experiments reveal that token representation in languages such as C++, Python,
and Java exhibit proximity to one another, whereas the same tokens in languages
such as Mathematica and R display significant dissimilarity. Our findings
suggest that this phenomenon can potentially result in performance challenges
when dealing with diverse languages. Thus, we recommend using our similarity
measure to select a diverse set of programming languages when training and
evaluating future models.",http://arxiv.org/pdf/2308.13354v1
2308.13576v1,cs.CL,An Ensemble Approach to Personalized Real Time Predictive Writing for Experts,2023-08-25 12:45:46+00:00,"Completing a sentence, phrase or word after typing few words / characters is
very helpful for Intuit financial experts, while taking notes or having a live
chat with users, since they need to write complex financial concepts more
efficiently and accurately many times in a day. In this paper, we tie together
different approaches like large language models, traditional Markov Models and
char level models to create an end-to-end system to provide personalised
sentence/word auto-complete suggestions to experts, under strict latency
constraints. Proposed system can auto-complete sentences, phrases or words
while writing with personalisation and can be trained with very less data and
resources with good efficiency. Our proposed system is not only efficient and
personalized but also robust as it leverages multiple machine learning
techniques along with transfer learning approach to fine tune large language
model with Intuit specific data. This ensures that even in cases of rare or
unusual phrases, the system can provide relevant auto-complete suggestions in
near real time. Survey has showed that this system saves expert note-taking
time and boosts expert confidence in their communication with teammates and
clients. Since enabling this predictive writing feature for QBLive experts,
more than a million keystrokes have been saved based on these suggestions. We
have done comparative study for our ensemble choice. Moreover this feature can
be integrated with any product which has writing facility within a very short
period of time.",http://arxiv.org/pdf/2308.13576v1
2308.13345v1,eess.AS,Decoupled Structure for Improved Adaptability of End-to-End Models,2023-08-25 12:31:12+00:00,"Although end-to-end (E2E) trainable automatic speech recognition (ASR) has
shown great success by jointly learning acoustic and linguistic information, it
still suffers from the effect of domain shifts, thus limiting potential
applications. The E2E ASR model implicitly learns an internal language model
(LM) which characterises the training distribution of the source domain, and
the E2E trainable nature makes the internal LM difficult to adapt to the target
domain with text-only data To solve this problem, this paper proposes decoupled
structures for attention-based encoder-decoder (Decoupled-AED) and neural
transducer (Decoupled-Transducer) models, which can achieve flexible domain
adaptation in both offline and online scenarios while maintaining robust
intra-domain performance. To this end, the acoustic and linguistic parts of the
E2E model decoder (or prediction network) are decoupled, making the linguistic
component (i.e. internal LM) replaceable. When encountering a domain shift, the
internal LM can be directly replaced during inference by a target-domain LM,
without re-training or using domain-specific paired speech-text data.
Experiments for E2E ASR models trained on the LibriSpeech-100h corpus showed
that the proposed decoupled structure gave 15.1% and 17.2% relative word error
rate reductions on the TED-LIUM 2 and AESRC2020 corpora while still maintaining
performance on intra-domain data.",http://arxiv.org/pdf/2308.13345v1
2308.13317v1,cs.AI,Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics,2023-08-25 11:41:05+00:00,"This paper presents a novel approach named Persona-Grouping-Intelligence
(PGI), which has been crafted to tackle the challenges posed by GPT models when
applied to real-world business issues. PGI leverages the inherent capabilities
of the GPT model to comprehend intricate language structures and generate
responses that are contextually relevant. The experiment occurred in a business
scenario where human intelligence was being underutilized due to less optimized
business processes. The primary objective of this approach is to leverage GPT
models to reduce the workload on humans in tasks that are extensive,
monotonous, and repetitive. Instead, the focus is redirected toward
decision-making activities. Remarkably, the experiment yielded an accuracy rate
of 93.81% in validating 4,000 responses generated by the model, underscoring
the effectiveness of the PGI strategies. Effectively addressing the issue of
underutilized human intelligence, this paradigm shift aligns business
environments with dynamic machine intelligence, enabling them to navigate the
intricacies of real-world challenges. This approach facilitates the practical
utilization of these models to tackle actual problems. The methodology offers
an opportunity to reshape the fundamental structure of business processes by
seamlessly integrating human decision-making with adaptable machine
intelligence. Consequently, this optimization enhances operational efficiency
and elevates strategic decision-making across diverse business contexts.",http://arxiv.org/pdf/2308.13317v1
2308.13315v1,cs.CL,Construction Grammar and Language Models,2023-08-25 11:37:56+00:00,"Recent progress in deep learning and natural language processing has given
rise to powerful models that are primarily trained on a cloze-like task and
show some evidence of having access to substantial linguistic information,
including some constructional knowledge. This groundbreaking discovery presents
an exciting opportunity for a synergistic relationship between computational
methods and Construction Grammar research. In this chapter, we explore three
distinct approaches to the interplay between computational methods and
Construction Grammar: (i) computational methods for text analysis, (ii)
computational Construction Grammar, and (iii) deep learning models, with a
particular focus on language models. We touch upon the first two approaches as
a contextual foundation for the use of computational methods before providing
an accessible, yet comprehensive overview of deep learning models, which also
addresses reservations construction grammarians may have. Additionally, we
delve into experiments that explore the emergence of constructionally relevant
information within these models while also examining the aspects of
Construction Grammar that may pose challenges for these models. This chapter
aims to foster collaboration between researchers in the fields of natural
language processing and Construction Grammar. By doing so, we hope to pave the
way for new insights and advancements in both these fields.",http://arxiv.org/pdf/2308.13315v1
2308.13259v1,cs.CL,Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering,2023-08-25 09:23:55+00:00,"Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown
impressive reasoning ability in various downstream tasks. Even so, suffering
from hallucinations and the inability to access external knowledge, LLMs often
come with incorrect or unfaithful intermediate reasoning steps, especially in
the context of answering knowledge-intensive tasks such as KBQA. To alleviate
this issue, we propose a framework called Knowledge-Driven Chain-of-Thought
(KD-CoT) to verify and modify reasoning traces in CoT via interaction with
external knowledge, and thus overcome the hallucinations and error propagation.
Concretely, we formulate the CoT rationale process of LLMs into a structured
multi-round QA format. In each round, LLMs interact with a QA system that
retrieves external knowledge and produce faithful reasoning traces based on
retrieved precise answers. The structured CoT reasoning of LLMs is facilitated
by our developed KBQA CoT collection, which serves as in-context learning
demonstrations and can also be utilized as feedback augmentation to train a
robust retriever. Extensive experiments on WebQSP and ComplexWebQuestion
datasets demonstrate the effectiveness of proposed KD-CoT in task-solving
reasoning generation, which outperforms the vanilla CoT ICL with an absolute
success rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented
retriever outperforms the state-of-the-art baselines for retrieving knowledge,
achieving significant improvement in Hit performance.",http://arxiv.org/pdf/2308.13259v1
2308.13207v1,cs.CL,LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models,2023-08-25 07:04:16+00:00,"The advent of Large Language Models (LLM) has revolutionized the field of
natural language processing, enabling significant progress in various
applications. One key area of interest is the construction of Knowledge Bases
(KB) using these powerful models. Knowledge bases serve as repositories of
structured information, facilitating information retrieval and inference tasks.
Our paper proposes LLM2KB, a system for constructing knowledge bases using
large language models, with a focus on the Llama 2 architecture and the
Wikipedia dataset. We perform parameter efficient instruction tuning for
Llama-2-13b-chat and StableBeluga-13B by training small injection models that
have only 0.05 % of the parameters of the base models using the Low Rank
Adaptation (LoRA) technique. These injection models have been trained with
prompts that are engineered to utilize Wikipedia page contexts of subject
entities fetched using a Dense Passage Retrieval (DPR) algorithm, to answer
relevant object entities for a given subject entity and relation. Our best
performing model achieved an average F1 score of 0.6185 across 21 relations in
the LM-KBC challenge held at the ISWC 2023 conference.",http://arxiv.org/pdf/2308.13207v1
2308.13198v1,cs.CL,Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons,2023-08-25 06:26:05+00:00,"Pre-trained language models (PLMs) contain vast amounts of factual knowledge,
but how the knowledge is stored in the parameters remains unclear. This paper
delves into the complex task of understanding how factual knowledge is stored
in multilingual PLMs, and introduces the Architecture-adapted Multilingual
Integrated Gradients method, which successfully localizes knowledge neurons
more precisely compared to current methods, and is more universal across
various architectures and languages. Moreover, we conduct an in-depth
exploration of knowledge neurons, leading to the following two important
discoveries: (1) The discovery of Language-Independent Knowledge Neurons, which
store factual knowledge in a form that transcends language. We design
cross-lingual knowledge editing experiments, demonstrating that the PLMs can
accomplish this task based on language-independent neurons; (2) The discovery
of Degenerate Knowledge Neurons, a novel type of neuron showing that different
knowledge neurons can store the same fact. Its property of functional overlap
endows the PLMs with a robust mastery of factual knowledge. We design
fact-checking experiments, proving that the degenerate knowledge neurons can
help the PLMs to detect wrong facts. Experiments corroborate these findings,
shedding light on the mechanisms of factual knowledge storage in multilingual
PLMs, and contribute valuable insights to the field. The source code will be
made publicly available for further research.",http://arxiv.org/pdf/2308.13198v1
2308.13192v1,cs.AI,Formalising Natural Language Quantifiers for Human-Robot Interactions,2023-08-25 06:05:57+00:00,"We present a method for formalising quantifiers in natural language in the
context of human-robot interactions. The solution is based on first-order logic
extended with capabilities to represent the cardinality of variables, operating
similarly to generalised quantifiers. To demonstrate the method, we designed an
end-to-end system able to receive input as natural language, convert it into a
formal logical representation, evaluate it, and return a result or send a
command to a simulated robot.",http://arxiv.org/pdf/2308.13192v1
2308.13191v1,cs.CL,"Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",2023-08-25 05:52:05+00:00,"Although dominant in natural language processing, transformer-based models
remain challenged by the task of long-sequence processing, because the
computational cost of self-attention operations in transformers swells
quadratically with the input sequence length. To alleviate the complexity of
long-sequence processing, we propose a simple framework to enable the
offthe-shelf pre-trained transformers to process much longer sequences, while
the computation and memory costs remain growing linearly with the input
sequence lengths. More specifically, our method divides each long-sequence
input into a batch of chunks, then aligns the interchunk information during the
encoding steps, and finally selects the most representative hidden states from
the encoder for the decoding process. To extract inter-chunk semantic
information, we align the start and end token embeddings among chunks in each
encoding transformer block. To learn an effective hidden selection policy, we
design a dual updating scheme inspired by reinforcement learning, which regards
the decoders of transformers as environments, and the downstream performance
metrics as the rewards to evaluate the hidden selection actions. Our empirical
results on real-world long-text summarization and reading comprehension tasks
demonstrate effective improvements compared to prior longsequence processing
baselines.",http://arxiv.org/pdf/2308.13191v1
2308.13569v1,cs.CL,Discovering Mental Health Research Topics with Topic Modeling,2023-08-25 05:25:05+00:00,"Mental health significantly influences various aspects of our daily lives,
and its importance has been increasingly recognized by the research community
and the general public, particularly in the wake of the COVID-19 pandemic. This
heightened interest is evident in the growing number of publications dedicated
to mental health in the past decade. In this study, our goal is to identify
general trends in the field and pinpoint high-impact research topics by
analyzing a large dataset of mental health research papers. To accomplish this,
we collected abstracts from various databases and trained a customized
Sentence-BERT based embedding model leveraging the BERTopic framework. Our
dataset comprises 96,676 research papers pertaining to mental health, enabling
us to examine the relationships between different topics using their abstracts.
To evaluate the effectiveness of the model, we compared it against two other
state-of-the-art methods: Top2Vec model and LDA-BERT model. The model
demonstrated superior performance in metrics that measure topic diversity and
coherence. To enhance our analysis, we also generated word clouds to provide a
comprehensive overview of the machine learning models applied in mental health
research, shedding light on commonly utilized techniques and emerging trends.
Furthermore, we provide a GitHub link* to the dataset used in this paper,
ensuring its accessibility for further research endeavors.",http://arxiv.org/pdf/2308.13569v1
2308.13177v1,cs.CV,How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection,2023-08-25 04:54:32+00:00,"Object detection (OD) in computer vision has made significant progress in
recent years, transitioning from closed-set labels to open-vocabulary detection
(OVD) based on large-scale vision-language pre-training (VLP). However, current
evaluation methods and datasets are limited to testing generalization over
object types and referral expressions, which do not provide a systematic,
fine-grained, and accurate benchmark of OVD models' abilities. In this paper,
we propose a new benchmark named OVDEval, which includes 9 sub-tasks and
introduces evaluations on commonsense knowledge, attribute understanding,
position understanding, object relation comprehension, and more. The dataset is
meticulously created to provide hard negatives that challenge models' true
understanding of visual and linguistic input. Additionally, we identify a
problem with the popular Average Precision (AP) metric when benchmarking models
on these fine-grained label datasets and propose a new metric called
Non-Maximum Suppression Average Precision (NMS-AP) to address this issue.
Extensive experimental results show that existing top OVD models all fail on
the new tasks except for simple object types, demonstrating the value of the
proposed dataset in pinpointing the weakness of current OVD models and guiding
future research. Furthermore, the proposed NMS-AP metric is verified by
experiments to provide a much more truthful evaluation of OVD models, whereas
traditional AP metrics yield deceptive results. Data is available at
\url{https://github.com/om-ai-lab/OVDEval}",http://arxiv.org/pdf/2308.13177v1
2308.13173v1,cs.CV,DISGO: Automatic End-to-End Evaluation for Scene Text OCR,2023-08-25 04:45:37+00:00,"This paper discusses the challenges of optical character recognition (OCR) on
natural scenes, which is harder than OCR on documents due to the wild content
and various image backgrounds. We propose to uniformly use word error rates
(WER) as a new measurement for evaluating scene-text OCR, both end-to-end (e2e)
performance and individual system component performances. Particularly for the
e2e metric, we name it DISGO WER as it considers Deletion, Insertion,
Substitution, and Grouping/Ordering errors. Finally we propose to utilize the
concept of super blocks to automatically compute BLEU scores for e2e OCR
machine translation. The small SCUT public test set is used to demonstrate WER
performance by a modularized OCR system.",http://arxiv.org/pdf/2308.13173v1
2308.13170v1,cs.CL,Measuring Spurious Correlation in Classification: 'Clever Hans' in Translationese,2023-08-25 04:19:58+00:00,"Recent work has shown evidence of 'Clever Hans' behavior in high-performance
neural translationese classifiers, where BERT-based classifiers capitalize on
spurious correlations, in particular topic information, between data and target
classification labels, rather than genuine translationese signals.
Translationese signals are subtle (especially for professional translation) and
compete with many other signals in the data such as genre, style, author, and,
in particular, topic. This raises the general question of how much of the
performance of a classifier is really due to spurious correlations in the data
versus the signals actually targeted for by the classifier, especially for
subtle target signals and in challenging (low resource) data settings. We focus
on topic-based spurious correlation and approach the question from two
directions: (i) where we have no knowledge about spurious topic information and
its distribution in the data, (ii) where we have some indication about the
nature of spurious topic correlations. For (i) we develop a measure from first
principles capturing alignment of unsupervised topics with target
classification labels as an indication of spurious topic information in the
data. We show that our measure is the same as purity in clustering and propose
a 'topic floor' (as in a 'noise floor') for classification. For (ii) we
investigate masking of known spurious topic carriers in classification. Both
(i) and (ii) contribute to quantifying and (ii) to mitigating spurious
correlations.",http://arxiv.org/pdf/2308.13170v1
2308.13149v1,cs.CL,SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research,2023-08-25 03:05:33+00:00,"Recently, there has been growing interest in using Large Language Models
(LLMs) for scientific research. Numerous benchmarks have been proposed to
evaluate the ability of LLMs for scientific research. However, current
benchmarks are mostly based on pre-collected objective questions. This design
suffers from data leakage problem and lacks the evaluation of subjective Q/A
ability. In this paper, we propose SciEval, a comprehensive and
multi-disciplinary evaluation benchmark to address these issues. Based on
Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate
scientific research ability. In particular, we design a ""dynamic"" subset based
on scientific principles to prevent evaluation from potential data leakage.
Both objective and subjective questions are included in SciEval. These
characteristics make SciEval a more effective benchmark for scientific research
ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs
show that, although GPT-4 achieves SOTA performance compared to other LLMs,
there is still substantial room for improvement, especially for dynamic
questions. The data and codes are now publicly available.",http://arxiv.org/pdf/2308.13149v1
2308.13139v1,cs.CL,MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification,2023-08-25 02:32:36+00:00,"The eXtreme Multi-label text Classification(XMC) refers to training a
classifier that assigns a text sample with relevant labels from an extremely
large-scale label set (e.g., millions of labels). We propose MatchXML, an
efficient text-label matching framework for XMC. We observe that the label
embeddings generated from the sparse Term Frequency-Inverse Document
Frequency(TF-IDF) features have several limitations. We thus propose label2vec
to effectively train the semantic dense label embeddings by the Skip-gram
model. The dense label embeddings are then used to build a Hierarchical Label
Tree by clustering. In fine-tuning the pre-trained encoder Transformer, we
formulate the multi-label text classification as a text-label matching problem
in a bipartite graph. We then extract the dense text representations from the
fine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also
extract the static dense sentence embeddings from a pre-trained Sentence
Transformer. Finally, a linear ranker is trained by utilizing the sparse TF-IDF
features, the fine-tuned dense text representations and static dense sentence
features. Experimental results demonstrate that MatchXML achieves
state-of-the-art accuracy on five out of six datasets. As for the speed,
MatchXML outperforms the competing methods on all the six datasets. Our source
code is publicly available at https://github.com/huiyegit/MatchXML.",http://arxiv.org/pdf/2308.13139v1
2308.13137v1,cs.LG,OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models,2023-08-25 02:28:35+00:00,"Large language models (LLMs) have revolutionized natural language processing
tasks. However, their practical deployment is hindered by their immense memory
and computation requirements. Although recent post-training quantization (PTQ)
methods are effective in reducing memory footprint and improving the
computational efficiency of LLM, they hand-craft quantization parameters, which
leads to low performance and fails to deal with extremely low-bit quantization.
To tackle this issue, we introduce an Omnidirectionally calibrated Quantization
(OmniQuant) technique for LLMs, which achieves good performance in diverse
quantization settings while maintaining the computational efficiency of PTQ by
efficiently optimizing various quantization parameters. OmniQuant comprises two
innovative components including Learnable Weight Clipping (LWC) and Learnable
Equivalent Transformation (LET). LWC modulates the extreme values of weights by
optimizing the clipping threshold. Meanwhile, LET tackles activation outliers
by shifting the challenge of quantization from activations to weights through a
learnable equivalent transformation. Operating within a differentiable
framework using block-wise error minimization, OmniQuant can optimize the
quantization process efficiently for both weight-only and weight-activation
quantization. For instance, the LLaMA-2 model family with the size of 7-70B can
be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using
128 samples. Extensive experiments validate OmniQuant's superior performance
across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16,
and W2A16. Additionally, OmniQuant demonstrates effectiveness in
instruction-tuned models and delivers notable improvements in inference speed
and memory reduction on real devices. Codes and models are available at
\url{https://github.com/OpenGVLab/OmniQuant}.",http://arxiv.org/pdf/2308.13137v1
2308.13566v1,cs.LG,MLLM-DataEngine: An Iterative Refinement Approach for MLLM,2023-08-25 01:41:04+00:00,"Despite the great advance of Multimodal Large Language Models (MLLMs) in both
instruction dataset building and benchmarking, the independence of training and
evaluation makes current MLLMs hard to further improve their capability under
the guidance of evaluation results with a relatively low human cost. In this
paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data
generation, model training, and evaluation. Within each loop iteration, the
MLLM-DataEngine first analyze the weakness of the model based on the evaluation
results, then generate a proper incremental dataset for the next training
iteration and enhance the model capability iteratively. Compared with previous
data collection methods which are separate from the benchmarking, the data
generated by MLLM-DataEngine shows better targeting, quality, and correctness.
For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts
the ratio of different types of data within each incremental dataset based on
the benchmarking results. For quality, we resort to GPT-4 to generate
high-quality data with each given data type. For correctness, prompt design is
critical for the data generation results. Rather than previous hand-crafted
prompt, we propose an Interactive Prompt Optimization strategy, which optimizes
the prompt with the multi-round interaction between human and GPT, and improve
the correctness of generated data greatly. Through extensive experiments, we
find our MLLM-DataEngine could boost the MLLM capability in a targeted and
automatic manner, with only a few human participation. The MLLM-DataEngine will
be released and we hope it could be a general solution for the following MLLMs
building.",http://arxiv.org/pdf/2308.13566v1
2308.13565v1,cs.CL,DARWIN Series: Domain Specific Large Language Models for Natural Science,2023-08-25 01:40:48+00:00,"Emerging tools bring forth fresh approaches to work, and the field of natural
science is no different. In natural science, traditional manual, serial, and
labour-intensive work is being augmented by automated, parallel, and iterative
processes driven by artificial intelligence-based experimental automation and
more. To add new capabilities in natural science, enabling the acceleration and
enrichment of automation of the discovery process, we present DARWIN, a series
of tailored LLMs for natural science, mainly in physics, chemistry, and
material science. This series relies on open-source LLM, incorporating
structured and unstructured scientific knowledge from public datasets and
literature. We fine-tuned the models using over 60,000 instruction data points,
emphasizing factual correctness. During the fine-tuning, we introduce the
Scientific Instruction Generation (SIG) model, automating instruction
generation from scientific texts. This eliminates the need for manual
extraction or domain-specific knowledge graphs and efficiently injects
scientific knowledge into the model. We also explore multi-task training
strategies, revealing interconnections between scientific tasks. DARWIN series
not only achieves state-of-the-art results on various scientific tasks but also
diminishes reliance on closed-source AI models. Our research showcases the
ability of LLM in the scientific domain, with the overarching goal of fostering
prosperity within the broader AI for science community.",http://arxiv.org/pdf/2308.13565v1
2308.13563v1,cs.CL,"Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4",2023-08-25 00:09:16+00:00,"In traffic safety research, extracting information from crash narratives
using text analysis is a common practice. With recent advancements of large
language models (LLM), it would be useful to know how the popular LLM
interfaces perform in classifying or extracting information from crash
narratives. To explore this, our study has used the three most popular publicly
available LLM interfaces- ChatGPT, BARD and GPT4. This study investigated their
usefulness and boundaries in extracting information and answering queries
related to accidents from 100 crash narratives from Iowa and Kansas. During the
investigation, their capabilities and limitations were assessed and their
responses to the queries were compared. Five questions were asked related to
the narratives: 1) Who is at-fault? 2) What is the manner of collision? 3) Has
the crash occurred in a work-zone? 4) Did the crash involve pedestrians? and 5)
What are the sequence of harmful events in the crash? For questions 1 through
4, the overall similarity among the LLMs were 70%, 35%, 96% and 89%,
respectively. The similarities were higher while answering direct questions
requiring binary responses and significantly lower for complex questions. To
compare the responses to question 5, network diagram and centrality measures
were analyzed. The network diagram from the three LLMs were not always similar
although they sometimes have the same influencing events with high in-degree,
out-degree and betweenness centrality. This study suggests using multiple
models to extract viable information from narratives. Also, caution must be
practiced while using these interfaces to obtain crucial safety related
information.",http://arxiv.org/pdf/2308.13563v1
2308.13116v1,cs.CL,Sentence Embedding Models for Ancient Greek Using Multilingual Knowledge Distillation,2023-08-24 23:38:44+00:00,"Contextual language models have been trained on Classical languages,
including Ancient Greek and Latin, for tasks such as lemmatization,
morphological tagging, part of speech tagging, authorship attribution, and
detection of scribal errors. However, high-quality sentence embedding models
for these historical languages are significantly more difficult to achieve due
to the lack of training data. In this work, we use a multilingual knowledge
distillation approach to train BERT models to produce sentence embeddings for
Ancient Greek text. The state-of-the-art sentence embedding approaches for
high-resource languages use massive datasets, but our distillation approach
allows our Ancient Greek models to inherit the properties of these models while
using a relatively small amount of translated sentence data. We build a
parallel sentence dataset using a sentence-embedding alignment method to align
Ancient Greek documents with English translations, and use this dataset to
train our models. We evaluate our models on translation search, semantic
similarity, and semantic retrieval tasks and investigate translation bias. We
make our training and evaluation datasets freely available at
https://github.com/kevinkrahn/ancient-greek-datasets .",http://arxiv.org/pdf/2308.13116v1
2308.13089v1,cs.CL,Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens,2023-08-24 21:19:48+00:00,"The rapid growth in the usage and applications of Natural Language Processing
(NLP) in various sociotechnical solutions has highlighted the need for a
comprehensive understanding of bias and its impact on society. While research
on bias in NLP has expanded, several challenges persist that require attention.
These include the limited focus on sociodemographic biases beyond race and
gender, the narrow scope of analysis predominantly centered on models, and the
technocentric implementation approaches. This paper addresses these challenges
and advocates for a more interdisciplinary approach to understanding bias in
NLP. The work is structured into three facets, each exploring a specific aspect
of bias in NLP.",http://arxiv.org/pdf/2308.13089v1
2308.13081v1,cs.CL,Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations,2023-08-24 20:57:07+00:00,"This document presents adequate formal terminology for the mathematical
specification of a subset of Agent Based Models (ABMs) in the field of
Demography. The simulation of the targeted ABMs follows a fixed-step
single-clocked pattern. The proposed terminology further improves the model
understanding and can act as a stand-alone methodology for the specification
and optionally the documentation of a significant set of (demographic) ABMs.
Nevertheless, it is imaginable the this terminology probably with further
extensions can be merged with the largely-informal widely-used model
documentation and communication O.D.D. protocol [Grimm and et al., 2020,
Amouroux et al., 2010] to reduce many sources of ambiguity, hindering model
replications by other modelers. A published demographic model documentation,
largely simplified version of the Lone Parent Model [Gostoli and Silverman,
2020] is separately published in [Elsheikh, 2023b] as illustration for the
formal terminology. The model was implemented in the Julia language [Elsheikh,
2023a] based on the Agents.jl julia package [Datseris et al., 2022].",http://arxiv.org/pdf/2308.13081v1
2308.13067v1,cs.AI,Causal Parrots: Large Language Models May Talk Causality But Are Not Causal,2023-08-24 20:23:13+00:00,"Some argue scale is all what is needed to achieve AI, covering even causal
models. We make it clear that large language models (LLMs) cannot be causal and
give reason onto why sometimes we might feel otherwise. To this end, we define
and exemplify a new subgroup of Structural Causal Model (SCM) that we call meta
SCM which encode causal facts about other SCM within their variables. We
conjecture that in the cases where LLM succeed in doing causal inference,
underlying was a respective meta SCM that exposed correlations between causal
facts in natural language on whose data the LLM was ultimately trained. If our
hypothesis holds true, then this would imply that LLMs are like parrots in that
they simply recite the causal knowledge embedded in the data. Our empirical
analysis provides favoring evidence that current LLMs are even weak `causal
parrots.'",http://arxiv.org/pdf/2308.13067v1
2308.13056v1,cs.CL,Lexical Diversity in Kinship Across Languages and Dialects,2023-08-24 19:49:30+00:00,"Languages are known to describe the world in diverse ways. Across lexicons,
diversity is pervasive, appearing through phenomena such as lexical gaps and
untranslatability. However, in computational resources, such as multilingual
lexical databases, diversity is hardly ever represented. In this paper, we
introduce a method to enrich computational lexicons with content relating to
linguistic diversity. The method is verified through two large-scale case
studies on kinship terminology, a domain known to be diverse across languages
and cultures: one case study deals with seven Arabic dialects, while the other
one with three Indonesian languages. Our results, made available as browseable
and downloadable computational resources, extend prior linguistics research on
kinship terminology, and provide insight into the extent of diversity even
within linguistically and culturally close communities.",http://arxiv.org/pdf/2308.13056v1
2308.13032v1,cs.CL,Financial News Analytics Using Fine-Tuned Llama 2 GPT Model,2023-08-24 18:58:10+00:00,"The paper considers the possibility to fine-tune Llama 2 Large Language Model
(LLM) for the multitask analysis of financial news. For fine-tuning, the
PEFT/LoRA based approach was used. In the study, the model was fine-tuned for
the following tasks: analysing a text from financial market perspectives,
highlighting main points of a text, summarizing a text and extracting named
entities with appropriate sentiments. The obtained results show that the
fine-tuned Llama 2 model can perform a multitask financial news analysis with a
specified structure of response, part of response can be a structured text and
another part of data can have JSON format for further processing. Extracted
sentiments for named entities can be considered as predictive features in
supervised machine learning models with quantitative target variables.",http://arxiv.org/pdf/2308.13032v1
2308.12966v1,cs.CV,Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities,2023-08-24 17:59:17+00:00,"We introduce the Qwen-VL series, a set of large-scale vision-language models
designed to perceive and understand both text and images. Comprising Qwen-VL
and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like
image captioning, question answering, visual localization, and flexible
interaction. The evaluation covers a wide range of tasks including zero-shot
captioning, visual or document visual question answering, and grounding. We
demonstrate the Qwen-VL outperforms existing Large Vision Language Models
(LVLMs). We present their architecture, training, capabilities, and
performance, highlighting their contributions to advancing multimodal
artificial intelligence. Code, demo and models are available at
https://github.com/QwenLM/Qwen-VL.",http://arxiv.org/pdf/2308.12966v1
2308.12950v2,cs.CL,Code Llama: Open Foundation Models for Code,2023-08-24 17:39:13+00:00,"We release Code Llama, a family of large language models for code based on
Llama 2 providing state-of-the-art performance among open models, infilling
capabilities, support for large input contexts, and zero-shot instruction
following ability for programming tasks. We provide multiple flavors to cover a
wide range of applications: foundation models (Code Llama), Python
specializations (Code Llama - Python), and instruction-following models (Code
Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained
on sequences of 16k tokens and show improvements on inputs with up to 100k
tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support
infilling based on surrounding content. Code Llama reaches state-of-the-art
performance among open models on several code benchmarks, with scores of up to
53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python
7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform
every other publicly available model on MultiPL-E. We release Code Llama under
a permissive license that allows for both research and commercial use.",http://arxiv.org/pdf/2308.12950v2
2308.12898v2,cs.MM,Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?,2023-08-24 16:17:40+00:00,"The multimedia community has shown a significant interest in perceiving and
representing the physical world with multimodal pretrained neural network
models, and among them, the visual-language pertaining (VLP) is, currently, the
most captivating topic. However, there have been few endeavors dedicated to the
exploration of 1) whether essential linguistic knowledge (e.g., semantics and
syntax) can be extracted during VLP, and 2) how such linguistic knowledge
impact or enhance the multimodal alignment. In response, here we aim to
elucidate the impact of comprehensive linguistic knowledge, including semantic
expression and syntactic structure, on multimodal alignment. Specifically, we
design and release the SNARE, the first large-scale multimodal alignment
probing benchmark, to detect the vital linguistic components, e.g., lexical,
semantic, and syntax knowledge, containing four tasks: Semantic structure,
Negation logic, Attribute ownership, and Relationship composition. Based on our
proposed probing benchmarks, our holistic analyses of five advanced VLP models
illustrate that the VLP model: i) shows insensitivity towards complex syntax
structures and relies on content words for sentence comprehension; ii)
demonstrates limited comprehension of combinations between sentences and
negations; iii) faces challenges in determining the presence of actions or
spatial relationships within visual information and struggles with verifying
the correctness of triple combinations. We make our benchmark and code
available at \url{https://github.com/WangFei-2019/SNARE/}.",http://arxiv.org/pdf/2308.12898v2
2308.12896v1,cs.CV,"Beyond Document Page Classification: Design, Datasets, and Challenges",2023-08-24 16:16:47+00:00,"This paper highlights the need to bring document classification benchmarking
closer to real-world applications, both in the nature of data tested ($X$:
multi-channel, multi-paged, multi-industry; $Y$: class distributions and label
set variety) and in classification tasks considered ($f$: multi-page document,
page stream, and document bundle classification, ...). We identify the lack of
public multi-page document classification datasets, formalize different
classification tasks arising in application scenarios, and motivate the value
of targeting efficient multi-page document representations. An experimental
study on proposed multi-page document classification datasets demonstrates that
current benchmarks have become irrelevant and need to be updated to evaluate
complete documents, as they naturally occur in practice. This reality check
also calls for more mature evaluation methodologies, covering calibration
evaluation, inference complexity (time-memory), and a range of realistic
distribution shifts (e.g., born-digital vs. scanning noise, shifting page
order). Our study ends on a hopeful note by recommending concrete avenues for
future improvements.}",http://arxiv.org/pdf/2308.12896v1
2308.12890v2,cs.CL,Large Language Models Vote: Prompting for Rare Disease Identification,2023-08-24 16:09:13+00:00,"The emergence of generative Large Language Models (LLMs) emphasizes the need
for accurate and efficient prompting approaches. LLMs are often applied in
Few-Shot Learning (FSL) contexts, where tasks are executed with minimal
training data. FSL has become popular in many Artificial Intelligence (AI)
subdomains, including AI for health. Rare diseases affect a small fraction of
the population. Rare disease identification from clinical notes inherently
requires FSL techniques due to limited data availability. Manual data
collection and annotation is both expensive and time-consuming. In this paper,
we propose Models-Vote Prompting (MVP), a flexible prompting approach for
improving the performance of LLM queries in FSL settings. MVP works by
prompting numerous LLMs to perform the same tasks and then conducting a
majority vote on the resulting outputs. This method achieves improved results
to any one model in the ensemble on one-shot rare disease identification and
classification tasks. We also release a novel rare disease dataset for FSL,
available to those who signed the MIMIC-IV Data Use Agreement (DUA).
Furthermore, in using MVP, each model is prompted multiple times, substantially
increasing the time needed for manual annotation, and to address this, we
assess the feasibility of using JSON for automating generative LLM evaluation.",http://arxiv.org/pdf/2308.12890v2
2308.12888v1,cs.CL,Inducing Causal Structure for Abstractive Text Summarization,2023-08-24 16:06:36+00:00,"The mainstream of data-driven abstractive summarization models tends to
explore the correlations rather than the causal relationships. Among such
correlations, there can be spurious ones which suffer from the language prior
learned from the training corpus and therefore undermine the overall
effectiveness of the learned model. To tackle this issue, we introduce a
Structural Causal Model (SCM) to induce the underlying causal structure of the
summarization data. We assume several latent causal factors and non-causal
factors, representing the content and style of the document and summary.
Theoretically, we prove that the latent factors in our SCM can be identified by
fitting the observed training data under certain conditions. On the basis of
this, we propose a Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq)
to learn the causal representations that can mimic the causal factors, guiding
us to pursue causal information for summary generation. The key idea is to
reformulate the Variational Auto-encoder (VAE) to fit the joint distribution of
the document and summary variables from the training corpus. Experimental
results on two widely used text summarization datasets demonstrate the
advantages of our approach.",http://arxiv.org/pdf/2308.12888v1
2308.14753v1,cs.CV,Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond,2023-08-28 17:59:47+00:00,"Visual similarities discovery (VSD) is an important task with broad
e-commerce applications. Given an image of a certain object, the goal of VSD is
to retrieve images of different objects with high perceptual visual similarity.
Although being a highly addressed problem, the evaluation of proposed methods
for VSD is often based on a proxy of an identification-retrieval task,
evaluating the ability of a model to retrieve different images of the same
object. We posit that evaluating VSD methods based on identification tasks is
limited, and faithful evaluation must rely on expert annotations. In this
paper, we introduce the first large-scale fashion visual similarity benchmark
dataset, consisting of more than 110K expert-annotated image pairs. Besides
this major contribution, we share insight from the challenges we faced while
curating this dataset. Based on these insights, we propose a novel and
efficient labeling procedure that can be applied to any dataset. Our analysis
examines its limitations and inductive biases, and based on these findings, we
propose metrics to mitigate those limitations. Though our primary focus lies on
visual similarity, the methodologies we present have broader applications for
discovering and evaluating perceptual similarity across various domains.",http://arxiv.org/pdf/2308.14753v1
2308.14742v1,math.OC,Minimizing Quasi-Self-Concordant Functions by Gradient Regularization of Newton Method,2023-08-28 17:43:04+00:00,"We study the composite convex optimization problems with a
Quasi-Self-Concordant smooth component. This problem class naturally
interpolates between classic Self-Concordant functions and functions with
Lipschitz continuous Hessian. Previously, the best complexity bounds for this
problem class were associated with trust-region schemes and implementations of
a ball-minimization oracle. In this paper, we show that for minimizing
Quasi-Self-Concordant functions we can use instead the basic Newton Method with
Gradient Regularization. For unconstrained minimization, it only involves a
simple matrix inversion operation (solving a linear system) at each step. We
prove a fast global linear rate for this algorithm, matching the complexity
bound of the trust-region scheme, while our method remains especially simple to
implement. Then, we introduce the Dual Newton Method, and based on it, develop
the corresponding Accelerated Newton Scheme for this problem class, which
further improves the complexity factor of the basic method. As a direct
consequence of our results, we establish fast global linear rates of simple
variants of the Newton Method applied to several practical problems, including
Logistic Regression, Soft Maximum, and Matrix Scaling, without requiring
additional assumptions on strong or uniform convexity for the target objective.",http://arxiv.org/pdf/2308.14742v1
2308.14740v1,cs.CV,Total Selfie: Generating Full-Body Selfies,2023-08-28 17:41:14+00:00,"We present a method to generate full-body selfies -- photos that you take of
yourself, but capturing your whole body as if someone else took the photo of
you from a few feet away. Our approach takes as input a pre-captured video of
your body, a target pose photo, and a selfie + background pair for each
location. We introduce a novel diffusion-based approach to combine all of this
information into high quality, well-composed photos of you with the desired
pose and background.",http://arxiv.org/pdf/2308.14740v1
2308.14705v1,stat.ML,Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning,2023-08-28 16:58:44+00:00,"Ensembling a neural network is a widely recognized approach to enhance model
performance, estimate uncertainty, and improve robustness in deep supervised
learning. However, deep ensembles often come with high computational costs and
memory demands. In addition, the efficiency of a deep ensemble is related to
diversity among the ensemble members which is challenging for large,
over-parameterized deep neural networks. Moreover, ensemble learning has not
yet seen such widespread adoption, and it remains a challenging endeavor for
self-supervised or unsupervised representation learning. Motivated by these
challenges, we present a novel self-supervised training regime that leverages
an ensemble of independent sub-networks, complemented by a new loss function
designed to encourage diversity. Our method efficiently builds a sub-model
ensemble with high diversity, leading to well-calibrated estimates of model
uncertainty, all achieved with minimal computational overhead compared to
traditional deep self-supervised ensembles. To evaluate the effectiveness of
our approach, we conducted extensive experiments across various tasks,
including in-distribution generalization, out-of-distribution detection,
dataset corruption, and semi-supervised settings. The results demonstrate that
our method significantly improves prediction reliability. Our approach not only
achieves excellent accuracy but also enhances calibration, surpassing baseline
performance across a wide range of self-supervised architectures in computer
vision, natural language processing, and genomics data.",http://arxiv.org/pdf/2308.14705v1
2308.14693v1,eess.SP,Hybrid PLS-ML Authentication Scheme for V2I Communication Networks,2023-08-28 16:34:50+00:00,"Vehicular communication networks are rapidly emerging as vehicles become
smarter. However, these networks are increasingly susceptible to various
attacks. The situation is exacerbated by the rise in automated vehicles
complicates, emphasizing the need for security and authentication measures to
ensure safe and effective traffic management. In this paper, we propose a novel
hybrid physical layer security (PLS)-machine learning (ML) authentication
scheme by exploiting the position of the transmitter vehicle as a device
fingerprint. We use a time-of-arrival (ToA) based localization mechanism where
the ToA is estimated at roadside units (RSUs), and the coordinates of the
transmitter vehicle are extracted at the base station (BS).Furthermore, to
track the mobility of the moving legitimate vehicle, we use ML model trained on
several system parameters. We try two ML models for this purpose, i.e., support
vector regression and decision tree. To evaluate our scheme, we conduct binary
hypothesis testing on the estimated positions with the help of the ground
truths provided by the ML model, which classifies the transmitter node as
legitimate or malicious. Moreover, we consider the probability of false alarm
and the probability of missed detection as performance metrics resulting from
the binary hypothesis testing, and mean absolute error (MAE), mean square error
(MSE), and coefficient of determination $\text{R}^2$ to further evaluate the ML
models. We also compare our scheme with a baseline scheme that exploits the
angle of arrival at RSUs for authentication. We observe that our proposed
position-based mechanism outperforms the baseline scheme significantly in terms
of missed detections.",http://arxiv.org/pdf/2308.14693v1
2308.14659v1,cs.LG,RESTORE: Graph Embedding Assessment Through Reconstruction,2023-08-28 15:41:30+00:00,"Following the success of Word2Vec embeddings, graph embeddings (GEs) have
gained substantial traction. GEs are commonly generated and evaluated
extrinsically on downstream applications, but intrinsic evaluations of the
original graph properties in terms of topological structure and semantic
information have been lacking. Understanding these will help identify the
deficiency of the various families of GE methods when vectorizing graphs in
terms of preserving the relevant knowledge or learning incorrect knowledge. To
address this, we propose RESTORE, a framework for intrinsic GEs assessment
through graph reconstruction. We show that reconstructing the original graph
from the underlying GEs yields insights into the relative amount of information
preserved in a given vector form. We first introduce the graph reconstruction
task. We generate GEs from three GE families based on factorization methods,
random walks, and deep learning (with representative algorithms from each
family) on the CommonSense Knowledge Graph (CSKG). We analyze their
effectiveness in preserving the (a) topological structure of node-level graph
reconstruction with an increasing number of hops and (b) semantic information
on various word semantic and analogy tests. Our evaluations show deep
learning-based GE algorithm (SDNE) is overall better at preserving (a) with a
mean average precision (mAP) of 0.54 and 0.35 for 2 and 3-hop reconstruction
respectively, while the factorization-based algorithm (HOPE) is better at
encapsulating (b) with an average Euclidean distance of 0.14, 0.17, and 0.11
for 1, 2, and 3-hop reconstruction respectively. The modest performance of
these GEs leaves room for further research avenues on better graph
representation learning.",http://arxiv.org/pdf/2308.14659v1
2308.14658v1,cs.LG,Adversarial Predictions of Data Distributions Across Federated Internet-of-Things Devices,2023-08-28 15:40:50+00:00,"Federated learning (FL) is increasingly becoming the default approach for
training machine learning models across decentralized Internet-of-Things (IoT)
devices. A key advantage of FL is that no raw data are communicated across the
network, providing an immediate layer of privacy. Despite this, recent works
have demonstrated that data reconstruction can be done with the locally trained
model updates which are communicated across the network. However, many of these
works have limitations with regard to how the gradients are computed in
backpropagation. In this work, we demonstrate that the model weights shared in
FL can expose revealing information about the local data distributions of IoT
devices. This leakage could expose sensitive information to malicious actors in
a distributed system. We further discuss results which show that injecting
noise into model weights is ineffective at preventing data leakage without
seriously harming the global model accuracy.",http://arxiv.org/pdf/2308.14658v1
2308.14650v1,astro-ph.EP,Comparison of automated crater catalogs for Mars from Benedix et al. (2020) and Lee and Hogan (2021),2023-08-28 15:22:15+00:00,"Crater mapping using neural networks and other automated methods has
increased recently with automated Crater Detection Algorithms (CDAs) applied to
planetary bodies throughout the solar system. A recent publication by Benedix
et al. (2020) showed high performance at small scales compared to similar
automated CDAs but with a net positive diameter bias in many crater candidates.
I compare the publicly available catalogs from Benedix et al. (2020) and Lee &
Hogan (2021) and show that the reported performance is sensitive to the metrics
used to test the catalogs. I show how the more permissive comparison methods
indicate a higher CDA performance by allowing worse candidate craters to match
ground-truth craters. I show that the Benedix et al. (2020) catalog has a
substantial performance loss with increasing latitude and identify an image
projection issue that might cause this loss. Finally, I suggest future
applications of neural networks in generating large scientific datasets be
validated using secondary networks with independent data sources or training
methods.",http://arxiv.org/pdf/2308.14650v1
2308.14647v1,cs.LG,Edge Generation Scheduling for DAG Tasks using Deep Reinforcement Learning,2023-08-28 15:19:18+00:00,"Directed acyclic graph (DAG) tasks are currently adopted in the real-time
domain to model complex applications from the automotive, avionics, and
industrial domain that implement their functionalities through chains of
intercommunicating tasks. This paper studies the problem of scheduling
real-time DAG tasks by presenting a novel schedulability test based on the
concept of trivial schedulability. Using this schedulability test, we propose a
new DAG scheduling framework (edge generation scheduling -- EGS) that attempts
to minimize the DAG width by iteratively generating edges while guaranteeing
the deadline constraint. We study how to efficiently solve the problem of
generating edges by developing a deep reinforcement learning algorithm combined
with a graph representation neural network to learn an efficient edge
generation policy for EGS. We evaluate the effectiveness of the proposed
algorithm by comparing it with state-of-the-art DAG scheduling heuristics and
an optimal mixed-integer linear programming baseline. Experimental results show
that the proposed algorithm outperforms the state-of-the-art by requiring fewer
processors to schedule the same DAG tasks.",http://arxiv.org/pdf/2308.14647v1
2308.14644v1,cs.RO,Human Comfortability Index Estimation in Industrial Human-Robot Collaboration Task,2023-08-28 15:16:35+00:00,"Fluent human-robot collaboration requires a robot teammate to understand,
learn, and adapt to the human's psycho-physiological state. Such collaborations
require a computing system that monitors human physiological signals during
human-robot collaboration (HRC) to quantitatively estimate a human's level of
comfort, which we have termed in this research as comfortability index (CI) and
uncomfortability index (unCI). Subjective metrics (surprise, anxiety, boredom,
calmness, and comfortability) and physiological signals were collected during a
human-robot collaboration experiment that varied robot behavior. The emotion
circumplex model is adapted to calculate the CI from the participant's
quantitative data as well as physiological data. To estimate CI/unCI from
physiological signals, time features were extracted from electrocardiogram
(ECG), galvanic skin response (GSR), and pupillometry signals. In this
research, we successfully adapt the circumplex model to find the location
(axis) of 'comfortability' and 'uncomfortability' on the circumplex model, and
its location match with the closest emotions on the circumplex model. Finally,
the study showed that the proposed approach can estimate human
comfortability/uncomfortability from physiological signals.",http://arxiv.org/pdf/2308.14644v1
2308.14642v1,cs.LG,Rate-Optimal Policy Optimization for Linear Markov Decision Processes,2023-08-28 15:16:09+00:00,"We study regret minimization in online episodic linear Markov Decision
Processes, and obtain rate-optimal $\widetilde O (\sqrt K)$ regret where $K$
denotes the number of episodes. Our work is the first to establish the optimal
(w.r.t.~$K$) rate of convergence in the stochastic setting with bandit feedback
using a policy optimization based approach, and the first to establish the
optimal (w.r.t.~$K$) rate in the adversarial setup with full information
feedback, for which no algorithm with an optimal rate guarantee is currently
known.",http://arxiv.org/pdf/2308.14642v1
2308.14632v1,cs.LG,Comparing AutoML and Deep Learning Methods for Condition Monitoring using Realistic Validation Scenarios,2023-08-28 14:57:29+00:00,"This study extensively compares conventional machine learning methods and
deep learning for condition monitoring tasks using an AutoML toolbox. The
experiments reveal consistent high accuracy in random K-fold cross-validation
scenarios across all tested models. However, when employing leave-one-group-out
(LOGO) cross-validation on the same datasets, no clear winner emerges,
indicating the presence of domain shift in real-world scenarios. Additionally,
the study assesses the scalability and interpretability of conventional methods
and neural networks. Conventional methods offer explainability with their
modular structure aiding feature identification. In contrast, neural networks
require specialized interpretation techniques like occlusion maps to visualize
important regions in the input data. Finally, the paper highlights the
significance of feature selection, particularly in condition monitoring tasks
with limited class variations. Low-complexity models prove sufficient for such
tasks, as only a few features from the input signal are typically needed. In
summary, these findings offer crucial insights into the strengths and
limitations of various approaches, providing valuable benchmarks and
identifying the most suitable methods for condition monitoring applications,
thereby enhancing their applicability in real-world scenarios.",http://arxiv.org/pdf/2308.14632v1
2308.14626v1,eess.IV,VesselShot: Few-shot learning for cerebral blood vessel segmentation,2023-08-28 14:48:49+00:00,"Angiography is widely used to detect, diagnose, and treat cerebrovascular
diseases. While numerous techniques have been proposed to segment the vascular
network from different imaging modalities, deep learning (DL) has emerged as a
promising approach. However, existing DL methods often depend on proprietary
datasets and extensive manual annotation. Moreover, the availability of
pre-trained networks specifically for medical domains and 3D volumes is
limited. To overcome these challenges, we propose a few-shot learning approach
called VesselShot for cerebrovascular segmentation. VesselShot leverages
knowledge from a few annotated support images and mitigates the scarcity of
labeled data and the need for extensive annotation in cerebral blood vessel
segmentation. We evaluated the performance of VesselShot using the publicly
available TubeTK dataset for the segmentation task, achieving a mean Dice
coefficient (DC) of 0.62(0.03).",http://arxiv.org/pdf/2308.14626v1
2308.14606v1,cs.LG,On the Tradeoff between Privacy Preservation and Byzantine-Robustness in Decentralized Learning,2023-08-28 14:20:53+00:00,"This paper jointly considers privacy preservation and Byzantine-robustness in
decentralized learning. In a decentralized network, honest-but-curious agents
faithfully follow the prescribed algorithm, but expect to infer their
neighbors' private data from messages received during the learning process,
while dishonest-and-Byzantine agents disobey the prescribed algorithm, and
deliberately disseminate wrong messages to their neighbors so as to bias the
learning process. For this novel setting, we investigate a generic
privacy-preserving and Byzantine-robust decentralized stochastic gradient
descent (SGD) framework, in which Gaussian noise is injected to preserve
privacy and robust aggregation rules are adopted to counteract Byzantine
attacks. We analyze its learning error and privacy guarantee, discovering an
essential tradeoff between privacy preservation and Byzantine-robustness in
decentralized learning -- the learning error caused by defending against
Byzantine attacks is exacerbated by the Gaussian noise added to preserve
privacy. Numerical experiments are conducted and corroborate our theoretical
findings.",http://arxiv.org/pdf/2308.14606v1
2308.14602v1,eess.SY,Recent Progress in Energy Management of Connected Hybrid Electric Vehicles Using Reinforcement Learning,2023-08-28 14:12:52+00:00,"The growing adoption of hybrid electric vehicles (HEVs) presents a
transformative opportunity for revolutionizing transportation energy systems.
The shift towards electrifying transportation aims to curb environmental
concerns related to fossil fuel consumption. This necessitates efficient energy
management systems (EMS) to optimize energy efficiency. The evolution of EMS
from HEVs to connected hybrid electric vehicles (CHEVs) represent a pivotal
shift. For HEVs, EMS now confronts the intricate energy cooperation
requirements of CHEVs, necessitating advanced algorithms for route
optimization, charging coordination, and load distribution. Challenges persist
in both domains, including optimal energy utilization for HEVs, and cooperative
eco-driving control (CED) for CHEVs across diverse vehicle types. Reinforcement
learning (RL) stands out as a promising tool for addressing these challenges at
hand. Specifically, within the realm of CHEVs, the application of multi-agent
reinforcement learning (MARL) emerges as a powerful approach for effectively
tackling the intricacies of CED control. Despite extensive research, few
reviews span from individual vehicles to multi-vehicle scenarios. This review
bridges the gap, highlighting challenges, advancements, and potential
contributions of RL-based solutions for future sustainable transportation
systems.",http://arxiv.org/pdf/2308.14602v1
2308.14601v1,cs.CY,Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery,2023-08-28 14:12:25+00:00,"As online music platforms grow, music recommender systems play a vital role
in helping users navigate and discover content within their vast musical
databases. At odds with this larger goal, is the presence of popularity bias,
which causes algorithmic systems to favor mainstream content over, potentially
more relevant, but niche items. In this work we explore the intrinsic
relationship between music discovery and popularity bias. To mitigate this
issue we propose a domain-aware, individual fairness-based approach which
addresses popularity bias in graph neural network (GNNs) based recommender
systems. Our approach uses individual fairness to reflect a ground truth
listening experience, i.e., if two songs sound similar, this similarity should
be reflected in their representations. In doing so, we facilitate meaningful
music discovery that is robust to popularity bias and grounded in the music
domain. We apply our BOOST methodology to two discovery based tasks, performing
recommendations at both the playlist level and user level. Then, we ground our
evaluation in the cold start setting, showing that our approach outperforms
existing fairness benchmarks in both performance and recommendation of
lesser-known content. Finally, our analysis explains why our proposed
methodology is a novel and promising approach to mitigating popularity bias and
improving the discovery of new and niche content in music recommender systems.",http://arxiv.org/pdf/2308.14601v1
2308.14597v1,cs.CV,Adversarial Attacks on Foundational Vision Models,2023-08-28 14:09:02+00:00,"Rapid progress is being made in developing large, pretrained, task-agnostic
foundational vision models such as CLIP, ALIGN, DINOv2, etc. In fact, we are
approaching the point where these models do not have to be finetuned
downstream, and can simply be used in zero-shot or with a lightweight probing
head. Critically, given the complexity of working at this scale, there is a
bottleneck where relatively few organizations in the world are executing the
training then sharing the models on centralized platforms such as HuggingFace
and torch.hub. The goal of this work is to identify several key adversarial
vulnerabilities of these models in an effort to make future designs more
robust. Intuitively, our attacks manipulate deep feature representations to
fool an out-of-distribution (OOD) detector which will be required when using
these open-world-aware models to solve closed-set downstream tasks. Our methods
reliably make in-distribution (ID) images (w.r.t. a downstream task) be
predicted as OOD and vice versa while existing in extremely
low-knowledge-assumption threat models. We show our attacks to be potent in
whitebox and blackbox settings, as well as when transferred across foundational
model types (e.g., attack DINOv2 with CLIP)! This work is only just the
beginning of a long journey towards adversarially robust foundational vision
models.",http://arxiv.org/pdf/2308.14597v1
2308.14596v1,cs.CV,LatentDR: Improving Model Generalization Through Sample-Aware Latent Degradation and Restoration,2023-08-28 14:08:42+00:00,"Despite significant advances in deep learning, models often struggle to
generalize well to new, unseen domains, especially when training data is
limited. To address this challenge, we propose a novel approach for
distribution-aware latent augmentation that leverages the relationships across
samples to guide the augmentation procedure. Our approach first degrades the
samples stochastically in the latent space, mapping them to augmented labels,
and then restores the samples from their corrupted versions during training.
This process confuses the classifier in the degradation step and restores the
overall class distribution of the original samples, promoting diverse
intra-class/cross-domain variability. We extensively evaluate our approach on a
diverse set of datasets and tasks, including domain generalization benchmarks
and medical imaging datasets with strong domain shift, where we show our
approach achieves significant improvements over existing methods for latent
space augmentation. We further show that our method can be flexibly adapted to
long-tail recognition tasks, demonstrating its versatility in building more
generalizable models. Code is available at
https://github.com/nerdslab/LatentDR.",http://arxiv.org/pdf/2308.14596v1
2308.14555v1,cs.LG,Kernel Limit of Recurrent Neural Networks Trained on Ergodic Data Sequences,2023-08-28 13:17:39+00:00,"Mathematical methods are developed to characterize the asymptotics of
recurrent neural networks (RNN) as the number of hidden units, data samples in
the sequence, hidden state updates, and training steps simultaneously grow to
infinity. In the case of an RNN with a simplified weight matrix, we prove the
convergence of the RNN to the solution of an infinite-dimensional ODE coupled
with the fixed point of a random algebraic equation. The analysis requires
addressing several challenges which are unique to RNNs. In typical mean-field
applications (e.g., feedforward neural networks), discrete updates are of
magnitude $\mathcal{O}(\frac{1}{N})$ and the number of updates is
$\mathcal{O}(N)$. Therefore, the system can be represented as an Euler
approximation of an appropriate ODE/PDE, which it will converge to as $N
\rightarrow \infty$. However, the RNN hidden layer updates are
$\mathcal{O}(1)$. Therefore, RNNs cannot be represented as a discretization of
an ODE/PDE and standard mean-field techniques cannot be applied. Instead, we
develop a fixed point analysis for the evolution of the RNN memory states, with
convergence estimates in terms of the number of update steps and the number of
hidden units. The RNN hidden layer is studied as a function in a Sobolev space,
whose evolution is governed by the data sequence (a Markov chain), the
parameter updates, and its dependence on the RNN hidden layer at the previous
time step. Due to the strong correlation between updates, a Poisson equation
must be used to bound the fluctuations of the RNN around its limit equation.
These mathematical methods give rise to the neural tangent kernel (NTK) limits
for RNNs trained on data sequences as the number of data samples and size of
the neural network grow to infinity.",http://arxiv.org/pdf/2308.14555v1
2308.14516v1,cs.LG,Prediction of Tourism Flow with Sparse Geolocation Data,2023-08-28 12:03:03+00:00,"Modern tourism in the 21st century is facing numerous challenges. Among these
the rapidly growing number of tourists visiting space-limited regions like
historical cities, museums and bottlenecks such as bridges is one of the
biggest. In this context, a proper and accurate prediction of tourism volume
and tourism flow within a certain area is important and critical for visitor
management tasks such as sustainable treatment of the environment and
prevention of overcrowding. Static flow control methods like conventional
low-level controllers or limiting access to overcrowded venues could not solve
the problem yet. In this paper, we empirically evaluate the performance of
state-of-the-art deep-learning methods such as RNNs, GNNs, and Transformers as
well as the classic statistical ARIMA method. Granular limited data supplied by
a tourism region is extended by exogenous data such as geolocation trajectories
of individual tourists, weather and holidays. In the field of visitor flow
prediction with sparse data, we are thereby capable of increasing the accuracy
of our predictions, incorporating modern input feature handling as well as
mapping geolocation data on top of discrete POI data.",http://arxiv.org/pdf/2308.14516v1
2308.14507v1,math.ST,Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing,2023-08-28 11:49:23+00:00,"We consider the problem of parameter estimation from observations given by a
generalized linear model. Spectral methods are a simple yet effective approach
for estimation: they estimate the parameter via the principal eigenvector of a
matrix obtained by suitably preprocessing the observations. Despite their wide
use, a rigorous performance characterization of spectral estimators, as well as
a principled way to preprocess the data, is available only for unstructured
(i.e., i.i.d. Gaussian and Haar) designs. In contrast, real-world design
matrices are highly structured and exhibit non-trivial correlations. To address
this problem, we consider correlated Gaussian designs which capture the
anisotropic nature of the measurements via a feature covariance matrix
$\Sigma$. Our main result is a precise asymptotic characterization of the
performance of spectral estimators in this setting. This then allows to
identify the optimal preprocessing that minimizes the number of samples needed
to meaningfully estimate the parameter. Remarkably, such an optimal spectral
estimator depends on $\Sigma$ only through its normalized trace, which can be
consistently estimated from the data. Numerical results demonstrate the
advantage of our principled approach over previous heuristic methods.
  Existing analyses of spectral estimators crucially rely on the rotational
invariance of the design matrix. This key assumption does not hold for
correlated Gaussian designs. To circumvent this difficulty, we develop a novel
strategy based on designing and analyzing an approximate message passing
algorithm whose fixed point coincides with the desired spectral estimator. Our
methodology is general, and opens the way to the precise characterization of
spiked matrices and of the corresponding spectral methods in a variety of
settings.",http://arxiv.org/pdf/2308.14507v1
2308.14486v1,cs.SI,Rebalancing Social Feed to Minimize Polarization and Disagreement,2023-08-28 10:59:05+00:00,"Social media have great potential for enabling public discourse on important
societal issues. However, adverse effects, such as polarization and echo
chambers, greatly impact the benefits of social media and call for algorithms
that mitigate these effects. In this paper, we propose a novel problem
formulation aimed at slightly nudging users' social feeds in order to strike a
balance between relevance and diversity, thus mitigating the emergence of
polarization, without lowering the quality of the feed. Our approach is based
on re-weighting the relative importance of the accounts that a user follows, so
as to calibrate the frequency with which the content produced by various
accounts is shown to the user. We analyze the convexity properties of the
problem, demonstrating the non-matrix convexity of the objective function and
the convexity of the feasible set. To efficiently address the problem, we
develop a scalable algorithm based on projected gradient descent. We also prove
that our problem statement is a proper generalization of the undirected-case
problem so that our method can also be adopted for undirected social networks.
As a baseline for comparison in the undirected case, we develop a semidefinite
programming approach, which provides the optimal solution. Through extensive
experiments on synthetic and real-world datasets, we validate the effectiveness
of our approach, which outperforms non-trivial baselines, underscoring its
ability to foster healthier and more cohesive online communities.",http://arxiv.org/pdf/2308.14486v1
2308.14481v1,cs.LG,Group Regression for Query Based Object Detection and Tracking,2023-08-28 10:43:53+00:00,"Group regression is commonly used in 3D object detection to predict box
parameters of similar classes in a joint head, aiming to benefit from
similarities while separating highly dissimilar classes. For query-based
perception methods, this has, so far, not been feasible. We close this gap and
present a method to incorporate multi-class group regression, especially
designed for the 3D domain in the context of autonomous driving, into existing
attention and query-based perception approaches. We enhance a transformer based
joint object detection and tracking model with this approach, and thoroughly
evaluate its behavior and performance. For group regression, the classes of the
nuScenes dataset are divided into six groups of similar shape and prevalence,
each being regressed by a dedicated head. We show that the proposed method is
applicable to many existing transformer based perception approaches and can
bring potential benefits. The behavior of query group regression is thoroughly
analyzed in comparison to a unified regression head, e.g. in terms of
class-switching behavior and distribution of the output parameters. The
proposed method offers many possibilities for further research, such as in the
direction of deep multi-hypotheses tracking.",http://arxiv.org/pdf/2308.14481v1
2308.14478v1,stat.ML,Some issues in robust clustering,2023-08-28 10:35:04+00:00,"Some key issues in robust clustering are discussed with focus on Gaussian
mixture model based clustering, namely the formal definition of outliers,
ambiguity between groups of outliers and clusters, the interaction between
robust clustering and the estimation of the number of clusters, the essential
dependence of (not only) robust clustering on tuning decisions, and
shortcomings of existing measurements of cluster stability when it comes to
outliers.",http://arxiv.org/pdf/2308.14478v1
2308.14456v1,eess.AS,Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads,2023-08-28 09:49:48+00:00,"Self-supervised learning (SSL) leverages large datasets of unlabeled speech
to reach impressive performance with reduced amounts of annotated data. The
high number of proposed approaches fostered the emergence of comprehensive
benchmarks that evaluate their performance on a set of downstream tasks
exploring various aspects of the speech signal. However, while the number of
considered tasks has been growing, most proposals rely upon a single downstream
architecture that maps the frozen SSL representations to the task labels. This
study examines how benchmarking results are affected by changes in the probing
head architecture. Interestingly, we found that altering the downstream
architecture structure leads to significant fluctuations in the performance
ranking of the evaluated models. Against common practices in speech SSL
benchmarking, we evaluate larger-capacity probing heads, showing their impact
on performance, inference costs, generalization and multi-level feature
exploitation.",http://arxiv.org/pdf/2308.14456v1
2308.14430v1,eess.AS,TextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models,2023-08-28 09:06:32+00:00,"Recently, there has been a growing interest in the field of controllable
Text-to-Speech (TTS). While previous studies have relied on users providing
specific style factor values based on acoustic knowledge or selecting reference
speeches that meet certain requirements, generating speech solely from natural
text prompts has emerged as a new challenge for researchers. This challenge
arises due to the scarcity of high-quality speech datasets with natural text
style prompt and the absence of advanced text-controllable TTS models. In light
of this, 1) we propose TextrolSpeech, which is the first large-scale speech
emotion dataset annotated with rich text attributes. The dataset comprises
236,220 pairs of style prompt in natural text descriptions with five style
factors and corresponding speech samples. Through iterative experimentation, we
introduce a multi-stage prompt programming approach that effectively utilizes
the GPT model for generating natural style descriptions in large volumes. 2)
Furthermore, to address the need for generating audio with greater style
diversity, we propose an efficient architecture called Salle. This architecture
treats text controllable TTS as a language model task, utilizing audio codec
codes as an intermediate representation to replace the conventional
mel-spectrogram. Finally, we successfully demonstrate the ability of the
proposed model by showing a comparable performance in the controllable TTS
task. Audio samples are available at https://sall-e.github.io/",http://arxiv.org/pdf/2308.14430v1
2308.14412v1,cs.LG,Task-Aware Machine Unlearning and Its Application in Load Forecasting,2023-08-28 08:50:12+00:00,"Data privacy and security have become a non-negligible factor in load
forecasting. Previous researches mainly focus on training stage enhancement.
However, once the model is trained and deployed, it may need to `forget' (i.e.,
remove the impact of) part of training data if the data is found to be
malicious or as requested by the data owner. This paper introduces machine
unlearning algorithm which is specifically designed to remove the influence of
part of the original dataset on an already trained forecaster. However, direct
unlearning inevitably degrades the model generalization ability. To balance
between unlearning completeness and performance degradation, a
performance-aware algorithm is proposed by evaluating the sensitivity of local
model parameter change using influence function and sample re-weighting.
Moreover, we observe that the statistic criterion cannot fully reflect the
operation cost of down-stream tasks. Therefore, a task-aware machine unlearning
is proposed whose objective is a tri-level optimization with dispatch and
redispatch problems considered. We theoretically prove the existence of the
gradient of such objective, which is key to re-weighting the remaining samples.
We test the unlearning algorithms on linear and neural network load forecasters
with realistic load dataset. The simulation demonstrates the balance on
unlearning completeness and operational cost. All codes can be found at
https://github.com/xuwkk/task_aware_machine_unlearning.",http://arxiv.org/pdf/2308.14412v1
2308.14409v1,cs.CV,Steerable Conditional Diffusion for Out-of-Distribution Adaptation in Imaging Inverse Problems,2023-08-28 08:47:06+00:00,"Denoising diffusion models have emerged as the go-to framework for solving
inverse problems in imaging. A critical concern regarding these models is their
performance on out-of-distribution (OOD) tasks, which remains an under-explored
challenge. Realistic reconstructions inconsistent with the measured data can be
generated, hallucinating image features that are uniquely present in the
training dataset. To simultaneously enforce data-consistency and leverage
data-driven priors, we introduce a novel sampling framework called Steerable
Conditional Diffusion. This framework adapts the denoising network specifically
to the available measured data. Utilising our proposed method, we achieve
substantial enhancements in OOD performance across diverse imaging modalities,
advancing the robust deployment of denoising diffusion models in real-world
applications.",http://arxiv.org/pdf/2308.14409v1
2308.14407v1,physics.optics,Identifying topology of leaky photonic lattices with machine learning,2023-08-28 08:42:06+00:00,"We show how machine learning techniques can be applied for the classification
of topological phases in leaky photonic lattices using limited measurement
data. We propose an approach based solely on bulk intensity measurements, thus
exempt from the need for complicated phase retrieval procedures. In particular,
we design a fully connected neural network that accurately determines
topological properties from the output intensity distribution in dimerized
waveguide arrays with leaky channels, after propagation of a spatially
localized initial excitation at a finite distance, in a setting that closely
emulates realistic experimental conditions.",http://arxiv.org/pdf/2308.14407v1
2308.14400v1,cs.CV,Semi-Supervised Semantic Depth Estimation using Symbiotic Transformer and NearFarMix Augmentation,2023-08-28 08:33:45+00:00,"In computer vision, depth estimation is crucial for domains like robotics,
autonomous vehicles, augmented reality, and virtual reality. Integrating
semantics with depth enhances scene understanding through reciprocal
information sharing. However, the scarcity of semantic information in datasets
poses challenges. Existing convolutional approaches with limited local
receptive fields hinder the full utilization of the symbiotic potential between
depth and semantics. This paper introduces a dataset-invariant semi-supervised
strategy to address the scarcity of semantic information. It proposes the Depth
Semantics Symbiosis module, leveraging the Symbiotic Transformer for achieving
comprehensive mutual awareness by information exchange within both local and
global contexts. Additionally, a novel augmentation, NearFarMix is introduced
to combat overfitting and compensate both depth-semantic tasks by strategically
merging regions from two images, generating diverse and structurally consistent
samples with enhanced control. Extensive experiments on NYU-Depth-V2 and KITTI
datasets demonstrate the superiority of our proposed techniques in indoor and
outdoor environments.",http://arxiv.org/pdf/2308.14400v1
2308.14388v1,stat.ML,Biclustering Methods via Sparse Penalty,2023-08-28 08:07:57+00:00,"In this paper, we first reviewed several biclustering methods that are used
to identify the most significant clusters in gene expression data. Here we
mainly focused on the SSVD(sparse SVD) method and tried a new sparse penalty
named ""Prenet penalty"" which has been used only in factor analysis to gain
sparsity. Then in the simulation study, we tried different types of generated
datasets (with different sparsity and dimension) and tried 1-layer
approximation then for k-layers which shows the mixed Prenet penalty is very
effective for non-overlapped data. Finally, we used some real gene expression
data to show the behavior of our methods.",http://arxiv.org/pdf/2308.14388v1
2308.14380v1,cs.LG,Self-Supervision for Tackling Unsupervised Anomaly Detection: Pitfalls and Opportunities,2023-08-28 07:55:01+00:00,"Self-supervised learning (SSL) is a growing torrent that has recently
transformed machine learning and its many real world applications, by learning
on massive amounts of unlabeled data via self-generated supervisory signals.
Unsupervised anomaly detection (AD) has also capitalized on SSL, by
self-generating pseudo-anomalies through various data augmentation functions or
external data exposure. In this vision paper, we first underline the importance
of the choice of SSL strategies on AD performance, by presenting evidences and
studies from the AD literature. Equipped with the understanding that SSL incurs
various hyperparameters (HPs) to carefully tune, we present recent developments
on unsupervised model selection and augmentation tuning for SSL-based AD. We
then highlight emerging challenges and future opportunities; on designing new
pretext tasks and augmentation functions for different data modalities,
creating novel model selection solutions for systematically tuning the SSL HPs,
as well as on capitalizing on the potential of pretrained foundation models on
AD through effective density estimation.",http://arxiv.org/pdf/2308.14380v1
2308.14377v1,cs.LG,Meta Attentive Graph Convolutional Recurrent Network for Traffic Forecasting,2023-08-28 07:49:30+00:00,"Traffic forecasting is a fundamental problem in intelligent transportation
systems. Existing traffic predictors are limited by their expressive power to
model the complex spatial-temporal dependencies in traffic data, mainly due to
the following limitations. Firstly, most approaches are primarily designed to
model the local shared patterns, which makes them insufficient to capture the
specific patterns associated with each node globally. Hence, they fail to learn
each node's unique properties and diversified patterns. Secondly, most existing
approaches struggle to accurately model both short- and long-term dependencies
simultaneously. In this paper, we propose a novel traffic predictor, named Meta
Attentive Graph Convolutional Recurrent Network (MAGCRN). MAGCRN utilizes a
Graph Convolutional Recurrent Network (GCRN) as a core module to model local
dependencies and improves its operation with two novel modules: 1) a
Node-Specific Meta Pattern Learning (NMPL) module to capture node-specific
patterns globally and 2) a Node Attention Weight Generation Module (NAWG)
module to capture short- and long-term dependencies by connecting the
node-specific features with the ones learned initially at each time step during
GCRN operation. Experiments on six real-world traffic datasets demonstrate that
NMPL and NAWG together enable MAGCRN to outperform state-of-the-art baselines
on both short- and long-term predictions.",http://arxiv.org/pdf/2308.14377v1
2308.14374v1,cs.LG,Online Continual Learning on Hierarchical Label Expansion,2023-08-28 07:42:26+00:00,"Continual learning (CL) enables models to adapt to new tasks and environments
without forgetting previously learned knowledge. While current CL setups have
ignored the relationship between labels in the past task and the new task with
or without small task overlaps, real-world scenarios often involve hierarchical
relationships between old and new tasks, posing another challenge for
traditional CL approaches. To address this challenge, we propose a novel
multi-level hierarchical class incremental task configuration with an online
learning constraint, called hierarchical label expansion (HLE). Our
configuration allows a network to first learn coarse-grained classes, with data
labels continually expanding to more fine-grained classes in various hierarchy
depths. To tackle this new setup, we propose a rehearsal-based method that
utilizes hierarchy-aware pseudo-labeling to incorporate hierarchical class
information. Additionally, we propose a simple yet effective memory management
and sampling strategy that selectively adopts samples of newly encountered
classes. Our experiments demonstrate that our proposed method can effectively
use hierarchy on our HLE setup to improve classification accuracy across all
levels of hierarchies, regardless of depth and class imbalance ratio,
outperforming prior state-of-the-art works by significant margins while also
outperforming them on the conventional disjoint, blurry and i-Blurry CL setups.",http://arxiv.org/pdf/2308.14374v1
2308.14355v1,cs.LG,Can Transformer and GNN Help Each Other?,2023-08-28 07:03:08+00:00,"Although Transformer has achieved great success in natural language process
and computer vision, it has difficulty generalizing to medium and large-scale
graph data for two important reasons: (i) High complexity. (ii) Failing to
capture the complex and entangled structure information. In graph
representation learning, Graph Neural Networks(GNNs) can fuse the graph
structure and node attributes but have limited receptive fields. Therefore, we
question whether can we combine Transformers and GNNs to help each other. In
this paper, we propose a new model named TransGNN where the Transformer layer
and GNN layer are used alternately to improve each other. Specifically, to
expand the receptive field and disentangle the information aggregation from
edges, we propose using Transformer to aggregate more relevant nodes'
information to improve the message passing of GNNs. Besides, to capture the
graph structure information, we utilize positional encoding and make use of the
GNN layer to fuse the structure into node attributes, which improves the
Transformer in graph data. We also propose to sample the most relevant nodes
for Transformer and two efficient samples update strategies to lower the
complexity. At last, we theoretically prove that TransGNN is more expressive
than GNNs only with extra linear complexity. The experiments on eight datasets
corroborate the effectiveness of TransGNN on node and graph classification
tasks.",http://arxiv.org/pdf/2308.14355v1
2308.14350v1,cs.LG,Simple Modification of the Upper Confidence Bound Algorithm by Generalized Weighted Averages,2023-08-28 06:53:31+00:00,"The multi-armed bandit (MAB) problem is a classical problem that models
sequential decision-making under uncertainty in reinforcement learning. In this
study, we propose a new generalized upper confidence bound (UCB) algorithm
(GWA-UCB1) by extending UCB1, which is a representative algorithm for MAB
problems, using generalized weighted averages, and present an effective
algorithm for various problem settings. GWA-UCB1 is a two-parameter
generalization of the balance between exploration and exploitation in UCB1 and
can be implemented with a simple modification of the UCB1 formula. Therefore,
this algorithm can be easily applied to UCB-based reinforcement learning
models. In preliminary experiments, we investigated the optimal parameters of a
simple generalized UCB1 (G-UCB1), prepared for comparison and GWA-UCB1, in a
stochastic MAB problem with two arms. Subsequently, we confirmed the
performance of the algorithms with the investigated parameters on stochastic
MAB problems when arm reward probabilities were sampled from uniform or normal
distributions and on survival MAB problems assuming more realistic situations.
GWA-UCB1 outperformed G-UCB1, UCB1-Tuned, and Thompson sampling in most problem
settings and can be useful in many situations. The code is available at
https://github.com/manome/python-mab.",http://arxiv.org/pdf/2308.14350v1
2308.14348v1,eess.SY,Label-free Deep Learning Driven Secure Access Selection in Space-Air-Ground Integrated Networks,2023-08-28 06:48:06+00:00,"In Space-air-ground integrated networks (SAGIN), the inherent openness and
extensive broadcast coverage expose these networks to significant eavesdropping
threats. Considering the inherent co-channel interference due to spectrum
sharing among multi-tier access networks in SAGIN, it can be leveraged to
assist the physical layer security among heterogeneous transmissions. However,
it is challenging to conduct a secrecy-oriented access strategy due to both
heterogeneous resources and different eavesdropping models. In this paper, we
explore secure access selection for a scenario involving multi-mode users
capable of accessing satellites, unmanned aerial vehicles, or base stations in
the presence of eavesdroppers. Particularly, we propose a Q-network
approximation based deep learning approach for selecting the optimal access
strategy for maximizing the sum secrecy rate. Meanwhile, the power optimization
is also carried out by an unsupervised learning approach to improve the secrecy
performance. Remarkably, two neural networks are trained by unsupervised
learning and Q-network approximation which are both label-free methods without
knowing the optimal solution as labels. Numerical results verify the efficiency
of our proposed power optimization approach and access strategy, leading to
enhanced secure transmission performance.",http://arxiv.org/pdf/2308.14348v1
2308.14343v1,stat.ML,Buy when? Survival machine learning model comparison for purchase timing,2023-08-28 06:40:02+00:00,"The value of raw data is unlocked by converting it into information and
knowledge that drives decision-making. Machine Learning (ML) algorithms are
capable of analysing large datasets and making accurate predictions. Market
segmentation, client lifetime value, and marketing techniques have all made use
of machine learning. This article examines marketing machine learning
techniques such as Support Vector Machines, Genetic Algorithms, Deep Learning,
and K-Means. ML is used to analyse consumer behaviour, propose items, and make
other customer choices about whether or not to purchase a product or service,
but it is seldom used to predict when a person will buy a product or a basket
of products. In this paper, the survival models Kernel SVM, DeepSurv, Survival
Random Forest, and MTLR are examined to predict tine-purchase individual
decisions. Gender, Income, Location, PurchaseHistory, OnlineBehavior,
Interests, PromotionsDiscounts and CustomerExperience all have an influence on
purchasing time, according to the analysis. The study shows that the DeepSurv
model predicted purchase completion the best. These insights assist marketers
in increasing conversion rates.",http://arxiv.org/pdf/2308.14343v1
2308.14340v1,cs.LG,HRGCN: Heterogeneous Graph-level Anomaly Detection with Hierarchical Relation-augmented Graph Neural Networks,2023-08-28 06:32:09+00:00,"This work considers the problem of heterogeneous graph-level anomaly
detection. Heterogeneous graphs are commonly used to represent behaviours
between different types of entities in complex industrial systems for capturing
as much information about the system operations as possible. Detecting
anomalous heterogeneous graphs from a large set of system behaviour graphs is
crucial for many real-world applications like online web/mobile service and
cloud access control. To address the problem, we propose HRGCN, an unsupervised
deep heterogeneous graph neural network, to model complex heterogeneous
relations between different entities in the system for effectively identifying
these anomalous behaviour graphs. HRGCN trains a hierarchical
relation-augmented Heterogeneous Graph Neural Network (HetGNN), which learns
better graph representations by modelling the interactions among all the system
entities and considering both source-to-destination entity (node) types and
their relation (edge) types. Extensive evaluation on two real-world application
datasets shows that HRGCN outperforms state-of-the-art competing anomaly
detection approaches. We further present a real-world industrial case study to
justify the effectiveness of HRGCN in detecting anomalous (e.g., congested)
network devices in a mobile communication service. HRGCN is available at
https://github.com/jiaxililearn/HRGCN.",http://arxiv.org/pdf/2308.14340v1
2308.14322v1,cs.LG,Machine Unlearning Methodology base on Stochastic Teacher Network,2023-08-28 06:05:23+00:00,"The rise of the phenomenon of the ""right to be forgotten"" has prompted
research on machine unlearning, which grants data owners the right to actively
withdraw data that has been used for model training, and requires the
elimination of the contribution of that data to the model. A simple method to
achieve this is to use the remaining data to retrain the model, but this is not
acceptable for other data owners who continue to participate in training.
Existing machine unlearning methods have been found to be ineffective in
quickly removing knowledge from deep learning models. This paper proposes using
a stochastic network as a teacher to expedite the mitigation of the influence
caused by forgotten data on the model. We performed experiments on three
datasets, and the findings demonstrate that our approach can efficiently
mitigate the influence of target data on the model within a single epoch. This
allows for one-time erasure and reconstruction of the model, and the
reconstruction model achieves the same performance as the retrained model.",http://arxiv.org/pdf/2308.14322v1
2308.14304v1,cs.LG,Solving Attention Kernel Regression Problem via Pre-conditioner,2023-08-28 04:37:38+00:00,"Large language models have shown impressive performance in many tasks. One of
the major features from the computation perspective is computing the attention
matrix. Previous works [Zandieh, Han, Daliri, and Karba 2023, Alman and Song
2023] have formally studied the possibility and impossibility of approximating
the attention matrix. In this work, we define and study a new problem which is
called the attention kernel regression problem. We show how to solve the
attention kernel regression in the input sparsity time of the data matrix.",http://arxiv.org/pdf/2308.14304v1
2308.14267v1,cs.LG,Unleash Model Potential: Bootstrapped Meta Self-supervised Learning,2023-08-28 02:49:07+00:00,"The long-term goal of machine learning is to learn general visual
representations from a small amount of data without supervision, mimicking
three advantages of human cognition: i) no need for labels, ii) robustness to
data scarcity, and iii) learning from experience. Self-supervised learning and
meta-learning are two promising techniques to achieve this goal, but they both
only partially capture the advantages and fail to address all the problems.
Self-supervised learning struggles to overcome the drawbacks of data scarcity,
while ignoring prior knowledge that can facilitate learning and generalization.
Meta-learning relies on supervised information and suffers from a bottleneck of
insufficient learning. To address these issues, we propose a novel Bootstrapped
Meta Self-Supervised Learning (BMSSL) framework that aims to simulate the human
learning process. We first analyze the close relationship between meta-learning
and self-supervised learning. Based on this insight, we reconstruct tasks to
leverage the strengths of both paradigms, achieving advantages i and ii.
Moreover, we employ a bi-level optimization framework that alternates between
solving specific tasks with a learned ability (first level) and improving this
ability (second level), attaining advantage iii. To fully harness its power, we
introduce a bootstrapped target based on meta-gradient to make the model its
own teacher. We validate the effectiveness of our approach with comprehensive
theoretical and empirical study.",http://arxiv.org/pdf/2308.14267v1
2308.14258v1,cs.LG,Breaking Boundaries: Distributed Domain Decomposition with Scalable Physics-Informed Neural PDE Solvers,2023-08-28 02:25:11+00:00,"Mosaic Flow is a novel domain decomposition method designed to scale
physics-informed neural PDE solvers to large domains. Its unique approach
leverages pre-trained networks on small domains to solve partial differential
equations on large domains purely through inference, resulting in high
reusability. This paper presents an end-to-end parallelization of Mosaic Flow,
combining data parallel training and domain parallelism for inference on
large-scale problems. By optimizing the network architecture and data parallel
training, we significantly reduce the training time for learning the Laplacian
operator to minutes on 32 GPUs. Moreover, our distributed domain decomposition
algorithm enables scalable inferences for solving the Laplace equation on
domains 4096 times larger than the training domain, demonstrating strong
scaling while maintaining accuracy on 32 GPUs. The reusability of Mosaic Flow,
combined with the improved performance achieved through the distributed-memory
algorithms, makes it a promising tool for modeling complex physical phenomena
and accelerating scientific discovery.",http://arxiv.org/pdf/2308.14258v1
2308.14245v1,cs.LG,A Comparison of Personalized and Generalized Approaches to Emotion Recognition Using Consumer Wearable Devices: Machine Learning Study,2023-08-28 01:21:22+00:00,"Background: Studies have shown the potential adverse health effects, ranging
from headaches to cardiovascular disease, associated with long-term negative
emotions and chronic stress. Since many indicators of stress are imperceptible
to observers, the early detection and intervention of stress remains a pressing
medical need. Physiological signals offer a non-invasive method of monitoring
emotions and are easily collected by smartwatches. Existing research primarily
focuses on developing generalized machine learning-based models for emotion
classification. Objective: We aim to study the differences between personalized
and generalized machine learning models for three-class emotion classification
(neutral, stress, and amusement) using wearable biosignal data. Methods: We
developed a convolutional encoder for the three-class emotion classification
problem using data from WESAD, a multimodal dataset with physiological signals
for 15 subjects. We compared the results between a subject-exclusive
generalized, subject-inclusive generalized, and personalized model. Results:
For the three-class classification problem, our personalized model achieved an
average accuracy of 95.06% and F1-score of 91.71, our subject-inclusive
generalized model achieved an average accuracy of 66.95% and F1-score of 42.50,
and our subject-exclusive generalized model achieved an average accuracy of
67.65% and F1-score of 43.05. Conclusions: Our results emphasize the need for
increased research in personalized emotion recognition models given that they
outperform generalized models in certain contexts. We also demonstrate that
personalized machine learning models for emotion classification are viable and
can achieve high performance.",http://arxiv.org/pdf/2308.14245v1
2308.14239v1,quant-ph,Quantum Next Generation Reservoir Computing: An Efficient Quantum Algorithm for Forecasting Quantum Dynamics,2023-08-28 00:34:40+00:00,"Next Generation Reservoir Computing (NG-RC) is a modern class of model-free
machine learning that enables an accurate forecasting of time series data
generated by dynamical systems. We demonstrate that NG-RC can accurately
predict full many-body quantum dynamics, instead of merely concentrating on the
dynamics of observables, which is the conventional application of reservoir
computing. In addition, we apply a technique which we refer to as skipping
ahead to predict far future states accurately without the need to extract
information about the intermediate states. However, adopting a classical NG-RC
for many-body quantum dynamics prediction is computationally prohibitive due to
the large Hilbert space of sample input data. In this work, we propose an
end-to-end quantum algorithm for many-body quantum dynamics forecasting with a
quantum computational speedup via the block-encoding technique. This proposal
presents an efficient model-free quantum scheme to forecast quantum dynamics
coherently, bypassing inductive biases incurred in a model-based approach.",http://arxiv.org/pdf/2308.14239v1
2308.14220v1,cs.LG,On Active Learning for Gaussian Process-based Global Sensitivity Analysis,2023-08-27 22:42:31+00:00,"This paper explores the application of active learning strategies to
adaptively learn Sobol indices for global sensitivity analysis. We demonstrate
that active learning for Sobol indices poses unique challenges due to the
definition of the Sobol index as a ratio of variances estimated from Gaussian
process surrogates. Consequently, learning strategies must either focus on
convergence in the numerator or the denominator of this ratio. However, rapid
convergence in either one does not guarantee convergence in the Sobol index. We
propose a novel strategy for active learning that focuses on resolving the main
effects of the Gaussian process (associated with the numerator of the Sobol
index) and compare this with existing strategies based on convergence in the
total variance (the denominator of the Sobol index). The new strategy,
implemented through a new learning function termed the MUSIC (minimize
uncertainty in Sobol index convergence), generally converges in Sobol index
error more rapidly than the existing strategies based on the Expected
Improvement for Global Fit (EIGF) and the Variance Improvement for Global Fit
(VIGF). Both strategies are compared with simple sequential random sampling and
the MUSIC learning function generally converges most rapidly for
low-dimensional problems. However, for high-dimensional problems, the
performance is comparable to random sampling. The new learning strategy is
demonstrated for a practical case of adaptive experimental design for
large-scale Boundary Layer Wind Tunnel experiments.",http://arxiv.org/pdf/2308.14220v1
2308.14216v1,cs.LG,Machine Learning for Administrative Health Records: A Systematic Review of Techniques and Applications,2023-08-27 22:34:10+00:00,"Machine learning provides many powerful and effective techniques for
analysing heterogeneous electronic health records (EHR). Administrative Health
Records (AHR) are a subset of EHR collected for administrative purposes, and
the use of machine learning on AHRs is a growing subfield of EHR analytics.
Existing reviews of EHR analytics emphasise that the data-modality of the EHR
limits the breadth of suitable machine learning techniques, and pursuable
healthcare applications. Despite emphasising the importance of data modality,
the literature fails to analyse which techniques and applications are relevant
to AHRs. AHRs contain uniquely well-structured, categorically encoded records
which are distinct from other data-modalities captured by EHRs, and they can
provide valuable information pertaining to how patients interact with the
healthcare system.
  This paper systematically reviews AHR-based research, analysing 70 relevant
studies and spanning multiple databases. We identify and analyse which machine
learning techniques are applied to AHRs and which health informatics
applications are pursued in AHR-based research. We also analyse how these
techniques are applied in pursuit of each application, and identify the
limitations of these approaches. We find that while AHR-based studies are
disconnected from each other, the use of AHRs in health informatics research is
substantial and accelerating. Our synthesis of these studies highlights the
utility of AHRs for pursuing increasingly complex and diverse research
objectives despite a number of pervading data- and technique-based limitations.
Finally, through our findings, we propose a set of future research directions
that can enhance the utility of AHR data and machine learning techniques for
health informatics research.",http://arxiv.org/pdf/2308.14216v1
2308.14215v1,cs.LG,TimeTrail: Unveiling Financial Fraud Patterns through Temporal Correlation Analysis,2023-08-27 22:27:57+00:00,"In the field of financial fraud detection, understanding the underlying
patterns and dynamics is important to ensure effective and reliable systems.
This research introduces a new technique, ""TimeTrail,"" which employs advanced
temporal correlation analysis to explain complex financial fraud patterns. The
technique leverages time-related insights to provide transparent and
interpretable explanations for fraud detection decisions, enhancing
accountability and trust.
  The ""TimeTrail"" methodology consists of three key phases: temporal data
enrichment, dynamic correlation analysis, and interpretable pattern
visualization. Initially, raw financial transaction data is enriched with
temporal attributes. Dynamic correlations between these attributes are then
quantified using innovative statistical measures. Finally, a unified
visualization framework presents these correlations in an interpretable manner.
To validate the effectiveness of ""TimeTrail,"" a study is conducted on a diverse
financial dataset, surrounding various fraud scenarios. Results demonstrate the
technique's capability to uncover hidden temporal correlations and patterns,
performing better than conventional methods in both accuracy and
interpretability. Moreover, a case study showcasing the application of
""TimeTrail"" in real-world scenarios highlights its utility for fraud detection.",http://arxiv.org/pdf/2308.14215v1
2308.14207v1,stat.ML,Predictive Sparse Manifold Transform,2023-08-27 21:25:45+00:00,"We present Predictive Sparse Manifold Transform (PSMT), a minimalistic,
interpretable and biologically plausible framework for learning and predicting
natural dynamics. PSMT incorporates two layers where the first sparse coding
layer represents the input sequence as sparse coefficients over an overcomplete
dictionary and the second manifold learning layer learns a geometric embedding
space that captures topological similarity and dynamic temporal linearity in
sparse coefficients. We apply PSMT on a natural video dataset and evaluate the
reconstruction performance with respect to contextual variability, the number
of sparse coding basis functions and training samples. We then interpret the
dynamic topological organization in the embedding space. We next utilize PSMT
to predict future frames compared with two baseline methods with a static
embedding space. We demonstrate that PSMT with a dynamic embedding space can
achieve better prediction performance compared to static baselines. Our work
establishes that PSMT is an efficient unsupervised generative framework for
prediction of future visual stimuli.",http://arxiv.org/pdf/2308.14207v1
2308.14175v1,cs.LG,Leveraging Linear Independence of Component Classifiers: Optimizing Size and Prediction Accuracy for Online Ensembles,2023-08-27 18:38:09+00:00,"Ensembles, which employ a set of classifiers to enhance classification
accuracy collectively, are crucial in the era of big data. However, although
there is general agreement that the relation between ensemble size and its
prediction accuracy, the exact nature of this relationship is still unknown. We
introduce a novel perspective, rooted in the linear independence of
classifier's votes, to analyze the interplay between ensemble size and
prediction accuracy. This framework reveals a theoretical link, consequently
proposing an ensemble size based on this relationship. Our study builds upon a
geometric framework and develops a series of theorems. These theorems clarify
the role of linear dependency in crafting ensembles. We present a method to
determine the minimum ensemble size required to ensure a target probability of
linearly independent votes among component classifiers. Incorporating real and
synthetic datasets, our empirical results demonstrate a trend: increasing the
number of classifiers enhances accuracy, as predicted by our theoretical
insights. However, we also identify a point of diminishing returns, beyond
which additional classifiers provide diminishing improvements in accuracy.
Surprisingly, the calculated ideal ensemble size deviates from empirical
results for certain datasets, emphasizing the influence of other factors. This
study opens avenues for deeper investigations into the complex dynamics
governing ensemble design and offers guidance for constructing efficient and
effective ensembles in practical scenarios.",http://arxiv.org/pdf/2308.14175v1
2308.14174v1,eess.SP,Integrated Approach of Gearbox Fault Diagnosis,2023-08-27 18:35:46+00:00,"Gearbox fault diagnosis is one of the most important parts in any industrial
systems. Failure of components inside gearbox can lead to a catastrophic
failure, uneven breakdown, and financial losses in industrial organization. In
that case intelligent maintenance of the gearbox comes into context. This paper
presents an integrated gearbox fault diagnosis approach which can easily deploy
in online condition monitoring. This work introduces a nonparametric data
preprocessing technique i.e., calculus enhanced energy operator (CEEO) to
preserve the characteristics frequencies in the noisy and inferred vibrational
signal. A set of time domain and spectral domain features are calculated from
the raw and CEEO vibration signal and inputted to the multiclass support vector
machine (MCSVM) to diagnose the faults on the system. An effective comparison
between raw signal and CEEO signal are presented to show the impact of CEEO in
gearbox fault diagnosis. The obtained results of this work look very promising
and can be implemented in any type of industrial system due to its
nonparametric nature.",http://arxiv.org/pdf/2308.14174v1
2308.14144v1,cs.LG,Learning end-to-end inversion of circular Radon transforms in the partial radial setup,2023-08-27 15:57:08+00:00,"We present a deep learning-based computational algorithm for inversion of
circular Radon transforms in the partial radial setup, arising in photoacoustic
tomography. We first demonstrate that the truncated singular value
decomposition-based method, which is the only traditional algorithm available
to solve this problem, leads to severe artifacts which renders the
reconstructed field as unusable. With the objective of overcoming this
computational bottleneck, we train a ResBlock based U-Net to recover the
inferred field that directly operates on the measured data. Numerical results
with augmented Shepp-Logan phantoms, in the presence of noisy full and limited
view data, demonstrate the superiority of the proposed algorithm.",http://arxiv.org/pdf/2308.14144v1
2308.14142v1,stat.ML,Integrated Variational Fourier Features for Fast Spatial Modelling with Gaussian Processes,2023-08-27 15:44:28+00:00,"Sparse variational approximations are popular methods for scaling up
inference and learning in Gaussian processes to larger datasets. For $N$
training points, exact inference has $O(N^3)$ cost; with $M \ll N$ features,
state of the art sparse variational methods have $O(NM^2)$ cost. Recently,
methods have been proposed using more sophisticated features; these promise
$O(M^3)$ cost, with good performance in low dimensional tasks such as spatial
modelling, but they only work with a very limited class of kernels, excluding
some of the most commonly used. In this work, we propose integrated Fourier
features, which extends these performance benefits to a very broad class of
stationary covariance functions. We motivate the method and choice of
parameters from a convergence analysis and empirical exploration, and show
practical speedup in synthetic and real world spatial regression tasks.",http://arxiv.org/pdf/2308.14142v1
2308.14129v1,cs.LG,SPEED: Streaming Partition and Parallel Acceleration for Temporal Interaction Graph Embedding,2023-08-27 15:11:44+00:00,"Temporal Interaction Graphs (TIGs) are widely employed to model intricate
real-world systems such as financial systems and social networks. To capture
the dynamism and interdependencies of nodes, existing TIG embedding models need
to process edges sequentially and chronologically. However, this requirement
prevents it from being processed in parallel and struggle to accommodate
burgeoning data volumes to GPU. Consequently, many large-scale temporal
interaction graphs are confined to CPU processing. Furthermore, a generalized
GPU scaling and acceleration approach remains unavailable. To facilitate
large-scale TIGs' implementation on GPUs for acceleration, we introduce a novel
training approach namely Streaming Edge Partitioning and Parallel Acceleration
for Temporal Interaction Graph Embedding (SPEED). The SPEED is comprised of a
Streaming Edge Partitioning Component (SEP) which addresses space overhead
issue by assigning fewer nodes to each GPU, and a Parallel Acceleration
Component (PAC) which enables simultaneous training of different sub-graphs,
addressing time overhead issue. Our method can achieve a good balance in
computing resources, computing time, and downstream task performance. Empirical
validation across 7 real-world datasets demonstrates the potential to expedite
training speeds by a factor of up to 19.29x. Simultaneously, resource
consumption of a single-GPU can be diminished by up to 69%, thus enabling the
multiple GPU-based training and acceleration encompassing millions of nodes and
billions of edges. Furthermore, our approach also maintains its competitiveness
in downstream tasks.",http://arxiv.org/pdf/2308.14129v1
2308.14119v1,cs.CV,Semi-Supervised Learning in the Few-Shot Zero-Shot Scenario,2023-08-27 14:25:07+00:00,"Semi-Supervised Learning (SSL) leverages both labeled and unlabeled data to
improve model performance. Traditional SSL methods assume that labeled and
unlabeled data share the same label space. However, in real-world applications,
especially when the labeled training set is small, there may be classes that
are missing from the labeled set. Existing frameworks aim to either reject all
unseen classes (open-set SSL) or to discover unseen classes by partitioning an
unlabeled set during training (open-world SSL). In our work, we construct a
classifier for points from both seen and unseen classes. Our approach is based
on extending an existing SSL method, such as FlexMatch, by incorporating an
additional entropy loss. This enhancement allows our method to improve the
performance of any existing SSL method in the classification of both seen and
unseen classes. We demonstrate large improvement gains over state-of-the-art
SSL, open-set SSL, and open-world SSL methods, on two benchmark image
classification data sets, CIFAR-100 and STL-10. The gains are most pronounced
when the labeled data is severely limited (1-25 labeled examples per class).",http://arxiv.org/pdf/2308.14119v1
2308.14114v1,cs.LG,Hybrid Transformer-RNN Architecture for Household Occupancy Detection Using Low-Resolution Smart Meter Data,2023-08-27 14:13:29+00:00,"Residential occupancy detection has become an enabling technology in today's
urbanized world for various smart home applications, such as building
automation, energy management, and improved security and comfort.
Digitalization of the energy system provides smart meter data that can be used
for occupancy detection in a non-intrusive manner without causing concerns
regarding privacy and data security. In particular, deep learning techniques
make it possible to infer occupancy from low-resolution smart meter data, such
that the need for accurate occupancy detection with privacy preservation can be
achieved. Our work is thus motivated to develop a privacy-aware and effective
model for residential occupancy detection in contemporary living environments.
Our model aims to leverage the advantages of both recurrent neural networks
(RNNs), which are adept at capturing local temporal dependencies, and
transformers, which are effective at handling global temporal dependencies. Our
designed hybrid transformer-RNN model detects residential occupancy using
hourly smart meter data, achieving an accuracy of nearly 92\% across households
with diverse profiles. We validate the effectiveness of our method using a
publicly accessible dataset and demonstrate its performance by comparing it
with state-of-the-art models, including attention-based occupancy detection
methods.",http://arxiv.org/pdf/2308.14114v1
2308.14104v1,cs.LG,Towards Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with Transferrable Local Policy,2023-08-27 13:22:50+00:00,"Machine learning has been adapted to help solve NP-hard combinatorial
optimization problems. One prevalent way is learning to construct solutions by
deep neural networks, which has been receiving more and more attention due to
the high efficiency and less requirement for expert knowledge. However, many
neural construction methods for Vehicle Routing Problems (VRPs) focus on
synthetic problem instances with limited scales and specified node
distributions, leading to poor performance on real-world problems which usually
involve large scales together with complex and unknown node distributions. To
make neural VRP solvers more practical in real-world scenarios, we design an
auxiliary policy that learns from the local transferable topological features,
named local policy, and integrate it with a typical constructive policy (which
learns from the global information of VRP instances) to form an ensemble
policy. With joint training, the aggregated policies perform cooperatively and
complementarily to boost generalization. The experimental results on two
well-known benchmarks, TSPLIB and CVRPLIB, of travelling salesman problem and
capacitated VRP show that the ensemble policy consistently achieves better
generalization than state-of-the-art construction methods and even works well
on real-world problems with several thousand nodes.",http://arxiv.org/pdf/2308.14104v1
2308.14085v1,cond-mat.dis-nn,"Sampling with flows, diffusion and autoregressive neural networks: A spin-glass perspective",2023-08-27 12:16:33+00:00,"Recent years witnessed the development of powerful generative models based on
flows, diffusion or autoregressive neural networks, achieving remarkable
success in generating data from examples with applications in a broad range of
areas. A theoretical analysis of the performance and understanding of the
limitations of these methods remain, however, challenging. In this paper, we
undertake a step in this direction by analysing the efficiency of sampling by
these methods on a class of problems with a known probability distribution and
comparing it with the sampling performance of more traditional methods such as
the Monte Carlo Markov chain and Langevin dynamics. We focus on a class of
probability distribution widely studied in the statistical physics of
disordered systems that relate to spin glasses, statistical inference and
constraint satisfaction problems.
  We leverage the fact that sampling via flow-based, diffusion-based or
autoregressive networks methods can be equivalently mapped to the analysis of a
Bayes optimal denoising of a modified probability measure. Our findings
demonstrate that these methods encounter difficulties in sampling stemming from
the presence of a first-order phase transition along the algorithm's denoising
path. Our conclusions go both ways: we identify regions of parameters where
these methods are unable to sample efficiently, while that is possible using
standard Monte Carlo or Langevin approaches. We also identify regions where the
opposite happens: standard approaches are inefficient while the discussed
generative methods work well.",http://arxiv.org/pdf/2308.14085v1
2308.14048v1,stat.ML,A Bayesian Non-parametric Approach to Generative Models: Integrating Variational Autoencoder and Generative Adversarial Networks using Wasserstein and Maximum Mean Discrepancy,2023-08-27 08:58:31+00:00,"Generative models have emerged as a promising technique for producing
high-quality images that are indistinguishable from real images. Generative
adversarial networks (GANs) and variational autoencoders (VAEs) are two of the
most prominent and widely studied generative models. GANs have demonstrated
excellent performance in generating sharp realistic images and VAEs have shown
strong abilities to generate diverse images. However, GANs suffer from ignoring
a large portion of the possible output space which does not represent the full
diversity of the target distribution, and VAEs tend to produce blurry images.
To fully capitalize on the strengths of both models while mitigating their
weaknesses, we employ a Bayesian non-parametric (BNP) approach to merge GANs
and VAEs. Our procedure incorporates both Wasserstein and maximum mean
discrepancy (MMD) measures in the loss function to enable effective learning of
the latent space and generate diverse and high-quality samples. By fusing the
discriminative power of GANs with the reconstruction capabilities of VAEs, our
novel model achieves superior performance in various generative tasks, such as
anomaly detection and data augmentation. Furthermore, we enhance the model's
capability by employing an extra generator in the code space, which enables us
to explore areas of the code space that the VAE might have overlooked. With a
BNP perspective, we can model the data distribution using an
infinite-dimensional space, which provides greater flexibility in the model and
reduces the risk of overfitting. By utilizing this framework, we can enhance
the performance of both GANs and VAEs to create a more robust generative model
suitable for various applications.",http://arxiv.org/pdf/2308.14048v1
2308.14030v1,cs.CV,Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-Supervised Contrastive Learning,2023-08-27 07:47:38+00:00,"Forensic pathology is critical in analyzing death manner and time from the
microscopic aspect to assist in the establishment of reliable factual bases for
criminal investigation. In practice, even the manual differentiation between
different postmortem organ tissues is challenging and relies on expertise,
considering that changes like putrefaction and autolysis could significantly
change typical histopathological appearance. Developing AI-based computational
pathology techniques to assist forensic pathologists is practically meaningful,
which requires reliable discriminative representation learning to capture
tissues' fine-grained postmortem patterns. To this end, we propose a framework
called FPath, in which a dedicated self-supervised contrastive learning
strategy and a context-aware multiple-instance learning (MIL) block are
designed to learn discriminative representations from postmortem
histopathological images acquired at varying magnification scales. Our
self-supervised learning step leverages multiple complementary contrastive
losses and regularization terms to train a double-tier backbone for
fine-grained and informative patch/instance embedding. Thereafter, the
context-aware MIL adaptively distills from the local instances a holistic
bag/image-level representation for the recognition task. On a large-scale
database of $19,607$ experimental rat postmortem images and $3,378$ real-world
human decedent images, our FPath led to state-of-the-art accuracy and promising
cross-domain generalization in recognizing seven different postmortem tissues.
The source code will be released on
\href{https://github.com/ladderlab-xjtu/forensic_pathology}{https://github.com/ladderlab-xjtu/forensic\_pathology}.",http://arxiv.org/pdf/2308.14030v1
2308.14020v1,cs.LG,A Comparison of Neural Networks for Wireless Channel Prediction,2023-08-27 06:39:46+00:00,"The performance of modern wireless communications systems depends critically
on the quality of the available channel state information (CSI) at the
transmitter and receiver. Several previous works have proposed concepts and
algorithms that help maintain high quality CSI even in the presence of high
mobility and channel aging, such as temporal prediction schemes that employ
neural networks. However, it is still unclear which neural network-based scheme
provides the best performance in terms of prediction quality, training
complexity and practical feasibility. To investigate such a question, this
paper first provides an overview of state-of-the-art neural networks applicable
to channel prediction and compares their performance in terms of prediction
quality. Next, a new comparative analysis is proposed for four promising neural
networks with different prediction horizons. The well-known tapped delay
channel model recommended by the Third Generation Partnership Program is used
for a standardized comparison among the neural networks. Based on this
comparative evaluation, the advantages and disadvantages of each neural network
are discussed and guidelines for selecting the best-suited neural network in
channel prediction applications are given.",http://arxiv.org/pdf/2308.14020v1
2308.14004v1,stat.ML,Online GentleAdaBoost -- Technical Report,2023-08-27 04:41:56+00:00,"We study the online variant of GentleAdaboost, where we combine a weak
learner to a strong learner in an online fashion. We provide an approach to
extend the batch approach to an online approach with theoretical justifications
through application of line search. Finally we compare our online boosting
approach with other online approaches across a variety of benchmark datasets.",http://arxiv.org/pdf/2308.14004v1
2308.13996v1,cs.LG,Improve in-situ life prediction and classification performance by capturing both the present state and evolution rate of battery aging,2023-08-27 03:24:37+00:00,"This study develops a methodology by capturing both the battery aging state
and degradation rate for improved life prediction performance. The aging state
is indicated by six physical features of an equivalent circuit model that are
extracted from the voltage relaxation data. And the degradation rate is
captured by two features extracted from the differences between the voltage
relaxation curves within a moving window (for life prediction), or the
differences between the capacity vs. voltage curves at different cycles (for
life classification). Two machine learning models, which are constructed based
on Gaussian Processes, are used to describe the relationships between these
physical features and battery lifetimes for the life prediction and
classification, respectively. The methodology is validated with the aging data
of 74 battery cells of three different types. Experimental results show that
based on only 3-12 minutes' sampling data, the method with novel features
predicts accurate battery lifetimes, with the prediction accuracy improved by
up to 67.09% compared with the benchmark method. And the batteries are
classified into three groups (long, medium, and short) with an overall accuracy
larger than 90% based on only two adjacent cycles' information, enabling the
highly efficient regrouping of retired batteries.",http://arxiv.org/pdf/2308.13996v1
2308.13991v1,cs.CV,JL-lemma derived Optimal Projections for Discriminative Dictionary Learning,2023-08-27 02:59:59+00:00,"To overcome difficulties in classifying large dimensionality data with a
large number of classes, we propose a novel approach called JLSPCADL. This
paper uses the Johnson-Lindenstrauss (JL) Lemma to select the dimensionality of
a transformed space in which a discriminative dictionary can be learned for
signal classification. Rather than reducing dimensionality via random
projections, as is often done with JL, we use a projection transformation
matrix derived from Modified Supervised PC Analysis (M-SPCA) with the
JL-prescribed dimension.
  JLSPCADL provides a heuristic to deduce suitable distortion levels and the
corresponding Suitable Description Length (SDL) of dictionary atoms to derive
an optimal feature space and thus the SDL of dictionary atoms for better
classification. Unlike state-of-the-art dimensionality reduction-based
dictionary learning methods, a projection transformation matrix derived in a
single step from M-SPCA provides maximum feature-label consistency of the
transformed space while preserving the cluster structure of the original data.
Despite confusing pairs, the dictionary for the transformed space generates
discriminative sparse coefficients, with fewer training samples.
Experimentation demonstrates that JLSPCADL scales well with an increasing
number of classes and dimensionality. Improved label consistency of features
due to M-SPCA helps to classify better. Further, the complexity of training a
discriminative dictionary is significantly reduced by using SDL.
Experimentation on OCR and face recognition datasets shows relatively better
classification performance than other supervised dictionary learning
algorithms.",http://arxiv.org/pdf/2308.13991v1
2308.14639v1,math.NA,A Rational Krylov Subspace Method for the Computation of the Matrix Exponential Operator,2023-08-28 15:09:13+00:00,"The computation of approximating e^tA B, where A is a large sparse matrix and
B is a rectangular matrix, serves as a crucial element in numerous scientific
and engineering calculations. A powerful way to consider this problem is to use
Krylov subspace methods. The purpose of this work is to approximate the matrix
exponential and some Cauchy-Stieltjes functions on a block vectors B of R^n*p
using a rational block Lanczos algorithm. We also derive some error estimates
and error bound for the convergence of the rational approximation and finally
numerical results attest to the computational efficiency of the proposed
method.",http://arxiv.org/pdf/2308.14639v1
2308.14573v1,math.NA,Linearizing Anhysteretic Magnetization Curves: A Novel Algorithm for Finding Simulation Parameters and Magnetic Moments,2023-08-28 13:37:43+00:00,"This paper proposes a new method for determining the simulation parameters of
the Jiles-Atherton Model used to simulate the first magnetization curve and
hysteresis loop in ferromagnetic materials. The Jiles-Atherton Model is an
important tool in engineering applications due to its relatively simple
differential formulation. However, determining the simulation parameters for
the anhysteretic curve is challenging. Several methods have been proposed,
primarily based on mathematical aspects of the anhysteretic and first
magnetization curves and hysteresis loops. This paper focuses on finding the
magnetic moments of the material, which are used to define the simulation
parameters for its anhysteretic curve. The proposed method involves using the
susceptibility of the material and a linear approximation of a paramagnet to
find the magnetic moments. The simulation parameters can then be found based on
the magnetic moments. The method is validated theoretically and experimentally
and offers a more physical approach to finding simulation parameters for the
anhysteretic curve and a simplified way of determining the magnetic moments of
the material.",http://arxiv.org/pdf/2308.14573v1
2308.14537v1,math.NA,Solving parametric elliptic interface problems via interfaced operator network,2023-08-28 12:49:08+00:00,"Learning operator mapping between infinite-dimensional Banach spaces via
neural networks has attracted a considerable amount of attention in recent
years. In this work, we propose an interfaced operator network (IONet) to solve
parametric elliptic interface PDEs, where different coefficients, source terms
and boundary conditions are considered as input features. To capture the
discontinuities of both input functions and output solutions across the
interface, IONet divides the entire domain into several separate sub-domains
according to the interface, and leverages multiple branch networks and truck
networks. Each branch network extracts latent representations of input
functions at a fixed number of sensors on a specific sub-domain, and each truck
network is responsible for output solutions on one sub-domain. In addition,
tailored physics-informed loss of IONet is proposed to ensure physical
consistency, which greatly reduces the requirement for training datasets and
makes IONet effective without any paired input-output observations in the
interior of the computational domain. Extensive numerical studies show that
IONet outperforms existing state-of-the-art deep operator networks in terms of
accuracy, efficiency, and versatility.",http://arxiv.org/pdf/2308.14537v1
2308.14490v1,math.NA,Efficient least squares approximation and collocation methods using radial basis functions,2023-08-28 11:07:38+00:00,"We describe an efficient method for the approximation of functions using
radial basis functions (RBFs), and extend this to a solver for boundary value
problems on irregular domains. The method is based on RBFs with centers on a
regular grid defined on a bounding box, with some of the centers outside the
computational domain. The equation is discretized using collocation with
oversampling, with collocation points inside the domain only, resulting in a
rectangular linear system to be solved in a least squares sense. The goal of
this paper is the efficient solution of that rectangular system. We show that
the least squares problem splits into a regular part, which can be expedited
with the FFT, and a low rank perturbation, which is treated separately with a
direct solver. The rank of the perturbation is influenced by the irregular
shape of the domain and by the weak enforcement of boundary conditions at
points along the boundary. The solver extends the AZ algorithm which was
previously proposed for function approximation involving frames and other
overcomplete sets. The solver has near optimal log-linear complexity for
univariate problems, and loses optimality for higher-dimensional problems but
remains faster than a direct solver.",http://arxiv.org/pdf/2308.14490v1
2308.14487v1,math.NA,Deep multi-step mixed algorithm for high dimensional non-linear PDEs and associated BSDEs,2023-08-28 11:00:32+00:00,"We propose a new multistep deep learning-based algorithm for the resolution
of moderate to high dimensional nonlinear backward stochastic differential
equations (BSDEs) and their corresponding parabolic partial differential
equations (PDE). Our algorithm relies on the iterated time discretisation of
the BSDE and approximates its solution and gradient using deep neural networks
and automatic differentiation at each time step. The approximations are
obtained by sequential minimisation of local quadratic loss functions at each
time step through stochastic gradient descent. We provide an analysis of
approximation error in the case of a network architecture with weight
constraints requiring only low regularity conditions on the generator of the
BSDE. The algorithm increases accuracy from its single step parent model and
has reduced complexity when compared to similar models in the literature.",http://arxiv.org/pdf/2308.14487v1
2308.14479v1,math.NA,A convergent interacting particle method for computing KPP front speeds in random flows,2023-08-28 10:38:31+00:00,"We aim to efficiently compute spreading speeds of
reaction-diffusion-advection (RDA) fronts in divergence free random flows under
the Kolmogorov-Petrovsky-Piskunov (KPP) nonlinearity. We study a stochastic
interacting particle method (IPM) for the reduced principal eigenvalue
(Lyapunov exponent) problem of an associated linear advection-diffusion
operator with spatially random coefficients. The Fourier representation of the
random advection field and the Feynman-Kac (FK) formula of the principal
eigenvalue (Lyapunov exponent) form the foundation of our method implemented as
a genetic evolution algorithm. The particles undergo advection-diffusion, and
mutation/selection through a fitness function originated in the FK semigroup.
We analyze convergence of the algorithm based on operator splitting, present
numerical results on representative flows such as 2D cellular flow and 3D
Arnold-Beltrami-Childress (ABC) flow under random perturbations. The 2D
examples serve as a consistency check with semi-Lagrangian computation. The 3D
results demonstrate that IPM, being mesh free and self-adaptive, is simple to
implement and efficient for computing front spreading speeds in the
advection-dominated regime for high-dimensional random flows on unbounded
domains where no truncation is needed.",http://arxiv.org/pdf/2308.14479v1
2308.14431v1,math.NA,Two-Scale Finite Element Approximation of a Homogenized Plate Model,2023-08-28 09:09:34+00:00,"This paper studies the discretization of a homogenization and dimension
reduction model for the elastic deformation of microstructured thin plates
proposed by Hornung, Neukamm, and Vel\v{c}i\'c in 2014. Thereby, a nonlinear
bending energy is based on a homogenized quadratic form which acts on the
second fundamental form associated with the elastic deformation. Convergence is
proven for a multi-affine finite element discretization of the involved
three-dimensional microscopic cell problems and a discrete Kirchhoff triangle
discretization of the two-dimensional isometry-constrained macroscopic problem.
Finally, the convergence properties are numerically verified in selected test
cases and qualitatively compared with deformation experiments for
microstructured sheets of paper.",http://arxiv.org/pdf/2308.14431v1
2308.14405v1,math.NA,Simulation of Permittivity and Conductivity Graded Materials for HVDC GIL for Different Voltage Forms,2023-08-28 08:40:21+00:00,"Functionally graded materials (FGM) are applied in HVDC gas insulated lines
(GIL) to control the electric field within the DC insulation system. In HVDC
GIL, FGM with a spatial distribution of the electric conductivity
(conductivity-FGM) is applied to control the electric field under DC steady
state condition. However, besides DC steady state, different DC conditions
occur, e.g. DC-on process, polarity reversal and lightning impulse. Under these
conditions conductivity-FGM is not sufficient to control the electric field,
since these conditions result in transient capacitive fields, where the
permittivity is decisive for the electric field. In this paper, we suggest
combining conductivity-FGM and a spatial distribution of permittivity
(permittivity-FGM) in the spacer material to control the electric field around
DC-GIL spacer for various DC-conditions, considering nonlinear material models
for the insulating gas and the epoxy spacer. A variation of the spatial
distribution of permittivity and conductivity in the spacer is investigated in
this paper for an effective field reduction. The results show a reduction of
the electric field intensity up to 65.8 %, when conductivity/permittivity-FGM
is applied.",http://arxiv.org/pdf/2308.14405v1
2308.14222v1,math.NA,Accurate complex Jacobi rotations,2023-08-27 22:46:18+00:00,"This note shows how to compute, to high relative accuracy under mild
assumptions, complex Jacobi rotations for diagonalization of Hermitian matrices
of order two, using the correctly rounded functions $\mathtt{cr\_hypot}$ and
$\mathtt{cr\_rsqrt}$, proposed for standardization in the C programming
language as recommended by the IEEE-754 floating-point standard. The rounding
to nearest (ties to even) and the non-stop arithmetic are assumed. The
numerical examples compare the observed with theoretical bounds on the relative
errors in the rotations' elements, and show that the maximal observed departure
of the rotations' determinants from unity is smaller than that of the
transformations computed by LAPACK.",http://arxiv.org/pdf/2308.14222v1
2308.14188v1,math.NA,Bayesian deep operator learning for homogenized to fine-scale maps for multiscale PDE,2023-08-27 19:36:53+00:00,"We present a new framework for computing fine-scale solutions of multiscale
Partial Differential Equations (PDEs) using operator learning tools. Obtaining
fine-scale solutions of multiscale PDEs can be challenging, but there are many
inexpensive computational methods for obtaining coarse-scale solutions.
Additionally, in many real-world applications, fine-scale solutions can only be
observed at a limited number of locations. In order to obtain approximations or
predictions of fine-scale solutions over general regions of interest, we
propose to learn the operator mapping from coarse-scale solutions to fine-scale
solutions using a limited number (and possibly noisy) observations of the
fine-scale solutions. The approach is to train multi-fidelity homogenization
maps using mathematically motivated neural operators. The operator learning
framework can efficiently obtain the solution of multiscale PDEs at any
arbitrary point, making our proposed framework a mesh-free solver. We verify
our results on multiple numerical examples showing that our approach is an
efficient mesh-free solver for multiscale PDEs.",http://arxiv.org/pdf/2308.14188v1
2308.14143v1,math.OC,Ensemble-localized Kernel Density Estimation with Applications to the Ensemble Gaussian Mixture Filter,2023-08-27 15:54:17+00:00,"The ensemble Gaussian mixture filter (EnGMF) is a powerful filter for highly
non-Gaussian and non-linear models that has practical utility in the case of a
small number of samples, and theoretical convergence to full Bayesian inference
in the ensemble limit. We aim to increase the utility of the EnGMF by
introducing an ensemble-local notion of covariance into the kernel density
estimation (KDE) step for the prior distribution. We prove that in the Gaussian
case, our new ensemble-localized KDE technique is exactly the same as more
traditional KDE techniques. We also show an example of a non-Gaussian
distribution that can fail to be approximated by canonical KDE methods, but can
be approximated exactly by our new KDE technique. We showcase our new KDE
technique on a simple bivariate problem, showing that it has nice qualitative
and quantitative properties, and significantly improves the estimate of the
prior and posterior distributions for all ensemble sizes tested. We
additionally show the utility of the proposed methodology for sequential
filtering for the Lorenz '63 equations, achieving a significant reduction in
error, and less conservative behavior in the uncertainty estimate with respect
to traditional techniques.",http://arxiv.org/pdf/2308.14143v1
2308.14127v1,math.NA,Information geometric regularization of the barotropic Euler equation,2023-08-27 15:03:32+00:00,"A key numerical difficulty in compressible fluid dynamics is the formation of
shock waves. Shock waves feature jump discontinuities in the velocity and
density of the fluid and thus preclude the existence of classical solutions to
the compressible Euler equations. Weak ""entropy"" solutions are commonly defined
by viscous regularization, but even small amounts of viscosity can
substantially change the long-term behavior of the solution. In this work, we
propose an inviscid regularization based on ideas from semidefinite programming
and information geometry. From a Lagrangian perspective, shock formation in
entropy solutions amounts to inelastic collisions of fluid particles. Their
trajectories are akin to that of projected gradient descent on a feasible set
of nonintersecting paths. We regularize these trajectories by replacing them
with solution paths of interior point methods based on log determinantal
barrier functions. These paths are geodesic curves with respect to the
information geometry induced by the barrier function. Thus, our regularization
amounts to replacing the Euclidean geometry of phase space with a suitable
information geometry. We extend this idea to infinite families of paths by
viewing Euler's equations as a dynamical system on a diffeomorphism manifold.
Our regularization embeds this manifold into an information geometric ambient
space, equipping it with a geodesically complete geometry. Expressing the
resulting Lagrangian equations in Eulerian form, we derive a regularized Euler
equation in conservation form. Numerical experiments on one and two-dimensional
problems show its promise as a numerical tool.",http://arxiv.org/pdf/2308.14127v1
2308.14080v1,math.OC,The Global R-linear Convergence of Nesterov's Accelerated Gradient Method with Unknown Strongly Convex Parameter,2023-08-27 11:56:20+00:00,"The Nesterov accelerated gradient (NAG) method is an important
extrapolation-based numerical algorithm that accelerates the convergence of the
gradient descent method in convex optimization. When dealing with an objective
function that is $\mu$-strongly convex, selecting extrapolation coefficients
dependent on $\mu$ enables global R-linear convergence. In cases $\mu$ is
unknown, a commonly adopted approach is to set the extrapolation coefficient
using the original NAG method, referred to as NAG-c. This choice allows for
achieving the optimal iteration complexity among first-order methods for
general convex problems. However, it remains an open question whether the NAG-c
method exhibits global R-linear convergence for strongly convex problems. In
this work, we answer this question positively by establishing the Q-linear
convergence of certain constructed Lyapunov sequences. Furthermore, we extend
our result to the global R-linear convergence of the accelerated proximal
gradient method, which is employed for solving strongly convex composite
optimization problems with nonsmooth terms in the objective function.
Interestingly, these results contradict the findings of the continuous
counterpart of the NAG-c method in [Su, Boyd, and Cand\'es, J. Mach. Learn.
Res., 2016, 17(153), 1-43], where the convergence rate by the suggested
ordinary differential equation cannot exceed $O(1/{\tt poly}(k))$ for strongly
convex functions.",http://arxiv.org/pdf/2308.14080v1
2308.13999v1,math.NA,A Milstein-type method for highly non-linear non-autonomous time-changed stochastic differential equations,2023-08-27 04:02:38+00:00,"A Milstein-type method is proposed for some highly non-linear non-autonomous
time-changed stochastic differential equations (SDEs). The spatial variables in
the coefficients of the time-changed SDEs satisfy the super-linear growth
condition and the temporal variables obey some H\""older's continuity condition.
The strong convergence in the finite time is studied and the convergence order
is obtained.",http://arxiv.org/pdf/2308.13999v1
2308.13986v1,math.NA,A Deep Learning Method for Computing Eigenvalues of the Fractional Schrdinger Operator,2023-08-27 02:17:07+00:00,"We present a novel deep learning method for computing eigenvalues of the
fractional Schr\""odinger operator. Our approach combines a newly developed loss
function with an innovative neural network architecture that incorporates prior
knowledge of the problem. These improvements enable our method to handle both
high-dimensional problems and problems posed on irregular bounded domains. We
successfully compute up to the first 30 eigenvalues for various fractional
Schr\""odinger operators. As an application, we share a conjecture to the
fractional order isospectral problem that has not yet been studied.",http://arxiv.org/pdf/2308.13986v1
2308.13840v1,math.NA,Optimal Transport-inspired Deep Learning Framework for Slow-Decaying Problems: Exploiting Sinkhorn Loss and Wasserstein Kernel,2023-08-26 10:24:43+00:00,"Reduced order models (ROMs) are widely used in scientific computing to tackle
high-dimensional systems. However, traditional ROM methods may only partially
capture the intrinsic geometric characteristics of the data. These
characteristics encompass the underlying structure, relationships, and
essential features crucial for accurate modeling.
  To overcome this limitation, we propose a novel ROM framework that integrates
optimal transport (OT) theory and neural network-based methods. Specifically,
we investigate the Kernel Proper Orthogonal Decomposition (kPOD) method
exploiting the Wasserstein distance as the custom kernel, and we efficiently
train the resulting neural network (NN) employing the Sinkhorn algorithm. By
leveraging an OT-based nonlinear reduction, the presented framework can capture
the geometric structure of the data, which is crucial for accurate learning of
the reduced solution manifold. When compared with traditional metrics such as
mean squared error or cross-entropy, exploiting the Sinkhorn divergence as the
loss function enhances stability during training, robustness against
overfitting and noise, and accelerates convergence.
  To showcase the approach's effectiveness, we conduct experiments on a set of
challenging test cases exhibiting a slow decay of the Kolmogorov n-width. The
results show that our framework outperforms traditional ROM methods in terms of
accuracy and computational efficiency.",http://arxiv.org/pdf/2308.13840v1
2308.13819v1,cs.LG,Guaranteed Stable Quadratic Models and their applications in SINDy and Operator Inference,2023-08-26 09:00:31+00:00,"Scientific machine learning for learning dynamical systems is a powerful tool
that combines data-driven modeling models, physics-based modeling, and
empirical knowledge. It plays an essential role in an engineering design cycle
and digital twinning. In this work, we primarily focus on an operator inference
methodology that builds dynamical models, preferably in low-dimension, with a
prior hypothesis on the model structure, often determined by known physics or
given by experts. Then, for inference, we aim to learn the operators of a model
by setting up an appropriate optimization problem. One of the critical
properties of dynamical systems is{stability. However, such a property is not
guaranteed by the inferred models. In this work, we propose inference
formulations to learn quadratic models, which are stable by design. Precisely,
we discuss the parameterization of quadratic systems that are locally and
globally stable. Moreover, for quadratic systems with no stable point yet
bounded (e.g., Chaotic Lorenz model), we discuss an attractive trapping region
philosophy and a parameterization of such systems. Using those
parameterizations, we set up inference problems, which are then solved using a
gradient-based optimization method. Furthermore, to avoid numerical derivatives
and still learn continuous systems, we make use of an integration form of
differential equations. We present several numerical examples, illustrating the
preservation of stability and discussing its comparison with the existing
state-of-the-art approach to infer operators. By means of numerical examples,
we also demonstrate how proposed methods are employed to discover governing
equations and energy-preserving models.",http://arxiv.org/pdf/2308.13819v1
2308.13709v1,cs.IT,Fast and Low-Memory Compressive Sensing Algorithms for Low Tucker-Rank Tensor Approximation from Streamed Measurements,2023-08-25 23:44:47+00:00,"In this paper we consider the problem of recovering a low-rank Tucker
approximation to a massive tensor based solely on structured random compressive
measurements. Crucially, the proposed random measurement ensembles are both
designed to be compactly represented (i.e., low-memory), and can also be
efficiently computed in one-pass over the tensor. Thus, the proposed
compressive sensing approach may be used to produce a low-rank factorization of
a huge tensor that is too large to store in memory with a total memory
footprint on the order of the much smaller desired low-rank factorization. In
addition, the compressive sensing recovery algorithm itself (which takes the
compressive measurements as input, and then outputs a low-rank factorization)
also runs in a time which principally depends only on the size of the sought
factorization, making its runtime sub-linear in the size of the large tensor
one is approximating. Finally, unlike prior works related to (streaming)
algorithms for low-rank tensor approximation from such compressive
measurements, we present a unified analysis of both Kronecker and Khatri-Rao
structured measurement ensembles culminating in error guarantees comparing the
error of our recovery algorithm's approximation of the input tensor to the best
possible low-rank Tucker approximation error achievable for the tensor by any
possible algorithm. We further include an empirical study of the proposed
approach that verifies our theoretical findings and explores various trade-offs
of parameters of interest.",http://arxiv.org/pdf/2308.13709v1
2308.13476v1,math.NA,Stand-alone Multigrid for Helmholtz Revisited: Towards Convergence Using Standard Components,2023-08-25 16:28:50+00:00,"Getting standard multigrid to work efficiently for the high-frequency
Helmholtz equation has been an open problem in applied mathematics for years.
Much effort has been dedicated to finding solution methods which can use
multigrid components to obtain solvers with a linear time complexity. In this
work we present one among the first stand-alone multigrid solvers for the 2D
Helmholtz equation using both a constant and non-constant wavenumber model
problem. We use standard smoothing techniques and do not impose any
restrictions on the number of grid points per wavelength on the coarse-grid. As
a result we are able to obtain a full V- and W-cycle algorithm. The key
features of the algorithm are the use of higher-order inter-grid transfer
operators combined with a complex constant in the coarsening process. Using
weighted-Jacobi smoothing, we obtain a solver which is $h-$independent and
scales linearly with the wavenumber $k$. Numerical results using 1 to 5
GMRES(3) smoothing steps approach $k-$ and $h-$ independent convergence, when
combined with the higher-order inter-grid transfer operators and a small or
even zero complex shift. The proposed algorithm provides an important step
towards the perpetuating branch of research in finding scalable solvers for
challenging wave propagation problems.",http://arxiv.org/pdf/2308.13476v1
2308.13333v1,cs.RO,Small Celestial Body Exploration with CubeSat Swarms,2023-08-25 12:09:31+00:00,"This work presents a large-scale simulation study investigating the
deployment and operation of distributed swarms of CubeSats for interplanetary
missions to small celestial bodies. Utilizing Taylor numerical integration and
advanced collision detection techniques, we explore the potential of large
CubeSat swarms in capturing gravity signals and reconstructing the internal
mass distribution of a small celestial body while minimizing risks and Delta V
budget. Our results offer insight into the applicability of this approach for
future deep space exploration missions.",http://arxiv.org/pdf/2308.13333v1
2308.13295v1,math.NA,Resolution-independent generative models based on operator learning for physics-constrained Bayesian inverse problems,2023-08-25 10:41:00+00:00,"The Bayesian inference approach is widely used to tackle inverse problems due
to its versatile and natural ability to handle ill-posedness. However, it often
faces challenges when dealing with situations involving continuous fields or
large-resolution discrete representations (high-dimensional). Moreover, the
prior distribution of unknown parameters is commonly difficult to be
determined. In this study, an Operator Learning-based Generative Adversarial
Network (OL-GAN) is proposed and integrated into the Bayesian inference
framework to handle these issues. Unlike most Bayesian approaches, the
distinctive characteristic of the proposed method is to learn the joint
distribution of parameters and responses. By leveraging the trained generative
model, the posteriors of the unknown parameters can theoretically be
approximated by any sampling algorithm (e.g., Markov Chain Monte Carlo, MCMC)
in a low-dimensional latent space shared by the components of the joint
distribution. The latent space is typically a simple and easy-to-sample
distribution (e.g., Gaussian, uniform), which significantly reduces the
computational cost associated with the Bayesian inference while avoiding prior
selection concerns. Furthermore, incorporating operator learning enables
resolution-independent in the generator. Predictions can be obtained at desired
coordinates, and inversions can be performed even if the observation data are
misaligned with the training data. Finally, the effectiveness of the proposed
method is validated through several numerical experiments.",http://arxiv.org/pdf/2308.13295v1
2308.13224v1,math.PR,Exponential Euler method for stiff stochastic differential equations with additive fractional Brownian noise,2023-08-25 07:41:59+00:00,"We discuss a system of stochastic differential equations with a stiff linear
term and additive noise driven by fractional Brownian motions (fBms) with Hurst
parameter H>1/2, which arise e. g., from spatial approximations of stochastic
partial differential equations. For their numerical approximation, we present
an exponential Euler scheme and show that it converges in the strong sense with
an exact rate close to the Hurst parameter H. Further, based on [2], we
conclude the existence of a unique stationary solution of the exponential Euler
scheme that is pathwise asymptotically stable.",http://arxiv.org/pdf/2308.13224v1
2308.13214v1,math.NA,Gl-QFOM and Gl-QGMRES: two efficient algorithms for quaternion linear systems with multiple right-hand sides,2023-08-25 07:24:53+00:00,"In this paper, we propose the global quaternion full orthogonalization
(Gl-QFOM) and global quaternion generalized minimum residual (Gl-QGMRES)
methods, which are built upon global orthogonal and oblique projections onto a
quaternion matrix Krylov subspace, for solving quaternion linear systems with
multiple right-hand sides. We first develop the global quaternion Arnoldi
procedure to preserve the quaternion Hessenberg form during the iterations. We
then establish the convergence analysis of the proposed methods, and show how
to apply them to solve the Sylvester quaternion matrix equation. Numerical
examples are provided to illustrate the effectiveness of our methods compared
with the traditional Gl-FOM and Gl-GMRES iterations for the real
representations of the original linear systems.",http://arxiv.org/pdf/2308.13214v1
2308.13195v1,math.NA,Preconditioning for Generalized Jacobians with the $$-Condition Number,2023-08-25 06:20:20+00:00,"Preconditioning is essential in iterative methods for solving linear systems
of equations. We study a nonclassic matrix condition number, the
$\omega$-condition number, in the context of optimal conditioning for low rank
updating of positive definite matrices. For a positive definite matrix, this
condition measure is the ratio of the arithmetic and geometric means of the
eigenvalues. In particular, we concentrate on linear systems with low rank
updates of positive definite matrices which are close to singular. These
systems arise in the contexts of nonsmooth Newton methods using generalized
Jacobians. We derive an explicit formula for the optimal
$\omega$-preconditioned update in this framework.
  Evaluating or estimating the classical condition number $\kappa$ can be
expensive. We show that the $\omega$-condition number can be evaluated exactly
following a Cholesky or LU factorization and it estimates the actual condition
of a linear system significantly better. Moreover, our empirical results show a
significant decrease in the number of iterations required for a requested
accuracy in the residual during an iterative method, i.e., these results
confirm the efficacy of using the $\omega$-condition number compared to the
classical condition number.",http://arxiv.org/pdf/2308.13195v1
2308.13152v1,math.NA,The time dimensional reduction method to determine the initial conditions without the knowledge of damping coefficients,2023-08-25 03:11:30+00:00,"This paper aims to reconstruct the initial condition of a hyperbolic equation
with an unknown damping coefficient. Our approach involves approximating the
hyperbolic equation's solution by its truncated Fourier expansion in the time
domain and using a polynomial-exponential basis. This truncation process
facilitates the elimination of the time variable, consequently, yielding a
system of quasi-linear elliptic equations. To globally solve the system without
needing an accurate initial guess, we employ the Carleman contraction
principle. We provide several numerical examples to illustrate the efficacy of
our method. The method not only delivers precise solutions but also showcases
remarkable computational efficiency.",http://arxiv.org/pdf/2308.13152v1
2308.13123v1,cs.CE,Multiscale modeling of thermal properties in Polyurethane incorporated with phase change materials composites: A case study,2023-08-25 00:29:56+00:00,"Polyurethane (PU) is an ideal thermal insulation material due to its
excellent thermal properties. The incorporation of Phase Change Materials
(PCMs) capsules into Polyurethane (PU) has been shown to be effective in
building envelopes. This design can significantly increase the stability of the
indoor thermal environment and reduce the fluctuation of indoor air
temperature. We develop a multiscale model of a PU-PCM foam composite and study
the thermal conductivity of this material. Later, the design of materials can
be optimized by obtaining thermal conductivity. We conduct a case study based
on the performance of this optimized material to fully consider the thermal
comfort of the occupants of a building envelope with the application of PU-PCMs
composites in a single room. At the same time, we also predict the energy
consumption of this case. All the outcomes show that this design is promising,
enabling the passive design of building energy and significantly improving
occupants' comfort.",http://arxiv.org/pdf/2308.13123v1
2308.13055v1,physics.flu-dyn,On the dynamics of the boundary vorticity for incompressible viscous flows,2023-08-24 19:42:54+00:00,"The dynamical equation of the boundary vorticity has been obtained, which
shows that the viscosity at a solid wall is doubled as if the fluid became more
viscous at the boundary. For certain viscous flows the boundary vorticity can
be determined via the dynamical equation up to bounded errors for all time,
without the need of knowing the details of the main stream flows. We then
validate the dynamical equation by carrying out stochastic direct numerical
simulations (i.e. the random vortex method for wall-bounded incompressible
viscous flows) by two different means of updating the boundary vorticity, one
using mollifiers of the Biot-Savart singular integral kernel, another using the
dynamical equations.",http://arxiv.org/pdf/2308.13055v1
2308.12939v1,cs.LG,Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries,2023-08-24 17:29:57+00:00,"Recently deep learning surrogates and neural operators have shown promise in
solving partial differential equations (PDEs). However, they often require a
large amount of training data and are limited to bounded domains. In this work,
we present a novel physics-informed neural operator method to solve
parametrized boundary value problems without labeled data. By reformulating the
PDEs into boundary integral equations (BIEs), we can train the operator network
solely on the boundary of the domain. This approach reduces the number of
required sample points from $O(N^d)$ to $O(N^{d-1})$, where $d$ is the domain's
dimension, leading to a significant acceleration of the training process.
Additionally, our method can handle unbounded problems, which are unattainable
for existing physics-informed neural networks (PINNs) and neural operators. Our
numerical experiments show the effectiveness of parametrized complex geometries
and unbounded problems.",http://arxiv.org/pdf/2308.12939v1
2308.12907v1,math.NA,New time domain decomposition methods for parabolic control problems I: Dirichlet-Neumann and Neumann-Dirichlet algorithms,2023-08-24 16:31:01+00:00,"We present new Dirichlet-Neumann and Neumann-Dirichlet algorithms with a time
domain decomposition applied to unconstrained parabolic optimal control
problems. After a spatial semi-discretization, we use the Lagrange multiplier
approach to derive a coupled forward-backward optimality system, which can then
be solved using a time domain decomposition. Due to the forward-backward
structure of the optimality system, three variants can be found for the
Dirichlet-Neumann and Neumann-Dirichlet algorithms. We analyze their
convergence behavior and determine the optimal relaxation parameter for each
algorithm. Our analysis reveals that the most natural algorithms are actually
only good smoothers, and there are better choices which lead to efficient
solvers. We illustrate our analysis with numerical experiments.",http://arxiv.org/pdf/2308.12907v1
2308.12891v1,math.NA,A class of Discontinuous Galerkin methods for nonlinear variational problems,2023-08-24 16:11:23+00:00,"In the context of Discontinuous Galerkin methods, we study approximations of
nonlinear variational problems associated with convex energies. We propose
element-wise nonconforming finite element methods to discretize the continuous
minimisation problem. Using $\Gamma$-convergence arguments we show that the
discrete minimisers converge to the unique minimiser of the continuous problem
as the mesh parameter tends to zero, under the additional contribution of
appropriately defined penalty terms at the level of the discrete energies. We
finally substantiate the feasibility of our methods by numerical examples.",http://arxiv.org/pdf/2308.12891v1
2308.12886v1,math.NA,Linear implicit approximations of invariant measures of semi-linear SDEs with non-globally Lipschitz coefficients,2023-08-24 16:03:40+00:00,"This article investigates the weak approximation towards the invariant
measure of semi-linear stochastic differential equations (SDEs) under
non-globally Lipschitz coefficients. For this purpose, we propose a
linear-theta-projected Euler (LTPE) scheme, which also admits an invariant
measure, to handle the potential influence of the linear stiffness. Under
certain assumptions, both the SDE and the corresponding LTPE method are shown
to converge exponentially to the underlying invariant measures, respectively.
Moreover, with time-independent regularity estimates for the corresponding
Kolmogorov equation, the weak error between the numerical invariant measure and
the original one can be guaranteed with an order one. Numerical experiments are
provided to verify our theoretical findings.",http://arxiv.org/pdf/2308.12886v1
2308.12884v1,math.NA,A second-order length-preserving and unconditionally energy stable rotational discrete gradient method for Oseen-Frank gradient flows,2023-08-24 16:01:25+00:00,"We present a second-order strictly length-preserving and unconditionally
energy-stable rotational discrete gradient (Rdg) scheme for the numerical
approximation of the Oseen-Frank gradient flows with anisotropic elastic energy
functional. Two essential ingredients of the Rdg method are reformulation of
the length constrained gradient flow into an unconstrained rotational form and
discrete gradient discretization for the energy variation. Besides the
well-known mean-value and Gonzalez discrete gradients, we propose a novel
Oseen-Frank discrete gradient, specifically designed for the solution of
Oseen-Frank gradient flow. We prove that the proposed Oseen-Frank discrete
gradient satisfies the energy difference relation, thus the resultant Rdg
scheme is energy stable. Numerical experiments demonstrate the efficiency and
accuracy of the proposed Rdg method and its capability for providing reliable
simulation results with highly disparate elastic coefficients.",http://arxiv.org/pdf/2308.12884v1
2308.12865v1,math.NA,A highly efficient and accurate divergence-free spectral method for curl-curl equation in two and three dimensions,2023-08-24 15:43:12+00:00,"In this paper, we present a fast divergence-free spectral algorithm (FDSA)
for the curl-curl problem. Divergence-free bases in two and three dimensions
are constructed by using the generalized Jacobi polynomials. An accurate
spectral method with exact preservation of the divergence-free constraint
point-wisely is then proposed, and its corresponding error estimate is
established. We then present a highly efficient solution algorithm based on a
combination of matrix-free preconditioned Krylov subspace iterative method and
a fully diagonalizable auxiliary problem, which is derived from the spectral
discretisations of generalized eigenvalue problems of Laplace and biharmonic
operators. We rigorously prove that the dimensions of the invariant subspace of
the preconditioned linear system resulting from the divergence-free spectral
method with respect to the dominate eigenvalue $1$, are $(N-3)^2$ and
$2(N-3)^3$ for two- and three-dimensional problems with $(N-1)^2$ and
$2(N-1)^3$ unknowns, respectively. Thus, the proposed method usually takes only
several iterations to converge, and astonishingly, as the problem size
(polynomial order) increases, the number of iterations will decrease, even for
highly indefinite system and oscillatory solutions. As a result, the
computational cost of the solution algorithm is only a small multiple of $N^3$
and $N^4$ floating number operations for 2D and 3D problems, respectively.
Plenty of numerical examples for solving the curl-curl problem with both
constant and variable coefficients in two and three dimensions are presented to
demonstrate the accuracy and efficiency of the proposed method.",http://arxiv.org/pdf/2308.12865v1
2308.12864v1,cs.LG,Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution,2023-08-24 15:39:01+00:00,"In this article, we present a novel data assimilation strategy in pore-scale
imaging and demonstrate that this makes it possible to robustly address
reactive inverse problems incorporating Uncertainty Quantification (UQ).
Pore-scale modeling of reactive flow offers a valuable opportunity to
investigate the evolution of macro-scale properties subject to dynamic
processes. Yet, they suffer from imaging limitations arising from the
associated X-ray microtomography (X-ray microCT) process, which induces
discrepancies in the properties estimates. Assessment of the kinetic parameters
also raises challenges, as reactive coefficients are critical parameters that
can cover a wide range of values. We account for these two issues and ensure
reliable calibration of pore-scale modeling, based on dynamical microCT images,
by integrating uncertainty quantification in the workflow.
  The present method is based on a multitasking formulation of reactive inverse
problems combining data-driven and physics-informed techniques in calcite
dissolution. This allows quantifying morphological uncertainties on the
porosity field and estimating reactive parameter ranges through prescribed PDE
models with a latent concentration field and dynamical microCT. The data
assimilation strategy relies on sequential reinforcement incorporating
successively additional PDE constraints. We guarantee robust and unbiased
uncertainty quantification by straightforward adaptive weighting of Bayesian
Physics-Informed Neural Networks (BPINNs), ensuring reliable micro-porosity
changes during geochemical transformations. We demonstrate successful Bayesian
Inference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT
images with meaningful posterior distribution on the reactive parameters and
dimensionless numbers.",http://arxiv.org/pdf/2308.12864v1
2308.12807v1,math.NA,Denoising Particle-In-Cell Data via Smoothness-Increasing Accuracy-Conserving Filters with Application to Bohm Speed Computation,2023-08-24 14:10:05+00:00,"The simulation of plasma physics is computationally expensive because the
underlying physical system is of high dimensions, requiring three spatial
dimensions and three velocity dimensions. One popular numerical approach is
Particle-In-Cell (PIC) methods owing to its ease of implementation and
favorable scalability in high-dimensional problems. An unfortunate drawback of
the method is the introduction of statistical noise resulting from the use of
finitely many particles. In this paper we examine the application of the
Smoothness-Increasing Accuracy-Conserving (SIAC) family of convolution kernel
filters as denoisers for moment data arising from PIC simulations. We show that
SIAC filtering is a promising tool to denoise PIC data in the physical space as
well as capture the appropriate scales in the Fourier space. Furthermore, we
demonstrate how the application of the SIAC technique reduces the amount of
information necessary in the computation of quantities of interest in plasma
physics such as the Bohm speed.",http://arxiv.org/pdf/2308.12807v1
2308.12781v1,math.NA,A Riemannian optimization method to compute the nearest singular pencil,2023-08-24 13:37:35+00:00,"Given a square pencil $A+ \lambda B$, where $A$ and $B$ are complex matrices,
we consider the problem of finding the singular pencil nearest to it in the
Frobenius distance. This problem is known to be very difficult, and the few
algorithms available in the literature can only deal efficiently with pencils
of very small size. We show that the problem is equivalent to minimizing a
certain objective function over the Riemannian manifold $SU(n) \times SU(n)$,
where $SU(n)$ denotes the special unitary group. With minor modifications, the
same approach extends to the case of finding a nearest singular pencil with a
specified minimal index. This novel perspective is based on the generalized
Schur form of pencils, and yields a competitive numerical method, by pairing it
with an algorithm capable of doing optimization on a Riemannian manifold. We
provide numerical experiments that show that the resulting method allows us to
deal with pencils of much larger size than alternative techniques, yielding
candidate minimizers of comparable or better quality. In the course of our
analysis, we also obtain a number of new theoretical results related to the
generalized Schur form of a (regular or singular) square pencil and to the
minimal index of a singular square pencil whose nullity is $1$.",http://arxiv.org/pdf/2308.12781v1
2308.12764v1,math.NA,Dirichlet-Neumann and Neumann-Neumann Methods for Elliptic Control Problems,2023-08-24 13:12:56+00:00,"We present the Dirichlet-Neumann (DN) and Neumann-Neumann (NN) methods
applied to the optimal control problems arising from elliptic partial
differential equations (PDEs) under the $H^{-1}$ regularization. We use the
Lagrange multiplier approach to derive a forward-backward optimality system
with the $L^2$ regularization, and a singular perturbed Poisson equation with
the $H^{-1}$ regularization. The $H^{-1}$ regularization thus avoids solving a
coupled bi-Laplacian problem, yet the solutions are less regular. The singular
perturbed Poisson equation is then solved by using the DN and NN methods, and a
detailed analysis is given both in the one-dimensional and two-dimensional
case. Finally, we provide some numerical experiments with conclusions.",http://arxiv.org/pdf/2308.12764v1
2308.12716v1,math.NA,Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks,2023-08-24 11:31:24+00:00,"This paper explores the ability of physics-informed neural networks (PINNs)
to solve forward and inverse problems of contact mechanics for small
deformation elasticity. We deploy PINNs in a mixed-variable formulation
enhanced by output transformation to enforce Dirichlet and Neumann boundary
conditions as hard constraints. Inequality constraints of contact problems,
namely Karush-Kuhn-Tucker (KKT) type conditions, are enforced as soft
constraints by incorporating them into the loss function during network
training. To formulate the loss function contribution of KKT constraints,
existing approaches applied to elastoplasticity problems are investigated and
we explore a nonlinear complementarity problem (NCP) function, namely
Fischer-Burmeister, which possesses advantageous characteristics in terms of
optimization. Based on the Hertzian contact problem, we show that PINNs can
serve as pure partial differential equation (PDE) solver, as data-enhanced
forward model, as inverse solver for parameter identification, and as
fast-to-evaluate surrogate model. Furthermore, we demonstrate the importance of
choosing proper hyperparameters, e.g. loss weights, and a combination of Adam
and L-BFGS-B optimizers aiming for better results in terms of accuracy and
training time.",http://arxiv.org/pdf/2308.12716v1
2308.12683v1,math.NA,The key to the enhanced performance of slab-like topologically interlocked structures with non-planar blocks,2023-08-24 09:48:38+00:00,"Topologically interlocked structures are assemblies of interlocking blocks
that hold together solely through contact. Such structures have been shown to
exhibit high strength, energy dissipation, and crack arrest properties. Recent
studies on beam-like topologically interlocked structures have shown that, with
non-planar blocks, it is possible to reach levels of strength and
work-to-failure which are otherwise possible only with unrealistically high
friction coefficients. While non-planar blocks have been extensively used for
slab-like assemblies, many questions in that context are still not fully
understood. Specifically, it is unclear what are the exact characteristics of
non-planar surface morphologies which can potentially improve the enhanced
mechanical response of slab-like assemblies. In addition, it is unclear if
slab-like structures with non-planar surface blocks can reach a saturated
response with realistic friction coefficient values, as is the case with
beam-like ones. Here, we investigate such fundamental questions using numerical
simulations. We show that, by using non-planar blocks, it is possible to reach
saturation to the response capacity of the structure with a realistic friction
coefficient. Furthermore, we show that the key morphology parameter responsible
for the enhanced performance is the local angle of inclination at the top of
the loaded block. Lastly, we show that non-planar morphologies lead to improved
work-to-failure and ultimate deflection, which cannot be attained with
planar-faced blocks. These findings shed new light on topologically interlocked
structures with non-planar blocks, allowing for a better understanding of their
strengths and potential applications.",http://arxiv.org/pdf/2308.12683v1
2308.12393v1,cs.LG,Machine learning in parameter estimation of nonlinear systems,2023-08-23 19:20:24+00:00,"Accurately estimating parameters in complex nonlinear systems is crucial
across scientific and engineering fields. We present a novel approach for
parameter estimation using a neural network with the Huber loss function. This
method taps into deep learning's abilities to uncover parameters governing
intricate behaviors in nonlinear equations. We validate our approach using
synthetic data and predefined functions that model system dynamics. By training
the neural network with noisy time series data, it fine-tunes the Huber loss
function to converge to accurate parameters. We apply our method to damped
oscillators, Van der Pol oscillators, Lotka-Volterra systems, and Lorenz
systems under multiplicative noise. The trained neural network accurately
estimates parameters, evident from closely matching latent dynamics. Comparing
true and estimated trajectories visually reinforces our method's precision and
robustness. Our study underscores the Huber loss-guided neural network as a
versatile tool for parameter estimation, effectively uncovering complex
relationships in nonlinear systems. The method navigates noise and uncertainty
adeptly, showcasing its adaptability to real-world challenges.",http://arxiv.org/pdf/2308.12393v1
2308.12279v1,cs.LG,On-Manifold Projected Gradient Descent,2023-08-23 17:50:50+00:00,"This work provides a computable, direct, and mathematically rigorous
approximation to the differential geometry of class manifolds for
high-dimensional data, along with nonlinear projections from input space onto
these class manifolds. The tools are applied to the setting of neural network
image classifiers, where we generate novel, on-manifold data samples, and
implement a projected gradient descent algorithm for on-manifold adversarial
training. The susceptibility of neural networks (NNs) to adversarial attack
highlights the brittle nature of NN decision boundaries in input space.
Introducing adversarial examples during training has been shown to reduce the
susceptibility of NNs to adversarial attack; however, it has also been shown to
reduce the accuracy of the classifier if the examples are not valid examples
for that class. Realistic ""on-manifold"" examples have been previously generated
from class manifolds in the latent of an autoencoder. Our work explores these
phenomena in a geometric and computational setting that is much closer to the
raw, high-dimensional input space than can be provided by VAE or other black
box dimensionality reductions. We employ conformally invariant diffusion maps
(CIDM) to approximate class manifolds in diffusion coordinates, and develop the
Nystr\""{o}m projection to project novel points onto class manifolds in this
setting. On top of the manifold approximation, we leverage the spectral
exterior calculus (SEC) to determine geometric quantities such as tangent
vectors of the manifold. We use these tools to obtain adversarial examples that
reside on a class manifold, yet fool a classifier. These misclassifications
then become explainable in terms of human-understandable manipulations within
the data, by expressing the on-manifold adversary in the semantic basis on the
manifold.",http://arxiv.org/pdf/2308.12279v1
2308.12255v1,math.NA,Absorbing boundary conditions for the Helmholtz equation using Gauss-Legendre quadrature reduced integrations,2023-08-23 17:15:20+00:00,"We introduce a new class of absorbing boundary conditions (ABCs) for the
Helmholtz equation. The proposed ABCs can be derived from a certain simple
class of perfectly matched layers using $L$ discrete layers and using the $Q_N$
Lagrange finite element in conjunction with the $N$-point Gauss-Legendre
quadrature reduced integration rule. The proposed ABCs are classified by a
tuple $(L,N)$, and achieve reflection error of order $O(R^{2LN})$ for some
$R<1$. The new ABCs generalise the perfectly matched discrete layers proposed
by Guddati and Lim [Int. J. Numer. Meth. Engng 66 (6) (2006) 949-977],
including them as type $(L,1)$. An analysis of the proposed ABCs is performed
motivated by the work of Ainsworth [J. Comput. Phys. 198 (1) (2004) 106-130].
The new ABCs facilitate numerical implementations of the Helmholtz problem with
ABCs if $Q_N$ finite elements are used in the physical domain. Moreover, giving
more insight, the analysis presented in this work potentially aids with
developing ABCs in related areas.",http://arxiv.org/pdf/2308.12255v1
2308.12164v1,math.NA,A robust family of exponential attractors for a linear time discretization of the Cahn-Hilliard equation with a source term,2023-08-23 14:28:21+00:00,"We consider a linear implicit-explicit (IMEX) time discretization of the
Cahn-Hilliard equation with a source term, endowed with Dirichlet boundary
conditions. For every time step small enough, we build an exponential attractor
of the discrete-in-time dynamical system associated to the discretization. We
prove that, as the time step tends to 0, this attractor converges for the
symmmetric Hausdorff distance to an exponential attractor of the
continuous-in-time dynamical system associated with the PDE. We also prove that
the fractal dimension of the exponential attractor (and consequently, of the
global attractor) is bounded by a constant independent of the time step. The
results also apply to the classical Cahn-Hilliard equation with Neumann
boundary conditions.",http://arxiv.org/pdf/2308.12164v1
2308.12145v1,math.NA,Modeling excitable cells with the EMI equations: spectral analysis and iterative solution strategy,2023-08-23 14:01:07+00:00,"In this work, we are interested in solving large linear systems stemming from
the Extra-Membrane-Intra (EMI) model, which is employed for simulating
excitable tissues at a cellular scale. After setting the related systems of
partial differential equations (PDEs) equipped with proper boundary conditions,
we provide numerical approximation schemes for the EMI PDEs and focus on the
resulting large linear systems. We first give a relatively complete spectral
analysis using tools from the theory of Generalized Locally Toeplitz matrix
sequences. The obtained spectral information is used for designing appropriate
(preconditioned) Krylov solvers. We show, through numerical experiments, that
the presented solution strategy is robust w.r.t. problem and discretization
parameters, efficient and scalable.",http://arxiv.org/pdf/2308.12145v1
2308.12130v1,math.NA,Space-time hybridizable discontinuous Galerkin method for advection-diffusion on deforming domains: The advection-dominated regime,2023-08-23 13:36:12+00:00,"We analyze a space-time hybridizable discontinuous Galerkin method to solve
the time-dependent advection-diffusion equation on deforming domains. We prove
stability of the discretization in the advection-dominated regime by using
weighted test functions and derive a priori space-time error estimates. A
numerical example illustrates the theoretical results.",http://arxiv.org/pdf/2308.12130v1
2308.12104v1,math.NA,Computational Modeling of Coupled Interactions of Fluid Membranes with Embedded Filaments,2023-08-23 12:47:26+00:00,"In this work, we present a computational formulation based on continuum
mechanics to study the interaction of fluid membranes embedded with
semiflexible filaments. This is motivated by systems in membrane biology, such
as cytoskeletal networks and protein filaments aiding the cell fission process.
We model the membrane as a fluid shell via the Helfrich-Canham energy and the
filament as a one-dimensional Cosserat continuum. We assume the filament to be
tethered to the surface of the membrane in a way that it is allowed to float on
the surface freely. The novel filament-membrane coupling, which is anticipated
to yield interesting physics, also gives rise to unique computational
challenges, which we address in this work. We present validation results and
apply the formulation to certain problems inspired by cellular biology.",http://arxiv.org/pdf/2308.12104v1
2308.11964v1,math.NA,On the Computation of the Logarithm of the Modified Bessel Function of the Second Kind,2023-08-23 07:09:03+00:00,"The modified Bessel function of the second kind K$\nu$ appears in a wide
variety of applied scientific fields. While its use is greatly facilitated by
an implementation in most numerical libraries, overflow issues can be
encountered especially for large value of $\nu$. After giving some necessary
and sufficient conditions for their occurrences, this technical note shows that
they can mostly be avoided by directly computing the logarithm of K$\nu$ thanks
to a simple and stable forward recursion. A statistical examples based on the
Gil-Pelaez inversion formula is given to illustrate the recursive method.",http://arxiv.org/pdf/2308.11964v1
2308.11925v1,math.OC,Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks,2023-08-23 05:18:19+00:00,"In this work, we present and analyze a numerical solver for optimal control
problems (without / with box constraint) for linear and semilinear second-order
elliptic problems. The approach is based on a coupled system derived from the
first-order optimality system of the optimal control problem, and applies
physics informed neural networks (PINNs) to solve the coupled system. We
present an error analysis of the numerical scheme, and provide $L^2(\Omega)$
error bounds on the state, control and adjoint state in terms of deep neural
network parameters (e.g., depth, width, and parameter bounds) and the number of
sampling points in the domain and on the boundary. The main tools in the
analysis include offset Rademacher complexity and boundedness and Lipschitz
continuity of neural network functions. We present several numerical examples
to illustrate the approach and compare it with three existing approaches.",http://arxiv.org/pdf/2308.11925v1
2308.11892v1,physics.comp-ph,Constrained Pressure-Temperature Residual (CPTR) Preconditioner Performance for Large-Scale Thermal CO2 Injection Simulation,2023-08-23 03:40:26+00:00,"This work studies the performance of a novel preconditioner, designed for
thermal reservoir simulation cases and recently introduced in Roy et al. (2020)
and Cremon et al. (2020), on large-scale thermal CO2 injection cases. For
Carbon Capture and Sequestration (CCS) projects, injecting CO2 under
supercritical conditions is typically tens of degrees colder than the reservoir
temperature. Thermal effects can have a significant impact on the simulation
results, but they also add many challenges for the solvers. More specifically,
the usual combination of an iterative linear solver (such as GMRES) and the
Constrained Pressure Residual (CPR) physics-based block-preconditioner is known
to perform rather poorly or fail to converge when thermal effects play a
significant role. The Constrained Pressure-Temperature Residual (CPTR)
preconditioner retains the 2x2 block structure (elliptic/hyperbolic) of CPR but
includes the temperature in the elliptic subsystem. The elliptic subsystem is
now formed by two equations, and is dealt with by the system-solver of
BoomerAMG (from the HYPRE library). Then a global smoother, ILU(0), is applied
to the full system to handle the local, hyperbolic temperature fronts. We
implemented CPTR in the multi-physics solver GEOS and present results on
various large-scale thermal CCS simulation cases, including both Cartesian and
fully unstructured meshes, up to tens of millions of degrees of freedom. The
CPTR preconditioner severely reduces the number of GMRES iterations and the
runtime, with cases timing out in 24h with CPR now requiring a few hours with
CPTR. We present strong scaling results using hundreds of CPU cores for
multiple cases, and show close to linear scaling. CPTR is also virtually
insensitive to the thermal Peclet number (which compares advection and
diffusion effects) and is suitable to any thermal regime.",http://arxiv.org/pdf/2308.11892v1
2308.11882v1,math.NA,The macroscopic finite-difference scheme and modified equations of the general propagation multiple-relaxation-time lattice Boltzmann model,2023-08-23 02:59:04+00:00,"In this paper, we first present the general propagation
multiple-relaxation-time lattice Boltzmann (GPMRT-LB) model and obtain the
corresponding macroscopic finite-difference (GPMFD) scheme on conservative
moments. Then based on the Maxwell iteration method, we conduct the analysis on
the truncation errors and modified equations (MEs) of the GPMRT-LB model and
GPMFD scheme at both diffusive and acoustic scalings. For the nonlinear
anisotropic convection-diffusion equation (NACDE) and Navier-Stokes equations
(NSEs), we also derive the first- and second-order MEs of the GPMRT-LB model
and GPMFD scheme. In particular, for the one-dimensional convection-diffusion
equation (CDE) with the constant velocity and diffusion coefficient, we can
develop a fourth-order GPMRT-LB (F-GPMRT-LB) model and the corresponding
fourth-order GPMFD (F-GPMFD) scheme at the diffusive scaling. Finally, two
benchmark problems, Gauss hill problem and Poiseuille flow in two-dimensional
space, are used to test the GPMRT-LB model and GPMFD scheme, and it is found
that the numerical results are not only in good agreement with corresponding
analytical solutions, but also have a second-order convergence rate in space.
Additionally, a numerical study on one-dimensional CDE also demonstrates that
the F-GPMRT-LB model and F-GPMFD scheme can achieve a fourth-order accuracy in
space, which is consistent with our theoretical analysis.",http://arxiv.org/pdf/2308.11882v1
2308.11821v1,math.NA,Multi-temporal decomposition for elastoplastic ratcheting solids,2023-08-22 23:01:11+00:00,"This paper presents a multi-temporal formulation for simulating elastoplastic
solids under cyclic loading. We leverage the proper generalized decomposition
(PGD) to decompose the displacements into multiple time scales, separating the
spatial and intra-cyclic dependence from the inter-cyclic variation. In
contrast with the standard incremental approach, which solves the (non-linear
and computationally intensive) mechanical balance equations at every time step,
the proposed PGD approach allows the mechanical balance equations to be solved
exclusively for the small-time intra-cyclic response, while the large-time
inter-cyclic response is described by simple scalar algebraic equations.
Numerical simulations exhibiting complex cyclic responses, including a 2D
problem and an application to a monopile foundation, demonstrate that PGD
solutions with a limited number of space-time degrees of freedom may be
obtained numerically, only requiring a few modes to accurately capture the
reference response.",http://arxiv.org/pdf/2308.11821v1
2308.11581v1,math.NA,Dynamically Orthogonal Approximation for Stochastic Differential Equations,2023-08-22 17:30:12+00:00,"In this paper, we set the mathematical foundations of the Dynamical Low Rank
Approximation (DLRA) method for high-dimensional stochastic differential
equations. DLRA aims at approximating the solution as a linear combination of a
small number of basis vectors with random coefficients (low rank format) with
the peculiarity that both the basis vectors and the random coefficients vary in
time. While the formulation and properties of DLRA are now well understood for
random/parametric equations, the same cannot be said for SDEs and this work
aims to fill this gap. We start by rigorously formulating a Dynamically
Orthogonal (DO) approximation (an instance of DLRA successfully used in
applications) for SDEs, which we then generalize to define a parametrization
independent DLRA for SDEs. We show local well-posedness of the DO equations and
their equivalence with the DLRA formulation. We also characterize the explosion
time of the DO solution by a loss of linear independence of the random
coefficients defining the solution expansion and give sufficient conditions for
global existence.",http://arxiv.org/pdf/2308.11581v1
2308.11580v1,physics.comp-ph,NIPG-DG schemes for transformed master equations modeling open quantum systems,2023-08-22 17:28:51+00:00,"This work presents a numerical analysis of a master equation modeling the
interaction of a system with a noisy environment in the particular context of
open quantum systems. It is shown that our transformed master equation has a
reduced computational cost in comparison to a Wigner-Fokker-Planck model of the
same system for the general case of any potential. Specifics of a NIPG-DG
numerical scheme adequate for the convection-diffusion system obtained are then
presented. This will let us solve computationally the transformed system of
interest modeling our open quantum system. A benchmark problem, the case of a
harmonic potential, is then presented, for which the numerical results are
compared against the analytical steady-state solution of this problem.",http://arxiv.org/pdf/2308.11580v1
2308.11533v1,math.NA,Lifting Sylvester equations: singular value decay for non-normal coefficients,2023-08-22 15:59:41+00:00,"We aim to find conditions on two Hilbert space operators $A$ and $B$ under
which the expression $AX-XB$ having low rank forces the operator $X$ itself to
admit a good low rank approximation. It is known that this can be achieved when
$A$ and $B$ are normal and have well-separated spectra. In this paper, we relax
this normality condition, using the idea of operator dilations. The basic
problem then becomes the lifting of Sylvester equations, which is reminiscent
of the classical commutant lifting theorem and its variations. Our approach
also allows us to show that the (factored) alternating direction implicit
method for solving Sylvester equaftions $AX-XB=C$ does not require too many
iterations, even without requiring $A$ to be normal.",http://arxiv.org/pdf/2308.11533v1
2308.11503v1,math.NA,Multi-level Neural Networks for Accurate Solutions of Boundary-Value Problems,2023-08-22 15:24:29+00:00,"The solution to partial differential equations using deep learning approaches
has shown promising results for several classes of initial and boundary-value
problems. However, their ability to surpass, particularly in terms of accuracy,
classical discretization methods such as the finite element methods, remains a
significant challenge. Deep learning methods usually struggle to reliably
decrease the error in their approximate solution. A new methodology to better
control the error for deep learning methods is presented here. The main idea
consists in computing an initial approximation to the problem using a simple
neural network and in estimating, in an iterative manner, a correction by
solving the problem for the residual error with a new network of increasing
complexity. This sequential reduction of the residual of the partial
differential equation allows one to decrease the solution error, which, in some
cases, can be reduced to machine precision. The underlying explanation is that
the method is able to capture at each level smaller scales of the solution
using a new network. Numerical examples in 1D and 2D are presented to
demonstrate the effectiveness of the proposed approach. This approach applies
not only to physics informed neural networks but to other neural network
solvers based on weak or strong formulations of the residual.",http://arxiv.org/pdf/2308.11503v1
2308.11469v1,math.NA,An iterative method for Helmholtz boundary value problems arising in wave propagation,2023-08-22 14:31:39+00:00,"The complex Helmholtz equation $(\Delta + k^2)u=f$ (where $k\in{\mathbb
R},u(\cdot),f(\cdot)\in{\mathbb C}$) is a mainstay of computational wave
simulation. Despite its apparent simplicity, efficient numerical methods are
challenging to design and, in some applications, regarded as an open problem.
Two sources of difficulty are the large number of degrees of freedom and the
indefiniteness of the matrices arising after discretisation. Seeking to meet
them within the novel framework of probabilistic domain decomposition, we set
out to rewrite the Helmholtz equation into a form amenable to the Feynman-Kac
formula for elliptic boundary value problems. We consider two typical
scenarios, the scattering of a plane wave and the propagation inside a cavity,
and recast them as a sequence of Poisson equations. By means of stochastic
arguments, we find a sufficient and simulatable condition for the convergence
of the iterations. Upon discretisation a necessary condition for convergence
can be derived by adding up the iterates using the harmonic series for the
matrix inverse -- we illustrate the procedure in the case of finite
differences.
  From a practical point of view, our results are ultimately of limited scope.
Nonetheless, this unexpected -- even paradoxical -- new direction of attack on
the Helmholtz equation proposed by this work offers a fresh perspective on this
classical and difficult problem. Our results show that there indeed exists a
predictable range $k<k_{max}$ in which this new ansatz works with $k_{max}$
being far below the challenging situation.",http://arxiv.org/pdf/2308.11469v1
2308.11449v1,math.NA,Convergence guarantee for consistency models,2023-08-22 13:57:35+00:00,"We provide the first convergence guarantees for the Consistency Models (CMs),
a newly emerging type of one-step generative models that can generate
comparable samples to those generated by Diffusion Models. Our main result is
that, under the basic assumptions on score-matching errors, consistency errors
and smoothness of the data distribution, CMs can efficiently sample from any
realistic data distribution in one step with small $W_2$ error. Our results (1)
hold for $L^2$-accurate score and consistency assumption (rather than
$L^\infty$-accurate); (2) do note require strong assumptions on the data
distribution such as log-Sobelev inequality; (3) scale polynomially in all
parameters; and (4) match the state-of-the-art convergence guarantee for
score-based generative models (SGMs). We also provide the result that the
Multistep Consistency Sampling procedure can further reduce the error comparing
to one step sampling, which support the original statement of ""Consistency
Models, Yang Song 2023"". Our result further imply a TV error guarantee when
take some Langevin-based modifications to the output distributions.",http://arxiv.org/pdf/2308.11449v1
2308.11390v1,math.NA,Statistical higher-order multi-scale method for nonlinear thermo-mechanical simulation of random composite materials with temperature-dependent properties,2023-08-22 12:28:14+00:00,"Stochastic multi-scale modeling and simulation for nonlinear
thermo-mechanical problems of composite materials with complicated random
microstructures remains a challenging issue. In this paper, we develop a novel
statistical higher-order multi-scale (SHOMS) method for nonlinear
thermo-mechanical simulation of random composite materials, which is designed
to overcome limitations of prohibitive computation involving the macro-scale
and micro-scale. By virtue of statistical multi-scale asymptotic analysis and
Taylor series method, the SHOMS computational model is rigorously derived for
accurately analyzing nonlinear thermo-mechanical responses of random composite
materials both in the macro-scale and micro-scale. Moreover, the local error
analysis of SHOMS solutions in the point-wise sense clearly illustrates the
crucial indispensability of establishing the higher-order asymptotic corrected
terms in SHOMS computational model for keeping the conservation of local energy
and momentum. Then, the corresponding space-time multi-scale numerical
algorithm with off-line and on-line stages is designed to efficiently simulate
nonlinear thermo-mechanical behaviors of random composite materials. Finally,
extensive numerical experiments are presented to gauge the efficiency and
accuracy of the proposed SHOMS approach.",http://arxiv.org/pdf/2308.11390v1
2308.11371v1,math.NA,Reduced Order Modeling based Inexact FETI-DP solver for lattice structures,2023-08-22 11:53:35+00:00,"This paper addresses the overwhelming computational resources needed with
standard numerical approaches to simulate architected materials. Those
multiscale heterogeneous lattice structures gain intensive interest in
conjunction with the improvement of additive manufacturing as they offer, among
many others, excellent stiffness-to-weight ratios. We develop here a dedicated
HPC solver that benefits from the specific nature of the underlying problem in
order to drastically reduce the computational costs (memory and time) for the
full fine-scale analysis of lattice structures. Our purpose is to take
advantage of the natural domain decomposition into cells and, even more
importantly, of the geometrical and mechanical similarities among cells. Our
solver consists in a so-called inexact FETI-DP method where the local,
cell-wise operators and solutions are approximated with reduced order modeling
techniques. Instead of considering independently every cell, we end up with
only few principal local problems to solve and make use of the corresponding
principal cell-wise operators to approximate all the others. It results in a
scalable algorithm that saves numerous local factorizations. Our solver is
applied for the isogeometric analysis of lattices built by spline composition,
which offers the opportunity to compute the reduced basis with macro-scale
data, thereby making our method also multiscale and matrix-free. The solver is
tested against various 2D and 3D analyses. It shows major gains with respect to
black-box solvers; in particular, problems of several millions of degrees of
freedom can be solved with a simple computer within few minutes.",http://arxiv.org/pdf/2308.11371v1
2308.11364v1,math.NA,Higher-order multi-scale method for high-accuracy nonlinear thermo-mechanical simulation of heterogeneous shells,2023-08-22 11:38:34+00:00,"In the present work, we consider multi-scale computation and convergence for
nonlinear time-dependent thermo-mechanical equations of inhomogeneous shells
possessing temperature-dependent material properties and orthogonal periodic
configurations. The first contribution is that a novel higher-order macro-micro
coupled computational model is rigorously devised via multi-scale asymptotic
technique and Taylor series approach for high-accuracy simulation of
heterogeneous shells. Benefitting from the higher-order corrected terms, the
higher-order multi-scale computational model keeps the conservation of local
energy and momentum for nonlinear thermo-mechanical simulation. Moreover, a
global error estimation with explicit rate of higher-order multi-scale
solutions is first derived in the energy norm sense. Furthermore, an efficient
space-time numerical algorithm with off-line and on-line stages is presented in
detail. Adequate numerical experiments are conducted to confirm the competitive
advantages of the presented multi-scale approach, exhibiting not only the
exceptional numerical accuracy, but also the less computational expense for
heterogeneous shells.",http://arxiv.org/pdf/2308.11364v1
2308.11314v1,math.NA,A Study of Particle Motion in the Presence of Clusters,2023-08-22 09:49:09+00:00,"The motivation for this study came from the task of analysing the kinetic
behavior of single molecules in a living cell based on Single Molecule
Localization Microscopy. Given measurements of both the motion of clusters and
molecules, the main task consists in detecting if a molecule belongs to a
cluster. While the exact size of the clusters is usually unknown, upper bounds
are available. In this study, we simulate the cluster movement by a Brownian
motion and those of the particles by a Gaussian mixture model with two modes
depending on the position of the particle within or outside a cluster. We
propose various variational models to detect if a particle lies within a
cluster based on the Wasserstein and maximum mean discrepancy distances between
measures. We compare the performance of the proposed models for simulated data.",http://arxiv.org/pdf/2308.11314v1
2308.11255v1,math.NA,A mathematical model for meniscus cartilage regeneration,2023-08-22 07:58:11+00:00,"We propose a continuous model for meniscus cartilage regeneration triggered
by two populations of cells migrating and (de)differentiating within an
artificial scaffold with a known structure. The described biological processes
are influenced by a fluid flow and therewith induced deformations of the
scaffold. Numerical simulations are done for the corresponding dynamics within
a bioreactor which was designed for performing the biological experiments.",http://arxiv.org/pdf/2308.11255v1
2308.11133v1,math.NA,Learning the solution operator of a nonlinear parabolic equation using physics informed deep operator network,2023-08-22 02:27:39+00:00,"This study focuses on addressing the challenges of solving analytically
intractable differential equations that arise in scientific and engineering
fields such as Hamilton-Jacobi-Bellman. Traditional numerical methods and
neural network approaches for solving such equations often require independent
simulation or retraining when the underlying parameters change. To overcome
this, this study employs a physics-informed DeepONet (PI-DeepONet) to
approximate the solution operator of a nonlinear parabolic equation.
PI-DeepONet integrates known physics into a deep neural network, which learns
the solution of the PDE.",http://arxiv.org/pdf/2308.11133v1
2308.10877v1,stat.CO,Monte Carlo on manifolds in high dimensions,2023-08-21 17:24:41+00:00,"We introduce an efficient numerical implementation of a Markov Chain Monte
Carlo method to sample a probability distribution on a manifold (introduced
theoretically in Zappa, Holmes-Cerfon, Goodman (2018)), where the manifold is
defined by the level set of constraint functions, and the probability
distribution may involve the pseudodeterminant of the Jacobian of the
constraints, as arises in physical sampling problems. The algorithm is easy to
implement and scales well to problems with thousands of dimensions and with
complex sets of constraints provided their Jacobian retains sparsity. The
algorithm uses direct linear algebra and requires a single matrix factorization
per proposal point, which enhances its efficiency over previously proposed
methods but becomes the computational bottleneck of the algorithm in high
dimensions. We test the algorithm on several examples inspired by soft-matter
physics and materials science to study its complexity and properties.",http://arxiv.org/pdf/2308.10877v1
2308.10748v1,math.NA,Iterative solution to the biharmonic equation in mixed form discretized by the Hybrid High-Order method,2023-08-21 14:30:06+00:00,"We consider the solution to the biharmonic equation in mixed form discretized
by the Hybrid High-Order (HHO) methods. The two resulting second-order elliptic
problems can be decoupled via the introduction of a new unknown, corresponding
to the boundary value of the solution of the first Laplacian problem. This
technique yields a global linear problem that can be solved iteratively via a
Krylov-type method. More precisely, at each iteration of the scheme, two
second-order elliptic problems have to be solved, and a normal derivative on
the boundary has to be computed. In this work, we specialize this scheme for
the HHO discretization. To this aim, an explicit technique to compute the
discrete normal derivative of an HHO solution of a Laplacian problem is
proposed. Moreover, we show that the resulting discrete scheme is well-posed.
Finally, a new preconditioner is designed to speed up the convergence of the
Krylov method. Numerical experiments assessing the performance of the proposed
iterative algorithm on both two- and three-dimensional test cases are
presented.",http://arxiv.org/pdf/2308.10748v1
2308.10720v1,math.NA,On the accuracy of interpolation based on single-layer artificial neural networks,2023-08-21 13:40:09+00:00,"In the present paper, we consider one-hidden layer ANNs with a feedforward
architecture, also referred to as shallow or two-layer networks, so that the
structure is determined by the number and types of neurons. The determination
of the parameters that define the function, called training, is done via the
resolution of the approximation problem, so by imposing the interpolation
through a set of specific nodes. We present the case where the parameters are
trained using a procedure that is referred to as Extreme Learning Machine (ELM)
that leads to a linear interpolation problem. In such hypotheses, the existence
of an ANN interpolating function is guaranteed. The focus is then on the
accuracy of the interpolation outside of the given sampling interpolation nodes
when they are the equispaced, the Chebychev, and the randomly selected ones.
The study is motivated by the well-known bell-shaped Runge example, which makes
it clear that the construction of a global interpolating polynomial is accurate
only if trained on suitably chosen nodes, ad example the Chebychev ones. In
order to evaluate the behavior when growing the number of interpolation nodes,
we raise the number of neurons in our network and compare it with the
interpolating polynomial. We test using Runge's function and other well-known
examples with different regularities. As expected, the accuracy of the
approximation with a global polynomial increases only if the Chebychev nodes
are considered. Instead, the error for the ANN interpolating function always
decays and in most cases we observe that the convergence follows what is
observed in the polynomial case on Chebychev nodes, despite the set of nodes
used for training.",http://arxiv.org/pdf/2308.10720v1
2308.10703v1,math.NA,Optimal error estimates for non-conforming approximations of linear parabolic problems with minimal regularity,2023-08-21 13:18:09+00:00,"We consider a general linear parabolic problem with extended time boundary
conditions (including initial value problems and periodic ones), and
approximate it by the implicit Euler scheme in time and the Gradient
Discretisation method in space; the latter is in fact a class of methods that
includes conforming and nonconforming finite elements, discontinuous Galerkin
methods and several others. The main result is an error estimate which holds
without supplementary regularity hypothesis on the solution. This result states
that the approximation error has the same order as the sum of the interpolation
error and the conformity error. The proof of this result relies on an inf-sup
inequality in Hilbert spaces which can be used both in the continuous and the
discrete frameworks. The error estimate result is illustrated by numerical
examples with low regularity of the solution.",http://arxiv.org/pdf/2308.10703v1
2308.10698v1,math.NA,High-Order Numerical Integration on Domains Bounded by Intersecting Level Sets,2023-08-21 13:07:03+00:00,"We present a high-order method that provides numerical integration on
volumes, surfaces, and lines defined implicitly by two smooth intersecting
level sets. To approximate the integrals, the method maps quadrature rules
defined on hypercubes to the curved domains of the integrals. This enables the
numerical integration of a wide range of integrands since integration on
hypercubes is a well known problem. The mappings are constructed by treating
the isocontours of the level sets as graphs of height functions. Numerical
experiments with smooth integrands indicate a high-order of convergence for
transformed Gauss quadrature rules on domains defined by polynomial, rational,
and trigonometric level sets. We show that the approach we have used can be
combined readily with adaptive quadrature methods. Moreover, we apply the
approach to numerically integrate on difficult geometries without requiring a
low-order fallback method.",http://arxiv.org/pdf/2308.10698v1
2308.10697v1,math.DS,Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems,2023-08-21 13:05:12+00:00,"Koopman operators linearize nonlinear dynamical systems, making their
spectral information of crucial interest. Numerous algorithms have been
developed to approximate these spectral properties, and Dynamic Mode
Decomposition (DMD) stands out as the poster child of projection-based methods.
Although the Koopman operator itself is linear, the fact that it acts in an
infinite-dimensional space of observables poses various challenges. These
include spurious modes, essential spectra, and the verification of Koopman mode
decompositions. While recent work has addressed these challenges for
deterministic systems, there remains a notable gap in verified DMD methods
tailored for stochastic systems, where the Koopman operator measures the
expectation of observables. We show that it is necessary to go beyond
expectations to address these issues. By incorporating variance into the
Koopman framework, we address these challenges. Through an additional DMD-type
matrix, we approximate the sum of a squared residual and a variance term, each
of which can be approximated individually using batched snapshot data. This
allows verified computation of the spectral properties of stochastic Koopman
operators, controlling the projection error. We also introduce the concept of
variance-pseudospectra to gauge statistical coherency. Finally, we present a
suite of convergence results for the spectral quantities of stochastic Koopman
operators. Our study concludes with practical applications using both simulated
and experimental data. In neural recordings from awake mice, we demonstrate how
variance-pseudospectra can reveal physiologically significant information
unavailable to standard expectation-based dynamical models.",http://arxiv.org/pdf/2308.10697v1
2308.10693v1,math.NA,About the ''accurate mode'' of the IEEE 1788-2015 standard for interval arithmetic,2023-08-21 12:59:53+00:00,"The IEEE 1788-2015 standard for interval arithmetic defines three accuracy
modes for the so-called set-based flavor: tightest, accurate and valid. This
work in progress focuses on the accurate mode.First, an introduction to
interval arithmetic and to the IEEE 1788-2015 standard is given, then the
accurate mode is defined. How can this accurate mode be tested, when a library
implementing interval arithmetic claims to provide this mode? The chosen
approach is unit testing, and the elaboration of testing pairs for this
approach is developed.A discussion closes this paper: how can the tester be
tested? And if we go to the roots of the subject, is the accurate mode really
relevant or should it be dropped off in the next version of the standard?",http://arxiv.org/pdf/2308.10693v1
2308.10644v1,cs.LG,Faster Training of Neural ODEs Using Gau-Legendre Quadrature,2023-08-21 11:31:15+00:00,"Neural ODEs demonstrate strong performance in generative and time-series
modelling. However, training them via the adjoint method is slow compared to
discrete models due to the requirement of numerically solving ODEs. To speed
neural ODEs up, a common approach is to regularise the solutions. However, this
approach may affect the expressivity of the model; when the trajectory itself
matters, this is particularly important. In this paper, we propose an
alternative way to speed up the training of neural ODEs. The key idea is to
speed up the adjoint method by using Gau{\ss}-Legendre quadrature to solve
integrals faster than ODE-based methods while remaining memory efficient. We
also extend the idea to training SDEs using the Wong-Zakai theorem, by training
a corresponding ODE and transferring the parameters. Our approach leads to
faster training of neural ODEs, especially for large models. It also presents a
new way to train SDE-based models.",http://arxiv.org/pdf/2308.10644v1
2308.10439v1,math.NA,On the Approximation of Singular Functions by Series of Non-integer Powers,2023-08-21 03:23:47+00:00,"In this paper, we describe an algorithm for approximating functions of the
form $f(x)=\int_{a}^{b} x^{\mu} \sigma(\mu) \, d \mu$ over $[0,1] \subset
\mathbb{R}$, where $0<a<b<\infty$ and $\sigma(\mu)$ is some signed Radon
measure over $[a,b]$ or some distribution supported on $[a,b]$. Given the
desired accuracy $\epsilon$ and the values of $a$ and $b$, our method
determines a priori a collection of non-integer powers $\{t_j\}_{j=1}^N$, so
that the functions are approximated by series of the form $f(x)\approx
\sum_{j=1}^N c_j x^{t_j}$, where the expansion coefficients can be found by
solving a square, low-dimensional Vandermonde-like linear system using the
collocation points $\{x_j\}_{j=1}^N$, also determined a priori by $\epsilon$
and the values of $a$ and $b$. We prove that our method has a small uniform
approximation error which is proportional to $\epsilon$ multiplied by some
small constants. We demonstrate the performance of our algorithm with several
numerical experiments, and show that the number of singular powers and
collocation points grows as $N=O(\log{\frac{1}{\epsilon}})$.",http://arxiv.org/pdf/2308.10439v1
2308.10430v1,math-ph,Modeling of electronic dynamics in twisted bilayer graphene,2023-08-21 02:50:13+00:00,"We consider the problem of numerically computing the quantum dynamics of an
electron in twisted bilayer graphene. The challenge is that atomic-scale models
of the dynamics are aperiodic for generic twist angles because of the
incommensurability of the layers. The Bistritzer-MacDonald PDE model, which is
periodic with respect to the bilayer's moir\'e pattern, has recently been shown
to rigorously describe these dynamics in a parameter regime. In this work, we
first prove that the dynamics of the tight-binding model of incommensurate
twisted bilayer graphene can be approximated by computations on finite domains.
The main ingredient of this proof is a speed of propagation estimate proved
using Combes-Thomas estimates. We then provide extensive numerical computations
which clarify the range of validity of the Bistritzer-MacDonald model.",http://arxiv.org/pdf/2308.10430v1
2308.10357v1,math.NA,An Explicit Fourth-Order Hybrid-Variable Method for Euler Equations with A Residual-Consistent Viscosity,2023-08-20 20:14:27+00:00,"In this paper we present a formally fourth-order accurate hybrid-variable
method for the Euler equations in the context of method of lines. The
hybrid-variable (HV) method seeks numerical approximations to both
cell-averages and nodal solutions and evolves them in time simultaneously; and
it is proved in previous work that these methods are inherent superconvergent.
Taking advantage of the superconvergence, the method is built on a third-order
discrete differential operator, which approximates the first spatial derivative
at each grid point, only using the information in the two neighboring cells.
Stability and accuracy analyses are conducted in the one-dimensional case for
the linear advection equation; whereas extension to nonlinear systems including
the Euler equations is achieved using characteristic decomposition and the
incorporation of a residual-consistent viscosity to capture strong
discontinuities. Extensive numerical tests are presented to assess the
numerical performance of the method for both 1D and 2D problems.",http://arxiv.org/pdf/2308.10357v1
2308.10130v1,math.NA,On the Approximation of Operator-Valued Riccati Equations in Hilbert Spaces,2023-08-20 01:34:20+00:00,"In this work, we present an abstract theory for the approximation of
operator-valued Riccati equations posed on Hilbert spaces. It is demonstrated
here, under the assumption of compactness in the coefficient operators, that
the error of the approximate solution to the operator-valued Riccati equation
is bounded above by the approximation error of the governing semigroup. One
significant outcome of this result is the correct prediction of optimal
convergence for finite element approximations of the operator-valued Riccati
equations for when the governing semigroup involves parabolic, as well as
hyperbolic processes. We derive the abstract theory for the time-dependent and
time-independent operator-valued Riccati equations in the first part of this
work. In the second part, we prove optimal convergence rates for the finite
element approximation of the functional gain associated with model
one-dimensional weakly damped wave and thermal LQR control systems. These
theoretical claims are then corroborated with computational evidence.",http://arxiv.org/pdf/2308.10130v1
2308.10083v1,physics.comp-ph,Poisson quadrature method of moments for 2D kinetic equations with velocity of constant magnitude,2023-08-19 18:13:54+00:00,"This work is concerned with kinetic equations with velocity of constant
magnitude. We propose a quadrature method of moments based on the Poisson
kernel, called Poisson-EQMOM. The derived moment closure systems are well
defined for all physically relevant moments and the resultant approximations of
the distribution function converge as the number of moments goes to infinity.
The convergence makes our method stand out from most existing moment methods.
Moreover, we devise a delicate moment inversion algorithm. As an application,
the Vicsek model is studied for overdamped active particles. Then the
Poisson-EQMOM is validated with a series of numerical tests including spatially
homogeneous, one-dimensional and two-dimensional problems.",http://arxiv.org/pdf/2308.10083v1
2308.10081v1,math.NA,Transporting Higher-Order Quadrature Rules: Quasi-Monte Carlo Points and Sparse Grids for Mixture Distributions,2023-08-19 18:09:27+00:00,"Integration against, and hence sampling from, high-dimensional probability
distributions is of essential importance in many application areas and has been
an active research area for decades. One approach that has drawn increasing
attention in recent years has been the generation of samples from a target
distribution $\mathbb{P}_{\mathrm{tar}}$ using transport maps: if
$\mathbb{P}_{\mathrm{tar}} = T_\# \mathbb{P}_{\mathrm{ref}}$ is the pushforward
of an easily-sampled probability distribution $\mathbb{P}_{\mathrm{ref}}$ under
the transport map $T$, then the application of $T$ to
$\mathbb{P}_{\mathrm{ref}}$-distributed samples yields
$\mathbb{P}_{\mathrm{tar}}$-distributed samples. This paper proposes the
application of transport maps not just to random samples, but also to
quasi-Monte Carlo points, higher-order nets, and sparse grids in order for the
transformed samples to inherit the original convergence rates that are often
better than $N^{-1/2}$, $N$ being the number of samples/quadrature nodes. Our
main result is the derivation of an explicit transport map for the case that
$\mathbb{P}_{\mathrm{tar}}$ is a mixture of simple distributions, e.g.\ a
Gaussian mixture, in which case application of the transport map $T$ requires
the solution of an \emph{explicit} ODE with \emph{closed-form} right-hand side.
Mixture distributions are of particular applicability and interest since many
methods proceed by first approximating $\mathbb{P}_{\mathrm{tar}}$ by a mixture
and then sampling from that mixture (often using importance reweighting).
Hence, this paper allows for the sampling step to provide a better convergence
rate than $N^{-1/2}$ for all such methods.",http://arxiv.org/pdf/2308.10081v1
2308.09997v1,math.NA,Additive Schwarz methods for semilinear elliptic problems with convex energy functionals: Convergence rate independent of nonlinearity,2023-08-19 12:24:32+00:00,"We investigate additive Schwarz methods for semilinear elliptic problems with
convex energy functionals, which have wide scientific applications. A key
observation is that the convergence rates of both one- and two-level additive
Schwarz methods have bounds independent of the nonlinear term in the problem.
That is, the convergence rates do not deteriorate by the presence of
nonlinearity, so that solving a semilinear problem requires no more iterations
than a linear problem. Moreover, the two-level method is scalable in the sense
that the convergence rate of the method depends on $H/h$ and $H/\delta$ only,
where $h$ and $H$ are the typical diameters of an element and a subdomain,
respectively, and $\delta$ measures the overlap among the subdomains. Numerical
results are provided to support our theoretical findings.",http://arxiv.org/pdf/2308.09997v1
2308.09994v2,math.NA,The extension of Weyl-type relative perturbation bounds,2023-08-19 12:09:21+00:00,"Relative perturbation theory for eigenvalues of Hermitian positive definite
matrices has been well-studied, and the major results were later derived
analogously for Hermitian non-singular matrices. In this dissertation we extend
several relative perturbation results to Hermitian matrices that are
potentially singular, and also develop a general class of relative bounds for
Hermitian matrices. As a result, corresponding relative bounds for singular
values of rank-deficient $m\times n$ matrices are also obtained using related
Jordan-Wielandt matrices. We also discuss a comparison between the main
relative bound derived and the Weyl's absolute perturbation bound in terms of
their sharpness and derivation in practice.",http://arxiv.org/pdf/2308.09994v2
2308.09956v1,math.NA,FEM-PIKFNNs for underwater acoustic propagation induced by structural vibrations in different ocean environments,2023-08-19 09:18:13+00:00,"In this paper, a novel hybrid method based on the finite element method (FEM)
and physics-informed kernel function neural networks (PIKFNNs) is proposed and
applied to the prediction of underwater acoustic propagation induced by
structural vibrations in the unbounded ocean, deep ocean and shallow ocean. In
the hybrid method, PIKFNNs are a class of improved shallow physics-informed
neural networks (PINNs) that replace the activation functions in PINNs with the
physics-informed kernel functions (PIKFs), thereby integrating prior physical
information into the neural network model. Moreover, this neural network
circumvents the step of embedding the governing equations into the loss
function in PINNs, and requires only training on boundary data. By using the
Green's functions as the PIKFs and the structural-acoustic coupling response
information obtained from the FEM as boundary training data, the PIKFNNs can
inherently capture the Sommerfeld radiation condition at infinity, which is
naturally suitable for predicting ocean acoustic propagation. Numerical
experiments demonstrate the accuracy and feasibility of the FEM-PIKFNNs in
comparison with the true solutions and finite element results.",http://arxiv.org/pdf/2308.09956v1
2308.09912v1,math.OC,Complexity Guarantees for Nonconvex Newton-MR Under Inexact Hessian Information,2023-08-19 05:36:00+00:00,"We consider extensions of the Newton-MR algorithm for nonconvex optimization
to the settings where Hessian information is approximated. Under additive noise
model on the Hessian matrix, we investigate the iteration and operation
complexities of these variants to achieve first and second-order sub-optimality
criteria. We show that, under certain conditions, the algorithms achieve
iteration and operation complexities that match those of the exact variant.
Focusing on the particular nonconvex problems satisfying Polyak-\L ojasiewicz
condition, we show that our algorithm achieves a linear convergence rate. We
finally compare the performance of our algorithms with several alternatives on
a few machine learning problems.",http://arxiv.org/pdf/2308.09912v1
2308.09864v1,math.NA,A novel reduced basis method for adjoint sensitivity analysis of dynamic topology optimization,2023-08-19 00:48:29+00:00,"In gradient-based time domain topology optimization, design sensitivity
analysis (DSA) of the dynamic response is essential, and requires high
computational cost to directly differentiate, especially for high-order dynamic
system. To address this issue, this study develops an efficient reduced basis
method (RBM)-based discrete adjoint sensitivity analysis method, which on the
one hand significantly improves the efficiency of sensitivity analysis and on
the other hand avoids the consistency errors caused by the continuum method. In
this algorithm, the basis functions of the adjoint problem are constructed in
the offline phase based on the greedy-POD method, and a novel model-based
estimation is developed to facilitate the acceleration of this process. Based
on these basis functions, a fast and reasonably accurate model is then built by
Galerkin projection for sensitivity analysis in each dynamic topology
optimization iteration. Finally, the effectiveness of the error measures, the
efficiency and the accuracy of the presented reduced-order method are verified
by 2D and 3D dynamic structure studies.",http://arxiv.org/pdf/2308.09864v1
2308.09839v2,math.NA,Performant low-order matrix-free finite element kernels on GPU architectures,2023-08-18 22:21:54+00:00,"Numerical methods such as the Finite Element Method (FEM) have been
successfully adapted to utilize the computational power of GPU accelerators.
However, much of the effort around applying FEM to GPU's has been focused on
high-order FEM due to higher arithmetic intensity and order of accuracy. For
applications such as the simulation of subsurface processes, high levels of
heterogeneity results in high-resolution grids characterized by highly
discontinuous (cell-wise) material property fields. Moreover, due to the
significant uncertainties in the characterization of the domain of interest,
e.g. geologic reservoirs, the benefits of high order accuracy are reduced, and
low-order methods are typically employed. In this study, we present a strategy
for implementing highly performant low-order matrix-free FEM operator kernels
in the context of the conjugate gradient (CG) method. Performance results of
matrix-free Laplace and isotropic elasticity operator kernels are presented and
are shown to compare favorably to matrix-based SpMV operators on V100, A100,
and MI250X GPUs.",http://arxiv.org/pdf/2308.09839v2
2308.09714v1,physics.flu-dyn,Explicit Runge-Kutta algorithm to solve non-local equations with memory effects: case of the Maxey-Riley-Gatignol equation,2023-08-18 17:59:26+00:00,"A standard approach to solve ordinary differential equations, when they
describe dynamical systems, is to adopt a Runge-Kutta or related scheme. Such
schemes, however, are not applicable to the large class of equations which do
not constitute dynamical systems. In several physical systems, we encounter
integro-differential equations with memory terms where the time derivative of a
state variable at a given time depends on all past states of the system.
Secondly, there are equations whose solutions do not have well-defined Taylor
series expansion. The Maxey-Riley-Gatignol equation, which describes the
dynamics of an inertial particle in nonuniform and unsteady flow, displays both
challenges. We use it as a test bed to address the questions we raise, but our
method may be applied to all equations of this class. We show that the
Maxey-Riley-Gatignol equation can be embedded into an extended Markovian system
which is constructed by introducing a new dynamical co-evolving state variable
that encodes memory of past states. We develop a Runge-Kutta algorithm for the
resultant Markovian system. The form of the kernels involved in deriving the
Runge-Kutta scheme necessitates the use of an expansion in powers of $t^{1/2}$.
Our approach naturally inherits the benefits of standard time-integrators,
namely a constant memory storage cost, a linear growth of operational effort
with simulation time, and the ability to restart a simulation with the final
state as the new initial condition.",http://arxiv.org/pdf/2308.09714v1
2308.09605v1,math.NA,Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks,2023-08-18 14:58:23+00:00,"Physics-informed neural networks (PINNs) have been demonstrated to be
efficient in solving partial differential equations (PDEs) from a variety of
experimental perspectives. Some recent studies have also proposed PINN
algorithms for PDEs on surfaces, including spheres. However, theoretical
understanding of the numerical performance of PINNs, especially PINNs on
surfaces or manifolds, is still lacking. In this paper, we establish rigorous
analysis of the physics-informed convolutional neural network (PICNN) for
solving PDEs on the sphere. By using and improving the latest approximation
results of deep convolutional neural networks and spherical harmonic analysis,
we prove an upper bound for the approximation error with respect to the Sobolev
norm. Subsequently, we integrate this with innovative localization complexity
analysis to establish fast convergence rates for PICNN. Our theoretical results
are also confirmed and supplemented by our experiments. In light of these
findings, we explore potential strategies for circumventing the curse of
dimensionality that arises when solving high-dimensional PDEs.",http://arxiv.org/pdf/2308.09605v1
2308.09598v1,math.NA,Enhancing multiplex global efficiency,2023-08-18 14:52:02+00:00,"Modeling complex systems that consist of different types of objects leads to
multilayer networks, in which vertices are connected by both inter-layer and
intra-layer edges. In this paper, we investigate multiplex networks, in which
vertices in different layers are identified with each other, and the only
inter-layer edges are those that connect a vertex with its copy in other
layers. Let the third-order adjacency tensor $\mathcal{A}\in\R^{N\times N\times
L}$ and the parameter $\gamma\geq 0$, which is associated with the ease of
communication between layers, represent a multiplex network with $N$ vertices
and $L$ layers. To measure the ease of communication in a multiplex network, we
focus on the average inverse geodesic length, which we refer to as the
multiplex global efficiency $e_\mathcal{A}(\gamma)$ by means of the multiplex
path length matrix $P\in\R^{N\times N}$. This paper generalizes the approach
proposed in \cite{NR23} for single-layer networks. We describe an algorithm
based on min-plus matrix multiplication to construct $P$, as well as variants
$P^K$ that only take into account multiplex paths made up of at most $K$
intra-layer edges. These matrices are applied to detect redundant edges and to
determine non-decreasing lower bounds $e_\mathcal{A}^K(\gamma)$ for
$e_\mathcal{A}(\gamma)$, for $K=1,2,\dots,N-2$.
  Finally, the sensitivity of $e_\mathcal{A}^K(\gamma)$ to changes of the
entries of the adjacency tensor $\mathcal{A}$ is investigated to determine
edges that should be strengthened to enhance the multiplex global efficiency
the most.",http://arxiv.org/pdf/2308.09598v1
2308.09571v1,cs.LG,Physics-Informed Boundary Integral Networks (PIBI-Nets): A Data-Driven Approach for Solving Partial Differential Equations,2023-08-18 14:03:34+00:00,"Partial differential equations (PDEs) can describe many relevant phenomena in
dynamical systems. In real-world applications, we commonly need to combine
formal PDE models with (potentially noisy) observations. This is especially
relevant in settings where we lack information about boundary or initial
conditions, or where we need to identify unknown model parameters. In recent
years, Physics-informed neural networks (PINNs) have become a popular tool for
problems of this kind. In high-dimensional settings, however, PINNs often
suffer from computational problems because they usually require dense
collocation points over the entire computational domain. To address this
problem, we present Physics-Informed Boundary Integral Networks (PIBI-Nets) as
a data-driven approach for solving PDEs in one dimension less than the original
problem space. PIBI-Nets only need collocation points at the computational
domain boundary, while still achieving highly accurate results, and in several
practical settings, they clearly outperform PINNs. Exploiting elementary
properties of fundamental solutions of linear differential operators, we
present a principled and simple way to handle point sources in inverse
problems. We demonstrate the excellent performance of PIBI-Nets for the Laplace
and Poisson equations, both on artificial data sets and within a real-world
application concerning the reconstruction of groundwater flows.",http://arxiv.org/pdf/2308.09571v1
2308.09556v1,math.OC,A Principle for Global Optimization with Gradients,2023-08-18 13:39:29+00:00,"This work demonstrates the utility of gradients for the global optimization
of certain differentiable functions with many suboptimal local minima. To this
end, a principle for generating search directions from non-local quadratic
approximants based on gradients of the objective function is analyzed.
Experiments measure the quality of non-local search directions as well as the
performance of a proposed simplistic algorithm, of the covariance matrix
adaptation evolution strategy (CMA-ES), and of a randomly reinitialized
Broyden-Fletcher-Goldfarb-Shanno (BFGS) method.",http://arxiv.org/pdf/2308.09556v1
2308.09460v1,stat.CO,Accelerated Bayesian imaging by relaxed proximal-point Langevin sampling,2023-08-18 10:55:49+00:00,"This paper presents a new accelerated proximal Markov chain Monte Carlo
methodology to perform Bayesian inference in imaging inverse problems with an
underlying convex geometry. The proposed strategy takes the form of a
stochastic relaxed proximal-point iteration that admits two complementary
interpretations. For models that are smooth or regularised by Moreau-Yosida
smoothing, the algorithm is equivalent to an implicit midpoint discretisation
of an overdamped Langevin diffusion targeting the posterior distribution of
interest. This discretisation is asymptotically unbiased for Gaussian targets
and shown to converge in an accelerated manner for any target that is
$\kappa$-strongly log-concave (i.e., requiring in the order of $\sqrt{\kappa}$
iterations to converge, similarly to accelerated optimisation schemes),
comparing favorably to [M. Pereyra, L. Vargas Mieles, K.C. Zygalakis, SIAM J.
Imaging Sciences, 13, 2 (2020), pp. 905-935] which is only provably accelerated
for Gaussian targets and has bias. For models that are not smooth, the
algorithm is equivalent to a Leimkuhler-Matthews discretisation of a Langevin
diffusion targeting a Moreau-Yosida approximation of the posterior distribution
of interest, and hence achieves a significantly lower bias than conventional
unadjusted Langevin strategies based on the Euler-Maruyama discretisation. For
targets that are $\kappa$-strongly log-concave, the provided non-asymptotic
convergence analysis also identifies the optimal time step which maximizes the
convergence speed. The proposed methodology is demonstrated through a range of
experiments related to image deconvolution with Gaussian and Poisson noise,
with assumption-driven and data-driven convex priors.",http://arxiv.org/pdf/2308.09460v1
2308.09394v1,physics.flu-dyn,An Eigenvalue-Free Implementation of the Log-Conformation Formulation,2023-08-18 08:51:31+00:00,"The log-conformation formulation, although highly successful, was from the
beginning formulated as a partial differential equation that contains an, for
PDEs unusual, eigenvalue decomposition of the unknown field. To this day, most
numerical implementations have been based on this or a similar eigenvalue
decomposition, with Knechtges et al. (2014) being the only notable exception
for two-dimensional flows.
  In this paper, we present an eigenvalue-free algorithm to compute the
constitutive equation of the log-conformation formulation that works for two-
and three-dimensional flows. Therefore, we first prove that the challenging
terms in the constitutive equations are representable as a matrix function of a
slightly modified matrix of the log-conformation field. We give a proof of
equivalence of this term to the more common log-conformation formulations.
Based on this formulation, we develop an eigenvalue-free algorithm to evaluate
this matrix function. The resulting full formulation is first discretized using
a finite volume method, and then tested on the confined cylinder and
sedimenting sphere benchmarks.",http://arxiv.org/pdf/2308.09394v1
2308.09367v1,math.NA,On the Approximation of Bi-Lipschitz Maps by Invertible Neural Networks,2023-08-18 08:01:45+00:00,"Invertible neural networks (INNs) represent an important class of deep neural
network architectures that have been widely used in several applications. The
universal approximation properties of INNs have also been established recently.
However, the approximation rate of INNs is largely missing. In this work, we
provide an analysis of the capacity of a class of coupling-based INNs to
approximate bi-Lipschitz continuous mappings on a compact domain, and the
result shows that it can well approximate both forward and inverse maps
simultaneously. Furthermore, we develop an approach for approximating
bi-Lipschitz maps on infinite-dimensional spaces that simultaneously
approximate the forward and inverse maps, by combining model reduction with
principal component analysis and INNs for approximating the reduced map, and we
analyze the overall approximation error of the approach. Preliminary numerical
results show the feasibility of the approach for approximating the solution
operator for parameterized second-order elliptic problems.",http://arxiv.org/pdf/2308.09367v1
2308.09330v1,math.NA,A non-overlapping Schwarz algorithm for the HDG method,2023-08-18 06:24:56+00:00,"In this paper, we present two non-overlapping Schwarz algorithms for the
hybridizable discontinuous Galerkin (HDG) method. The first algorithm is based
on the Neumann-Neumann method. The second one is an iterative algorithm uses
both trace and flux interface unknowns on interfaces between subdomains.
Numerical results are provided to verify the validity of our algorithms.",http://arxiv.org/pdf/2308.09330v1
2308.09288v1,math.AP,Damping for fractional wave equations and applications to water waves,2023-08-18 04:18:34+00:00,"Motivated by numerically modeling surface waves for inviscid Euler equations,
we analyze linear models for damped water waves and establish decay properties
for the energy for sufficiently regular initial configurations. Our findings
give the explicit decay rates for the energy, but do not address
reflection/transmission of waves at the interface of the damping. Still for a
subset of the models considered, this represents the first result proving the
decay of the energy of the surface wave models.",http://arxiv.org/pdf/2308.09288v1
2308.09265v1,math.NA,On a numerical artifact of solving shallow water equations with a discontinuous bottom: Analysis and a nontransonic fix,2023-08-18 02:48:33+00:00,"In this paper, we study a numerical artifact of solving the nonlinear shallow
water equations with a discontinuous bottom topography. For various first-order
schemes, the numerical solution of the momentum will form a spurious spike at
the discontinuous points of the bottom, which should not exist in the exact
solution. The height of the spike cannot be reduced even after the mesh is
refined. For subsonic problems, this numerical artifact may cause the wrong
convergence to a function far away from the exact solution. To explain the
formation of the spurious spike, we perform a convergence analysis by proving a
Lax--Wendroff type theorem. It is shown that the spurious spike is caused by
the numerical viscosity in the computation of the water height at the
discontinuous bottom. The height of the spike is proportional to the magnitude
of the viscosity constant in the Lax--Friedrichs flux. Motivated by this
conclusion, we propose a modified scheme by adopting the central flux at the
bottom discontinuity in the equation of mass conservation, and show that this
numerical artifact can be removed in many cases. For various numerical tests
with nontransonic Riemann solutions, we observe that the modified scheme is
able to retrieve the correct convergence.",http://arxiv.org/pdf/2308.09265v1
2308.09250v1,cs.LG,Capacity Bounds for Hyperbolic Neural Network Representations of Latent Tree Structures,2023-08-18 02:24:32+00:00,"We study the representation capacity of deep hyperbolic neural networks
(HNNs) with a ReLU activation function. We establish the first proof that HNNs
can $\varepsilon$-isometrically embed any finite weighted tree into a
hyperbolic space of dimension $d$ at least equal to $2$ with prescribed
sectional curvature $\kappa<0$, for any $\varepsilon> 1$ (where $\varepsilon=1$
being optimal). We establish rigorous upper bounds for the network complexity
on an HNN implementing the embedding. We find that the network complexity of
HNN implementing the graph representation is independent of the representation
fidelity/distortion. We contrast this result against our lower bounds on
distortion which any ReLU multi-layer perceptron (MLP) must exert when
embedding a tree with $L>2^d$ leaves into a $d$-dimensional Euclidean space,
which we show at least $\Omega(L^{1/d})$; independently of the depth, width,
and (possibly discontinuous) activation function defining the MLP.",http://arxiv.org/pdf/2308.09250v1
2308.09232v2,math.NA,Hadamard integrator for time-dependent wave equations: Lagrangian formulation via ray tracing,2023-08-18 01:25:38+00:00,"We propose a novel Hadamard integrator for the self-adjoint time-dependent
wave equation in an inhomogeneous medium. First, we create a new asymptotic
series based on the Gelfand-Shilov function, dubbed Hadamard's ansatz, to
approximate the Green's function of the time-dependent wave equation. Second,
incorporating the leading term of Hadamard's ansatz into the Kirchhoff-Huygens
representation, we develop an original Hadamard integrator for the Cauchy
problem of the time-dependent wave equation and derive the corresponding
Lagrangian formulation in geodesic polar coordinates. Third, to construct the
Hadamard integrator in the Lagrangian formulation efficiently, we use a
short-time ray tracing method to obtain wavefront locations accurately, and we
further develop fast algorithms to compute Chebyshev-polynomial based low-rank
representations of both wavefront locations and variants of Hadamard
coefficients. Fourth, equipped with these low-rank representations, we apply
the Hadamard integrator to efficiently solve time-dependent wave equations with
highly oscillatory initial conditions, where the time step size is independent
of the initial conditions. By judiciously choosing the medium-dependent time
step, our new Hadamard integrator can propagate wave field beyond caustics
implicitly and advance spatially overturning waves in time naturally. Moreover,
since the integrator is independent of initial conditions, the Hadamard
integrator can be applied to many different initial conditions once it is
constructed. Both two-dimensional and three-dimensional numerical examples
illustrate the accuracy and performance of the proposed method.",http://arxiv.org/pdf/2308.09232v2
2308.09224v1,math.OC,Geometric characterizations for strong minima with applications to nuclear norm minimization problems,2023-08-18 00:56:58+00:00,"In this paper, we introduce several geometric characterizations for strong
minima of optimization problems. Applying these results to nuclear norm
minimization problems allows us to obtain new necessary and sufficient
quantitative conditions for this important property. Our characterizations for
strong minima are weaker than the Restricted Injectivity and Nondegenerate
Source Condition, which are usually used to identify solution uniqueness of
nuclear norm minimization problems. Consequently, we obtain the minimum (tight)
bound on the number of measurements for (strong) exact recovery of low-rank
matrices.",http://arxiv.org/pdf/2308.09224v1
2308.09208v1,math.NA,A hybrid PML formulation for the 2D three-field dynamic poroelastic equations,2023-08-17 23:27:49+00:00,"Simulation of wave propagation in poroelastic half-spaces presents a common
challenge in fields like geomechanics and biomechanics, requiring Absorbing
Boundary Conditions (ABCs) at the semi-infinite space boundaries. Perfectly
Matched Layers (PML) are a popular choice due to their excellent wave
absorption properties. However, PML implementation can lead to problems with
unknown stresses or strains, time convolutions, or PDE systems with Auxiliary
Differential Equations (ADEs), which increases computational complexity and
resource consumption.
  This article presents two new PML formulations for arbitrary poroelastic
domains. The first formulation is a fully-mixed form that employs time-history
variables instead of ADEs, reducing the number of unknowns and mathematical
operations. The second formulation is a hybrid form that restricts the
fully-mixed formulation to the PML domain, resulting in smaller matrices for
the solver while preserving governing equations in the interior domain. The
fully-mixed formulation introduces three scalar variables over the whole
domain, whereas the hybrid form confines them to the PML domain.
  The proposed formulations were tested in three numerical experiments in
geophysics using realistic parameters for soft sites with free surfaces. The
results were compared with numerical solutions from extended domains and
simpler ABCs, such as paraxial approximation, demonstrating the accuracy,
efficiency, and precision of the proposed methods. The article also discusses
the applicability of these methods to complex media and their extension to the
Multiaxial PML formulation.
  The codes for the simulations are available for download from
\url{https://github.com/hmella/POROUS-HYBRID-PML}.",http://arxiv.org/pdf/2308.09208v1
2308.09169v1,math.OC,A DPG method for linear quadratic optimal control problems,2023-08-17 19:56:43+00:00,"The DPG method with optimal test functions for solving linear quadratic
optimal control problems with control constraints is studied. We prove
existence of a unique optimal solution of the nonlinear discrete problem and
characterize it through first order optimality conditions. Furthermore, we
systematically develop a priori as well as a posteriori error estimates. Our
proposed method can be applied to a wide range of constrained optimal control
problems subject to, e.g., scalar second-order PDEs and the Stokes equations.
Numerical experiments that illustrate our theoretical findings are presented.",http://arxiv.org/pdf/2308.09169v1
2308.09068v1,math.NA,Close to optimal column approximations with a single SVD,2023-08-17 15:54:41+00:00,"The best column approximation in the Frobenius norm with $r$ columns has an
error at most $\sqrt{r+1}$ times larger than the truncated singular value
decomposition. Reaching this bound in practice involves either expensive random
volume sampling or at least $r$ executions of singular value decomposition. In
this paper it will be shown that the same column approximation bound can be
reached with only a single SVD (which can also be replaced with approximate
SVD). As a corollary, it will be shown how to find a highly nondegenerate
submatrix in $r$ rows of size $N$ in just $O(Nr^2)$ operations, which mostly
has the same properties as the maximum volume submatrix.",http://arxiv.org/pdf/2308.09068v1
2308.14618v1,physics.chem-ph,Seniority and Hierarchy Configuration Interaction for Radicals and Excited States,2023-08-28 14:42:13+00:00,"Hierarchy configuration interaction (hCI) has been recently introduced as an
alternative configuration interaction (CI) route combining excitation degree
and seniority number, which showed to efficiently recover both dynamic and
static correlations for closed-shell molecular systems
[\href{https://doi.org/10.1021/acs.jpclett.2c00730}{\textit{J.~Phys.~Chem.~Lett.}~\textbf{2022},
\textit{13}, 4342}]. Here, we generalize hCI for an arbitrary reference
determinant, allowing calculations for radicals and for excited states in a
state-specific way. We gauge this route against excitation-based CI (eCI) and
seniority-based CI (sCI) by evaluating how different ground-state properties of
radicals converge to the full CI limit. We find that hCI outperforms or matches
eCI, whereas sCI is far less accurate, in line with previous observations for
closed-shell molecules. Employing the second-order Epstein-Nesbet perturbation
theory as a correction significantly accelerates the convergence of hCI and
eCI. We further explore various hCI and sCI models to calculate excitation
energies of closed- and open-shell systems. Our results underline that both the
choice of the reference determinant and the set of orbitals drive the fine
balance between correlation of ground and excited states. State-specific hCI2
and higher order models perform similarly to their eCI counterparts, whereas
lower orders of hCI deliver poor results. In turn, sCI1 produces decent
excitation energies for radicals, encouraging the development of related
seniority-based coupled cluster methods.",http://arxiv.org/pdf/2308.14618v1
2308.13863v1,physics.comp-ph,Full-scale ab initio simulations of laser-driven atomistic dynamics,2023-08-26 12:46:45+00:00,"The coupling of excited states and ionic dynamics is the basic and
challenging point for the materials response at extreme conditions. In
laboratory, the intense laser produces transient nature and complexity with
highly nonequilibrium states, making it extremely difficult and interesting for
both experimental measurements and theoretical methods. With the inclusion of
laser-excited states, we extended ab initio method into the direct simulations
of whole laser-driven microscopic dynamics from solid to liquid. We constructed
the framework of combining the electron-temperaturedependent deep neural
network potential energy surface with hybrid atomistic-continuum approach,
controlling non-adiabatic energy exchange and atomistic dynamics, which enables
consistent interpretation of experimental data. By large scale ab inito
simulations, we demonstrate that the nonthermal effects introduced by hot
electrons play a dominant role in modulating the lattice dynamics,
thermodynamic pathway, and structural transformation. We highlight that the
present work provides a path to realistic computational studies of laser-driven
processes, thus bridging the gap between experiments and simulations.",http://arxiv.org/pdf/2308.13863v1
2308.13727v1,physics.plasm-ph,Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: II. dynamics forecasting,2023-08-26 01:48:29+00:00,"In part I of the article, we demonstrated that a variant of the Dynamic Mode
Decomposition (DMD) algorithm based on variable projection optimization, called
Optimized DMD (OPT-DMD), enables a robust identification of the dominant
spatiotemporally coherent modes underlying the data across various test cases
representing different physical parameters in an ExB simulation configuration.
As the OPT-DMD can be constrained to produce stable reduced-order models (ROMs)
by construction, in this paper, we extend the application of the OPT-DMD and
investigate the capabilities of the linear ROM from this algorithm toward
forecasting in time of the plasma dynamics in configurations representative of
the radial-azimuthal and axial-azimuthal cross-sections of a Hall thruster and
over a range of simulation parameters in each test case. The predictive
capacity of the OPT-DMD ROM is assessed primarily in terms of short-term
dynamics forecast or, in other words, for large ratios of training-to-test
data. However, the utility of the ROM for long-term dynamics forecasting is
also presented for an example case in the radial-azimuthal configuration. The
model's predictive performance is heterogeneous across various test cases.
Nonetheless, a remarkable predictiveness is observed in the test cases that do
not exhibit highly transient behaviors. Moreover, in all investigated cases,
the error between the ground-truth and the reconstructed data from the OPT-DMD
ROM remains bounded over time within both the training and the test window. As
a result, despite its limitation in terms of generalized applicability to all
plasma conditions, the OPT-DMD is proven as a reliable method to develop low
computational cost and highly predictive data-driven reduced-order models in
systems with a quasi-periodic global evolution of the plasma state.",http://arxiv.org/pdf/2308.13727v1
2308.13726v1,physics.plasm-ph,Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: I. Extraction of spatiotemporally coherent patterns,2023-08-26 01:37:52+00:00,"In this two-part article, we evaluate the utility and the generalizability of
the Dynamic Mode Decomposition (DMD) algorithm for data-driven analysis and
reduced-order modelling of plasma dynamics in cross-field ExB configurations.
The DMD algorithm is an interpretable data-driven method that finds a best-fit
linear model describing the time evolution of spatiotemporally coherent
structures (patterns) in data. We have applied the DMD to extensive
high-fidelity datasets generated using a particle-in-cell (PIC) code based on a
cost-efficient reduced-order PIC scheme. In this part, we first provide an
overview of the concept of DMD and its underpinning Proper Orthogonal and
Singular Value Decomposition methods. Two of the main DMD variants are next
introduced. We then present and discuss the results of the DMD application in
terms of the identification and extraction of the dominant spatiotemporal modes
from high-fidelity data over a range of simulation conditions. We demonstrate
that the DMD variant based on variable projection optimization (OPT-DMD)
outperforms the basic DMD method in identification of the modes underlying the
data, leading to notably more reliable reconstruction of the ground-truth.
Furthermore, we show in multiple test cases that the discrete frequency
spectrum of OPT-DMD-extracted modes is consistent with the temporal spectrum
from the Fast Fourier Transform of the data. This observation implies that the
OPT-DMD augments the conventional spectral analyses by being able to uniquely
reveal the spatial structure of the dominant modes in the frequency spectra,
thus, yielding more accessible, comprehensive information on the spatiotemporal
characteristics of the plasma phenomena.",http://arxiv.org/pdf/2308.13726v1
2308.13692v1,cond-mat.mtrl-sci,Enhanced Spin Hall Ratio in Two-Dimensional III-V Semiconductors,2023-08-25 22:21:27+00:00,"Spin Hall effect plays a critical role in spintronics since it can convert
charge current to spin current. Using state-of-the-art ab initio calculations
including quadrupole and spin-orbit coupling, the charge and spin transports
have been investigated in pristine and doped two-dimensional III-V
semiconductors. Valence bands induce a strong scattering which limits charge
conductivity in the hole-doped system, where spin Hall conductivity is enhanced
by the spin-orbit splitting, yielding an ultrahigh spin Hall ratio
$\xi\approx0.9$ in GaAs monolayer at room temperature.",http://arxiv.org/pdf/2308.13692v1
2308.13432v1,physics.acc-ph,Dephasingless laser wakefield acceleration in the bubble regime,2023-08-25 15:25:17+00:00,"Laser wakefield accelerators (LWFAs) have electric fields that are orders of
magnitude larger than those of conventional accelerators, promising an
attractive, small-scale alternative for next-generation light sources and
lepton colliders. The maximum energy gain in a single-stage LWFA is limited by
dephasing, which occurs when the trapped particles outrun the accelerating
phase of the wakefield. Here, we demonstrate that a single space-time
structured laser pulse can be used for ionization injection and electron
acceleration over many dephasing lengths in the bubble regime. Simulations of a
dephasingless laser wakefield accelerator driven by a 6.2-J laser pulse show 25
pC of injected charge accelerated over 20 dephasing lengths (1.3 cm) to a
maximum energy of 2.1 GeV. The space-time structured laser pulse features an
ultrashort, programmable-trajectory focus. Accelerating the focus, reducing the
focused spot-size variation, and mitigating unwanted self-focusing stabilize
the electron acceleration, which improves beam quality and leads to projected
energy gains of 125 GeV in a single, sub-meter stage driven by a 500-J pulse.",http://arxiv.org/pdf/2308.13432v1
2308.13299v1,physics.flu-dyn,Microstructure-based prediction of hydrodynamic forces in stationary particle assemblies,2023-08-25 10:50:17+00:00,"In the work, we derive novel hydrodynamic force models to describe the
interaction of a flow with particles in an assembly when only an averaged
resolution of the flow is available. These force models are able to predict the
average drag on the particle assembly, as well as the deviations from the
average drag force and the lift force for each individual particle in the
assembly. To achieve this, PR-DNS of various particle assemblies and flow
regimes are carried out, varying the particle volume fraction up to 0.6, and
the mean particle flow Reynolds number up to 300. To characterize the structure
of the particles in the assembly, a Voronoi tessellation is carried out, and a
number of scalars, vectors and tensors are defined based upon this
tessellation. The microstructure informed hydrodynamic force models are based
on symbolic regressions of these quantities derived from the Voronoi
tessellation, the global particle volume fraction of the particle assembly and
the flow regime represented by the Reynolds number, and the forces on the
individual particles in the assembly.
  The resulting hydrodynamic force models are single expressions can be
directly employed in a Lagrangian particle tracking (LPT) or computational
fluid dynamics/discrete element model (CFD/DEM) framework. By comparing the
results of the newly proposed hydrodynamic force models with an averaged force
model, as is usually adopted in Lagrangian particle tracking simulations, we
show that a significant increase in accuracy can be achieved, without
significantly increasing the cost of the simulation.",http://arxiv.org/pdf/2308.13299v1
2308.13280v1,physics.ao-ph,AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning,2023-08-25 10:02:26+00:00,"The atmosphere affects humans in a multitude of ways, from loss of life due
to adverse weather effects to long-term social and economic impacts on
societies. Computer simulations of atmospheric dynamics are, therefore, of
great importance for the well-being of our and future generations. Here, we
propose AtmoRep, a novel, task-independent stochastic computer model of
atmospheric dynamics that can provide skillful results for a wide range of
applications. AtmoRep uses large-scale representation learning from artificial
intelligence to determine a general description of the highly complex,
stochastic dynamics of the atmosphere from the best available estimate of the
system's historical trajectory as constrained by observations. This is enabled
by a novel self-supervised learning objective and a unique ensemble that
samples from the stochastic model with a variability informed by the one in the
historical record. The task-independent nature of AtmoRep enables skillful
results for a diverse set of applications without specifically training for
them and we demonstrate this for nowcasting, temporal interpolation, model
correction, and counterfactuals. We also show that AtmoRep can be improved with
additional data, for example radar observations, and that it can be extended to
tasks such as downscaling. Our work establishes that large-scale neural
networks can provide skillful, task-independent models of atmospheric dynamics.
With this, they provide a novel means to make the large record of atmospheric
observations accessible for applications and for scientific inquiry,
complementing existing simulations based on first principles.",http://arxiv.org/pdf/2308.13280v1
2308.13222v1,physics.comp-ph,Bayesian Reasoning for Physics Informed Neural Networks,2023-08-25 07:38:50+00:00,"Physics informed neural network (PINN) approach in Bayesian formulation is
presented. We adopt the Bayesian neural network framework formulated by MacKay
(Neural Computation 4 (3) (1992) 448). The posterior densities are obtained
from Laplace approximation. For each model (fit), the so-called evidence is
computed. It is a measure that classifies the hypothesis. The most optimal
solution has the maximal value of the evidence. The Bayesian framework allows
us to control the impact of the boundary contribution to the total loss.
Indeed, the relative weights of loss components are fine-tuned by the Bayesian
algorithm. We solve heat, wave, and Burger's equations. The obtained results
are in good agreement with the exact solutions. All solutions are provided with
the uncertainties computed within the Bayesian framework.",http://arxiv.org/pdf/2308.13222v1
2308.13096v1,cond-mat.mtrl-sci,Electronic Structure Prediction of Multi-million Atom Systems Through Uncertainty Quantification Enabled Transfer Learning,2023-08-24 21:41:29+00:00,"The ground state electron density - obtainable using Kohn-Sham Density
Functional Theory (KS-DFT) simulations - contains a wealth of material
information, making its prediction via machine learning (ML) models attractive.
However, the computational expense of KS-DFT scales cubically with system size
which tends to stymie training data generation, making it difficult to develop
quantifiably accurate ML models that are applicable across many scales and
system configurations. Here, we address these fundamental challenges using
Bayesian neural networks and employ transfer learning to leverage the
multi-scale nature of the training data. Our ML models employ descriptors
involving simple scalar products, comprehensively sample system configurations
through thermalization, and quantify uncertainty in electron density
predictions. We show that our models incur significantly lower data generation
costs while allowing confident - and when verifiable, accurate - predictions
for a wide variety of bulk systems well beyond training, including systems with
defects, different alloy compositions, and at unprecedented, multi-million-atom
scales.",http://arxiv.org/pdf/2308.13096v1
2308.12754v1,cond-mat.soft,PyMembrane: A flexible framework for efficient simulations of elastic and liquid membranes,2023-08-24 13:01:12+00:00,"PyMembrane is a software package for simulating liquid and elastic membranes
using a discretisation of the continuum description based on unstructured
triangulated two-dimensional meshes embedded in three-dimensional space. The
package is written in C++, with a flexible and intuitive Python interface,
allowing for a quick setup, execution and analysis of complex simulations.
PyMembrane follows modern software engineering principles and features a
modular design that allows for straightforward implementation of custom
extensions while ensuring consistency and enabling inexpensive maintenance. A
hallmark feature of this design is the use of a standardized C++ interface
which streamlines adding new functionalities. Furthermore, PyMembrane uses data
structures optimised for unstructured meshes, ensuring efficient mesh
operations and force calculations. By providing several templates for typical
simulations supplemented by extensive documentation, the users can seamlessly
set up and run research-level simulations and extend the package to integrate
additional features, underscoring PyMembrane's commitment to user-centric
design.",http://arxiv.org/pdf/2308.12754v1
2308.13555v1,cond-mat.stat-mech,Probabilistic description of dissipative chaotic scattering,2023-08-24 12:19:32+00:00,"We investigate the extent to which the probabilistic properties of a chaotic
scattering system with dissipation can be understood from the properties of the
dissipation-free system. For large energies $E$, a fully chaotic scattering
leads to an exponential decay of the survival probability $P(t) \sim e^{-\kappa
t}$ with an escape rate $\kappa$ that decreases with $E$. Dissipation
$\gamma>0$ leads to the appearance of different finite-time regimes in $P(t)$.
We show how these different regimes can be understood for small $\gamma\ll 1$
and $t\gg 1/\kappa_0$ from the effective escape rate
$\kappa_\gamma(t)=\kappa_0(E(t))$ (including the non-hyperbolic regime) until
the energy reaches a critical value $E_c$ at which no escape is possible. More
generally, we argue that for small dissipation $\gamma$ and long times $t$ the
surviving trajectories in the dissipative system are distributed according to
the conditionally invariant measure of the conservative system at the
corresponding energy $E(t)<E(0)$. Quantitative predictions of our general
theory are compared with numerical simulations in the Henon-Heiles model.",http://arxiv.org/pdf/2308.13555v1
2308.12717v1,physics.chem-ph,Erfonium: A Hooke Atom with Soft Interaction Potential,2023-08-24 11:31:55+00:00,"Properties of erfonium, a Hooke atom with the Coulomb interaction potential
$1/r$ replaced by a non-singular $\text{erf}(\mu r)/r$ potential are
investigated. The structure of the Hooke atom potential and properties of its
energy spectrum, relative to the ones of the spherical harmonic oscillator and
of harmonium, are analyzed. It is shown, that at a certain value of $\mu$ the
system changes its behavior from a harmonium-like regime to a
harmonic-oscillator-like regime.",http://arxiv.org/pdf/2308.12717v1
2308.12358v1,cond-mat.str-el,variPEPS -- a versatile tensor network library for variational ground state simulations in two spatial dimensions,2023-08-23 18:03:14+00:00,"Tensor networks capture large classes of ground states of phases of quantum
matter faithfully and efficiently. Their manipulation and contraction has
remained a challenge over the years, however. For most of the history, ground
state simulations of two-dimensional quantum lattice systems using (infinite)
projected entangled pair states have relied on what is called a time-evolving
block decimation. In recent years, multiple proposals for the variational
optimization of the quantum state have been put forward, overcoming accuracy
and convergence problems of previously known methods. The incorporation of
automatic differentiation in tensor networks algorithms has ultimately enabled
a new, flexible way for variational simulation of ground states and excited
states. In this work, we review the state of the art of the variational iPEPS
framework. We present and explain the functioning of an efficient,
comprehensive and general tensor network library for the simulation of infinite
two-dimensional systems using iPEPS, with support for flexible unit cells and
different lattice geometries.",http://arxiv.org/pdf/2308.12358v1
2308.12206v1,cond-mat.mtrl-sci,"Plastic deformation mechanisms during nanoindentation of W, Mo, V body-centered cubic single crystals and their corresponding W-Mo, W-V equiatomic random solid solutions",2023-08-23 15:43:21+00:00,"Deformation plasticity mechanisms in alloys and compounds may unveil the
material capacity towards optimal mechanical properties. We conduct a series of
molecular dynamics (MD) simulations to investigate plasticity mechanisms due to
nanoindentation in pure tungsten, molybdenum and vanadium body-centered cubic
single crystals, as well as the also body-centered cubic, equiatomic, random
solid solutions (RSS) of tungsten--molybdenum and tungsten--vanadium alloys.
Our analysis focuses on a thorough, side-by-side comparison of dynamic
deformation processes, defect nucleation, and evolution, along with
corresponding stress--strain curves. We also check the surface morphology of
indented samples through atomic shear strain mapping. As expected, the presence
of Mo and V atoms in W matrices introduces lattice strain and distortion,
increasing material resistance to deformation and slowing down dislocation
mobility of dislocation loops with a Burgers vector of 1/2 $\langle 111
\rangle$. Our side-by-side comparison displays a remarkable suppression of the
plastic zone size in equiatomic W--V RSS, but not in equiatomic W--Mo RSS
alloys, displaying a clear prediction for optimal hardening response equiatomic
W--V RSS alloys. If the small-depth nanoindentation plastic response is
indicative of overall mechanical performance, it is possible to conceive a
novel MD-based pathway towards material design for mechanical applications in
complex, multi-component alloys.",http://arxiv.org/pdf/2308.12206v1
2308.12002v1,cs.LG,Neural oscillators for magnetic hysteresis modeling,2023-08-23 08:41:24+00:00,"Hysteresis is a ubiquitous phenomenon in science and engineering; its
modeling and identification are crucial for understanding and optimizing the
behavior of various systems. We develop an ordinary differential equation-based
recurrent neural network (RNN) approach to model and quantify the hysteresis,
which manifests itself in sequentiality and history-dependence. Our neural
oscillator, HystRNN, draws inspiration from coupled-oscillatory RNN and
phenomenological hysteresis models to update the hidden states. The performance
of HystRNN is evaluated to predict generalized scenarios, involving first-order
reversal curves and minor loops. The findings show the ability of HystRNN to
generalize its behavior to previously untrained regions, an essential feature
that hysteresis models must have. This research highlights the advantage of
neural oscillators over the traditional RNN-based methods in capturing complex
hysteresis patterns in magnetic materials, where traditional rate-dependent
methods are inadequate to capture intrinsic nonlinearity.",http://arxiv.org/pdf/2308.12002v1
2308.12312v1,physics.comp-ph,Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas,2023-08-23 07:00:56+00:00,"The Vlasov-Poisson system is employed in its reduced form version (1D1V) as a
test bed for the applicability of Physics Informed Neural Network (PINN) to the
wave-particle resonance. Two examples are explored: the Landau damping and the
bump-on-tail instability. PINN is first tested as a compression method for the
solution of the Vlasov-Poisson system and compared to the standard neural
networks. Second, the application of PINN to solving the Vlasov-Poisson system
is also presented with the special emphasis on the integral part, which
motivates the implementation of a PINN variant, called Integrable PINN
(I-PINN), based on the automatic-differentiation to solve the partial
differential equation and on the automatic-integration to solve the integral
equation.",http://arxiv.org/pdf/2308.12312v1
2308.11865v1,physics.plasm-ph,Resistive Hose modes in Tokamak Runaway Electron Beams,2023-08-23 01:56:17+00:00,"Beams of energetic runaway electrons are generated during disruptions in
tokamaks, and fluid models are used to study their effects on macroscale
dynamics. Linear computations of a massless, runaway electron beam coupled to
MHD plasma show that resistive hose instabilities grow faster than tearing
modes at large resistivity. Eigenvalue results with reduced models of the
resistive hose instability are compared with results from the full MHD and beam
system, showing that the resistive hose decouples from any plasma response. An
estimate of plasma temperature at which growth of the resistive hose dominates
tearing for post-disruption DIII-D plasma parameters is in a physically
relevant regime",http://arxiv.org/pdf/2308.11865v1
2308.11724v1,physics.comp-ph,MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations,2023-08-22 18:30:53+00:00,"Molecular Dynamics (MD) simulations are ubiquitous in cutting-edge
physio-chemical research. They provide critical insights into how a physical
system evolves over time given a model of interatomic interactions.
Understanding a system's evolution is key to selecting the best candidates for
new drugs, materials for manufacturing, and countless other practical
applications. With today's technology, these simulations can encompass millions
of unit transitions between discrete molecular structures, spanning up to
several milliseconds of real time. Attempting to perform a brute-force analysis
with data-sets of this size is not only computationally impractical, but would
not shed light on the physically-relevant features of the data. Moreover, there
is a need to analyze simulation ensembles in order to compare similar processes
in differing environments. These problems call for an approach that is
analytically transparent, computationally efficient, and flexible enough to
handle the variety found in materials based research. In order to address these
problems, we introduce MolSieve, a progressive visual analytics system that
enables the comparison of multiple long-duration simulations. Using MolSieve,
analysts are able to quickly identify and compare regions of interest within
immense simulations through its combination of control charts, data-reduction
techniques, and highly informative visual components. A simple programming
interface is provided which allows experts to fit MolSieve to their needs. To
demonstrate the efficacy of our approach, we present two case studies of
MolSieve and report on findings from domain collaborators.",http://arxiv.org/pdf/2308.11724v1
2308.11666v1,physics.soc-ph,Generalized dimension reduction approach for heterogeneous networked systems with time-delay,2023-08-22 04:49:56+00:00,"Networks of interconnected agents are essential to study complex networked
systems' state evolution, stability, resilience, and control. Nevertheless, the
high dimensionality and nonlinear dynamics are vital factors preventing us from
theoretically analyzing them. Recently, the dimension-reduction approaches
reduced the system's size by mapping the original system to a one-dimensional
system such that only one effective representative can capture its macroscopic
dynamics. However, the approaches dramatically fail as the network becomes
heterogeneous and has multiple community structures. Here, we bridge the gap by
developing a generalized dimension reduction approach, which enables us to map
the original system to a $m$-dimensional system that consists of $m$
interacting components. Notably, by validating it on various dynamical models,
this approach accurately predicts the original system state and the tipping
point, if any. Furthermore, the numerical results demonstrate that this
approach approximates the system evolution and identifies the critical points
for complex networks with time delay.",http://arxiv.org/pdf/2308.11666v1
2308.11151v1,cond-mat.soft,Predicting Pair Correlation Functions of Glasses using Machine Learning,2023-08-22 03:13:07+00:00,"Glasses offer a broad range of tunable thermophysical properties that are
linked to their compositions. However, it is challenging to establish a
universal composition-property relation of glasses due to their enormous
composition and chemical space. Here, we address this problem and develop a
metamodel of composition-atomistic structure relation of a class of glassy
material via a machine learning (ML) approach. Within this ML framework, an
unsupervised deep learning technique, viz. convolutional neural network (CNN)
autoencoder, and a regression algorithm, viz. random forest (RF), are
integrated into a fully automated pipeline to predict the spatial distribution
of atoms in a glass. The RF regression model predicts the pair correlation
function of a glass in a latent space. Subsequently, the decoder of the CNN
converts the latent space representation to the actual pair correlation
function of the given glass. The atomistic structures of silicate (SiO2) and
sodium borosilicate (NBS) based glasses with varying compositions and dopants
are collected from molecular dynamics (MD) simulations to establish and
validate this ML pipeline. The model is found to predict the atom pair
correlation function for many unknown glasses very accurately. This method is
very generic and can accelerate the design, discovery, and fundamental
understanding of composition-atomistic structure relations of glasses and other
materials.",http://arxiv.org/pdf/2308.11151v1
2308.10886v1,cond-mat.stat-mech,A streamlined molecular-dynamics workflow for computing solubilities of molecular and ionic crystals,2023-08-21 17:36:44+00:00,"Computing the solubility of crystals in a solvent using atomistic simulations
is notoriously challenging due to the complexities and convergence issues
associated with free-energy methods, as well as the slow equilibration in
direct-coexistence simulations. This paper introduces a molecular-dynamics
workflow that simplifies and robustly computes the solubility of molecular or
ionic crystals. This method is considerably more straightforward than the
state-of-the-art, as we have streamlined and optimised each step of the
process. Specifically, we calculate the chemical potential of the crystal using
the gas-phase molecule as a reference state, and employ the S0 method to
determine the concentration dependence of the chemical potential of the solute.
We use this workflow to predict the solubilities of sodium chloride in water,
urea polymorphs in water, and paracetamol polymorphs in both water and ethanol.
Our findings indicate that the predicted solubility is sensitive to the chosen
potential energy surface. Furthermore, we note that the harmonic approximation
often fails for both molecular crystals and gas molecules at or above room
temperature, and that the assumption of an ideal solution becomes less valid
for highly soluble substances.",http://arxiv.org/pdf/2308.10886v1
2308.10752v1,physics.comp-ph,Comprehensive Molecular Representation from Equivariant Transformer,2023-08-21 14:39:29+00:00,"We implement an equivariant transformer that embeds molecular net charge and
spin state without additional neural network parameters. The model trained on a
singlet/triplet non-correlated \ce{CH2} dataset can identify different spin
states and shows state-of-the-art extrapolation capability. We found that
Softmax activation function utilised in the self-attention mechanism of graph
networks outperformed ReLU-like functions in prediction accuracy. Additionally,
increasing the attention temperature from $\tau = \sqrt{d}$ to $\sqrt{2d}$
further improved the extrapolation capability. We also purposed a weight
initialisation method that sensibly accelerated the training process.",http://arxiv.org/pdf/2308.10752v1
2308.10723v1,physics.comp-ph,Controllable Weyl nodes and Fermi arcs in a light-irradiated carbon allotrope,2023-08-21 13:47:32+00:00,"The precise control of Weyl physics in realistic materials oers a promising
avenue to construct accessible topological quantum systems, and thus draw
widespread attention in condensed-matter physics. Here, based on rst-principles
calculations, maximally localized Wannier functions based tight-binding model,
and Floquet theorem, we study the light-manipulated evolution of Weyl physics
in a carbon allotrope C6 crystallizing a face-centered orthogonal structure
(fco-C6), an ideal Weyl semimetal with two pairs of Weyl nodes, under the
irradiation of a linearly polarized light (LPL). We show that the positions of
Weyl nodes and Fermi arcs can be accurately controlled by changing light
intensity. Moreover, we employ a low-energy eective k p model to understand
light-controllable Weyl physics. The results indicate that the symmetry of
light-irradiated fco-C6 can be selectively preserved, which guarantees that the
light-manipulated Weyl nodes can only move in the highsymmetry plane in
momentum space. Our work not only demonstrates the ecacy of employing periodic
driving light elds as an ecient approach to manipulate Weyl physics, but also
paves a reliable pathway for designing accessible topological states under
light irradiation.",http://arxiv.org/pdf/2308.10723v1
2308.11658v1,physics.optics,Analytical Method for Metasurface-Based Cloaking Under Arbitrary Oblique Illumination,2023-08-21 12:01:23+00:00,"The performance of antennas can severely deteriorate in the presence of
adjacent electrically-large scatterers. In this work, we use a conducting
hollow cylinder to shield the scatterer. The cylinder is shelled with single
layer dielectric and electromagnetic metasurface. The scattering field analysis
with respect to the surface impedance is derived. By optimizing the anisotropic
impedance distribution, the scattering cross-section can be effectively
reduced. The proposed method is valid for both TMz, TEz and non-TM/TE incident
field. The accuracy and effectiveness of the method are verified by four
cloaking scenarios in microwave regime. We demonstrate that with the surface
impedance obtained by our method, a metasurface is designed with physical
subwavelength structure. We also show a cloaking scenario under magnetic dipole
radiation, which is closer to the case of a realistic antenna. This method can
be further applied to cloaking tasks in terahertz and optical regimes.",http://arxiv.org/pdf/2308.11658v1
2308.10659v1,cond-mat.supr-con,"Magnetic phases of electron-doped, infinite-layer Sr$_{1-x}$La$_x$CuO$_2$ from first-principles density functional calculations",2023-08-21 11:52:53+00:00,"The magnetic phases of electron-doped, infinite-layer
$\mathrm{Sr}_{1-x}\mathrm{La}_{x}\mathrm{CuO}_2$ are elucidated by
first-principles density functional calculations. We describe the
antiferromagnetic parent state, metallic phase transition, lattice structure
and magnetic anisotropy evolution upon doping, as well as pressure-induced
changes to the density of states at Fermi level that are consistent with
experiments where comparison is possible. We investigate low-energy states with
multiple magnetic configurations and study their specific heat coefficients and
magnetic exchange coupling, as well as the density of states at Fermi level.
The latter quantity is used to study the effects of spin fluctuations on the
electronic structure of this strongly correlated material.",http://arxiv.org/pdf/2308.10659v1
2308.11657v1,physics.comp-ph,Multiple scattering of 855 MeV electrons in amorphous and crystalline silicon: simulations versus experiment,2023-08-21 10:51:07+00:00,"The angular distribution function of multiple scattering experienced by 855
MeV electrons passing through an amorphous silicon plate and an oriented
silicon crystal has been studied by means of relativistic molecular dynamics
simulations using two types of the potentials that describe electron-atom
interaction. The differences in the angular distributions of the beam particles
in both media are analysed. The results obtained are compared to the
experimental data and to the results of Monte Carlo simulations.",http://arxiv.org/pdf/2308.11657v1
2308.10364v1,cs.LG,SE(3) Equivariant Augmented Coupling Flows,2023-08-20 20:49:15+00:00,"Coupling normalizing flows allow for fast sampling and density evaluation,
making them the tool of choice for probabilistic modeling of physical systems.
However, the standard coupling architecture precludes endowing flows that
operate on the Cartesian coordinates of atoms with the SE(3) and permutation
invariances of physical systems. This work proposes a coupling flow that
preserves SE(3) and permutation equivariance by performing coordinate splits
along additional augmented dimensions. At each layer, the flow maps atoms'
positions into learned SE(3) invariant bases, where we apply standard flow
transformations, such as monotonic rational-quadratic splines, before returning
to the original basis. Crucially, our flow preserves fast sampling and density
evaluation, and may be used to produce unbiased estimates of expectations with
respect to the target distribution via importance sampling. When trained on the
DW4, LJ13 and QM9-positional datasets, our flow is competitive with equivariant
continuous normalizing flows, while allowing sampling two orders of magnitude
faster. Moreover, to the best of our knowledge, we are the first to learn the
full Boltzmann distribution of alanine dipeptide by only modeling the Cartesian
positions of its atoms. Lastly, we demonstrate that our flow can be trained to
approximately sample from the Boltzmann distribution of the DW4 and LJ13
particle systems using only their energy functions.",http://arxiv.org/pdf/2308.10364v1
2308.10327v1,quant-ph,Quantum State Tomography using Quantum Machine Learning,2023-08-20 17:51:24+00:00,"Quantum State Tomography (QST) is a fundamental technique in Quantum
Information Processing (QIP) for reconstructing unknown quantum states.
However, the conventional QST methods are limited by the number of measurements
required, which makes them impractical for large-scale quantum systems. To
overcome this challenge, we propose the integration of Quantum Machine Learning
(QML) techniques to enhance the efficiency of QST. In this paper, we conduct a
comprehensive investigation into various approaches for QST, encompassing both
classical and quantum methodologies; We also implement different QML approaches
for QST and demonstrate their effectiveness on various simulated and
experimental quantum systems, including multi-qubit networks. Our results show
that our QML-based QST approach can achieve high fidelity (98%) with
significantly fewer measurements than conventional methods, making it a
promising tool for practical QIP applications.",http://arxiv.org/pdf/2308.10327v1
2308.10283v1,cs.LG,Adaptive Uncertainty-Guided Model Selection for Data-Driven PDE Discovery,2023-08-20 14:36:45+00:00,"We propose a new parameter-adaptive uncertainty-penalized Bayesian
information criterion (UBIC) to prioritize the parsimonious partial
differential equation (PDE) that sufficiently governs noisy spatial-temporal
observed data with few reliable terms. Since the naive use of the BIC for model
selection has been known to yield an undesirable overfitted PDE, the UBIC
penalizes the found PDE not only by its complexity but also the quantified
uncertainty, derived from the model supports' coefficient of variation in a
probabilistic view. We also introduce physics-informed neural network learning
as a simulation-based approach to further validate the selected PDE flexibly
against the other discovered PDE. Numerical results affirm the successful
application of the UBIC in identifying the true governing PDE. Additionally, we
reveal an interesting effect of denoising the observed data on improving the
trade-off between the BIC score and model complexity. Code is available at
https://github.com/Pongpisit-Thanasutives/UBIC.",http://arxiv.org/pdf/2308.10283v1
2308.10164v1,cond-mat.soft,Molecular Dynamics Simulation of Apolipoprotein E3 Lipid Nanodiscs,2023-08-20 05:14:01+00:00,"Nanodiscs are binary discoidal complexes of a phospholipid bilayer
circumscribed by belt-like helical scaffold proteins. Using coarse-grained and
all-atom molecular dynamics simulations, we explore the stability, size, and
structure of nanodiscs formed between the N-terminal domain of apolipoprotein
E3 (apoE3-NT) and variable number of
1,2-dimyristoyl-sn-glycero-3-phosphocholine (DMPC) molecules. We study both
parallel and antiparallel double-belt configurations, consisting of four
proteins per nanodisc. Our simulations predict nanodiscs containing between 240
and 420 DMPC molecules to be stable. The antiparallel configurations exhibit an
average of 1.6 times more amino acid interactions between protein chains and 2
times more ionic contacts, compared to the parallel configuration. With one
exception, DMPC order parameters are consistently larger in the antiparallel
configuration than in the parallel one. In most cases, the root mean square
deviation of the positions of the protein backbone atoms is smaller in the
antiparallel configuration. We further report nanodisc size, thickness, radius
of gyration, and solvent accessible surface area. Combining all investigated
parameters, we hypothesize the antiparallel protein configuration leading to
more stable and more rigid nanodiscs than the parallel one.",http://arxiv.org/pdf/2308.10164v1
2308.09837v1,cs.SC,Field theory with the Maxima computer algebra system,2023-08-18 22:12:18+00:00,"The Maxima computer algebra system, the open-source successor to MACSYMA, the
first general-purpose computer algebra system that was initially developed at
the Massachusetts Institute of Technology in the late 1960s and later
distributed by the United States Department of Energy, has some remarkable
capabilities, some of which are implemented in the form of add-on, ""share""
packages that are distributed along with the core Maxima system. One such share
package is itensor, for indicial tensor manipulation. One of the more
remarkable features of itensor is functional differentiation. Through this, it
is possible to use itensor to develop a Lagrangian field theory and derive the
corresponding field equations. In the present note, we demonstrate this
capability by deriving Maxwell's equations from the Maxwell Lagrangian, and
exploring the properties of the system, including current conservation.",http://arxiv.org/pdf/2308.09837v1
2308.09782v1,cond-mat.mes-hall,Velocity-gauge real-time time-dependent density functional tight-binding for large-scale condensed matter systems,2023-08-18 19:12:48+00:00,"We present a new velocity-gauge real-time, time-dependent density functional
tight-binding (VG-rtTDDFTB) implementation in the open-source DFTB+ software
package (https://dftbplus.org) for probing electronic excitations in large,
condensed matter systems. Our VG-rtTDDFTB approach enables real-time electron
dynamics simulations of large, periodic, condensed matter systems containing
thousands of atoms with a favorable computational scaling as a function of
system size. We provide computational details and benchmark calculations to
demonstrate its accuracy and computational parallelizability on a variety of
large material systems. As a representative example, we calculate laser-induced
electron dynamics in a 512-atom amorphous silicon supercell to highlight the
large periodic systems that can be examined with our implementation. Taken
together, our VG-rtTDDFTB approach enables new electron dynamics simulations of
complex systems that require large periodic supercells, such as crystal
defects, complex surfaces, nanowires, and amorphous materials.",http://arxiv.org/pdf/2308.09782v1
2308.09628v1,physics.flu-dyn,Numerical Simulation of Shock Wave Propagation Over a Dense Particle Layer Using the Baer-Nunziato Model,2023-08-18 15:41:41+00:00,"The present study examines the possibility of numerical simulation of a
strong shock wave propagating over the surface of a dense layer of particles
poured onto an impermeable wall using the Baer-Nunizato two-phase flow model.
The setting of the problem follows the full-scale experiment. The mathematical
model is based on a two-dimensional system of Baer-Nunziato equations and takes
into account intergranular stresses arising in the solid phase of particles.
The computational algorithm is based on the HLLC method with a pressure
relaxation procedure. The developed algorithm proved to be efficient for
two-phase problems with explicit interfacial boundaries and strong shock waves.
These issues are typical of problems arising from the interaction of a shock
wave with a bed or a layer of particles. A comparison with the simulations and
full-scale experiments of other authors is carried out. A reasonable agreement
with the experiment is obtained for the angles of the transmitted compaction
wave and granular contact, including their dependency on the intensity of the
propagating shock wave. The granular contact angle increases with the incident
shock wave Mach number, while the transmitted compaction wave angle decreases.
The explanation of the phenomenon of the decrease in thickness of the compacted
region in the layer with the increase in intensity of the propagating shock
wave is given. The main reason is that the maximal value of the particle volume
fraction in the plug of compacted particles in the layer rises with the
increase in shock wave intensity.",http://arxiv.org/pdf/2308.09628v1
2308.09492v1,physics.comp-ph,Predicting Properties of Oxide Glasses Using Informed Neural Networks,2023-08-18 12:02:11+00:00,"Many modern-day applications require the development of new materials with
specific properties. In particular, the design of new glass compositions is of
great industrial interest. Current machine learning methods for learning the
composition-property relationship of glasses promise to save on expensive
trial-and-error approaches. Even though quite large datasets on the composition
of glasses and their properties already exist (i.e., with more than 350,000
samples), they cover only a very small fraction of the space of all possible
glass compositions. This limits the applicability of purely data-driven models
for property prediction purposes and necessitates the development of models
with high extrapolation power. In this paper, we propose a neural network model
which incorporates prior scientific and expert knowledge in its learning
pipeline. This informed learning approach leads to an improved extrapolation
power compared to blind (uninformed) neural network models. To demonstrate
this, we train our models to predict three different material properties, that
is, the glass transition temperature, the Young's modulus (at room
temperature), and the shear modulus of binary oxide glasses which do not
contain sodium. As representatives for conventional blind neural network
approaches we use five different feed-forward neural networks of varying widths
and depths. For each property, we set up model ensembles of multiple trained
models and show that, on average, our proposed informed model performs better
in extrapolating the three properties of previously unseen sodium borate glass
samples than all five conventional blind models.",http://arxiv.org/pdf/2308.09492v1
2308.09439v1,physics.flu-dyn,A low Diffusion HLL-CPS Scheme for all Mach number flows,2023-08-18 10:08:13+00:00,"A low diffusion version of the HLL-CPS scheme for resolving the shear layers
and the flow features at low Mach numbers is presented here. The low diffusion
HLL-CPS scheme is obtained by reconstructing the velocities at the cell
interface with the face normal Mach number and a pressure function. Asymptotic
analysis of the modified scheme shows a correct scaling of the pressure at low
Mach numbers and a significant reduction in numerical dissipation. The
robustness of the HLL-CPS scheme for strong shock is improved by reducing the
contribution of the contact wave in the vicinity of the shock. The improvement
in robustness for strong shock is demonstrated analytically through linear
perturbation and matrix stability analyses. A set of numerical test cases are
solved to demonstrate the efficacy of the proposed scheme over a wide range of
Mach numbers.",http://arxiv.org/pdf/2308.09439v1
2308.09271v1,cond-mat.mtrl-sci,Raman and IR spectra of water under graphene nanoconfinement at ambient and extreme pressure-temperature conditions: a first-principles study,2023-08-18 03:22:52+00:00,"The nanoconfinement of water can result in dramatic differences in its
physical and chemical properties compared to bulk water. However, a detailed
molecular-level understanding of these properties is still lacking. Vibrational
spectroscopy, such as Raman and infrared, is a popular experimental tool for
studying the structure and dynamics of water, and is often complemented by
atomistic simulations to interpret experimental spectra, but there have been
few theoretical spectroscopy studies of nanoconfined water using
first-principles methods at ambient conditions, let alone under extreme
pressure-temperature conditions. Here, we computed the Raman and IR spectra of
water nanoconfined by graphene at ambient and extreme pressure-temperature
conditions using ab intio simulations. Our results revealed alterations in the
Raman stretching and low-frequency bands due to the graphene confinement. We
also found spectroscopic evidence indicating that nanoconfinement considerably
changes the tetrahedral hydrogen bond network, which is typically found in bulk
water. Furthermore, we observed an unusual bending band in the Raman spectrum
at ~10 GPa and 1000 K, which is attributed to the unique molecular structure of
confined ionic water. Additionally, we found that at ~20 GPa and 1000 K,
confined water transformed into a superionic fluid, making it challenging to
identify the IR stretching band. Finally, we computed the ionic conductivity of
confined water in the ionic and superionic phases. Our results highlight the
efficacy of Raman and IR spectroscopy in studying the structure and dynamics of
nanoconfined water in a large pressure-temperature range. Our predicted Raman
and IR spectra can serve as a valuable guide for future experiments.",http://arxiv.org/pdf/2308.09271v1
2308.09142v1,physics.chem-ph,Accurate machine learning force fields via experimental and simulation data fusion,2023-08-17 18:22:19+00:00,"Machine Learning (ML)-based force fields are attracting ever-increasing
interest due to their capacity to span spatiotemporal scales of classical
interatomic potentials at quantum-level accuracy. They can be trained based on
high-fidelity simulations or experiments, the former being the common case.
However, both approaches are impaired by scarce and erroneous data resulting in
models that either do not agree with well-known experimental observations or
are under-constrained and only reproduce some properties. Here we leverage both
Density Functional Theory (DFT) calculations and experimentally measured
mechanical properties and lattice parameters to train an ML potential of
titanium. We demonstrate that the fused data learning strategy can concurrently
satisfy all target objectives, thus resulting in a molecular model of higher
accuracy compared to the models trained with a single data source. The
inaccuracies of DFT functionals at target experimental properties were
corrected, while the investigated off-target properties remained largely
unperturbed. Our approach is applicable to any material and can serve as a
general strategy to obtain highly accurate ML potentials.",http://arxiv.org/pdf/2308.09142v1
2308.09071v1,cs.NE,Pattern recognition using spiking antiferromagnetic neurons,2023-08-17 16:00:57+00:00,"Spintronic devices offer a promising avenue for the development of nanoscale,
energy-efficient artificial neurons for neuromorphic computing. It has
previously been shown that with antiferromagnetic (AFM) oscillators, ultra-fast
spiking artificial neurons can be made that mimic many unique features of
biological neurons. In this work, we train an artificial neural network of AFM
neurons to perform pattern recognition. A simple machine learning algorithm
called spike pattern association neuron (SPAN), which relies on the temporal
position of neuron spikes, is used during training. In under a microsecond of
physical time, the AFM neural network is trained to recognize symbols composed
from a grid by producing a spike within a specified time window. We further
achieve multi-symbol recognition with the addition of an output layer to
suppress undesirable spikes. Through the utilization of AFM neurons and the
SPAN algorithm, we create a neural network capable of high-accuracy recognition
with overall power consumption on the order of picojoules.",http://arxiv.org/pdf/2308.09071v1
2308.08703v1,physics.chem-ph,Flexible DMRG-based framework for anharmonic vibrational calculations,2023-08-16 23:31:59+00:00,"We present a novel formulation of the vibrational density matrix
renormalization group (vDMRG) algorithm tailored to strongly anharmonic
molecules described by general high-dimensional model representations of
potential energy surfaces. For this purpose, we extend the vDMRG framework to
support vibrational Hamiltonians expressed in the so-called $n$-mode
second-quantization formalism. The resulting $n$-mode vDMRG method offers full
flexibility with respect to both the functional form of the PES and the choice
of single-particle basis set. We leverage this framework to apply, for the
first time, vDMRG based on an anharmonic modal basis set optimized with the
vibrational self-consistent field algorithm on an on-the-fly constructed PES.
We also extend the $n$-mode vDMRG framework to include excited-state targeting
algorithms in order to efficiently calculate anharmonic transition frequencies.
We demonstrate the capabilities of our novel $n$-mode vDMRG framework for
methyloxirane, a challenging molecule with 24 coupled vibrational modes.",http://arxiv.org/pdf/2308.08703v1
2308.08617v1,physics.flu-dyn,Stability and Order of Accuracy Analysis of High-Order Schemes Formulated Using the Flux Reconstruction Approach,2023-08-16 18:21:46+00:00,"A stability analysis is performed on high-order schemes formulated using the
Flux Reconstruction (FR) approach. The one-dimensional advection model equation
is used for the assessment of the stability region of these schemes when
coupled with Runge-Kutta-type time-march procedures. Schemes are created using
different numbers of internal points for each cell of the discrete domain, in
such a way that a broad spectrum of spatial discretization orders can be
analyzed. Multiple correction functions are employed so that well-known
schemes, for instance the Discontinuous Galerkin (DG) and the Staggered-Grid
schemes, may also be included in the study. The stability analysis is performed
by identifying the behavior, in the complex plane, of the eigenvalues
associated with each one of the considered cases. It is observed that, as the
order of the scheme increases, a significant decrease in its domain of
stability occurs, followed by a significant reduction in the amount of
artificial dissipation that is introduced to the solution.",http://arxiv.org/pdf/2308.08617v1
2308.08615v1,physics.comp-ph,Scalable Lattice Sampling using Factorized Generative Models,2023-08-16 18:18:22+00:00,"Boltzmann distributions over lattices are pervasive in Computational Physics.
Sampling them becomes increasingly difficult with the increase in the number of
dimensions, especially near critical regions, e.g., phase transitions or
continuum limits. Conditional generative models are emerging as promising tools
for sampling in critical regions. When conditioned on the parameters to be
varied, they can be efficaciously extended to critical regions without the need
for retraining. However, current approaches do not scale well to large
lattices. We present a novel approach called Parallelizable Block
Metropolis-within-Gibbs (PBMG) for generating samples for any local lattice
model. It factorizes the joint distribution of lattice into local parametric
kernels, thereby allowing efficient sampling of very large lattices. We
optimize the model with reverse Kullback-Leibler divergence (RKLD) to avoid the
need for ground truth samples. Since the local distributions are simpler, the
model is not affected by mode collapse, which generally occurs while training
with RKLD. We validate our approach on the XY model and the Scalar $\phi^4$
theory. PBMG achieves high acceptance rates and the observable statistics
estimated from the samples match the ground truth.",http://arxiv.org/pdf/2308.08615v1
2308.08594v1,cond-mat.str-el,Automatic Order Detection and Restoration Through Systematically Improvable Variational Wave Functions,2023-08-16 18:00:01+00:00,"Variational wave function ansatze are an invaluable tool to study the
properties of strongly correlated systems. We propose such a wave function,
based on the theory of auxiliary fields and combining aspects of
auxiliary-field quantum Monte Carlo and modern variational optimization
techniques including automatic differentiation. The resulting ansatz,
consisting of several slices of optimized projectors, is highly expressive and
systematically improvable. We benchmark this form on the two-dimensional
Hubbard model, using both cylindrical and large, fully periodic supercells. The
computed ground-state energies are competitive with the best variational
results. Moreover, the optimized wave functions predict the correct
ground-state order with near full symmetry restoration (i.e. translation
invariance) despite initial states with incorrect orders. The ansatz can become
a tool for local order prediction, leading to a new paradigm for variational
studies of bulk systems. It can also be viewed as an approach to produce
accurate and systematically improvable wave functions in a convenient form of
non-orthogonal Slater determinants (e.g., for quantum chemistry) at polynomial
computational cost.",http://arxiv.org/pdf/2308.08594v1
2308.08509v1,cond-mat.mtrl-sci,Surface Phase Diagrams from Nested Sampling,2023-08-16 17:06:40+00:00,"Atomic-scale modeling of surface phase equilibria often focuses on
temperatures near zero Kelvin due to the difficulty in computing the free
energy of surfaces at finite temperatures. The Bayesian-inference-based nested
sampling (NS) algorithm allows modeling surface phase equilibria at arbitrary
temperatures by directly and efficiently calculating the partition function,
whose relationship with free energy is well known. In this work, we extend NS
to calculate surface phase diagrams, including all relevant translational,
rotational, and vibrational contributions to the free energy. We apply NS to
the surfaces of the Lennard-Jones solid, recording energies through the
iterative compression of surface phase space rather than a specific cooling
schedule. We construct the partition function from these recorded energies to
calculate ensemble averages of thermodynamic properties, such as the
constant-volume heat capacity and temperature-dependent order parameters that
characterize the surface structure. Key results include determining the nature
of phase transitions on flat and stepped surfaces, which typically feature an
enthalpy-driven condensation at higher temperatures and an entropy-driven
reordering process at lower temperatures, and the presence of critical points
on the phase diagrams of most of the flatter facets. Overall, we demonstrate
the ability and potential of NS for surface modeling and, ultimately, materials
discovery.",http://arxiv.org/pdf/2308.08509v1
2308.08468v1,cs.LG,An Expert's Guide to Training Physics-informed Neural Networks,2023-08-16 16:19:25+00:00,"Physics-informed neural networks (PINNs) have been popularized as a deep
learning framework that can seamlessly synthesize observational data and
partial differential equation (PDE) constraints. Their practical effectiveness
however can be hampered by training pathologies, but also oftentimes by poor
choices made by users who lack deep learning expertise. In this paper we
present a series of best practices that can significantly improve the training
efficiency and overall accuracy of PINNs. We also put forth a series of
challenging benchmark problems that highlight some of the most prominent
difficulties in training PINNs, and present comprehensive and fully
reproducible ablation studies that demonstrate how different architecture
choices and training strategies affect the test accuracy of the resulting
models. We show that the methods and guiding principles put forth in this study
lead to state-of-the-art results and provide strong baselines that future
studies should use for comparison purposes. To this end, we also release a
highly optimized library in JAX that can be used to reproduce all results
reported in this paper, enable future research studies, as well as facilitate
easy adaptation to new use-case scenarios.",http://arxiv.org/pdf/2308.08468v1
2308.08447v1,cond-mat.mes-hall,Accelerating micromagnetic and atomistic simulations using multiple GPUs,2023-08-16 15:59:50+00:00,"It is shown micromagnetic and atomistic spin dynamics simulations can use
multiple GPUs in order to reduce computation time, but also to allow for a
larger simulation size than is possible on a single GPU. Whilst interactions
which depend on neighbouring spins, such as exchange interactions, may be
implemented efficiently by transferring data between GPUs using halo regions,
or alternatively using direct memory accesses, implementing the long-range
demagnetizing interaction is the main difficulty in achieving good performance
scaling, where the data transfer rate between GPUs is a significant bottleneck.
A multi-GPU convolution algorithm is developed here, which relies on single-GPU
FFTs executed in parallel. It is shown that even for micromagnetic simulations
where the demagnetizing interaction computation time dominates, good
performance scaling may be achieved, with speedup factors up to 1.8, 2.5, and
3.1, for 2, 3, and 4 GPUs respectively. The code developed here can be used for
any number of GPUs in parallel, with performance scaling strongly dependent on
inter-GPU data transfer rate and connection topology. This is further improved
in micromagnetic simulations which include a spin transport solver, obtaining
speedup factors up to 1.96, 2.8, and 3.7, for 2, 3, and 4 GPUs respectively.
The best case scenario is obtained for atomistic spin dynamics simulations,
where the demagnetizing interaction is implemented with spin-averaged cells.
Using a single workstation with 4 GPUs, it is shown atomistic spin dynamics
simulations with up to 1 billion spins, and atomistic Monte Carlo simulations
with up to 2 billion spins are possible, with a near-ideal performance scaling.",http://arxiv.org/pdf/2308.08447v1
2308.08425v1,cond-mat.soft,On fusogenicity of positively charged phased-separated lipid vesicles: experiments and computational simulations,2023-08-16 15:16:50+00:00,"This paper studies the fusogenicity of cationic liposomes in relation to
their surface distribution of cationic lipids and utilizes membrane phase
separation to control this surface distribution. It is found that concentrating
the cationic lipids into small surface patches on liposomes, through
phase-separation, can enhance liposome's fusogenicity. Further concentrating
these lipids into smaller patches on the surface of liposomes led to an
increased level of fusogenicity. These experimental findings are supported by
numerical simulations using a mathematical model for phase-separated charged
liposomes. Findings of this study may be used for design and development of
highly fusogenic liposomes with minimal level of toxicity.",http://arxiv.org/pdf/2308.08425v1
2308.08301v1,physics.flu-dyn,Proposal for Numerical Benchmarking of Fluid-Structure Interaction in Cerebral Aneurysms,2023-08-16 12:02:01+00:00,"Computational fluid dynamics is intensively used to deepen the understanding
of aneurysm growth and rupture in the attempt to support physicians during
therapy planning. Numerous studies have assumed fully-rigid vessel walls in
their simulations, whose sole hemodynamics may fail to provide a satisfactory
criterion for rupture risk assessment. Moreover, direct in-vivo observations of
intracranial aneurysm pulsation have been recently reported, encouraging the
development of fluid-structure interaction for their modelling and for new
assessments. In this work, we describe a new fluid-structure interaction
benchmark setting for the careful evaluation of different aneurysm shapes. The
studied configurations consist of three real aneurysm domes positioned on a
toroidal channel. All geometric features, meshing characteristics, flow
quantities, comparisons with a rigid-wall model and corresponding plots are
provided. Reported results emphasize the alteration of flow patterns and
hemodynamic descriptors when moving from the rigid-wall model to the complete
fluid-structure interaction framework, thereby underlining the importance of
the coupling between hemodynamics and the surrounding vessel tissue.",http://arxiv.org/pdf/2308.08301v1
2308.08211v1,cond-mat.mtrl-sci,Biaxial strain modulated electronic structures of layered two-dimensional MoSiGeN4 Rashba systems,2023-08-16 08:28:38+00:00,"The two-dimensional (2D) MA2Z4 family has received extensive attention in
manipulating its electronic structure and achieving intriguing physical
properties. However, engineering the electronic properties remains a challenge.
Herein, based on first-principles calculations, we systematically investigate
the effect of biaxial strains on the electronic structures of 2D Rashba
MoSiGeN4 (MSGN), and further explore how the interlayer interactions affect the
Rashba spin splitting in such strained layered MSGNs. After applying biaxial
strains, the band gap decreases monotonically with increasing tensile strains
but increases when the compressive strains are applied. An
indirect-direct-indirect band gap transition is induced by applying a moderate
compressive strain (< 5%) in the MSGNs. Due to the symmetry breaking and
moderate spin-orbit coupling (SOC), the monolayer MSGN possess an isolated
Rashba spin splitting (R) near the Fermi level, which could be effectively
regulated to the Lifshitz transition (L) by biaxial strain. For instance, a
L-R-L transformation of Fermi surface is presented in monolayer and a more
complex and changeable L-R-L-R evolution is observed in bilayer and trilayer
MSGNs as the biaxial strain vary from -8% to 12%, which actually depend on the
appearance, variation, and vanish of the Mexican hat band in the absence of SOC
under different strains. The contribution of Mo-dz2 orbital hybridized with
N-pz orbital in the highest valence band plays a dominant role on the band
evolution under biaxial strains, where the R-L evolution corresponds to the
decreased Mo-dz2 orbital contribution. Our study highlights the biaxial strain
controllable Rashba spin splitting, in particular the introduction and even the
evolution of Lifshitz transition near Fermi surface, which makes the strained
MSGNs as promising candidates for future applications in spintronic devices.",http://arxiv.org/pdf/2308.08211v1
2308.08149v1,cond-mat.str-el,"Correlated flat bands in the paramagnetic phase of triangular antiferromagnets Na$_2$BaX(PO$_4$)$_2$ (X = Mn, Co, Ni)",2023-08-16 05:04:35+00:00,"Flat band systems in condensed matter physics are intriguing because they can
exhibit exotic phases and unconventional properties. In this work, we studied
three correlated magnetic systems, Na$_2$BaX(PO$_4$)$_2$ (X = Mn, Co, Ni), and
revealed their unusual electronic structure and magnetic properties. Despite
their different effective angular momentum, our first-principles calculations
showed a similar electronic structure among them. However, their different
valence configurations led to different responses to electronic correlations in
the high-temperature paramagnetic phase. Using the dynamical mean-field method,
we found that all systems can be understood as a multi-band Hubbard model with
Hund'ss coupling. Our calculations of spin susceptibility and the {\it
ab-initio} estimation of magnetic exchange coupling indicated strong
intra-plane antiferromagnetic coupling and weak inter-plane coupling in all
systems. The ground states of these systems are largely degenerate. It is
likely that none of these magnetic states would dominate over the others,
leading to the possibility of quantum spin liquid states in these systems. Our
work unifies the understanding of these three structurally similar systems and
opens new avenues for exploring correlated flat bands with distinct electronic
and magnetic responses.",http://arxiv.org/pdf/2308.08149v1
2308.08132v1,physics.comp-ph,Accuracy of Kohn-Sham density functional theory for warm- and hot-dense matter equation of state,2023-08-16 03:48:21+00:00,"We study the accuracy of Kohn-Sham density functional theory (DFT) for warm-
and hot-dense matter (WDM and HDM). Specifically, considering a wide range of
systems, we perform accurate ab initio molecular dynamics simulations with
temperature-independent local/semilocal density functionals to determine the
equations of state at compression ratios of 3x--7x and temperatures near 1 MK.
We find very good agreement with path integral Monte Carlo (PIMC) benchmarks,
while having significantly smaller error bars and smoother data, demonstrating
the fidelity of DFT for the study of WDM and HDM.",http://arxiv.org/pdf/2308.08132v1
2308.08101v1,physics.app-ph,The interface states in gate-all-around transistors (GAAFETs),2023-08-16 02:20:13+00:00,"The atomic-level structural detail and the quantum effects are becoming
crucial to device performance as the emerging advanced transistors,
representatively GAAFETs, are scaling down towards sub-3nm nodes. However, a
multiscale simulation framework based on atomistic models and ab initio quantum
simulation is still absent. Here, we propose such a simulation framework by
fulfilling three challenging tasks, i.e., building atomistic all-around
interfaces between semiconductor and amorphous gate-oxide, conducting
large-scale first-principles calculations on the interface models containing up
to 2796 atoms, and finally bridging the state-of-the-art atomic level
calculation to commercial TCAD. With this framework, two unnoticed origins of
interface states are demonstrated, and their tunability by changing channel
size, orientation and geometry is confirmed. The quantitative study of
interface states and their effects on device performance explains why the
nanosheet channel is preferred in industry. We believe such a bottom-up
framework is necessary and promising for the accurate simulation of emerging
advanced transistors.",http://arxiv.org/pdf/2308.08101v1
2308.08085v1,physics.comp-ph,GPU-Native Adaptive Mesh Refinement with Application to Lattice Boltzmann Simulations,2023-08-16 01:22:02+00:00,"The Lattice Boltzmann Method (LBM) has garnered significant interest in
General-Purpose Graphics Processing Unit (GPGPU) programming for computational
fluid dynamics due to its straightforward GPU parallelization and could benefit
greatly from Adaptive Mesh Refinement (AMR). AMR can assist in efficiently
resolving flows with regions of interest requiring a high degree of resolution.
An AMR scheme that could manage a computational mesh entirely on the GPU
without intermediate data transfers to/from the host device would provide a
substantial speedup to GPU-accelerated solvers, however, implementations
commonly employ CPU/hybrid frameworks instead, due to lack of a recursive data
structure. A block-based GPU-native algorithm will be presented for AMR in the
context of GPGPU programming and implemented in an open-source C++ code. The
meshing code is equipped with a Lattice Boltzmann solver for assessing
performance. Different AMR approaches and consequences in implementation are
considered before careful selection of data structures enabling efficient
refinement and coarsening compatible with single instruction multiple data
architecture is detailed. Inter-level communication is achieved by tricubic
interpolation and standard spatial averaging. Although the present open-source
implementation is tailored for LBM simulations, the outlined grid refinement
procedure is compatible with solvers for cell-centered block-structured grids.
  Link to repository: https://github.com/KhodrJ/AGAL",http://arxiv.org/pdf/2308.08085v1
2308.08573v1,physics.comp-ph,Fourier modal method for inverse design of metasurface-enhanced micro-LEDs,2023-08-15 21:14:52+00:00,"We present a simulation capability for micro-scale light-emitting diodes
(uLEDs) that achieves comparable accuracy to CPU-based finite-difference
time-domain simulation but is more than 10^7 times faster. Our approach is
based on the Fourier modal method (FMM) -- which, as we demonstrate, is well
suited to modeling thousands of incoherent sources -- with extensions that
allow rapid convergence for uLED structures that are challenging to model with
standard approaches. The speed of our method makes the inverse design of uLEDs
tractable, which we demonstrate by designing a metasurface-enhanced uLED that
doubles the light extraction efficiency of an unoptimized device.",http://arxiv.org/pdf/2308.08573v1
2308.07999v1,physics.comp-ph,IceCube experience using XRootD-based Origins with GPU workflows in PNRP,2023-08-15 19:20:23+00:00,"The IceCube Neutrino Observatory is a cubic kilometer neutrino telescope
located at the geographic South Pole. Understanding detector systematic effects
is a continuous process. This requires the Monte Carlo simulation to be updated
periodically to quantify potential changes and improvements in science results
with more detailed modeling of the systematic effects. IceCube's largest
systematic effect comes from the optical properties of the ice the detector is
embedded in. Over the last few years there have been considerable improvements
in the understanding of the ice, which require a significant processing
campaign to update the simulation. IceCube normally stores the results in a
central storage system at the University of Wisconsin-Madison, but it ran out
of disk space in 2022. The Prototype National Research Platform (PNRP) project
thus offered to provide both GPU compute and storage capacity to IceCube in
support of this activity. The storage access was provided via XRootD-based OSDF
Origins, a first for IceCube computing. We report on the overall experience
using PNRP resources, with both successes and pain points.",http://arxiv.org/pdf/2308.07999v1
2308.07975v1,physics.comp-ph,Optimizing the size of array for modern discrete Fourier transform libraries,2023-08-15 18:25:21+00:00,"The problem of optimization of the array size for modern discrete Fourier
transform libraries is considered and reformulated as an integer linear
programming problem. Acceleration of finding an optimal solution using standard
freely available library with respect to brute force approach is demonstrated.
Ad hoc recursive algorithm of finding the optimal solution is proposed,
complexity scaling of the algorithm is estimated analytically. The problem can
be used in a linear programming class as an example of purely integer
programming problem (continuous linear programming solution has no sense),
simple enough to be solved using even interpreting programming languages like
Python or Matlab.",http://arxiv.org/pdf/2308.07975v1
2308.07964v3,quant-ph,Quantum computing for chemistry and physics applications from a Monte Carlo perspective,2023-08-15 18:01:28+00:00,"This Perspective focuses on the several overlaps between quantum algorithms
and Monte Carlo methods in the domains of physics and chemistry. We will
analyze the challenges and possibilities of integrating established quantum
Monte Carlo solutions in quantum algorithms. These include refined energy
estimators, parameter optimization, real and imaginary-time dynamics, and
variational circuits. Conversely, we will review new ideas in utilizing quantum
hardware to accelerate the sampling in statistical classical models, with
applications in physics, chemistry, optimization, and machine learning. This
review aims to be accessible to both communities and intends to foster further
algorithmic developments at the intersection of quantum computing and Monte
Carlo methods. Most of the works discussed in this Perspective have emerged
within the last two years, indicating a rapidly growing interest in this
promising area of research.",http://arxiv.org/pdf/2308.07964v3
2308.08569v1,physics.comp-ph,Fullwave design of cm-scale cylindrical metasurfaces via fast direct solvers,2023-08-15 17:54:10+00:00,"Large-scale metasurfaces promise nanophotonic performance improvements to
macroscopic optics functionality, for applications from imaging to analog
computing. Yet the size scale mismatch of centimeter-scale chips versus
micron-scale wavelengths prohibits use of conventional full-wave simulation
techniques, and has necessitated dramatic approximations. Here, we show that
tailoring ""fast direct"" integral-equation simulation techniques to the form
factor of metasurfaces offers the possibility for accurate and efficient
full-wave, large-scale metasurface simulations. For cylindrical
(two-dimensional) metasurfaces, we demonstrate accurate simulations whose
solution time scales \emph{linearly} with the metasurface diameter. Moreover,
the solver stores compressed information about the simulation domain that is
reusable over many design iterations. We demonstrate the capabilities of our
solver through two designs: first, a high-efficiency, high-numerical-aperture
metalens that is 20,000 wavelengths in diameter. Second, a high-efficiency,
large-beam-width grating coupler. The latter corresponds to millimeter-scale
beam design at standard telecommunications wavelengths, while the former, at a
visible wavelength of 500 nm, corresponds to a design diameter of 1 cm, created
through full simulations of Maxwell's equations.",http://arxiv.org/pdf/2308.08569v1
2308.07907v1,hep-th,Sequential Monte Carlo with Cross-validated Neural Networks for Complexity of Hyperbolic Black Hole Solutions in 4D,2023-08-15 17:48:35+00:00,"This paper investigates the self-similar solutions of the
Einstein-axion-dilaton configuration from type IIB string theory and the global
SL(2,R) symmetry. We consider the Continuous Self Similarity (CSS), where the
scale transformation is controlled by an SL(2, R) boost or hyperbolic
translation. The solutions stay invariant under the combination of space-time
dilation with internal SL(2,R) transformations. We develop a new formalism
based on Sequential Monte Carlo (SMC) and artificial neural networks (NNs) to
estimate the self-similar solutions to the equations of motion in the
hyperbolic class in four dimensions. Due to the complex and highly nonlinear
patterns, researchers typically have to use various constraints and numerical
approximation methods to estimate the equations of motion; thus, they have to
overlook the measurement errors in parameter estimation. Through a Bayesian
framework, we incorporate measurement errors into our models to find the
solutions to the hyperbolic equations of motion. It is well known that the
hyperbolic class suffers from multiple solutions where the critical collapse
functions have overlap domains for these solutions. To deal with this
complexity, for the first time in literature on the axion-dilaton system, we
propose the SMC approach to obtain the multi-modal posterior distributions.
Through a probabilistic perspective, we confirm the deterministic $\alpha$ and
$\beta$ solutions available in the literature and determine all possible
solutions that may occur due to measurement errors. We finally proposed the
penalized Leave-One-Out Cross-validation (LOOCV) to combine the Bayesian
NN-based estimates optimally. The approach enables us to determine the optimum
weights while dealing with the co-linearity issue in the NN-based estimates and
better predict the critical functions corresponding to multiple solutions of
the equations of motion.",http://arxiv.org/pdf/2308.07907v1
2308.07841v1,math.DS,No-propagate algorithm for linear responses of random chaotic systems,2023-08-15 15:37:12+00:00,"We develop the no-propagate algorithm for sampling the linear response of
random dynamical systems, which are non-uniform hyperbolic deterministic
systems perturbed by noise with smooth density. We first derive a Monte-Carlo
type formula and then the algorithm, which is different from the ensemble
(stochastic gradient) algorithms, finite-element algorithms, and fast-response
algorithms; it does not involve the propagation of vectors or covectors, and
only the density of the noise is differentiated, so the formula is not cursed
by gradient explosion, dimensionality, or non-hyperbolicity. We demonstrate our
algorithm on a tent map perturbed by noise and a chaotic neural network with 51
layers $\times$ 9 neurons.
  By itself, this algorithm approximates the linear response of non-hyperbolic
deterministic systems, with an additional error proportional to the noise. We
also discuss the potential of using this algorithm as a part of a bigger
algorithm with smaller error.",http://arxiv.org/pdf/2308.07841v1
2308.07564v1,math.NA,MSAT: Matrix stability analysis tool for shock-capturing schemes,2023-08-15 04:20:30+00:00,"The simulation of supersonic or hypersonic flows often suffers from numerical
shock instabilities if the flow field contains strong shocks, limiting the
further application of shock-capturing schemes. In this paper, we develop the
unified matrix stability analysis method for schemes with three-point stencils
and present MSAT, an open-source tool to quantitatively analyze the shock
instability problem. Based on the finite-volume approach on the structured
grid, MSAT can be employed to investigate the mechanism of the shock
instability problem, evaluate the robustness of numerical schemes, and then
help to develop robust schemes. Also, MSAT has the ability to analyze the
practical simulation of supersonic or hypersonic flows, evaluate whether it
will suffer from shock instabilities, and then assist in selecting appropriate
numerical schemes accordingly. As a result, MSAT is a helpful tool that can
investigate the shock instability problem and help to cure it.",http://arxiv.org/pdf/2308.07564v1
2308.07561v3,physics.optics,Inverse Design of Terahertz Nanoresonators through Physics-Informed Machine Learning,2023-08-15 04:08:11+00:00,"The rapid development of 6G communications using terahertz (THz)
electromagnetic waves has created a demand for highly sensitive THz
nanoresonators capable of detecting these waves. Among the potential
candidates, THz nanogap loop arrays show promising characteristics but require
significant computational resources for accurate simulation. This requirement
arises because their unit cells are 10 times smaller than millimeter
wavelengths, with nanogap regions that are 1,000,000 times smaller. To address
this challenge, we propose a rapid inverse design method for terahertz
nanoresonators using physics-informed machine learning, specifically employing
double deep Q-learning combined with an analytical model of the THz nanogap
loop array. Through approximately 200,000 iterations in about 39 hours on a
middle-level personal computer (CPU: 3.40 GHz, 6 cores, 12 threads, RAM: 16 GB,
GPU: NVIDIA GeForce GTX 1050), our approach successfully identifies the optimal
structure, resulting in an experimental electric field enhancement of 32,000 at
0.2 THz, 300% stronger than previous achievements. By leveraging our analytical
model-based approach, we significantly reduce the computational resources
required, providing a viable alternative to the impractical numerical
simulation-based inverse design that was previously impractical.",http://arxiv.org/pdf/2308.07561v3
2308.07544v1,physics.chem-ph,Earth Mover's Distance as a metric to evaluate the extent of charge transfer in excitations using discretized real-space densities,2023-08-15 03:21:05+00:00,"This paper presents a novel theoretical measure, $\mu^{\text{EMD}}$, based on
the Earth Mover's Distance, for quantifying the density shift caused by
electronic excitations in molecules. As input, the EMD metric uses only the
discretized ground and excited state electron densities in real space,
rendering it compatible with almost all electronic structure methods used to
calculate excited states. The EMD metric is compared against other popular
theoretical metrics for describing the extent of electron-hole separation in a
wide range of excited states (valence, Rydberg, charge-transfer, etc). The
results showcase the EMD metric's effectiveness across all excitation types and
suggest that it is useful as an additional tool to characterize electronic
excitations. The study also reveals that $\mu^{\text{EMD}}$ can function as a
promising diagnostic tool for predicting the failure of pure
exchange-correlation functionals. Specifically, we show statistical
relationships between the functional-driven errors, the exact exchange content
within the functional, and the magnitude of $\mu^{\text{EMD}}$ values.",http://arxiv.org/pdf/2308.07544v1
2308.07487v1,physics.comp-ph,Kernel Fusion in Atomistic Spin Dynamics Simulations on Nvidia GPUs using Tensor Core,2023-08-14 22:37:27+00:00,"In atomistic spin dynamics simulations, the time cost of constructing the
space- and time-displaced pair correlation function in real space increases
quadratically as the number of spins $N$, leading to significant computational
effort. The GEMM subroutine can be adopted to accelerate the calculation of the
dynamical spin-spin correlation function, but the computational cost of
simulating large spin systems ($>40000$ spins) on CPUs remains expensive. In
this work, we perform the simulation on the graphics processing unit (GPU), a
hardware solution widely used as an accelerator for scientific computing and
deep learning. We show that GPUs can accelerate the simulation up to 25-fold
compared to multi-core CPUs when using the GEMM subroutine on both. To hide
memory latency, we fuse the element-wise operation into the GEMM kernel using
$\mathtt{CUTLASS}$ that can improve the performance by 26% $\sim$ 33% compared
to implementation based on $\mathtt{cuBLAS}$. Furthermore, we perform the
on-the-fly calculation in the epilogue of the GEMM subroutine to avoid saving
intermediate results on global memory, which makes the large-scale atomistic
spin dynamics simulation feasible and affordable.",http://arxiv.org/pdf/2308.07487v1
2308.07311v1,cond-mat.mtrl-sci,"Stability, mechanisms and kinetics of emergence of Au surface reconstructions using Bayesian force fields",2023-08-14 17:55:07+00:00,"Metal surfaces have long been known to reconstruct, significantly influencing
their structural and catalytic properties. Many key mechanistic aspects of
these subtle transformations remain poorly understood due to limitations of
previous simulation approaches. Using active learning of Bayesian
machine-learned force fields trained from ab initio calculations, we enable
large-scale molecular dynamics simulations to describe the thermodynamics and
time evolution of the low-index mesoscopic surface reconstructions of Au (e.g.,
the Au(111)-`Herringbone,' Au(110)-(1$\times$2)-`Missing-Row,' and
Au(100)-`Quasi-Hexagonal' reconstructions). This capability yields direct
atomistic understanding of the dynamic emergence of these surface states from
their initial facets, providing previously inaccessible information such as
nucleation kinetics and a complete mechanistic interpretation of reconstruction
under the effects of strain and local deviations from the original
stoichiometry. We successfully reproduce previous experimental observations of
reconstructions on pristine surfaces and provide quantitative predictions of
the emergence of spinodal decomposition and localized reconstruction in
response to strain at non-ideal stoichiometries. A unified mechanistic
explanation is presented of the kinetic and thermodynamic factors driving
surface reconstruction. Furthermore, we study surface reconstructions on Au
nanoparticles, where characteristic (111) and (100) reconstructions
spontaneously appear on a variety of high-symmetry particle morphologies.",http://arxiv.org/pdf/2308.07311v1
2308.07277v1,cond-mat.mtrl-sci,Quantum MASALA: Quantum MAterialS Ab initio eLectronic-structure pAckage,2023-08-14 17:06:33+00:00,"We present QuantumMASALA, a compact package that implements different
electronic structure methods in Python. Within just 8000 lines of pure Python
code, we have implemented Density Functional Theory (DFT), Time dependent
Density Functional Theory (TD-DFT) and the GW Method. The program can run
across multiple process cores and in Graphical Processing Units (GPU) with the
help of easily-accessible Python libraries. With QuantumESPRESSO and BerkeleyGW
I/O interfaces implemented, it can also be used as a substitute for small scale
calculations, making it a perfect learning tool for ab initio methods. The
package is aimed to provide a framework with its modular and simple code design
to rapidly build and test new methods for first-principles calculation.",http://arxiv.org/pdf/2308.07277v1
2308.07178v1,quant-ph,de Broglie-Bohm analysis of a nonlinear membrane: From quantum to classical chaos,2023-08-14 14:38:48+00:00,"Within the de Broglie-Bohm theory, we numerically study a generic
two-dimensional anharmonic oscillator including cubic and quartic interactions.
Our analysis of the quantum velocity fields and trajectories reveals the
emergence of dynamical vortices. In their vicinity, fingerprints of chaotic
behavior such as unpredictability and sensitivity to initial conditions are
detected. The simultaneous presence of off-diagonal and nonlinear terms leads
to robust quantum chaos very analogous to its classical version.",http://arxiv.org/pdf/2308.07178v1
2308.07352v1,cs.LG,Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer,2023-08-14 09:32:21+00:00,"Globally, there are many polluted groundwater sites that need an active
remediation plan for the restoration of local ecosystem and environment.
Engineered nanoparticles (ENPs) have proven to be an effective reactive agent
for the in-situ degradation of pollutants in groundwater. While the performance
of these ENPs has been highly promising on the laboratory scale, their
application in real field case conditions is still limited. The complex
transport and retention mechanisms of ENPs hinder the development of an
efficient remediation strategy. Therefore, a predictive tool to comprehend the
transport and retention behavior of ENPs is highly required. The existing tools
in the literature are dominated with numerical simulators, which have limited
flexibility and accuracy in the presence of sparse datasets and the aquifer
heterogeneity. This work uses a Bayesian Physics-Informed Neural Network
(B-PINN) framework to model the nano-particles mobility within an aquifer. The
result from the forward model demonstrates the effective capability of B-PINN
in accurately predicting the ENPs mobility and quantifying the uncertainty. The
inverse model output is then used to predict the governing parameters for the
ENPs mobility in a small-scale aquifer. The research demonstrates the
capability of the tool to provide predictive insights for developing an
efficient groundwater remediation strategy.",http://arxiv.org/pdf/2308.07352v1
2308.06840v1,physics.flu-dyn,Mesh-Free Hydrodynamic Stability,2023-08-13 19:49:29+00:00,"A specialized mesh-free radial basis function-based finite difference
(RBF-FD) discretization is used to solve the large eigenvalue problems arising
in hydrodynamic stability analyses of flows in complex domains. Polyharmonic
spline functions with polynomial augmentation (PHS+poly) are used to construct
the discrete linearized incompressible and compressible Navier-Stokes operators
on scattered nodes. Rigorous global and local eigenvalue stability studies of
these global operators and their constituent RBF stencils provide a set of
parameters that guarantee stability while balancing accuracy and computational
efficiency. Specialized elliptical stencils to compute boundary-normal
derivatives are introduced and the treatment of the pole singularity in
cylindrical coordinates is discussed. The numerical framework is demonstrated
and validated on a number of hydrodynamic stability methods ranging from
classical linear theory of laminar flows to state-of-the-art non-modal
approaches that are applicable to turbulent mean flows. The examples include
linear stability, resolvent, and wavemaker analyses of cylinder flow at
Reynolds numbers ranging from 47 to 180, and resolvent and wavemaker analyses
of the self-similar flat-plate boundary layer at a Reynolds number as well as
the turbulent mean of a high-Reynolds-number transonic jet at Mach number 0.9.
All previously-known results are found in close agreement with the literature.
Finally, the resolvent-based wavemaker analyses of the Blasius boundary layer
and turbulent jet flows offer new physical insight into the modal and non-modal
growth in these flows.",http://arxiv.org/pdf/2308.06840v1
2308.06708v1,cs.LG,Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model,2023-08-13 07:55:46+00:00,"This paper presents an ensemble data assimilation method using the pseudo
ensembles generated by denoising diffusion probabilistic model. Since the model
is trained against noisy and sparse observation data, this model can produce
divergent ensembles close to observations. Thanks to the variance in generated
ensembles, our proposed method displays better performance than the
well-established ensemble data assimilation method when the simulation model is
imperfect.",http://arxiv.org/pdf/2308.06708v1
2308.06675v1,physics.comp-ph,"Learning fast, accurate, and stable closures of a kinetic theory of an active fluid",2023-08-13 03:33:40+00:00,"Important classes of active matter systems can be modeled using kinetic
theories. However, kinetic theories can be high dimensional and challenging to
simulate. Reduced-order representations based on tracking only low-order
moments of the kinetic model serve as an efficient alternative, but typically
require closure assumptions to model unrepresented higher-order moments. In
this study, we present a learning framework based on neural networks that
exploit rotational symmetries in the closure terms to learn accurate closure
models directly from kinetic simulations. The data-driven closures demonstrate
excellent a-priori predictions comparable to the state-of-the-art Bingham
closure. We provide a systematic comparison between different neural network
architectures and demonstrate that nonlocal effects can be safely ignored to
model the closure terms. We develop an active learning strategy that enables
accurate prediction of the closure terms across the entire parameter space
using a single neural network without the need for retraining. We also propose
a data-efficient training procedure based on time-stepping constraints and a
differentiable pseudo-spectral solver, which enables the learning of stable
closures suitable for a-posteriori inference. The coarse-grained simulations
equipped with data-driven closure models faithfully reproduce the mean velocity
statistics, scalar order parameters, and velocity power spectra observed in
simulations of the kinetic theory. Our differentiable framework also
facilitates the estimation of parameters in coarse-grained descriptions
conditioned on data.",http://arxiv.org/pdf/2308.06675v1
2308.06516v1,math.NA,Runge--Kutta methods determined from extended phase space methods for Hamiltonian systems,2023-08-12 09:45:11+00:00,"We study two existing extended phase space integrators for Hamiltonian
systems, the {\em midpoint projection method} and the {\em symmetric projection
method}, showing that the first is a pseudosymplectic and pseudosymmetric
Runge--Kutta method and the second is a monoimplicit symplectic Runge--Kutta
method.",http://arxiv.org/pdf/2308.06516v1
2308.06462v1,physics.comp-ph,On the Atomic Cluster Expansion: interatomic potentials and beyond,2023-08-12 04:03:45+00:00,"The Atomic Cluster Expansion (ACE) [R. Drautz, Phys. Rev. B, 99:014104
(2019)] provides a systematically improvable, universal descriptor for the
environment of an atom that is invariant to permutation, translation and
rotation. ACE is being used extensively in newly emerging interatomic
potentials based on machine learning. This commentary discusses the ACE
framework and its potential impact.",http://arxiv.org/pdf/2308.06462v1
2308.06196v2,physics.flu-dyn,Towards full molecular gas dynamics simulations of complex flows via the Boltzmann equation,2023-08-11 15:40:52+00:00,"This work explores the capability of simulating complex fluid flows by
directly solving the Boltzmann equation. Due to the high-dimensionality of the
governing equation, the substantial computational cost of solving the Boltzmann
equation has generally limited its application to simpler, two-dimensional flow
problems. Utilizing a combination of high-order spatial discretizations and
discretely-conservative velocity models along with their highly-efficient
implementation on massively-parallel GPU computing architectures, we
demonstrate the current ability of directly solving the Boltzmann equation
augmented with the BGK collision model for complex, three-dimensional flows.
Numerical results are presented for a variety of these problems including
rarefied microchannels, transitional and turbulent flows, and high-speed
atmospheric re-entry vehicles, showcasing the ability of the approach in
accurately predicting complex nonlinear flow phenomena and non-equilibrium
effects.",http://arxiv.org/pdf/2308.06196v2
2308.06071v1,physics.comp-ph,Fermionic physics from ab initio path integral Monte Carlo simulations of fictitious identical particles,2023-08-11 11:22:40+00:00,"The \emph{ab initio} path integral Monte Carlo (PIMC) method is one of the
most successful methods in statistical physics, quantum chemistry and related
fields, but its application to quantum degenerate Fermi systems is severely
hampered by an exponential computational bottleneck: the notorious fermion sign
problem. Very recently, Xiong and Xiong [J. Chem. Phys. 157, 094112 (2022)]
have suggested to partially circumvent the sign problem by carrying out PIMC
simulations of fictitious systems which contain an interpolating continuous
variable $\xi\in[-1,1]$ in their partition function, with the physical Fermi-
and Bose-statistics corresponding to the endpoint limits $\xi=-1$ and $\xi=1$.
It has been proposed that thermodynamic information about the fermionic limit
might be obtained by path integral calculations within the bosonic sector
$\xi>0$ combined with a quadratic $\xi$ extrapolation throughout the fermionic
sector $\xi<0$, essentially bypassing the sign problem. In this work, we show
how the inclusion of the artificial parameter $\xi$ can be interpreted as an
effective penalty on the formation of permutation cycles in the PIMC
simulation. We empirically demonstrate that the proposed extrapolation method
breaks down for moderate to high quantum degeneracy. Instead, the method
constitutes a valuable tool for the description of large Fermi-systems of weak
quantum degeneracy. This is demonstrated for electrons in a 2D harmonic trap
and for the archetypal uniform electron gas (UEG), where we find excellent
agreement ($\sim0.5\%$) with exact configuration PIMC results in the
high-density regime while attaining a speed-up exceeding eleven orders of
magnitude. Finally, we extend the idea beyond the energy and analyze the radial
density distribution (2D trap), as well as the static structure factor and
imaginary-time density-density correlation function (UEG).",http://arxiv.org/pdf/2308.06071v1
2308.05999v1,cs.LG,Does AI for science need another ImageNet Or totally different benchmarks? A case study of machine learning force fields,2023-08-11 08:06:58+00:00,"AI for science (AI4S) is an emerging research field that aims to enhance the
accuracy and speed of scientific computing tasks using machine learning
methods. Traditional AI benchmarking methods struggle to adapt to the unique
challenges posed by AI4S because they assume data in training, testing, and
future real-world queries are independent and identically distributed, while
AI4S workloads anticipate out-of-distribution problem instances. This paper
investigates the need for a novel approach to effectively benchmark AI for
science, using the machine learning force field (MLFF) as a case study. MLFF is
a method to accelerate molecular dynamics (MD) simulation with low
computational cost and high accuracy. We identify various missed opportunities
in scientifically meaningful benchmarking and propose solutions to evaluate
MLFF models, specifically in the aspects of sample efficiency, time domain
sensitivity, and cross-dataset generalization capabilities. By setting up the
problem instantiation similar to the actual scientific applications, more
meaningful performance metrics from the benchmark can be achieved. This suite
of metrics has demonstrated a better ability to assess a model's performance in
real-world scientific applications, in contrast to traditional AI benchmarking
methodologies. This work is a component of the SAIBench project, an AI4S
benchmarking suite. The project homepage is
https://www.computercouncil.org/SAIBench.",http://arxiv.org/pdf/2308.05999v1
2308.05900v1,cond-mat.mtrl-sci,QERaman: An open-source program for calculating resonance Raman spectra based on Quantum ESPRESSO,2023-08-11 01:25:02+00:00,"We present an open-source program QERaman that computes first-order resonance
Raman spectroscopy of materials using the output data from Quantum ESPRESSO.
Complex values of Raman tensors are calculated based on the quantum description
of the Raman scattering from calculations of electron-photon and
electron-phonon matrix elements, which are obtained by using the modified
Quantum ESPRESSO. Our program also calculates the resonant Raman spectra as a
function of incident laser energy for linearly- or circularly-polarized light.
Hands-on tutorials for graphene and MoS$_2$ are given to show how to run
QERaman. All codes, examples, and scripts are available on the GitHub
repository.",http://arxiv.org/pdf/2308.05900v1
2308.05845v1,physics.comp-ph,Treatment of long-range interactions arising in the Enskog-Vlasov description of dense fluids,2023-08-10 19:55:56+00:00,"The kinetic theory of rarefied gases and numerical schemes based on the
Boltzmann equation have evolved to the cornerstone of non-equilibrium gas
dynamics. However, their counterparts in the dense regime remain rather exotic
for practical non-continuum scenarios. This problem is partly due to the fact
that long-range interactions arising from the attractive tail of molecular
potentials, lead to a computationally demanding Vlasov integral. This study
focuses on numerical remedies for efficient stochastic particle simulations
based on the Enskog-Vlasov kinetic equation. In particular, we devise a
Poisson-type elliptic equation that governs the underlying long-range
interactions. The idea comes through fitting a Green function to the molecular
potential, and hence deriving an elliptic equation for the associated
fundamental solution. Through this transformation of the Vlasov integral,
efficient Poisson-type solvers can be readily employed in order to compute the
mean field forces. Besides the technical aspects of different numerical schemes
for the treatment of the Vlasov integral, simulation results for the
evaporation of a liquid slab into the vacuum are presented. It is shown that
the proposed formulation leads to accurate predictions with a reasonable
computational cost.",http://arxiv.org/pdf/2308.05845v1
2308.05454v1,cond-mat.mtrl-sci,"Finite-temperature ductility-brittleness and electronic structures of Al$_{n}$Sc (n=1, 2 and 3)",2023-08-10 09:25:01+00:00,"Finite-temperature ductility-brittleness and electronic structures of
Al$_3$Sc, Al$_2$Sc and AlSc are studied comparatively by first-principles
calculations and ab-initio molecular dynamics. Results show that Al$_3$Sc and
Al$_2$Sc are inherently brittle at both ground state and finite temperatures.
By contrast, AlSc possesses a significantly superior ductility evaluated from
all Pugh's, Pettifor's and Poisson's ductility-brittleness criteria. At ground
state, AlSc meets the criteria of ductile according to Pugh's and Poisson's
theories, while it is categorized as the brittle in the frame of Pettifor's
picture. With the increasing temperature, the ductility of all the studied
compounds exhibits a noticeable improvement. In particular, as the temperature
rises, the Cauchy pressure of AlSc undergoes a transition from negative to
positive. Thus, at high temperatures (T > 600 K), AlSc is unequivocally
classified as the ductile from all criteria considered. In all Al$_3$Sc,
Al$_2$Sc and AlSc, the Al-Al bond, originated from s-p and p-p orbital
hybridizations, and the Al-Sc bond, dominated by p-d covalent hybridization,
are the first and second strongest chemical bonds, respectively. To explain the
difference in mechanical properties of the studied compounds, the mean bond
strength (MBS) is evaluated. The weaker Al-Al bond in AlSc, leading to a
smaller MBS, could be the origin for the softer elastic stiffness and superior
intrinsic ductility. The longer length of the Al-Al bond in AlSc is responsible
for its weaker bond strength. Furthermore, the enhanced metallicity of the
Al-Al bond in AlSc would also contribute to its exceptional ductility.",http://arxiv.org/pdf/2308.05454v1
2308.05297v1,physics.chem-ph,Exceptional spatial variation of charge injection energies on plasmonic surfaces,2023-08-10 02:39:41+00:00,"Charge injection into a molecule on a metallic interface is a key step in
many photo-activated reactions. The energy barrier for injection is paralleled
with the lowest particle and hole addition energies. We employ Green's function
formalism of the many-body perturbation theory and compute the excitation
energies, which include non-local correlations due to charge density
fluctuations on the surface, i.e., the plasmons. We explore a prototypical
system: CO$_2$ molecule on nanoscale plasmonic Au infinite and nanoparticle
surface with nearly 3,000 electrons. In contrast to widely used density
functional theory, we demonstrate that the energy barrier varies significantly
depending on the molecular position on the surface, creating ""hot spots"" for
possible carrier injection. These areas arise due to an intertwined competition
between purely plasmonic couplings (charge density fluctuations on the
substrate surface alone) and the degree of hybridization between the molecule
and the substrate. There are multiple positions found with the lowest energy
barrier for the electron/hole injection. We identify that the charge injection
barrier to the adsorbate on the plasmonic surface trends down from the facet
edge to the facet center -- here, the change in molecular orbitals overshadows
the role of the charge fluctuations in the substrate. This finding contrasts
the typical picture in which the electric field enhancement on the nanoparticle
edges is considered the most critical factor.",http://arxiv.org/pdf/2308.05297v1
2308.05193v1,physics.comp-ph,"Motifs in seismic networks: Romania, Italy, United States of America, and Japan",2023-08-09 19:11:06+00:00,"We present a detailed description of seismic activity in Romania, Italy, and
Japan, as well as the California seismic zone in the United States of America,
based on the statistical analysis of the underlying seismic networks used to
model the aforementioned zones. Our results on network connectivity and simple
network motifs allow for a complex description of seismic zones, while at the
same time reinforcing the current understanding of seismicity as a critical
phenomenon. The reported distributions on node connectivity, three-, and
four-event motifs are consistent with power-law, i.e., scale-free,
distributions over large intervals and are robust across seismic networks
obtained from different discretizations of the seismic zones of interest. In
our analysis of the distributions of node connectivity and simple motifs, we
distinguish between the global distribution and the power-law part of it with
the help of maximum likelihood estimation (MLE) method and complementary
cumulative distribution functions (CCDF). The main message is that the
distributions reported for the aforementioned seismic zones are not
intrinsically power laws, but have large power-law components, extending over
some orders of magnitude, independent of discretization. All the results were
obtained using publicly-available databases and open-source software, as well
as a new toolbox available on GitHub, specifically designed to automatically
analyze earthquake databases.",http://arxiv.org/pdf/2308.05193v1
2308.05186v1,physics.acc-ph,Fast modeling of high-gain cavity-based x-ray free-electron lasers,2023-08-09 18:44:15+00:00,"Cavity-based x-ray free-electron lasers (CBXFELs) are expected to be a
dramatic leap ahead in x-ray source brightness and coherence properties
compared to single pass XFELs, unlocking previously impossible experimental
capabilities. One particular challenge of the CBXFEL is electron-optical
overlap due to its relatively large (hundreds of meters) footprint. Numerical
modeling of CBXFELs with angular and positional errors is critical for
designing stable cavities, as well as to predict signatures of specific
misalignment effects. Full-scale simulations of CBXFELs are incredibly
time-consuming, making large-scale parameter searches intractable on reasonable
timescales. In this paper, we present a semi-analytical model that allows to
investigate realistic scenarios - x-ray cavity without gain (""cold cavity""),
x-ray cavity with a high gain XFEL amplifier, and an x-ray cavity with an
oscillating e-beam trajectory in an XFEL - all in the presence of angular and
positional errors. We especially focus on fast modeling of the XFEL process and
x-ray optics, while capturing effects pertaining to actual experimental setups
at the Linac Coherent Light Source (LCLS) at SLAC.",http://arxiv.org/pdf/2308.05186v1
2308.05163v1,cond-mat.mtrl-sci,Neural network potentials for modeling nonstoichiometric materials: a case of Chromium Sulfides Cr$_{(1-x)}$S,2023-08-09 18:08:57+00:00,"Deviation from stoichiometry can yield a diverse range of stable phases with
distinct physical and chemical properties. To comprehensively explore
nonstoichiometric materials, it is crucial to investigate their compositional
and structural domains with precision and cost-effectiveness. However, the
extensive diversity in these domains render first-principles methods, such as
density functional theory (DFT), inappropriate for such endeavors. In this
study, we propose a generic framework that utilizes neural network potentials
(NNPs) to model nonstoichiometric materials with chemical accuracy at realistic
length and time scales. We apply our framework to analyze nonstoichiometric
Cr$_{(1-x)}$S materials, a compelling material category with significant
potential in the field of two-dimensional (2D) magnetic materials applications.
The efficacy of the NNP model is shown to outperform the conventional cluster
expansion (CE) model, exhibiting near-DFT accuracy and robust transferability
to unexplored crystal structures and compositions. Furthermore, we employ the
NNP model in simulated annealing (SA) optimizations to predict the low-energy
Cr$_{(1-x)}$S structures across diverse compositions. A notable structural
transition is discerned at the Cr$_{0.5}$S composition, characterized by a
preferential migration of half of the Cr atoms to the van der Waals (vdW) gaps.
This highlights the experimentally observed non-vdW nature of CrS$_2$ and
emphasizes the pivotal role of excess Cr atoms beyond the composition ratio of
Cr/S = $1/2$ in stabilizing the vdW gaps. Additionally, we employ the NNP model
in a large-scale vacancy diffusion Monte Carlo (MC) simulation to emphasize the
impact of lateral compressive strains in catalyzing the formation of vdW gaps
within 2D CrS$_2$ slabs. This provides a direct pathway for more facile
exfoliation of ultrathin CrS$_2$ nanosheets through strain engineering.",http://arxiv.org/pdf/2308.05163v1
2308.05047v1,quant-ph,Large-scale simulation of Shor's quantum factoring algorithm,2023-08-09 16:19:52+00:00,"Shor's factoring algorithm is one of the most anticipated applications of
quantum computing. However, the limited capabilities of today's quantum
computers only permit a study of Shor's algorithm for very small numbers. Here
we show how large GPU-based supercomputers can be used to assess the
performance of Shor's algorithm for numbers that are out of reach for current
and near-term quantum hardware. First, we study Shor's original factoring
algorithm. While theoretical bounds suggest success probabilities of only 3-4
%, we find average success probabilities above 50 %, due to a high frequency of
""lucky"" cases, defined as successful factorizations despite unmet sufficient
conditions. Second, we investigate a powerful post-processing procedure, by
which the success probability can be brought arbitrarily close to one, with
only a single run of Shor's quantum algorithm. Finally, we study the
effectiveness of this post-processing procedure in the presence of typical
errors in quantum processing hardware. We find that the quantum factoring
algorithm exhibits a particular form of universality and resilience against the
different types of errors. The largest semiprime that we have factored by
executing Shor's algorithm on a GPU-based supercomputer, without exploiting
prior knowledge of the solution, is 549755813701 = 712321 * 771781. We put
forward the challenge of factoring, without oversimplification, a non-trivial
semiprime larger than this number on any quantum computing device.",http://arxiv.org/pdf/2308.05047v1
2308.05039v2,physics.optics,Symmetry Broken Vectorial Kerr Frequency Combs from Fabry-Prot Resonators,2023-08-09 16:10:30+00:00,"Optical frequency combs find many applications in metrology, frequency
standards, communications and photonic devices. We consider field polarization
properties and describe a vector comb generation through the spontaneous
symmetry breaking of temporal cavity solitons within coherently driven,
passive, Fabry-P\'erot cavities with Kerr nonlinearity. Global coupling effects
due to the interactions of counter-propagating light restrict the maximum
number of soliton pairs within the cavity - even down to a single soliton pair
- and force long range polarization conformity in trains of vector solitons.",http://arxiv.org/pdf/2308.05039v2
2308.05023v1,physics.chem-ph,High-energy nitrogen rings stabilized by superatom properties,2023-08-09 15:38:45+00:00,"How to stabilize nitrogen-rich high-energy-density molecules under
conventional conditions is particularly important for the energy storage and
conversion of such systems and has attracted extensive attention. In this work,
our theoretical study showed for the first time that the stabilization
mechanism of the nitrogen ring conformed to the superatomic properties at the
atomic level. This result occurred because the stabilized anionic nitrogen
rings generally showed planar high symmetry and the injected electrons occupied
the superatomic molecular orbitals (SAMOs) of the nitrogen rings. According to
these results, we identified the typical stabilized anionic nitrogen ring
structures N64-, N5- and N42-, and their superatomic electronic configurations
were 1S21P41D41F22S21P21F21D42P41G41F4, 1S21P41D41P22S21F41D42P4 and
1S21P41D21P21D22S22P41D4, respectively. On this basis, we further designed a
pathway to stabilize nitrogen rings by introducing metal atoms as electron
donors to form neutral ThN6, LiN5 and MgN4 structures, thereby replacing the
anionization of systems. Our study highlights the importance of developing
nitrogen-rich energetic materials from the perspective of superatoms.",http://arxiv.org/pdf/2308.05023v1
2308.04897v1,physics.optics,Fast simulation of light scattering and harmonic generation in axially symmetric structures in COMSOL,2023-08-09 11:57:18+00:00,"In the field of optics and nanophotonics, simulation of electromagnetic
scattering plays a major role in the study of complex nanostructures and
optical devices. The numerical analysis of scattering spectra, even for
nanocavities with simple geometry, is associated with significant computational
difficulties. However, when the system exhibits certain symmetries, it becomes
possible to simplify the problem through the process of separation of
variables, which leads to a decrease in its dimension. In this paper, we aim to
provide a practical guide to a fast simulation of linear and non-linear
scattering problems in COMSOL Multiphysics for axisymmetric objects including
computation of scattering cross-section as well as its multipolar
decomposition, optical forces, and second harmonic generation. We also
accompany the provided guide with the ready-to-run COMSOL models.",http://arxiv.org/pdf/2308.04897v1
2308.04869v1,physics.plasm-ph,RBG-Maxwell Framework: Simulation of Collisional Plasma Systems via Coupled Boltzmann-Maxwell equations on GPU,2023-08-09 11:02:39+00:00,"This paper presents the RBG-Maxwell framework, a relativistic collisional
plasma simulator on GPUs. We provide detailed discussions on the fundamental
equations, numerical algorithms, implementation specifics, and key testing
outcomes. The RBG-Maxwell framework is a robust numerical code designed for
simulating the evolution of plasma systems through a kinetic approach on
large-scale GPUs. It offers easy adaptability to a wide range of physical
systems. Given the appropriate initial distributions, particle masses, charges,
differential cross-sections, and external forces (which are not confined to
electromagnetic forces), the RBG-Maxwell framework can direct the evolution of
a particle system from a non-equilibrium state to a thermal state.",http://arxiv.org/pdf/2308.04869v1
2308.04845v1,physics.comp-ph,Interaction-induced directional transport on periodically driven chains,2023-08-09 10:12:51+00:00,"We study a driven system in which interaction between particles causes their
directional, coupled movement. In that model system, two particles move
alternatingly in time on two coupled chains. Without interaction, both
particles diffuse along their respective chains, independent from one another.
Interaction between them, no matter if attractive or repellent, leads to an
energetic separation of configurations where the particles are close to each
other and those where they are farther separated. The energy difference causes
close-by particles to remain bound together, forming a doublon. Their relative
position in the starting configuration determines whether the doublon moves to
the left or right or remains stationary due to the periodic driving.",http://arxiv.org/pdf/2308.04845v1
2308.04827v1,cond-mat.soft,Tuning the Stability of a Model Quasicrystal and its Approximants with a Periodic Substrate,2023-08-09 09:26:47+00:00,"Quasicrystals and their periodic approximants are complex phases, which have
by now been observed in many metallic alloys, soft matter systems, and particle
simulations. In recent experiments of thin-film perovskites on solid
substrates, the type of complex phase was found to change depending on
thermodynamic conditions and the type of substrate used. Here, we investigate
the effect of a substrate on the relative stability of a two-dimensional model
quasicrystal and its approximants. Our numerical methods are molecular dynamics
simulations and free energy calculations that take into account phason flips
explicitly. For weak substrates, we observe an incommensurate-commensurate
transition, in which a continuous series of QC approximants locks into a
discrete number of approximants. For stronger substrates, an enhancement of the
stability of the dodecagonal quasicrystal and a variants of square lattices
were found. All phenomena can be explained by the interplay of the model system
with the substrate. Our results demonstrate that designing novel complex
periodic and quasiperiodic structures by choice of suitable substrates is a
promising strategy.",http://arxiv.org/pdf/2308.04827v1
2308.04745v1,physics.acc-ph,A Laser-Plasma Ion Beam Booster Based on Hollow-Channel Magnetic Vortex Acceleration,2023-08-09 07:27:23+00:00,"Laser-driven ion acceleration can provide ultra-short, high-charge,
low-emittance beams. Although undergoing extensive research, demonstrated
maximum energies for laser-ion sources are non-relativistic, complicating
injection into high-$\beta$ accelerator elements and stopping short of
desirable energies for pivotal applications, such as proton tumor therapy. In
this work, we decouple the efforts towards relativistic beam energies from a
single laser-plasma source via a proof-of-principle concept, boosting the beam
into this regime through only a few plasma stages. We employ full 3D
particle-in-cell simulations to demonstrate the capability for capture of
high-charge beams as produced by laser-driven sources, where both source and
booster stages utilize readily available laser pulse parameters.",http://arxiv.org/pdf/2308.04745v1
2308.04690v1,math.NA,Finite Element Operator Network for Solving Parametric PDEs,2023-08-09 03:56:07+00:00,"Partial differential equations (PDEs) underlie our understanding and
prediction of natural phenomena across numerous fields, including physics,
engineering, and finance. However, solving parametric PDEs is a complex task
that necessitates efficient numerical methods. In this paper, we propose a
novel approach for solving parametric PDEs using a Finite Element Operator
Network (FEONet). Our proposed method leverages the power of deep learning in
conjunction with traditional numerical methods, specifically the finite element
method, to solve parametric PDEs in the absence of any paired input-output
training data. We demonstrate the effectiveness of our approach on several
benchmark problems and show that it outperforms existing state-of-the-art
methods in terms of accuracy, generalization, and computational flexibility.
Our FEONet framework shows potential for application in various fields where
PDEs play a crucial role in modeling complex domains with diverse boundary
conditions and singular behavior. Furthermore, we provide theoretical
convergence analysis to support our approach, utilizing finite element
approximation in numerical analysis.",http://arxiv.org/pdf/2308.04690v1
2308.04633v1,cond-mat.mtrl-sci,Topological interfacial states in ferroelectric domain walls of two-dimensional bismuth,2023-08-08 23:55:01+00:00,"Using machine learning method, we investigate various domain walls for the
recently discovered single-element ferroelectrics bismuth monolayer (Nature
617, 67 (2023)). We find the charged domain wall configuration has a lower
energy than the uncharged domain wall structure due to its low electrostatic
repusion potential. Two stable charged domain wall configurations exhibit
topological interfacial states near their domain walls, which is caused by the
change of the Z_2 number between ferroelectric and paraelectric states.
Interestingly, different from the edge states of topological insulators, the
energies of topological interfacial states for these two structure are splited
due to the build-in electric fields in ferroelectrics. We also find a stable
uncharged domain wall configutation can reduce band gap which is caused by the
domain wall. Our works indicate that domain walls in two-dimensional bismuth
may be a good platform for ferroelectric domain wall devices.",http://arxiv.org/pdf/2308.04633v1
2308.04488v1,hep-lat,Hadrons in (1+1)D Hamiltonian hardcore lattice QCD,2023-08-08 18:00:05+00:00,"We study 2-flavor Hamiltonian lattice QCD in (1+1)D with hardcore gluons, at
zero and finite density, by means of matrix product states. We introduce a
formulation of the theory where gauge redundancy is absent and construct a
gauge invariant tensor network ansatz. We show that the model is critical in an
extended subregion of parameter space and identify at least two distinct
phases, one of which embeds the continuum limit location. We reconstruct a
subset of the particle spectrum in each phase, identifying edge and bulk
gapless modes. We thereby show that the studied model provides a minimal SU(3)
gauge theory whilst reproducing known phenomena of (3+1)D QCD. Most notably,
its particle spectrum features charged pions.",http://arxiv.org/pdf/2308.04488v1
2308.13636v1,cond-mat.stat-mech,Non-parametric learning critical behavior in Ising partition functions: PCA entropy and intrinsic dimension,2023-08-25 19:06:22+00:00,"We provide and critically analyze a framework to learn critical behavior in
classical partition functions through the application of non-parametric methods
to data sets of thermal configurations. We illustrate our approach in phase
transitions in 2D and 3D Ising models. First, we extend previous studies on the
intrinsic dimension of 2D partition function data sets, by exploring the effect
of volume in 3D Ising data. We find that as opposed to 2D systems for which
this quantity has been successfully used in unsupervised characterizations of
critical phenomena, in the 3D case its estimation is far more challenging. To
circumvent this limitation, we then use the principal component analysis (PCA)
entropy, a ``Shannon entropy'' of the normalized spectrum of the covariance
matrix. We find a striking qualitative similarity to the thermodynamic entropy,
which the PCA entropy approaches asymptotically. The latter allows us to
extract -- through a conventional finite-size scaling analysis with modest
lattice sizes -- the critical temperature with less than $1\%$ error for both
2D and 3D models while being computationally efficient. The PCA entropy can
readily be applied to characterize correlations and critical phenomena in a
huge variety of many-body problems and suggests a (direct) link between
easy-to-compute quantities and entropies.",http://arxiv.org/pdf/2308.13636v1
2308.13604v1,cond-mat.dis-nn,Network science Ising states of matter,2023-08-25 18:01:03+00:00,"Network science provides very powerful tools for extracting information from
interacting data. Although recently the unsupervised detection of phases of
matter using machine learning has raised significant interest, the full
prediction power of network science has not yet been systematically explored in
this context. Here we fill this gap by providing an in-depth statistical,
combinatorial, geometrical and topological characterization of 2D Ising
snapshot networks (IsingNets) extracted from Monte Carlo simulations of the 2D
Ising model at different temperatures, going across the phase transition. Our
analysis reveals the complex organization properties of IsingNets in both the
ferromagnetic and paramagnetic phases and demonstrates the significant
deviations of the IsingNets with respect to randomized null models. In
particular percolation properties of the IsingNets reflect the existence of the
symmetry between configurations with opposite magnetization below the critical
temperature and the very compact nature of the two emerging giant clusters
revealed by our persistent homology analysis of the IsingNets. Moreover, the
IsingNets display a very broad degree distribution and significant
degree-degree correlations and weight-degree correlations demonstrating that
they encode relevant information present in the configuration space of the 2D
Ising model. The geometrical organization of the critical IsingNets is
reflected in their spectral properties deviating from the one of the null
model. This work reveals the important insights that network science can bring
to the characterization of phases of matter. The set of tools described hereby
can be applied as well to numerical and experimental data.",http://arxiv.org/pdf/2308.13604v1
2308.13117v1,physics.data-an,Probabilistic Mixture Model-Based Spectral Unmixing,2023-08-24 23:46:16+00:00,"Identifying pure components in mixtures is a common yet challenging problem.
This unmixing process requires that mixing preserves the identity of the
components (endmembers), i.e., mixing is linear, and the endmembers must be
spectrally distinct. Even with these requirements met, extracting the
endmembers from a single mixture may be impossible; an ensemble of mixtures
with sufficient diversity is needed. Several spectral unmixing approaches have
been proposed, many of which are connected to hyperspectral imaging. However,
most of them assume highly diverse collections of mixtures and extremely
low-loss spectroscopic measurements. Additionally, these frameworks do not
incorporate the uncertainty inherent in unmixing. We propose a probabilistic
inference approach that explicitly incorporates noise and uncertainty, enabling
us to unmix endmembers in collections of mixtures with limited diversity. We
use a Bayesian mixture model to jointly extract endmember spectra and mixing
parameters while explicitly modeling observation noise and the resulting
inference uncertainties. We obtain approximate distributions over endmember
coordinates for each set of observed spectra while remaining robust to
inference biases from the lack of pure observations and presence of
non-isotropic Gaussian noise. Access to reliable uncertainties on the unmixing
solutions would enable robust solutions as well as informed decision making.",http://arxiv.org/pdf/2308.13117v1
2308.13028v1,quant-ph,Training Neural Networks with Universal Adiabatic Quantum Computing,2023-08-24 18:51:50+00:00,"The training of neural networks (NNs) is a computationally intensive task
requiring significant time and resources. This paper presents a novel approach
to NN training using Adiabatic Quantum Computing (AQC), a paradigm that
leverages the principles of adiabatic evolution to solve optimisation problems.
We propose a universal AQC method that can be implemented on gate quantum
computers, allowing for a broad range of Hamiltonians and thus enabling the
training of expressive neural networks. We apply this approach to various
neural networks with continuous, discrete, and binary weights. Our results
indicate that AQC can very efficiently find the global minimum of the loss
function, offering a promising alternative to classical training methods.",http://arxiv.org/pdf/2308.13028v1
2308.13027v1,quant-ph,Efficient characterization of blinking quantum emitters from scarce data sets via machine learning,2023-08-24 18:51:30+00:00,"Single photon emitters are core building blocks of quantum technologies, with
established and emerging applications ranging from quantum computing and
communication to metrology and sensing. Regardless of their nature, quantum
emitters universally display fluorescence intermittency or photoblinking:
interaction with the environment can cause the emitters to undergo quantum
jumps between on and off states that correlate with higher and lower
photoemission events, respectively. Understanding and quantifying the mechanism
and dynamics of photoblinking is important for both fundamental and practical
reasons. However, the analysis of blinking time traces is often afflicted by
data scarcity. Blinking emitters can photo-bleach and cease to fluoresce over
time scales that are too short for their photodynamics to be captured by
traditional statistical methods. Here, we demonstrate two approaches based on
machine learning that directly address this problem. We present a multi-feature
regression algorithm and a genetic algorithm that allow for the extraction of
blinking on/off switching rates with >85% accuracy, and with >10x less data and
>20x higher precision than traditional methods based on statistical inference.
Our algorithms effectively extend the range of surveyable blinking systems and
trapping dynamics to those that would otherwise be considered too short-lived
to be investigated. They are therefore a powerful tool to help gain a better
understanding of the physical mechanism of photoblinking, with practical
benefits for applications based on quantum emitters that rely on either
mitigating or harnessing the phenomenon.",http://arxiv.org/pdf/2308.13027v1
2308.12724v1,hep-ex,Jet energy calibration with deep learning as a Kubeflow pipeline,2023-08-24 12:02:09+00:00,"Precise measurements of the energy of jets emerging from particle collisions
at the LHC are essential for a vast majority of physics searches at the CMS
experiment. In this study, we leverage well-established deep learning models
for point clouds and CMS open data to improve the energy calibration of
particle jets. To enable production-ready machine learning based jet energy
calibration an end-to-end pipeline is built on the Kubeflow cloud platform. The
pipeline allowed us to scale up our hyperparameter tuning experiments on cloud
resources, and serve optimal models as REST endpoints. We present the results
of the parameter tuning process and analyze the performance of the served
models in terms of inference time and overhead, providing insights for future
work in this direction. The study also demonstrates improvements in both flavor
dependence and resolution of the energy response when compared to the standard
jet energy corrections baseline.",http://arxiv.org/pdf/2308.12724v1
2308.11952v1,physics.med-ph,iGLU 4.0: A continuous glucose monitoring and balancing paradigm with physiological parameters,2023-08-23 06:49:32+00:00,"The conventional method of glucose measurement such as pricking blood from
the body is prevalent which brings pain and trauma. Invasive methods of
measurement sometimes raise the risk of blood infection to the patient.
Sometimes, some of the physiological parameters such as body temperature and
systolic blood pressure (SBP) are responsible for blood glucose level
fluctuations. Moreover, diabetes for a long duration usually becomes a critical
issue. In such situation, patients need to consult diabetologist frequently,
which is not possible in normal life. Therefore, it is required to develop
non-invasive glucose balancing paradigm, which measures blood glucose without
pricking blood along with physiological parameters measurement and decision
model. The proposed paradigm helps to doctor, who is even available at remote
location. There will not be any need to consult frequently. In the way of
optimized non-invasive system design, an NIRS technique with specific
wavelengths along with physiological parameters is taken to predict the precise
glucose value. The all parameters (glucose, Blood pressure and body
temperature), food intake and insulin levels are parts of decision model, which
would help to the doctor to take decision related to the further medicine doses
and diet plan. The patients would have suggestions according to maintain their
blood glucose level. The proposed system demonstrated an accurate model with
MARD and AvgE 12.50% and 12.10% respectively using DNN model. Coefficient of
determination R2 has been found 0.97.",http://arxiv.org/pdf/2308.11952v1
2308.11763v1,physics.data-an,An efficient set-theoretic algorithm for high-order Forman-Ricci curvature,2023-08-22 20:09:53+00:00,"Differential geometric approaches are ubiquitous in several fields of
mathematics, physics and engineering, and their discretizations enable the
development of network-based mathematical and computational frameworks, which
are essential for large-scale data science. The Forman-Ricci curvature (FRC) -
a statistical measure based on Riemannian geometry and designed for networks -
is known for its high capacity for extracting geometric information from
complex networks. However, extracting information from dense networks is still
challenging due to the combinatorial explosion of high-order network
structures. Motivated by this challenge we sought a set-theoretic
representation theory for high-order network cells and FRC, as well as their
associated concepts and properties, which together provide an alternative and
efficient formulation for computing high-order FRC in complex networks. We
provide a pseudo-code, a software implementation coined FastForman, as well as
a benchmark comparison with alternative implementations. Crucially, our
representation theory reveals previous computational bottlenecks and also
accelerates the computation of FRC. As a consequence, our findings open new
research possibilities in complex systems where higher-order geometric
computations are required.",http://arxiv.org/pdf/2308.11763v1
2308.11700v1,physics.ins-det,SuperCalo: Calorimeter shower super-resolution,2023-08-22 18:00:00+00:00,"Calorimeter shower simulation is a major bottleneck in the Large Hadron
Collider computational pipeline. There have been recent efforts to employ
deep-generative surrogate models to overcome this challenge. However, many of
best performing models have training and generation times that do not scale
well to high-dimensional calorimeter showers. In this work, we introduce
SuperCalo, a flow-based super-resolution model, and demonstrate that
high-dimensional fine-grained calorimeter showers can be quickly upsampled from
coarse-grained showers. This novel approach presents a way to reduce
computational cost, memory requirements and generation time associated with
fast calorimeter simulation models. Additionally, we show that the showers
upsampled by SuperCalo possess a high degree of variation. This allows a large
number of high-dimensional calorimeter showers to be upsampled from much fewer
coarse showers with high-fidelity, which results in additional reduction in
generation time.",http://arxiv.org/pdf/2308.11700v1
2308.11309v1,q-bio.QM,TrajPy: empowering feature engineering for trajectory analysis across domains,2023-08-22 09:37:48+00:00,"Trajectories, sequentially measured quantities that form a path, are an
important presence in many different fields, from hadronic beams in physics to
electrocardiograms in medicine. Trajectory anal-ysis requires the
quantification and classification of curves either using statistical
descriptors or physics-based features. To date, there is no extensive and
user-friendly package for trajectory anal-ysis available, despite its
importance and potential application across domains. We developed a free
open-source python package named TrajPy as a complementary tool to empower
trajectory analysis. The package showcases a friendly graphic user interface
and provides a set of physical descriptors that help characterizing these
intricate structures. In combina-tion with image analysis, it was already
successfully applied to the study of mitochondrial motility in neuroblastoma
cell lines and to the analysis of in silico models for cell migration. The
TrajPy package was developed in Python 3 and released under the GNU GPL-3
license. Easy installation is available through PyPi and the development source
code can be found in the repository https://github.com/ocbe-uio/TrajPy/. The
package release is automatically archived under the DOI 10.5281/zenodo.3656044.",http://arxiv.org/pdf/2308.11309v1
2308.11304v1,physics.data-an,Self-consistent autocorrelation for finite-area bias correction in roughness measurement,2023-08-22 09:32:54+00:00,"Scan line levelling, a ubiquitous and often necessary step in AFM data
processing, can cause a severe bias on measured roughness parameters such as
mean square roughness or correlation length. This work exploits the observation
that the bias of autocorrelation function (ACF) is expressed in terms of the
function itself, permitting a self-consistent formulation of the problem. Using
this formulation, two correction approaches are proposed, both with the aim to
obtain convenient formulae which can be applied to practical correction. The
first modifies standard analytical models of ACF to incorporate, in
expectation, the bias and thus match the bias of the data the models are used
to fit. The second inverts the relation between true and estimated ACF to
realise a model-free correction.",http://arxiv.org/pdf/2308.11304v1
2308.10856v2,cs.LG,Majorana Demonstrator Data Release for AI/ML Applications,2023-08-21 16:50:59+00:00,"The enclosed data release consists of a subset of the calibration data from
the Majorana Demonstrator experiment. Each Majorana event is accompanied by raw
Germanium detector waveforms, pulse shape discrimination cuts, and calibrated
final energies, all shared in an HDF5 file format along with relevant metadata.
This release is specifically designed to support the training and testing of
Artificial Intelligence (AI) and Machine Learning (ML) algorithms upon our
data. This document is structured as follows. Section I provides an overview of
the dataset's content and format; Section II outlines the location of this
dataset and the method for accessing it; Section III presents the NPML Machine
Learning Challenge associated with this dataset; Section IV contains a
disclaimer from the Majorana collaboration regarding the use of this dataset;
Appendix A contains technical details of this data release. Please direct
questions about the material provided within this release to liaobo77@ucsd.edu
(A. Li).",http://arxiv.org/pdf/2308.10856v2
2308.10674v1,physics.optics,Live Iterative Ptychography,2023-08-21 12:17:06+00:00,"We demonstrate live-updating ptychographic reconstruction with ePIE, an
iterative ptychography method, during ongoing data acquisition. The
reconstruction starts with a small subset of the total data, and as the
acquisition proceeds the data used for reconstruction is extended. This creates
a live-updating view of object and illumination that allows monitoring the
ongoing experiment and adjusting parameters with quick turn-around. This is
particularly advantageous for long-running acquisitions. We show that such a
gradual reconstruction yields interpretable results already with a small subset
of the data. We show simulated live processing with various scan patterns,
parallelized reconstruction, and real-world live processing at the hard X-ray
ptychographic nanoanalytical microscope PtyNAMi at the PETRA III beamline.",http://arxiv.org/pdf/2308.10674v1
2308.10588v1,hep-ph,Persistent homology of collider observations: when (w)hole matters,2023-08-21 09:41:31+00:00,"Topological invariants have played a fundamental role in the advancement of
theoretical high energy physics. Physicists have used several kinematic
techniques to distinguish new physics predictions from the Standard Model (SM)
of particle physics at Large Hadron Collider (LHC). However, the study of
global topological invariants of the collider signals has not yet attracted
much attention. In this article, we present, a novel approach to study collider
signals using persistent homology. The global topological properties of the
ensemble of events as expressed by measures like persistent entropy, Betti
area, etc. are worth considering in addition to the traditional approach of
using kinematic variables event by event. In this exploratory study, we first
explore the characteristic topological signature of a few SM electroweak
resonant productions. Next, we use the framework to distinguish global
properties of the invisible Higgs decay processes in the SM and a real singlet
extension of the SM featuring stable singlet scalar dark matter.",http://arxiv.org/pdf/2308.10588v1
2308.09575v1,physics.data-an,KinFit -- A Kinematic Fitting Package for Hadron Physics Experiments,2023-08-18 14:05:59+00:00,"A kinematic fitting package, KinFit, based on the Lagrange multiplier
technique has been implemented for generic hadron physics experiments. It is
particularly suitable for experiments where the interaction point is unknown,
such as experiments with extended target volumes. The KinFit package includes
vertex finding tools and fitting with kinematic constraints, such as mass
hypothesis and four-momentum conservation, as well as combinations of these
constraints. The new package is distributed as an open source software via
GitHub.
  This paper presents a comprehensive description of the KinFit package and its
features, as well as a benchmark study using Monte Carlo simulations of the
$pp\rightarrow pK^+\Lambda \rightarrow pK^+p\pi^-$ reaction. The results show
that KinFit improves the parameter resolution and provides an excellent basis
for event selection.",http://arxiv.org/pdf/2308.09575v1
2308.09452v1,cond-mat.stat-mech,Dimensional measures of generalized entropy,2023-08-18 10:33:18+00:00,"Entropy is useful in statistical problems as a measure of irreversibility,
randomness, mixing, dispersion, and number of microstates. However, there
remains ambiguity over the precise mathematical formulation of entropy,
generalized beyond the additive definition pioneered by Boltzmann, Gibbs, and
Shannon (applicable to thermodynamic equilibria). For generalized entropies to
be applied rigorously to nonequilibrium statistical mechanics, we suggest that
there is a need for a physically interpretable (dimensional) framework that can
be connected to dynamical processes operating in phase space. In this work, we
introduce dimensional measures of entropy that admit arbitrary invertible
weight functions (subject to curvature and convergence requirements). These
""dimensional entropies"" have physical dimensions of phase-space volume and
represent the extent of level sets of the distribution function. Dimensional
entropies with power-law weight functions (related to R\'{e}nyi and Tsallis
entropies) are particularly robust, as they do not require any internal
dimensional parameters due to their scale invariance. We also point out the
existence of composite entropy measures that can be constructed from
functionals of dimensional entropies. We calculate the response of the
dimensional entropies to perturbations, showing that for a structured
distribution, perturbations have the largest impact on entropies weighted at a
similar phase-space scale. This elucidates the link between dynamics
(perturbations) and statistics (entropies). Finally, we derive corresponding
generalized maximum-entropy distributions. Dimensional entropies may be useful
as a diagnostic (for irreversibility) and for theoretical modeling (if the
underlying irreversible processes in phase space are understood) in chaotic and
complex systems, such as collisionless systems of particles with long-range
interactions.",http://arxiv.org/pdf/2308.09452v1
2308.08805v1,physics.geo-ph,Bayesian Variational Time-lapse Full-waveform Inversion,2023-08-17 06:32:05+00:00,"Time-lapse seismic full-waveform inversion (FWI) provides estimates of
dynamic changes in the subsurface by performing multiple seismic surveys at
different times. Since FWI problems are highly non-linear and non-unique, it is
important to quantify uncertainties in such estimates to allow robust decision
making. Markov chain Monte Carlo (McMC) methods have been used for this
purpose, but due to their high computational cost, those studies often require
an accurate baseline model and estimates of the locations of potential velocity
changes, and neglect uncertainty in the baseline velocity model. Such detailed
and accurate prior information is not always available in practice.
  In this study we use an efficient optimization method called stochastic Stein
variational gradient descent (sSVGD) to solve time-lapse FWI problems without
assuming such prior knowledge, and to estimate uncertainty both in the baseline
velocity model and the velocity change. We test two Bayesian strategies:
separate Bayesian inversions for each seismic survey, and a single join
inversion for baseline and repeat surveys, and compare the methods with the
standard linearised double difference inversion. The results demonstrate that
all three methods can produce accurate velocity change estimates in the case of
having fixed (exactly repeatable) acquisition geometries, but that the two
Bayesian methods generate more accurate results when the acquisition geometry
changes between surveys. Furthermore the joint inversion provides the most
accurate velocity change and uncertainty estimates in all cases. We therefore
conclude that Bayesian time-lapse inversion, especially adopting a joint
inversion strategy, may be useful to image and monitor the subsurface changes,
in particular where uncertainty in the results might lead to significantly
different decisions.",http://arxiv.org/pdf/2308.08805v1
2308.08437v2,physics.flu-dyn,Sensitivity Analysis and Parametric Optimization of Micro-Plasma Actuators: A Mini Review,2023-08-16 15:42:22+00:00,"The Dielectric Barrier Discharge (DBD) micro-plasma actuator stands out as a
highly promising tool for active fluid flow control. Researchers specializing
in flow control have taken a keen interest in this actuator due to its
economical manufacturing, low energy consumption, compact size, lightweight
nature, straightforward implementation, and absence of movable components or
pneumatic/hydraulic systems. Given its extensive application, achieving the
best design for plasma actuators necessitates a more profound grasp of how
diverse physical factors (like electrode thickness, electrode length,
dielectric thickness, and dielectric materials) and operational variables (such
as applied voltage, frequency, and waveform) impact its performance. Within
this article, we delve into a comprehensive assessment of both numerical and
experimental investigations focused on optimizing actuator parameters. These
studies can be categorized into two main groups. The initial group involves
fundamental test cases conducted on flat plates, while the subsequent group
pertains to modeling controlled flow in real-world scenarios, including curved
surfaces.",http://arxiv.org/pdf/2308.08437v2
2308.08389v1,math-ph,On the probability distributions of the force and potential energy for a system with an infinite number of random point sources,2023-08-16 14:19:45+00:00,"In this work, we study the probability distribution for the force and
potential energy of a test particle interacting with $N$ point random sources
in the limit $N\rightarrow\infty$. The interaction is given by a central
potential $V(R)=k/R^{\delta-1}$ in a $ d$-dimensional euclidean space, where
$R$ is the random relative distance between the source and the test particle,
$\delta$ is the force exponent, and $k$ is the coupling parameter. In order to
assure a well-defined limit for the probability distribution of the force and
potential energy, we { must} renormalize the coupling parameter and/or the
system size as a function of the number $N$ of sources.
  We show the existence of three non-singular limits, depending on the exponent
$\delta$ and the spatial dimension $d$. (i) For $\delta<d$ the force and
potential energy { converge} to their respective mean values. This limit is
called Mean Field Limit. (ii) For $\delta>d+1$ the potential energy converges
to a random variable and the force to a random vector. This limit is called
Thermodynamic Limit. (iii) For $d<\delta<d+1$ the potential energy converges to
its mean and the force to a random vector. This limit is called Mixed Limit
  Also, we show the existence of two singular limits: (iv) for $\delta=d$ the
potential energy converges to its mean and the force to zero, and (v) for
$\delta=d+1$ the energy converges to a finite value and the force to a random
vector.",http://arxiv.org/pdf/2308.08389v1
2308.07989v1,cond-mat.stat-mech,Interplay between particle trapping and heterogeneity in anomalous diffusion,2023-08-15 18:46:34+00:00,"Heterogeneous media diffusion is often described using position-dependent
diffusion coefficients and estimated indirectly through mean squared
displacement in experiments. This approach may overlook other mechanisms and
their interaction with position-dependent diffusion, potentially leading to
erroneous conclusions. Here, we introduce a hybrid diffusion model that merges
a position-dependent diffusion coefficient with the trapping mechanism of the
comb model. We derive exact solutions for position distributions and mean
squared displacements, validated through simulations of Langevin equations. Our
model shows that the trapping mechanism attenuates the impact of media
heterogeneity. Superdiffusion occurs when the position-dependent coefficient
increases superlinearly, while subdiffusion occurs for sublinear and inverse
power-law relations. This nontrivial interplay between heterogeneity and
state-independent mechanisms also leads to anomalous yet Brownian and
non-Brownian yet Gaussian regimes. These findings emphasize the need for
cautious interpretations of experiments and highlight the limitations of
relying solely on mean squared displacements or position distributions for
diffusion characterization.",http://arxiv.org/pdf/2308.07989v1
2308.07604v1,astro-ph.EP,Searching for Novel Chemistry in Exoplanetary Atmospheres using Machine Learning for Anomaly Detection,2023-08-15 07:19:54+00:00,"The next generation of telescopes will yield a substantial increase in the
availability of high-resolution spectroscopic data for thousands of exoplanets.
The sheer volume of data and number of planets to be analyzed greatly motivate
the development of new, fast and efficient methods for flagging interesting
planets for reobservation and detailed analysis. We advocate the application of
machine learning (ML) techniques for anomaly (novelty) detection to exoplanet
transit spectra, with the goal of identifying planets with unusual chemical
composition and even searching for unknown biosignatures. We successfully
demonstrate the feasibility of two popular anomaly detection methods (Local
Outlier Factor and One Class Support Vector Machine) on a large public database
of synthetic spectra. We consider several test cases, each with different
levels of instrumental noise. In each case, we use ROC curves to quantify and
compare the performance of the two ML techniques.",http://arxiv.org/pdf/2308.07604v1
2308.07521v1,physics.acc-ph,Machine Learning Based Alignment For LCLS-II-HE Optics,2023-08-15 01:23:53+00:00,"The hard X-ray instruments at the Linac Coherent Light Source are in the
design phase for upgrades that will take full advantage of the high repetition
rates that will become available with LCLS-II-HE. The current X-ray Correlation
Spectroscopy instrument will be converted to the Dynamic X-ray Scattering
instrument, and will feature a meV-scale high-resolution monochromator at its
front end with unprecedented coherent flux. With the new capability come many
engineering and design challenges, not least of which is the sensitivity to
long-term drift of the optics. With this in mind, we have estimated the system
tolerance to angular drift and vibration for all the relevant optics ($\sim$10
components) in terms of how the central energy out of the monochromator will be
affected to inform the mechanical design. Additionally, we have started
planning for methods to correct for such drifts using available (both invasive
and non-invasive) X-ray beam diagnostics. In simulations, we have demonstrated
the ability of trained Machine Learning models to correct misalignments to
maintain the desired central energy and optical axis within the necessary
tolerances. Additionally, we exhibit the use of Bayesian Optimization to
minimize the impact of thermal deformations of crystals as well as beam
alignment from scratch. The initial results are very promising and efforts to
further extend this work are ongoing.",http://arxiv.org/pdf/2308.07521v1
2308.07049v1,hep-ex,Projections of Discovery Potentials from Expected Background,2023-08-14 10:18:39+00:00,"Background with certain uncertainties are usually known in searches of novel
phenomena prior to the experiments are conducted at their design stage. We
quantitatively study the projected sensitivities in terms of discovery
potentials. These are essential for the optimizations of the experimental
specifications as well as of the cost-effectiveness in various investment.
Sensitivities in counting analysis are derived with complete Poisson statistics
and its continuous approximation, and are compared with those using maximum
likelihood analysis in which additional measurables are included as signatures.
The roles and effects due to uncertainties in the background estimates are
studied. Two expected features to establish positive effects are verified and
quantified: (i) In counting-only experiments, the required signal strength can
be derived with complete Poisson analysis, and the continuous approximation
would underestimate the results. (ii) Incorporating continuous variables as
additional constraints would reduce the required signal strength relative to
that of counting-only analysis. The formulations are applied to the case on the
experimental searches of neutrinoless double beta decay in which both ambient
and two-neutrino background are considered.",http://arxiv.org/pdf/2308.07049v1
2308.12382v1,math.DS,Constructing low-dimensional ordinary differential equations from chaotic time series of high/infinite-dimensional systems using radial function-based regression,2023-08-12 05:12:45+00:00,"In our previous study (N. Tsutsumi, K. Nakai and Y. Saiki (2022)) we proposed
a method of constructing a system of differential equations of chaotic behavior
only from observable deterministic time series, which we will call radial
function-based regression (RfR) method. The RfR method employs a regression
using Gaussian radial basis functions together with polynomial terms to
facilitate the robust modeling of chaotic behavior. In this paper, we apply the
RfR method to several types of relatively high-dimensional deterministic time
series generated by a partial differential equation, a delay differential
equation, a turbulence model, and intermittent dynamics. The case when the
observation includes noise is also tested. We have effectively constructed a
system of differential equations for each of these examples, which is assessed
from the point of view of time series forecast, reconstruction of invariant
sets, and invariant densities. We find that in some of the models, an
appropriate trajectory is realized on the chaotic saddle and is identified by
the Stagger-and-Step method.",http://arxiv.org/pdf/2308.12382v1
2308.06433v1,physics.data-an,Robust reconstruction of sparse network dynamics,2023-08-12 01:54:52+00:00,"Reconstruction of the network interaction structure from multivariate time
series is an important problem in multiple fields of science. This problem is
ill-posed for large networks leading to the reconstruction of false
interactions. We put forward the Ergodic Basis Pursuit (EBP) method that uses
the network dynamics' statistical properties to ensure the exact reconstruction
of sparse networks when a minimum length of time series is attained. We show
that this minimum time series length scales quadratically with the node degree
being probed and logarithmic with the network size. Our approach is robust
against noise and allows us to treat the noise level as a parameter. We show
the reconstruction power of the EBP in experimental multivariate time series
from optoelectronic networks.",http://arxiv.org/pdf/2308.06433v1
2308.06252v1,physics.optics,Fundamental Limits on Subwavelength Range Resolution,2023-08-11 17:38:10+00:00,"We establish fundamental bounds on subwavelength resolution for the radar
ranging problem, ``super radar''. Information theoretical metrics are applied
to probe the resolution limits for the case of both direct electric field
measurement and photon-counting measurements. To establish fundamental limits,
we begin with the simplest case of range resolution of two point targets from a
metrology perspective. These information-based metrics establish fundamental
bounds on both the minimal discrimination distance of two targets as well as
the precision on the separation of two subwavelength resolved targets. For the
minimal separation distance, both the direct field method and photon counting
method show that the discriminability vanishes quadratically as the target
separation goes to zero, and is proportional to the variance of the second
derivative of the electromagnetic field profile. Nevertheless, robust
subwavelength estimation is possible. Several different band-limited function
classes are introduced to optimize discrimination. We discuss the application
of maximum likelihood estimation to improve the range precision with optimal
performance. The general theory of multi-parameter estimation is analyzed, and
a simple example of estimating both the separation and relative strength of the
two point reflectors is presented.",http://arxiv.org/pdf/2308.06252v1
2308.06149v1,stat.ML,Gaussian Process Regression for Maximum Entropy Distribution,2023-08-11 14:26:29+00:00,"Maximum-Entropy Distributions offer an attractive family of probability
densities suitable for moment closure problems. Yet finding the Lagrange
multipliers which parametrize these distributions, turns out to be a
computational bottleneck for practical closure settings. Motivated by recent
success of Gaussian processes, we investigate the suitability of Gaussian
priors to approximate the Lagrange multipliers as a map of a given set of
moments. Examining various kernel functions, the hyperparameters are optimized
by maximizing the log-likelihood. The performance of the devised data-driven
Maximum-Entropy closure is studied for couple of test cases including
relaxation of non-equilibrium distributions governed by Bhatnagar-Gross-Krook
and Boltzmann kinetic equations.",http://arxiv.org/pdf/2308.06149v1
2308.07773v1,stat.ME,A Mathematical Analysis of Benford's Law and its Generalization,2023-08-11 07:45:59+00:00,"We explain Kossovsky's generalization of Benford's law which is a formula
that approximates the distribution of leftmost digits in finite sequences of
natural data and apply it to six sequences of data including populations of US
cities and towns and times between earthquakes. We model the natural logarithms
of these two data sequences as samples of random variables having normal and
reflected Gumbel densities respectively. We show that compliance with the
general law depends on how nearly constant the periodized density functions are
and that the models are generally more compliant than the natural data. This
surprising result suggests that the generalized law might be used to improve
density estimation which is the basis of statistical pattern recognition,
machine learning and data science.",http://arxiv.org/pdf/2308.07773v1
2308.05664v1,q-bio.NC,Information decomposition reveals hidden high-order contributions to temporal irreversibility,2023-08-10 16:04:23+00:00,"Temporal irreversibility, often referred to as the arrow of time, is a
fundamental concept in statistical mechanics. Markers of irreversibility also
provide a powerful characterisation of information processing in biological
systems. However, current approaches tend to describe temporal irreversibility
in terms of a single scalar quantity, without disentangling the underlying
dynamics that contribute to irreversibility. Here we propose a broadly
applicable information-theoretic framework to characterise the arrow of time in
multivariate time series, which yields qualitatively different types of
irreversible information dynamics. This multidimensional characterisation
reveals previously unreported high-order modes of irreversibility, and
establishes a formal connection between recent heuristic markers of temporal
irreversibility and metrics of information processing. We demonstrate the
prevalence of high-order irreversibility in the hyperactive regime of a
biophysical model of brain dynamics, showing that our framework is both
theoretically principled and empirically useful. This work challenges the view
of the arrow of time as a monolithic entity, enhancing both our theoretical
understanding of irreversibility and our ability to detect it in practical
applications.",http://arxiv.org/pdf/2308.05664v1
2308.05597v1,physics.data-an,Bounds on the rates of statistical divergences and mutual information,2023-08-10 14:17:12+00:00,"Statistical divergences are important tools in data analysis, information
theory, and statistical physics, and there exist well known inequalities on
their bounds. However, in many circumstances involving temporal evolution, one
needs limitations on the rates of such quantities, instead. Here, several
general upper bounds on the rates of some f-divergences are derived, valid for
any type of stochastic dynamics (both Markovian and non-Markovian), in terms of
information-like and/or thermodynamic observables. As special cases, the
analytical bounds on the rate of mutual information are obtained, which may
provide explicit and simple alternatives to the existing numerical algorithms
for its estimation. The major role in all those limitations is played by
temporal Fisher information, and some of them contain entropy production,
suggesting a link with stochastic thermodynamics. Overall, the derived bounds
can be applied to any complex network of interacting elements, where
predictability of network dynamics is of prime concern.",http://arxiv.org/pdf/2308.05597v1
2308.05510v1,astro-ph.IM,Advancing Space-Based Gravitational Wave Astronomy: Rapid Detection and Parameter Estimation Using Normalizing Flows,2023-08-10 11:42:33+00:00,"Gravitational wave (GW) astronomy is witnessing a transformative shift from
terrestrial to space-based detection, with missions like Taiji at the
forefront. While the transition brings unprecedented opportunities to explore
massive black hole binaries (MBHBs), it also imposes complex challenges in data
analysis, particularly in parameter estimation amidst confusion noise.
Addressing this gap, we utilize scalable Normalizing Flow models to achieve
rapid and accurate inference within the Taiji environment. Innovatively, our
approach simplifies the data's complexity, employs a transformation mapping to
overcome the year-period time-dependent response function, and unveils
additional multimodality in the arrival time parameter. Our method estimates
MBHBs several orders of magnitude faster than conventional techniques,
maintaining high accuracy even in complex backgrounds. These findings
significantly enhance the efficiency of GW data analysis, paving the way for
rapid detection and alerting systems and enriching our ability to explore the
universe through space-based GW observation.",http://arxiv.org/pdf/2308.05510v1
2308.04051v1,stat.ML,Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization,2023-08-08 04:57:58+00:00,"Our work presents a novel approach to shape optimization, that has the
twofold objective to improve the efficiency of global optimization algorithms
while promoting the generation of high-quality designs during the optimization
process free of geometrical anomalies. This is accomplished by reducing the
number of the original design variables defining a new reduced subspace where
the geometrical variance is maximized and modeling the underlying generative
process of the data via probabilistic linear latent variable models such as
Factor Analysis and Probabilistic Principal Component Analysis. We show that
the data follows approximately a Gaussian distribution when the shape
modification method is linear and the design variables are sampled uniformly at
random, due to the direct application of the central limit theorem. The model
uncertainty is measured in terms of Mahalanobis distance, and the paper
demonstrates that anomalous designs tend to exhibit a high value of this
metric. This enables the definition of a new optimization model where anomalous
geometries are penalized and consequently avoided during the optimization loop.
The procedure is demonstrated for hull shape optimization of the DTMB 5415
model, extensively used as an international benchmark for shape optimization
problems. The global optimization routine is carried out using Bayesian
Optimization and the DIRECT algorithm. From the numerical results, the new
framework improves the convergence of global optimization algorithms, while
only designs with high-quality geometrical features are generated through the
optimization routine thereby avoiding the wastage of precious computationally
expensive simulations.",http://arxiv.org/pdf/2308.04051v1
2308.03876v2,physics.ins-det,CaloDiffusion with GLaM for High Fidelity Calorimeter Simulation,2023-08-07 19:09:33+00:00,"Simulation is crucial for all aspects of collider data analysis, but the
available computing budget in the High Luminosity LHC era will be severely
constrained. Generative machine learning models may act as surrogates to
replace physics-based full simulation of particle detectors, and diffusion
models have recently emerged as the state of the art for other generative
tasks. We introduce CaloDiffusion, a denoising diffusion model trained on the
public CaloChallenge datasets to generate calorimeter showers. Our algorithm
employs 3D cylindrical convolutions, which take advantage of symmetries of the
underlying data representation. To handle irregular detector geometries, we
augment the diffusion model with a new geometry latent mapping (GLaM) layer to
learn forward and reverse transformations to a regular geometry that is
suitable for cylindrical convolutions. The showers generated by our approach
are nearly indistinguishable from the full simulation, as measured by several
different metrics.",http://arxiv.org/pdf/2308.03876v2
2308.02941v1,cs.LG,Towards the Development of an Uncertainty Quantification Protocol for the Natural Gas Industry,2023-08-05 18:54:59+00:00,"Simulations using machine learning (ML) models and mechanistic models are
often run to inform decision-making processes. Uncertainty estimates of
simulation results are critical to the decision-making process because
simulation results of specific scenarios may have wide, but unspecified,
confidence bounds that may impact subsequent analyses and decisions. The
objective of this work is to develop a protocol to assess uncertainties in
predictions of machine learning and mechanistic simulation models. The protocol
will outline an uncertainty quantification workflow that may be used to
establish credible bounds of predictability on computed quantities of interest
and to assess model sufficiency. The protocol identifies key sources of
uncertainties in machine learning and mechanistic modeling, defines applicable
methods of uncertainty propagation for these sources, and includes
statistically rational estimators for output uncertainties. The work applies
the protocol to test cases relevant to the gas distribution industry and
presents learnings from its application. The paper concludes with a brief
discussion outlining a pathway to the wider adoption of uncertainty
quantification within the industry",http://arxiv.org/pdf/2308.02941v1
2308.02016v1,physics.space-ph,Variability of interplanetary shock and associated energetic particle properties as a function of the time window around the shock,2023-08-03 20:14:57+00:00,"We study the effect of sampling windows on derived shock and associated
energetic storm particle (ESP) properties in 296 fast-forward interplanetary
shocks using ACE measurements at 1 au between 02/1998 - 08/2013. We vary the
time windows from 2-mins to 20-mins for the shock properties and from 2-mins to
540-mins for ESP properties. Variability is quantified by the median absolute
deviation (MAD) statistic. We find that the magnetic, density, and temperature
compression ratios vary from their median values by 17.03%, 20.05%, 25.91%,
respectively; shock speed by 16.26%, speed jump by 45.46%, Alfvenic Mach number
by 31.53%, and shock obliquity by 24.25%. Spectral indices in the 2-min to
540-min windows downstream of the shock vary from the median value of 1.79 by
26.05%, and by 30.53% from the 1.70 median value upstream of the shock.
Similarity of ESP spectral indices upstream and downstream of the shock suggest
that these ESP populations are likely locally accelerated at the shock.
Furthermore, we find that for a moving sampling window around the shock, values
for the density ratio hold for ~10-mins; the magnetic ratio and shock speed
jump hold for ~30-mins, and ~60-mins, respectively. Fixing the upstream window
to 2-mins and moving only in the downstream direction, then the density ratio
holds for ~60-mins downstream, magnetic ratio for ~30-mins, and the shock speed
jump holds for ~110-mins. Beyond these time windows, derived shock properties
no longer representative of shock properties. These results provide constraints
for modeling and forecasting efforts of shock and ESP-associated properties.",http://arxiv.org/pdf/2308.02016v1
2308.01750v1,cs.SI,Entropy-based detection of Twitter echo chambers,2023-08-03 13:12:50+00:00,"The presence of echo chambers, i.e. clusters of users exposed to news or
opinions in line with their previous beliefs, was observed in many online
debates on social platforms. Users form an echo chamber when two different
phenomena appear at the same time: 1. users interact with ones sharing similar
opinions; 2. users with similar opinions refer to the same pieces of news. We
propose a completely unbiased entropy-based procedure to spot echo chambers.
Remarkably, the method is completely agnostic about the nature of the data. In
the Italian Twitter debate about Covid-19 vaccination, we find a limited
presence of users in echo chambers (around 0.35% of all users), due to the
limited number of validated users who are exposed to the same news.
Nevertheless, their impact on the formation of a common discourse is strong,
since echo chambers are responsible for nearly one-third of retweets of their
discursive communities.",http://arxiv.org/pdf/2308.01750v1
2308.01949v1,physics.ins-det,Analysis of Light Attenuation Length Measurement of a High Quality Linear Alkylbenzene for the JUNO Experiment,2023-08-03 09:33:53+00:00,"Jiangmen Underground Neutrino Observatory (JUNO) is the next generation
neutrino experiment which aims at neutrino mass hierarchy problem along with
many other cutting-edge studies concerning neutrinos. Located 700m underground
in Jiangmen China, JUNO's central detector is an acrylic sphere filled with
20kt liquid scintillator with linear alkylbenzene(LAB) as scintillator solvent.
To ensure that an unprecedented energy resolution of $\sigma_E/E \leqslant 3\%$
can be reached, LAB used in JUNO must have excellent transparency at the
wavelength ranging from 350nm to 450nm.
  In the past decade much effort has been devoted to the development of high
transparency LAB based on the measurement of light attenuation length. Through
a close cooperation with Jingling Petrochemical Corporation in Nanjing,
transparency of LAB samples prepared for JUNO has been improved progressively.
However, this improvement is also pushing our apparatus towards approaching its
measuring limit, undermining the credibility of our measurement. In order to
get a result accurate and precise, an apparatus upgrading and a more detailed
error analysis is inevitable.
  In this article, we present an analysis of how apparatus upgrading helps with
decreasing measuring errors, and we conducted measurements using the new
apparatus on several samples. A detailed error analysis is followed to validate
the results. We propose to apply statistical methods featuring Monte Carlo
simulation to estimate systematic uncertainties. Deviations caused by fit
models is also considered and the overall uncertainty is obtained by combining
two independent measurements. We finally report the light attenuation length of
a newly improved LAB sample to be $29.90\pm 0.95$m, which gives a new high of
all the preceding samples we tested. This study may provide a strong evidence
of JUNO's feasibility to reach its energy resolution.",http://arxiv.org/pdf/2308.01949v1
2308.01524v1,physics.data-an,Unsupervised Learning of Part Similarity for Goal-Guided Accelerated Experiment Design in Metal Additive Manufacturing,2023-08-03 04:03:01+00:00,"Metal additive manufacturing is gaining broad interest and increased use in
the industrial and academic fields. However, the quantification and
commercialization of standard parts usually require extensive experiments and
expensive post-characterization, which impedes the rapid development and
adaptation of metal AM technologies. In this work, a similarity-based
acceleration (S-acceleration) method for design of experiments is developed to
reduce the time and costs associated with unveiling process-property (porosity
defects) relationships during manufacturing. With S-acceleration, part semantic
features from machine-setting parameters and physics-effects informed
characteristics are explored for measuring mutual part similarities. A
user-defined simplification rate of experiments is proposed to purposely remove
redundant parts before conducting experiments printing without sacrificing
information gain as original full factorial experiment design. This
S-acceleration design of experiments is demonstrated on a Concept Laser M2
machine for the experimental plan of modeling relationships between process
parameters and part porosity defects. The printed part has 2 mm diameter by 4
mm tall pin geometry considering variations in build location and orientation,
laser settings and powder feedstock are held constant. In total, 242 parts are
measured to create a ground truth data set of porosity levels by using X-ray
tomography microscopy. The S-acceleration method is assessed for performance
considering 40%, 50%, and 60% of user-defined experiment simplification rates.
The repeated experiments are removed without ignoring the minority experiments
outlier, assuring a similar process-property relation in the original
experiment plan. The experiment number is significantly reduced based on part
similarity with minimal compromise of model accuracy and obtained knowledge.",http://arxiv.org/pdf/2308.01524v1
2308.01438v1,cs.LG,Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations,2023-08-02 21:22:17+00:00,"Cost-effective sensors are capable of real-time capturing a variety of air
quality-related modalities from different pollutant concentrations to
indoor/outdoor humidity and temperature. Machine learning (ML) models are
capable of performing air-quality ""ahead-of-time"" approximations. Undoubtedly,
accurate indoor air quality approximation significantly helps provide a healthy
indoor environment, optimize associated energy consumption, and offer human
comfort. However, it is crucial to design an ML architecture to capture the
domain knowledge, so-called problem physics. In this study, we propose six
novel physics-based ML models for accurate indoor pollutant concentration
approximations. The proposed models include an adroit combination of
state-space concepts in physics, Gated Recurrent Units, and Decomposition
techniques. The proposed models were illustrated using data collected from five
offices in a commercial building in California. The proposed models are shown
to be less complex, computationally more efficient, and more accurate than
similar state-of-the-art transformer-based models. The superiority of the
proposed models is due to their relatively light architecture (computational
efficiency) and, more importantly, their ability to capture the underlying
highly nonlinear patterns embedded in the often contaminated sensor-collected
indoor air quality temporal data.",http://arxiv.org/pdf/2308.01438v1
2308.00584v1,cond-mat.stat-mech,Probability density function of the unbalanced impulse in Langevin theory of Brownian motion,2023-08-01 15:01:57+00:00,"This paper attempts to find a probability distribution for the white noise
(rapidly fluctuating unbalanced force) in the Langevin Equation. Unbalanced
force is the resultant impulse provided to the brownian particle by the
colliding fluid molecules. Therefore, a probability distribution of the speed
of the particles after each impact will have the same probability distribution
of the white noise. Such a distribution is discovered in this work by
constructing a simple model based on thermal molecules colliding with the
particle from all directions. The molecules obey Maxwell-Boltzmann speed
distribution law. At low temperatures, for bigger brownian particles, existence
of some non-random distribution for the unbalanced impulse, in itself is an
interesting result. The distribution takes a near half gaussian form at these
limits. At high temperatures, for small brownian particles(e.g: pollen grains),
the distribution is shown to approach uniform distribution, and hence
consistent with bulk of well established theoretical assumptions and
experimental results in the literature that claims the unbalanced force to be a
random white noise.",http://arxiv.org/pdf/2308.00584v1
2308.00718v1,physics.data-an,Beam Detection Based on Machine Learning Algorithms,2023-08-01 02:25:08+00:00,"The positions of free electron laser beams on screens are precisely
determined by a sequence of machine learning models. Transfer training is
conducted in a self-constructed convolutional neural network based on VGG16
model. Output of intermediate layers are passed as features to a support vector
regression model. With this sequence, 85.8% correct prediction is achieved on
test data.",http://arxiv.org/pdf/2308.00718v1
2308.00171v1,physics.chem-ph,Confidence Interval and Uncertainty Propagation Analysis of SAFT-type Equations of State,2023-07-31 22:10:08+00:00,"Thermodynamic models and, in particular, SAFT-type equations are vital in
characterizing complex systems. This paper presents a framework for sampling
parameter distributions in PC-SAFT and SAFT-VR Mie equations of state to
understand parameter confidence intervals and correlations. We identify
conserved quantities contributing to significant correlations. Comparing the
equations of state, we find that additional parameters introduced in the
SAFT-VR Mie equation increase relative uncertainties (1\%-2\% to 3\%-4\%) and
introduce more correlations. When incorporating association through additional
parameters, relative uncertainties increase, but correlations slightly
decrease. We investigate how uncertainties propagate to derived properties and
observe small uncertainties for that data with which the parameters were
regressed, especially for saturated-liquid volumes. However, extrapolating to
saturated-vapour volumes yields larger uncertainties due to the larger
isothermal compressibility. Near the critical point, uncertainties in saturated
volumes diverge due to increased sensitivity of the isothermal compressibility
to parameter uncertainties. This effect significantly impacts bulk properties,
particularly isobaric heat capacity, where uncertainties near the critical
point become extremely large, even when these uncertainties are small. We
emphasize that even small uncertainties near the critical point lead to
divergences in predicted properties.",http://arxiv.org/pdf/2308.00171v1
2307.16804v1,physics.ao-ph,Assimilation of SMAP Observations Over Land Improves the Simulation and Prediction of Tropical Cyclone Idai,2023-07-31 16:15:51+00:00,"Soil moisture conditions can influence the evolution of a tropical cyclone
(TC) that is partially or completely over land. Hence, better constraining soil
moisture initial conditions in a numerical weather prediction model can
potentially improve predictions of TC evolution near or over land. This study
examines the impact of assimilating observations from the NASA Soil Moisture
Active Passive (SMAP) mission into the NASA Goddard Earth Observing System
(GEOS) global weather model on the prediction of South-West Indian Ocean TC
Idai (2019). Two sets of retrospective forecasts of TC Idai are compared in an
Observing System Experiment framework: (i) forecasts initialized from an
analysis that is comparable to the GEOS operational analysis and (ii) forecasts
initialized from an analysis that additionally assimilates SMAP brightness
temperature observations over land. Results indicate that SMAP assimilation
leads to pronounced improvements in the representation of TC Idai structure and
prediction of its intensity and track. The wind speed radius (a measure for TC
compactness) is reduced by up to 18% in the analysis with SMAP assimilation
relative to the control experiment without SMAP assimilation. The forecast
intensity error, measured against the observed intensity, is reduced by up to
23%. The forecast along-track error is reduced by up to 34%, indicating a more
accurate propagation speed, while the impact of SMAP assimilation on the
forecast cross-track error is neutral. These results provide a valuable
demonstration that SMAP assimilation can have a highly beneficial impact on TC
prediction in global weather forecast models.",http://arxiv.org/pdf/2307.16804v1
2307.16344v1,physics.chem-ph,"Automated Preparation of Nanoscopic Structures: Graph-Based Sequence Analysis, Mismatch Detection, and pH-Consistent Protonation with Uncertainty Estimates",2023-07-30 23:43:43+00:00,"Structure and function in nanoscale atomistic assemblies are tightly coupled,
and every atom with its specific position and even every electron will have a
decisive effect on the electronic structure, and hence, on the molecular
properties. Molecular simulations of nanoscopic atomistic structures therefore
require accurately resolved three-dimensional input structures. If extracted
from experiment, these structures often suffer from severe uncertainties, of
which the lack of information on hydrogen atoms is a prominent example. Hence,
experimental structures require careful review and curation, which is a
time-consuming and error-prone process. Here, we present a fast and robust
protocol for the automated structure analysis, and pH-consistent protonation,
in short, ASAP. For biomolecules as a target, the ASAP protocol integrates
sequence analysis and error assessment of a given input structure. ASAP allows
for pKa prediction from reference data through Gaussian process regression
including uncertainty estimation and connects to system-focused atomistic
modeling described in (J. Chem. Theory Comput. 16, 2020, 1646). Although
focused on biomolecules, ASAP can be extended to other nanoscopic objects,
because most of its design elements rely on a general graph-based foundation
guaranteeing transferability. The modular character of the underlying pipeline
supports different degrees of automation, which allows for (i) efficient
feedback loops for human-machine interaction with a low entrance barrier and
for (ii) integration into autonomous procedures such as automated force field
parametrizations. This facilitates fast switching of the pH-state through
on-the-fly system-focused reparametrization during a molecular simulation at
virtually no extra computational cost.",http://arxiv.org/pdf/2307.16344v1
2307.16036v2,physics.plasm-ph,Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data,2023-07-29 17:57:22+00:00,"Researchers in the field of ultra-intense laser science are beginning to
embrace machine learning methods. In this study we consider three different
machine learning methods -- a two-hidden layer neural network, Support Vector
Regression and Gaussian Process Regression -- and compare how well they can
learn from a synthetic data set for proton acceleration in the Target Normal
Sheath Acceleration regime. The synthetic data set was generated from a
previously published theoretical model by Fuchs et al. 2005 that we modified.
Once trained, these machine learning methods can assist with efforts to
maximize the peak proton energy, or with the more general problem of
configuring the laser system to produce a proton energy spectrum with desired
characteristics. In our study we focus on both the accuracy of the machine
learning methods and the performance on one GPU including the memory
consumption. Although it is arguably the least sophisticated machine learning
model we considered, Support Vector Regression performed very well in our
tests.",http://arxiv.org/pdf/2307.16036v2
2307.15998v1,physics.data-an,Sorting ECGs by lag irreversibility,2023-07-29 14:51:08+00:00,"In this work we introduce the lag irreversibility function as a method to
assess time-irreversibility in discrete time series. It quantifies the degree
of time-asymmetry for the joint probability function of the state variable
under study and the state variable lagged in time. We test its performance in a
time-irreversible Markov chain model for which theoretical results are known.
Moreover, we use our approach to analyze electrocardiographic recordings of
four groups of subjects: healthy young individuals, healthy elderly
individuals, and persons with two different disease conditions, namely,
congestive heart failure and atrial fibrillation. We find that by studying
jointly the variability of the amplitudes of the different waves in the
electrocardiographic signals, one can obtain an efficient method to
discriminate between the groups already mentioned. Finally, we test the
accuracy of our method using the ROC analysis.",http://arxiv.org/pdf/2307.15998v1
2307.15479v1,cond-mat.stat-mech,"Thermal transitions in a one-dimensional, finite-size Ising model",2023-07-28 11:07:50+00:00,"We revisit the one-dimensional ferromagnetic Ising spin-chain with a finite
number of spins and periodic boundaries and derive analytically and verify
numerically its various stationary and dynamical properties at different
temperatures. In particular, we determine the probability distributions of
magnetization, the number of domain walls, and the corresponding residence
times for different chain lengths and magnetic fields. While we study finite
systems at thermal equilibrium, we identify several critical temperatures
similar to those for first-order phase transitions. At critical temperature
$T_{cr}^{(1)}$ the ferromagnetic state emerges as a metastable state, while at
$T_{cr}^{(2)}\leq T_{cr}^{(1)}$ it acquires the same probability as the
paramagnetic state. These two critical temperature merge for sufficiently
strong magnetic field that polarizes the spins. We also identify a dynamical
critical temperature $T_{cr}^{(\mathrm{d})}> T_{cr}^{(1)}$ below which the
ferromagnetic state has longer residence time, and thus is dynamically more
stable, than the paramagnetic state. We illustrate the utility of our results
by their application to structural transitions in biopolymers having
non-trivial intermediate equilibrium states.",http://arxiv.org/pdf/2307.15479v1
2307.15406v1,hep-lat,Stochastic automatic differentiation for Monte Carlo processes,2023-07-28 08:59:01+00:00,"Monte Carlo methods represent a cornerstone of computer science. They allow
to sample high dimensional distribution functions in an efficient way. In this
paper we consider the extension of Automatic Differentiation (AD) techniques to
Monte Carlo process, addressing the problem of obtaining derivatives (and in
general, the Taylor series) of expectation values. Borrowing ideas from the
lattice field theory community, we examine two approaches. One is based on
reweighting while the other represents an extension of the Hamiltonian approach
typically used by the Hybrid Monte Carlo (HMC) and similar algorithms. We show
that the Hamiltonian approach can be understood as a change of variables of the
reweighting approach, resulting in much reduced variances of the coefficients
of the Taylor series. This work opens the door to find other variance reduction
techniques for derivatives of expectation values.",http://arxiv.org/pdf/2307.15406v1
2307.15365v1,cs.SI,The Role of the IRA in Twitter during the 2016 US Presidential Election: Unveiling Amplification and Influence of Suspended Accounts,2023-07-28 07:31:23+00:00,"The impact of the social media campaign conducted by the Internet Research
Agency (IRA) during the 2016 U.S. presidential election continues to be a topic
of ongoing debate. While it is widely acknowledged that the objective of this
campaign was to support Donald Trump, the true extent of its influence on
Twitter users remains uncertain. Previous research has primarily focused on
analyzing the interactions between IRA users and the broader Twitter community
to assess the campaign's impact. In this study, we propose an alternative
perspective that suggests the existing approach may underestimate the true
extent of the IRA campaign. Our analysis uncovers the presence of a notable
group of suspended Twitter users, whose size surpasses the IRA user group size
by a factor of 60. These suspended users exhibit close interactions with IRA
accounts, suggesting potential collaboration or coordination. Notably, our
findings reveal the significant role played by these previously unnoticed
accounts in amplifying the impact of the IRA campaign, surpassing even the
reach of the IRA accounts themselves by a factor of 10. In contrast to previous
findings, our study reveals that the combined efforts of the Internet Research
Agency (IRA) and the identified group of suspended Twitter accounts had a
significant influence on individuals categorized as undecided or weak
supporters, probably with the intention of swaying their opinions.",http://arxiv.org/pdf/2307.15365v1
2307.15289v1,astro-ph.HE,"Public Kaggle Competition ""IceCube -- Neutrinos in Deep Ice""",2023-07-28 04:02:19+00:00,"The reconstruction of neutrino events in the IceCube experiment is crucial
for many scientific analyses, including searches for cosmic neutrino sources.
The Kaggle competition ""IceCube -- Neutrinos in Deep ice"" was a public machine
learning challenge designed to encourage the development of innovative
solutions to improve the accuracy and efficiency of neutrino event
reconstruction. Participants worked with a dataset of simulated neutrino events
and were tasked with creating a suitable model to predict the direction vector
of incoming neutrinos. From January to April 2023, hundreds of teams competed
for a total of $50k prize money, which was awarded to the best performing few
out of the many thousand submissions. In this contribution I will present some
insights into the organization of this large outreach project, and summarize
some of the main findings, results and takeaways.",http://arxiv.org/pdf/2307.15289v1
2307.15281v1,physics.geo-ph,Earthquake detection capacity of Dense Oceanfloor Network system for Earthquakes and Tsunamis (DONET),2023-07-28 03:26:33+00:00,"We adopted the Probability-based Magnitude of Completeness (PMC) method and
performed a case analysis of the Nankai Trough, a target region monitored for
future megathrust earthquakes. JAMSTEC (Japan Agency for Marine-Earth Science
and Technology) has created a seismicity catalog that includes events in this
region observed by DONET. Using seismicity data for 2015-2019, we found
spatiotemporal variability of completeness magnitude Mp. Mp was lower than 1 in
one of the areas where stations are densely deployed, whereas Mp was larger
than 2 at the periphery and outside of the DONET area. We then evaluated the
temporal evolution of Mp, highlighting how the failure of sets of observing
stations influenced Mp if not repaired. Stations are aggregated around the 12
science nodes (hubs that connect the stations) and connected through the two
oceanfloor backbone cables to JAMSTEC. We explored the possible use of PMC as a
tool with simulation computation of node malfunction. A simulation showed that
completeness estimates in the area near failure nodes were about 1 magnitude
larger. If such failure occurred for nodes near the region which straddles the
rupture zones of the previous Tonankai and Nankai earthquakes in 1940's, it
would most pronouncedly affect earthquake monitoring among nodes' failures. It
is desirable to repair these nodes or replace with new ones when their
malfunction occurs. We then demonstrated an example of how to use Mp
information as prior knowledge to seismicity-related studies. We used the b
value of the Gutenberg-Richter distribution, and computed it taking Mp into
consideration. We found that the spatial and temporal changes in b were
strongly correlated to the magnitude-6 class slow slip that grew over two years
on the Nankai Trough plate boundary, indicating the b value as a proxy that can
help to image stress heterogeneity when there is a slow slip event.",http://arxiv.org/pdf/2307.15281v1
2307.14515v1,gr-qc,Modeling the complexity of Elliptic Black Hole Solution In 4D Using Hamiltonian Monte Carlo with Stacked Neural Networks,2023-07-26 21:29:42+00:00,"In this paper, we study the black hole solution of self-similar gravitational
collapse in the Einstein-axion-dilaton system for the elliptic class in four
dimensions. The solution is invariant under space-time dilation, which is
combined with internal SL(2,R) transformations. Due to the complex and highly
nonlinear pattern of the equations of motion in the physics of black holes,
researchers typically have to use various numerical techniques to make the
equations tractable to estimate the parameters and the critical solutions. To
this end, they have to ignore the numerical measurement errors in estimating
the parameters. To our knowledge, for the first time in the literature on
axion-dilation systems, we propose to estimate the critical collapse functions
in a Bayesian framework. We develop a novel methodology to translate the
modelling of the complexity of the elliptic black hole to a sampling problem
using Hamiltonian Monte Carlo with stacked neural networks. Unlike methods in
the literature, this probabilistic approach enables us not only to recover the
available deterministic solution but also to explore possibly all physically
distinguishable self-similar solutions that may occur due to numerical
measurement errors.",http://arxiv.org/pdf/2307.14515v1
2307.14032v1,cond-mat.mtrl-sci,Advances of Machine Learning in Materials Science: Ideas and Techniques,2023-07-26 08:35:09+00:00,"In this big data era, the use of large dataset in conjunction with machine
learning (ML) has been increasingly popular in both industry and academia. In
recent times, the field of materials science is also undergoing a big data
revolution, with large database and repositories appearing everywhere.
Traditionally, materials science is a trial-and-error field, in both the
computational and experimental departments. With the advent of machine
learning-based techniques, there has been a paradigm shift: materials can now
be screened quickly using ML models and even generated based on materials with
similar properties; ML has also quietly infiltrated many sub-disciplinary under
materials science. However, ML remains relatively new to the field and is
expanding its wing quickly. There are a plethora of readily-available big data
architectures and abundance of ML models and software; The call to integrate
all these elements in a comprehensive research procedure is becoming an
important direction of material science research. In this review, we attempt to
provide an introduction and reference of ML to materials scientists, covering
as much as possible the commonly used methods and applications, and discussing
the future possibilities.",http://arxiv.org/pdf/2307.14032v1
2307.15083v2,q-bio.PE,Reaction Diffusion TAP,2023-07-25 20:52:07+00:00,"The recently introduced Theory of the Adjacent Possible (TAP) is a model of
combinatorial innovation aiming to explain the ""hockey-stick"" upward trend of
human technological evolution, where an explosion in the number of produced
items with increasing complexity suddenly occurs. In addition, the TAP model
was also used to explain the rapidly emerging biological complexity. Inspired
by TAP here we propose a reaction-diffusion system aiming to extend the model
in both space and time. We show that the new model exhibits similar
characteristics to the TAP model, like the sudden increase in the production of
items, after a longer period of slow growth. The new model also exhibits wave
propagation of ""innovation"", resulting in self-sustained complex interference
patterns.",http://arxiv.org/pdf/2307.15083v2
2307.13544v1,physics.soc-ph,A model for efficient dynamical ranking in networks,2023-07-25 14:47:36+00:00,"We present a physics-inspired method for inferring dynamic rankings in
directed temporal networks - networks in which each directed and timestamped
edge reflects the outcome and timing of a pairwise interaction. The inferred
ranking of each node is real-valued and varies in time as each new edge,
encoding an outcome like a win or loss, raises or lowers the node's estimated
strength or prestige, as is often observed in real scenarios including
sequences of games, tournaments, or interactions in animal hierarchies. Our
method works by solving a linear system of equations and requires only one
parameter to be tuned. As a result, the corresponding algorithm is scalable and
efficient. We test our method by evaluating its ability to predict interactions
(edges' existence) and their outcomes (edges' directions) in a variety of
applications, including both synthetic and real data. Our analysis shows that
in many cases our method's performance is better than existing methods for
predicting dynamic rankings and interaction outcomes.",http://arxiv.org/pdf/2307.13544v1
2307.13508v1,physics.soc-ph,Statistical properties of COVID-19 transmission intervals in Republic of Korea,2023-07-25 14:00:10+00:00,"A transmission interval for an infectious disease is important to understand
epidemic processes in complex networks. The transmission interval is defined as
a time interval between one person's infection and their infection to another
person. To study statistical properties of transmission intervals, we analyze a
COVID-19 dataset of confirmed cases in Republic of Korea that has been
collected for two years since the confirmation of the first case on 19 January
2020. Utilizing demographic information of confirmed individuals, such as sex,
age, residence location, and the nature of relation between infectors and
infectees, we find that transmission intervals are rarely affected by sexes,
but they tend to have larger values for the youngest and oldest age groups than
other groups. We also find some metropolitan cities or provinces with
relatively larger (smaller) transmission intervals than other locations. These
empirical findings might help us to better understand dynamical mechanisms of
epidemic processes in complex social systems.",http://arxiv.org/pdf/2307.13508v1
2307.13060v1,cs.CE,On the characteristics of natural hydraulic dampers: An image-based approach to study the fluid flow behaviour inside the human meniscal tissue,2023-07-24 18:19:39+00:00,"The meniscal tissue is a layered material with varying properties influenced
by collagen content and arrangement. Understanding the relationship between
structure and properties is crucial for disease management, treatment
development, and biomaterial design. The internal layer of the meniscus is
softer and more deformable than the outer layers, thanks to interconnected
collagen channels that guide fluid flow. To investigate these relationships, we
propose a novel approach that combines Computational Fluid Dynamics (CFD) with
Image Analysis (CFD-IA). We analyze fluid flow in the internal architecture of
the human meniscus across a range of inlet velocities (0.1mm/s to 1.6m/s) using
high-resolution 3D micro-computed tomography scans. Statistical correlations
are observed between architectural parameters (tortuosity, connectivity,
porosity, pore size) and fluid flow parameters (Re number distribution,
permeability). Some channels exhibit Re values of 1400 at an inlet velocity of
1.6m/s, and a transition from Darcy's regime to a non-Darcian regime occurs
around an inlet velocity of 0.02m/s. Location-dependent permeability ranges
from 20-32 Darcy. Regression modelling reveals a strong correlation between
fluid velocity and tortuosity at high inlet velocities, as well as with channel
diameter at low inlet velocities. At higher inlet velocities, flow paths
deviate more from the preferential direction, resulting in a decrease in the
concentration parameter by an average of 0.4. This research provides valuable
insights into the fluid flow behaviour within the meniscus and its structural
influences.",http://arxiv.org/pdf/2307.13060v1
2307.12841v1,physics.data-an,Information temperature as a parameter of random sequence complexity,2023-07-24 14:39:39+00:00,"In this study, we continue our exploration of the concept of information
temperature as a characteristic of random sequences. We describe methods for
introducing the information temperature in the context of binary high-order
Markov chain with step-wise memory and investigate the application of the
temperature as a parameter of the sequence complexity. We aim to define
complexity based on the derivative of entropy with respect to information
temperature, drawing an analogy to thermodynamic heat capacity. The maximum
complexity of a random sequence is achieved when its information ""heat
capacity"" approaches its highest possible value, which is directly influenced
by the sequence memory depth. We also discuss the potential of utilizing
information temperature as an indicator of the intellectual level exhibited by
any text-generating agent.",http://arxiv.org/pdf/2307.12841v1
2307.12744v1,q-fin.ST,"Memory Effects, Multiple Time Scales and Local Stability in Langevin Models of the S&P500 Market Correlation",2023-07-24 12:35:45+00:00,"The analysis of market correlations is crucial for optimal portfolio
selection of correlated assets, but their memory effects have often been
neglected. In this work, we analyse the mean market correlation of the S&P500
which corresponds to the main market mode in principle component analysis. We
fit a generalised Langevin equation (GLE) to the data whose memory kernel
implies that there is a significant memory effect in the market correlation
ranging back at least three trading weeks. The memory kernel improves the
forecasting accuracy of the GLE compared to models without memory and hence,
such a memory effect has to be taken into account for optimal portfolio
selection to minimise risk or for predicting future correlations. Moreover, a
Bayesian resilience estimation provides further evidence for non-Markovianity
in the data and suggests the existence of a hidden slow time scale that
operates on much slower times than the observed daily market data. Assuming
that such a slow time scale exists, our work supports previous research on the
existence of locally stable market states.",http://arxiv.org/pdf/2307.12744v1
2307.12636v1,eess.SY,Identifying drivers and mitigators for congestion and redispatch in the German electric power system with explainable AI,2023-07-24 09:19:38+00:00,"The transition to a sustainable energy supply challenges the operation of
electric power systems in manifold ways. Transmission grid loads increase as
wind and solar power are often installed far away from the consumers. In
extreme cases, system operators must intervene via countertrading or redispatch
to ensure grid stability. In this article, we provide a data-driven analysis of
congestion in the German transmission grid. We develop an explainable machine
learning model to predict the volume of redispatch and countertrade on an
hourly basis. The model reveals factors that drive or mitigate grid congestion
and quantifies their impact. We show that, as expected, wind power generation
is the main driver, but hydropower and cross-border electricity trading also
play an essential role. Solar power, on the other hand, has no mitigating
effect. Our results suggest that a change to the market design would alleviate
congestion.",http://arxiv.org/pdf/2307.12636v1
2307.12538v1,cond-mat.stat-mech,Vicsek Model Meets DBSCAN: Cluster Phases in the Vicsek Model,2023-07-24 06:00:54+00:00,"The Vicsek model, which was originally proposed to explain the dynamics of
bird flocking, exhibits a phase transition with respect to the absolute value
of the mean velocity. Although clusters of agents can be easily observed via
numerical simulations of the Vicsek model, qualitative studies are lacking. We
study the clustering structure of the Vicsek model by applying DBSCAN, a
recently-introduced clustering algorithm, and report that the Vicsek model
shows a phase transition with respect to the number of clusters: from O(N) to
O(1), with N being the number of agents, when increasing the magnitude of noise
for a fixed radius that specifies the interaction of the Vicsek model. We also
report that the combination of the order parameter proposed by Vicsek et al.
and the number of clusters defines at least four phases of the Vicsek model.",http://arxiv.org/pdf/2307.12538v1
2307.12505v1,q-bio.NC,Optimizing parameter search for community detection in time evolving networks of complex systems,2023-07-24 03:38:34+00:00,"Network representations have been effectively employed to analyze complex
systems across various areas and applications, leading to the development of
network science as a core tool to study systems with multiple components and
complex interactions. There is a growing interest in understanding the temporal
dynamics of complex networks to decode the underlying dynamic processes through
the temporal changes in network structure. Community detection algorithms,
which are specialized clustering algorithms, have been instrumental in studying
these temporal changes. They work by grouping nodes into communities based on
the structure and intensity of network connections over time aiming to maximize
modularity of the network partition. However, the performance of these
algorithms is highly influenced by the selection of resolution parameters of
the modularity function used, which dictate the scale of the represented
network, both in size of communities and the temporal resolution of dynamic
structure. The selection of these parameters has often been subjective and
heavily reliant on the characteristics of the data used to create the network
structure. Here, we introduce a method to objectively determine the values of
the resolution parameters based on the elements of self-organization. We
propose two key approaches: (1) minimization of the biases in spatial scale
network characterization and (2) maximization of temporal scale-freeness. We
demonstrate the effectiveness of these approaches using benchmark network
structures as well as real-world datasets. To implement our method, we also
provide an automated parameter selection software package that can be applied
to a wide range of complex systems.",http://arxiv.org/pdf/2307.12505v1
2307.12352v1,physics.data-an,"Statistical characterization of residual noise in the low-rank approximation filter framework, general theory and application to hyperpolarized tracer spectroscopy",2023-07-23 15:29:47+00:00,"The use of low-rank approximation filters in the field of NMR is increasing
due to their flexibility and effectiveness. Despite their ability to reduce the
Mean Square Error between the processed signal and the true signal is well
known, the statistical distribution of the residual noise is still undescribed.
In this article, we show that low-rank approximation filters are equivalent to
linear filters, and we calculate the mean and the covariance matrix of the
processed data. We also show how to use this knowledge to build a maximum
likelihood estimator, and we test the estimator's performance with a Montecarlo
simulation of a 13C pyruvate metabolic tracer. While the article focuses on NMR
spectroscopy experiment with hyperpolarized tracer, we also show that the
results can be applied to tensorial data (e.g. using HOSVD) or 1D data (e.g.
Cadzow filter).",http://arxiv.org/pdf/2307.12352v1
2308.00099v1,physics.med-ph,Autonomous Electron Tomography Reconstruction with Machine Learning,2023-07-21 19:20:38+00:00,"Modern electron tomography has progressed to higher resolution at lower doses
by leveraging compressed sensing methods that minimize total variation (TV).
However, these sparsity-emphasized reconstruction algorithms introduce tunable
parameters that greatly influence the reconstruction quality. Here, Pareto
front analysis shows that high-quality tomograms are reproducibly achieved when
TV minimization is heavily weighted. However, in excess, compressed sensing
tomography creates overly smoothed 3D reconstructions. Adding momentum into the
gradient descent during reconstruction reduces the risk of over-smoothing and
better ensures that compressed sensing is well behaved. For simulated data, the
tedious process of tomography parameter selection is efficiently solved using
Bayesian optimization with Gaussian processes. In combination, Bayesian
optimization with momentum-based compressed sensing greatly reduces the
required compute time$-$an 80% reduction was observed for the 3D reconstruction
of SrTiO$_3$ nanocubes. Automated parameter selection is necessary for large
scale tomographic simulations that enable the 3D characterization of a wider
range of inorganic and biological materials.",http://arxiv.org/pdf/2308.00099v1
2307.11608v2,cond-mat.soft,Learning minimal representations of stochastic processes with variational autoencoders,2023-07-21 14:25:06+00:00,"Stochastic processes have found numerous applications in science, as they are
broadly used to model a variety of natural phenomena. Due to their intrinsic
randomness and uncertainty, they are however difficult to characterize. Here,
we introduce an unsupervised machine learning approach to determine the minimal
set of parameters required to effectively describe the dynamics of a stochastic
process. Our method builds upon an extended $\beta$-variational autoencoder
architecture. By means of simulated datasets corresponding to paradigmatic
diffusion models, we showcase its effectiveness in extracting the minimal
relevant parameters that accurately describe these dynamics. Furthermore, the
method enables the generation of new trajectories that faithfully replicate the
expected stochastic behavior. Overall, our approach enables for the autonomous
discovery of unknown parameters describing stochastic processes, hence
enhancing our comprehension of complex phenomena across various fields.",http://arxiv.org/pdf/2307.11608v2
2307.11423v1,cs.IT,Attention to Entropic Communication,2023-07-21 08:33:55+00:00,"The concept of attention, numerical weights that emphasize the importance of
particular data, has proven to be very relevant in artificial intelligence.
Relative entropy (RE, aka Kullback-Leibler divergence) plays a central role in
communication theory. Here we combine these concepts, attention and RE. RE
guides optimal encoding of messages in bandwidth-limited communication as well
as optimal message decoding via the maximum entropy principle (MEP). In the
coding scenario, RE can be derived from four requirements, namely being
analytical, local, proper, and calibrated. Weighted RE, used for attention
steering in communications, turns out to be improper. To see how proper
attention communication can emerge, we analyze a scenario of a message sender
who wants to ensure that the receiver of the message can perform well-informed
actions. If the receiver decodes the message using the MEP, the sender only
needs to know the receiver's utility function to inform optimally, but not the
receiver's initial knowledge state. In case only the curvature of the utility
function maxima are known, it becomes desirable to accurately communicate an
attention function, in this case a by this curvature weighted and re-normalized
probability function. Entropic attention communication is here proposed as the
desired generalization of entropic communication that permits weighting while
being proper, thereby aiding the design of optimal communication protocols in
technical applications and helping to understand human communication. For
example, our analysis shows how to derive the level of cooperation expected
under misaligned interests of otherwise honest communication partners.",http://arxiv.org/pdf/2307.11423v1
2307.11157v1,hep-ph,The Interplay of Machine Learning--based Resonant Anomaly Detection Methods,2023-07-20 18:00:04+00:00,"Machine learning--based anomaly detection (AD) methods are promising tools
for extending the coverage of searches for physics beyond the Standard Model
(BSM). One class of AD methods that has received significant attention is
resonant anomaly detection, where the BSM is assumed to be localized in at
least one known variable. While there have been many methods proposed to
identify such a BSM signal that make use of simulated or detected data in
different ways, there has not yet been a study of the methods' complementarity.
To this end, we address two questions. First, in the absence of any signal, do
different methods pick the same events as signal-like? If not, then we can
significantly reduce the false-positive rate by comparing different methods on
the same dataset. Second, if there is a signal, are different methods fully
correlated? Even if their maximum performance is the same, since we do not know
how much signal is present, it may be beneficial to combine approaches. Using
the Large Hadron Collider (LHC) Olympics dataset, we provide quantitative
answers to these questions. We find that there are significant gains possible
by combining multiple methods, which will strengthen the search program at the
LHC and beyond.",http://arxiv.org/pdf/2307.11157v1
2307.10509v1,stat.ME,An Iterative Wavelet Threshold for Signal Denoising,2023-07-20 00:18:38+00:00,"This paper introduces an adaptive filtering process based on shrinking
wavelet coefficients from the corresponding signal wavelet representation. The
filtering procedure considers a threshold method determined by an iterative
algorithm inspired by the control charts application, which is a tool of the
statistical process control (SPC). The proposed method, called SpcShrink, is
able to discriminate wavelet coefficients that significantly represent the
signal of interest. The SpcShrink is algorithmically presented and numerically
evaluated according to Monte Carlo simulations. Two empirical applications to
real biomedical data filtering are also included and discussed. The SpcShrink
shows superior performance when compared with competing algorithms.",http://arxiv.org/pdf/2307.10509v1
2307.10494v1,physics.data-an,A statistical learning framework for mapping indirect measurements of ergodic systems to emergent properties,2023-07-19 23:14:09+00:00,"The discovery of novel experimental techniques often lags behind contemporary
theoretical understanding. In particular, it can be difficult to establish
appropriate measurement protocols without analytic descriptions of the
underlying system-of-interest. Here we propose a statistical learning framework
that avoids the need for such descriptions for ergodic systems. We validate
this framework by using Monte Carlo simulation and deep neural networks to
learn a mapping between low-field nuclear magnetic resonance spectra and proton
exchange rates in ethanol-water mixtures. We found that trained networks
exhibited normalized-root-mean-square errors of less than 1% for exchange rates
under 150 s-1 but performed poorly for rates above this range. This
differential performance occurred because low-field measurements are
indistinguishable from one another at fast exchange. Nonetheless, where a
discoverable relationship between indirect measurements and emergent dynamics
exists, we demonstrate the possibility of approximating it without the need for
precise analytic descriptions, allowing experimental science to flourish in the
midst of ongoing theoretical work",http://arxiv.org/pdf/2307.10494v1
2307.10477v1,hep-ex,3D track reconstruction of low-energy electrons in the MIGDAL low pressure optical time projection chamber,2023-07-19 22:15:03+00:00,"We demonstrate three-dimensional track reconstruction of electrons in a low
pressure (50 Torr) optical TPC consisting of two glass GEMs with an ITO strip
readout in CF4 and CF4/Ar mixtures. The reconstructed tracks show a variety of
event topologies, including short tracks from photoelectrons induced by 55Fe
5.9 keV X-rays and long tracks from gamma ray interactions and beta decays.
Algorithms for event identification and track ridge detection are discussed as
well as multiple methods for integrating information from the camera image and
ITO waveforms with the goal of full 3D reconstruction of the track.",http://arxiv.org/pdf/2307.10477v1
2307.10040v1,cond-mat.quant-gas,Data-driven discovery of relevant information in quantum simulators,2023-07-19 15:20:11+00:00,"Quantum simulators offer powerful means to investigate strongly correlated
quantum matter. However, interpreting measurement outcomes in such systems
poses significant challenges. Here, we present a theoretical framework for
information extraction in synthetic quantum matter, illustrated for the case of
a quantum quench in a spinor Bose-Einstein condensate experiment. Employing
non-parametric unsupervised learning tools that provide different measures of
information content, we demonstrate a system-agnostic approach to identify
dominant degrees of freedom. This enables us to rank operators according to
their relevance, akin to effective field theory. To characterize the
corresponding effective description, we then explore the intrinsic dimension of
data sets as a measure of the complexity of the dynamics. This reveals a
simplification of the data structure, which correlates with the emergence of
time-dependent universal behavior in the studied system. Our assumption-free
approach can be immediately applied in a variety of experimental platforms.",http://arxiv.org/pdf/2307.10040v1
2307.09504v1,astro-ph.CO,Field-Level Inference with Microcanonical Langevin Monte Carlo,2023-07-18 18:00:01+00:00,"Field-level inference provides a means to optimally extract information from
upcoming cosmological surveys, but requires efficient sampling of a
high-dimensional parameter space. This work applies Microcanonical Langevin
Monte Carlo (MCLMC) to sample the initial conditions of the Universe, as well
as the cosmological parameters $\sigma_8$ and $\Omega_m$, from simulations of
cosmic structure. MCLMC is shown to be over an order of magnitude more
efficient than traditional Hamiltonian Monte Carlo (HMC) for a $\sim 2.6 \times
10^5$ dimensional problem. Moreover, the efficiency of MCLMC compared to HMC
greatly increases as the dimensionality increases, suggesting gains of many
orders of magnitude for the dimensionalities required by upcoming cosmological
surveys.",http://arxiv.org/pdf/2307.09504v1
2307.09483v1,cs.LG,Forecasting the steam mass flow in a powerplant using the parallel hybrid network,2023-07-18 17:59:25+00:00,"Efficient and sustainable power generation is a crucial concern in the energy
sector. In particular, thermal power plants grapple with accurately predicting
steam mass flow, which is crucial for operational efficiency and cost
reduction. In this study, we use a parallel hybrid neural network architecture
that combines a parametrized quantum circuit and a conventional feed-forward
neural network specifically designed for time-series prediction in industrial
settings to enhance predictions of steam mass flow 15 minutes into the future.
Our results show that the parallel hybrid model outperforms standalone
classical and quantum models, achieving more than 5.7 and 4.9 times lower mean
squared error (MSE) loss on the test set after training compared to pure
classical and pure quantum networks, respectively. Furthermore, the hybrid
model demonstrates smaller relative errors between the ground truth and the
model predictions on the test set, up to 2 times better than the pure classical
model. These findings contribute to the broader scientific understanding of how
integrating quantum and classical machine learning techniques can be applied to
real-world challenges faced by the energy sector, ultimately leading to
optimized power plant operations.",http://arxiv.org/pdf/2307.09483v1
2307.09294v1,physics.comp-ph,Dynamical and statistical properties of estimated high-dimensional ODE models: The case of the Lorenz'05 type II model,2023-07-18 14:36:32+00:00,"The performance of estimated models is often evaluated in terms of their
predictive capability. In this study, we investigate another important aspect
of estimated model evaluation: the disparity between the statistical and
dynamical properties of estimated models and their source system. Specifically,
we focus on estimated models obtained via the regression method, sparse
identification of nonlinear dynamics (SINDy), one of the promising algorithms
for determining equations of motion from time series of dynamical systems. We
chose our data source dynamical system to be a higher-dimensional instance of
the Lorenz 2005 type II model, an important meteorological toy model. We
examine how the dynamical and statistical properties of the estimated models
are affected by the standard deviation of white Gaussian noise added to the
numerical data on which the estimated models were fitted. Our results show that
the dynamical properties of the estimated models match those of the source
system reasonably well within a range of data-added noise levels, where the
estimated models do not generate divergent (unbounded) trajectories.
Additionally, we find that the dynamics of the estimated models become
increasingly less chaotic as the data-added noise level increases. We also
perform a variance analysis of the (SINDy) estimated model's free parameters,
revealing strong correlations between parameters belonging to the same
component of the estimated model's ordinary differential equation.",http://arxiv.org/pdf/2307.09294v1
2307.09060v1,physics.ao-ph,Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning,2023-07-18 08:25:14+00:00,"We present a data-driven emulator, stochastic weather generator (SWG),
suitable for estimating probabilities of prolonged heatwaves in France and
Scandinavia. This emulator is based on the method of analogs of circulation to
which we add temperature and soil moisture as predictor fields. We train the
emulator on an intermediate complexity climate model run and show that it is
capable of predicting conditional probabilities (forecasting) of heatwaves out
of sample. Special attention is payed that this prediction is evaluated using
proper score appropriate for rare events. To accelerate the computation of
analogs dimensionality reduction techniques are applied and the performance is
evaluated. The probabilistic prediction achieved with SWG is compared with the
one achieved with
  Convolutional Neural Network (CNN). With the availability of hundreds of
years of training data CNNs perform better at the task of probabilistic
prediction. In addition, we show that the SWG emulator trained on 80 years of
data is capable of estimating extreme return times of order of thousands of
years for heatwaves longer than several days more precisely than the fit based
on generalised extreme value distribution. Finally, the quality of its
synthetic extreme teleconnection patterns obtained with stochastic weather
generator is studied. We showcase two examples of such synthetic teleconnection
patterns for heatwaves in France and Scandinavia that compare favorably to the
very long climate model control run.",http://arxiv.org/pdf/2307.09060v1
2307.08936v1,physics.data-an,Renormalization Group-Motivated Learning,2023-07-18 02:55:02+00:00,"We introduce an RG-inspired coarse-graining for extracting the collective
features of data. The key to successful coarse-graining lies in finding
appropriate pairs of data sets. We coarse-grain the two closest data in a
regular real-space RG in a lattice while considers the overall information loss
in momentum-space RG. Here we compromise the two measures for the non-spatial
data set. For weakly correlated data close to Gaussian, we use the correlation
of data as a metric for the proximity of data points, but minimize an overall
projection error for optimal coarse-graining steps. It compresses the data to
maximize the correlation between the two data points to be compressed while
minimizing the correlation between the paired data and other data points. We
show that this approach can effectively reduce the dimensionality of the data
while preserving the essential features. We extend our method to incorporate
non-linear features by replacing correlation measures with mutual information.
This results in an information-bottleneck-like trade-off: maximally compress
the data while preserving the information among the compressed data and the
rest. Indeed, our approach can be interpreted as an exact form of
information-bottleneck-like trade off near linear data. We examine our method
with random Gaussian data and the Ising model to demonstrate its validity and
apply glass systems. Our approach has potential applications in various fields,
including machine learning and statistical physics.",http://arxiv.org/pdf/2307.08936v1
2307.07783v1,nlin.AO,Analytical solution for the long- and short-range every-pair-interactions model,2023-07-15 11:57:58+00:00,"Many physical, biological, and social systems exhibit emergent properties
that arise from the interactions between their components (cells). In this
study, we systematically treat every-pair interactions (a) that exhibit
power-law dependence on the Euclidean distance and (b) act in structures that
can be characterized using fractal geometry. We analytically derive the mean
interaction field of the cells and find that (i) in a long-range interaction
regime, the mean interaction field increases following a power law with the
size of the system, (ii) in a short-range interaction regime, the field
saturates, and (iii) in the intermediate range
  it follows a logarithmic behaviour. To validate our analytical solution, we
perform numerical simulations. In the case of short-range interactions, we
observe that discreteness significantly impacts the continuum approximation
used in the derivation, leading to incorrect asymptotic behaviour in this
regime. To address this issue, we propose an expansion that substantially
improves the accuracy of the analytical expression. Furthermore, our results
motivate us to explore a framework for estimating the fractal dimension of
unknown structures. This approach offers an alternative to established methods
such as box-counting or sandbox methods. Overall, we believe that our
analytical work will have broad applicability in systems where every-pair
interactions play a crucial role. The insights gained from this study can
contribute to a better understanding of various complex systems and facilitate
more accurate modelling and analysis in a wide range of disciplines.",http://arxiv.org/pdf/2307.07783v1
2307.07429v1,quant-ph,Variational dynamics of open quantum systems in phase space,2023-07-14 15:48:31+00:00,"We present a method to simulate the dynamics of large driven-dissipative
many-body open quantum systems using a variational encoding of the Wigner or
Husimi-Q quasi-probability distributions. The method relies on Monte-Carlo
sampling to maintain a polynomial computational complexity while allowing for
several quantities to be estimated efficiently. As a first application, we
present a proof of principle investigation into the physics of the
driven-dissipative Bose-Hubbard model with weak nonlinearity, providing
evidence for the high efficiency of the phase space variational approach.",http://arxiv.org/pdf/2307.07429v1
2307.07188v1,astro-ph.HE,Developing New Analysis Tools for Near Surface Radio-based Neutrino Detectors,2023-07-14 06:43:37+00:00,"The ARIANNA experiment is an Askaryan radio detector designed to measure
high-energy neutrino induced cascades within the Antarctic ice.
Ultra-high-energy neutrinos above $10^{16}$ eV have an extremely low flux, so
experimental data captured at trigger level need to be classified correctly to
retain more neutrino signal. We first describe two new physics-based neutrino
selection methods, or ""cuts"", (the updown and dipole cut) that extend a
previously published analysis to a specialized ARIANNA station with 8 antenna
channels, which is double the number used in the prior analysis. The new cuts
produce a neutrino efficiency of > 95% per station-year, while rejecting 99.93%
of the background (corresponding to 53 remaining events). When the new cuts are
combined with a previously developed cut using neutrino waveform templates, all
background is removed at no change of efficiency. In addition, the neutrino
efficiency is extrapolated to 1,000 station-years of operation, obtaining 91%.
This work then introduces a new selection method (the deep learning cut) to
augment the identification of neutrino events by using deep learning methods
and compares the efficiency to the physics-based analysis. The deep learning
cut gives 99% signal efficiency per station-year of operation while rejecting
99.997% of the background (corresponding to 2 remaining experimental background
events), which are subsequently removed by the waveform template cut at no
significant change in efficiency. The results of the deep learning cut were
verified using measured cosmic rays which shows that the simulations do not
introduce artifacts with respect to experimental data. The paper demonstrates
that the background rejection and signal efficiency of near surface antennas
meets the requirements of a large scale future array, as considered in baseline
design of the radio component of IceCube-Gen2.",http://arxiv.org/pdf/2307.07188v1
2307.06996v2,hep-ph,Spey: smooth inference for reinterpretation studies,2023-07-13 18:00:06+00:00,"Statistical models are at the heart of any empirical study for hypothesis
testing. We present a new cross-platform Python-based package which employs
different likelihood prescriptions through a plug-in system. This framework
empowers users to propose, examine, and publish new likelihood prescriptions
without developing software infrastructure, ultimately unifying and
generalising different ways of constructing likelihoods and employing them for
hypothesis testing, all in one place. Within this package, we propose a new
simplified likelihood prescription that surpasses its predecessors'
approximation accuracy by incorporating asymmetric uncertainties. Furthermore,
our package facilitates the inclusion of various likelihood combination
routines, thereby broadening the scope of independent studies through a
meta-analysis. By remaining agnostic to the source of the likelihood
prescription and the signal hypothesis generator, our platform allows for the
seamless implementation of packages with different likelihood prescriptions,
fostering compatibility and interoperability.",http://arxiv.org/pdf/2307.06996v2
2307.06509v1,astro-ph.EP,Information Gain as a Tool for Assessing Biosignature Missions,2023-07-13 01:16:55+00:00,"We propose the mathematical notion of information gain as a way of
quantitatively assessing the value of biosignature missions. This makes it
simple to determine how mission value depends on design parameters, prior
knowledge, and input assumptions. We demonstrate the utility of this framework
by applying it to a plethora of case examples: the minimal number of samples
needed to determine a trend in the occurrence rate of a signal as a function of
an environmental variable, and how much cost should be allocated to each class
of object; the relative impact of false positives and false negatives, with
applications to Enceladus data and how best to combine two signals; the optimum
tradeoff between resolution and coverage in the search for lurkers or other
spatially restricted signals, with application to our current state of
knowledge for solar system bodies; the best way to deduce a habitability
boundary; the optimal amount of money to spend on different mission aspects;
when to include an additional instrument on a mission; the optimal mission
lifetime; and when to follow/challenge the predictions of a habitability model.
In each case, we generate concrete, quantitative recommendations for optimising
mission design, mission selection, and/or target selection.",http://arxiv.org/pdf/2307.06509v1
2307.06074v1,physics.flu-dyn,Turbulent flows are not uniformly multifractal,2023-07-12 10:51:44+00:00,"The Frisch-Parisi multifractal formalism remains the most compelling
rationalisation for anomalous scaling in fully developed turbulence. We now
show that this formalism can be adapted locally to reveal the spatial
distribution of generalized dimensions and of how multifractal the energy
dissipation field is. In particular, we show that most regions of the flow are
close to being mono-fractal and these are interspersed with islands of
multifractality corresponding to the most singular structures in the flow. By
defining a suitable measure $\Phi ({\bf x})$ of the spatial variation of
multifractality, we show that this grows logarithmically with the extent to
which the energy dissipation varies locally around ${\bf x}$. These results
suggest ways to understand how singularities could arise in disparate regions
of a flow and provides new directions in understanding anomalous dissipation
and intermittency. We then employ the same technique to a non-intermittent,
model turbulent flow to check the robustness of our conclusions.",http://arxiv.org/pdf/2307.06074v1
2307.05739v1,physics.soc-ph,Unveiling the connectivity of complex networks using ordinal transition methods,2023-07-11 19:07:17+00:00,"Ordinal measures provide a valuable collection of tools for analyzing
correlated data series. However, using these methods to understand the
information interchange in networks of dynamical systems, and uncover the
interplay between dynamics and structure during the synchronization process,
remains relatively unexplored. Here, we compare the ordinal permutation
entropy, a standard complexity measure in the literature, and the permutation
entropy of the ordinal transition probability matrix that describes the
transitions between the ordinal patterns derived from a time series. We find
that the permutation entropy based on the ordinal transition matrix outperforms
the rest of the tested measures in discriminating the topological role of
networked chaotic R\""ossler systems. Since the method is based on permutation
entropy measures, it can be applied to arbitrary real-world time series
exhibiting correlations originating from an existing underlying unknown network
structure. In particular, we show the effectiveness of our method using
experimental datasets of networks of nonlinear oscillators.",http://arxiv.org/pdf/2307.05739v1
2307.05735v1,cs.LG,GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models,2023-07-11 19:03:17+00:00,"Scientific Machine Learning (SciML) is a burgeoning field that
synergistically combines domain-aware and interpretable models with agnostic
machine learning techniques. In this work, we introduce GOKU-UI, an evolution
of the SciML generative model GOKU-nets. The GOKU-UI broadens the original
model's spectrum to incorporate other classes of differential equations, such
as Stochastic Differential Equations (SDEs), and integrates a distributed, i.e.
ubiquitous, inference through attention mechanisms and a novel multiple
shooting training strategy in the latent space. These enhancements have led to
a significant increase in its performance in both reconstruction and forecast
tasks, as demonstrated by our evaluation of simulated and empirical data.
Specifically, GOKU-UI outperformed all baseline models on synthetic datasets
even with a training set 32-fold smaller, underscoring its remarkable data
efficiency. Furthermore, when applied to empirical human brain data, while
incorporating stochastic Stuart-Landau oscillators into its dynamical core, it
not only surpassed state-of-the-art baseline methods in the reconstruction
task, but also demonstrated better prediction of future brain activity up to 12
seconds ahead. By training GOKU-UI on resting-state fMRI data, we encoded
whole-brain dynamics into a latent representation, learning an effective
low-dimensional dynamical system model that could offer insights into brain
functionality and open avenues for practical applications such as mental state
or psychiatric condition classification. Ultimately, our research provides
further impetus for the field of Scientific Machine Learning, showcasing the
potential for advancements when established scientific insights are interwoven
with modern machine learning.",http://arxiv.org/pdf/2307.05735v1
2307.05334v1,physics.data-an,Exploring Model Misspecification in Statistical Finite Elements via Shallow Water Equations,2023-07-11 15:25:45+00:00,"The abundance of observed data in recent years has increased the number of
statistical augmentations to complex models across science and engineering. By
augmentation we mean coherent statistical methods that incorporate measurements
upon arrival and adjust the model accordingly. However, in this research area
methodological developments tend to be central, with important assessments of
model fidelity often taking second place. Recently, the statistical finite
element method (statFEM) has been posited as a potential solution to the
problem of model misspecification when the data are believed to be generated
from an underlying partial differential equation system. Bayes nonlinear
filtering permits data driven finite element discretised solutions that are
updated to give a posterior distribution which quantifies the uncertainty over
model solutions. The statFEM has shown great promise in systems subject to mild
misspecification but its ability to handle scenarios of severe model
misspecification has not yet been presented. In this paper we fill this gap,
studying statFEM in the context of shallow water equations chosen for their
oceanographic relevance. By deliberately misspecifying the governing equations,
via linearisation, viscosity, and bathymetry, we systematically analyse
misspecification through studying how the resultant approximate posterior
distribution is affected, under additional regimes of decreasing spatiotemporal
observational frequency. Results show that statFEM performs well with
reasonable accuracy, as measured by theoretically sound proper scoring rules.",http://arxiv.org/pdf/2307.05334v1
2307.05645v1,cond-mat.stat-mech,Description length of canonical and microcanonical models,2023-07-11 11:40:40+00:00,"Non-equivalence between the canonical and the microcanonical ensemble has
been shown to arise for models defined by an extensive number of constraints
(e.g. the Configuration Model). Here, we focus on the framework induced by
entropy maximization and study the extent to which ensemble non-equivalence
affects the description length of binary, canonical, and microcanonical models.
Specifically, we consider the description length induced by the Normalized
Maximum Likelihood (NML), which consists of two terms, i.e. a model
log-likelihood and its complexity: while the effects of ensemble
non-equivalence on the log-likelihood term are well understood, its effects on
the complexity term have not been systematically studied yet. Here, we find
that i) microcanonical models are always more complex than their canonical
counterparts and ii) the difference between the canonical and the
microcanonical description length is strongly influenced by the degree of
non-equivalence, a result suggesting that non-equivalence should be taken into
account when selecting models. Finally, we compare the NML-based approach to
model selection with the Bayesian one induced by Jeffreys prior, showing that
the two cannot be reconciled when non-equivalence holds.",http://arxiv.org/pdf/2307.05645v1
2307.04890v1,cs.SI,Temporal network compression via network hashing,2023-07-10 20:25:41+00:00,"Pairwise temporal interactions between entities can be represented as
temporal networks, which code the propagation of processes such as epidemic
spreading or information cascades, evolving on top of them. The largest outcome
of these processes is directly linked to the structure of the underlying
network. Indeed, a node of a network at given time cannot affect more nodes in
the future than it can reach via time-respecting paths. This set of nodes
reachable from a source defines an out-component, which identification is
costly. In this paper, we propose an efficient matrix algorithm to tackle this
issue and show that it outperforms other state-of-the-art methods. Secondly, we
propose a hashing framework to coarsen large temporal networks into smaller
proxies on which out-components are easier to estimate, and then recombined to
obtain the initial components. Our graph hashing solution has implications in
privacy respecting representation of temporal networks.",http://arxiv.org/pdf/2307.04890v1
2307.04755v1,cs.LG,Information decomposition to identify relevant variation in complex systems with machine learning,2023-07-10 17:57:32+00:00,"One of the fundamental steps toward understanding a complex system is
identifying variation at the scale of the system's components that is most
relevant to behavior on a macroscopic scale. Mutual information is a natural
means of linking variation across scales of a system due to its independence of
the particular functional relationship between variables. However, estimating
mutual information given high-dimensional, continuous-valued data is
notoriously difficult, and the desideratum -- to reveal important variation in
a comprehensible manner -- is only readily achieved through exhaustive search.
Here we propose a practical, efficient, and broadly applicable methodology to
decompose the information contained in a set of measurements by lossily
compressing each measurement with machine learning. Guided by the distributed
information bottleneck as a learning objective, the information decomposition
sorts variation in the measurements of the system state by relevance to
specified macroscale behavior, revealing the most important subsets of
measurements for different amounts of predictive information. Additional
granularity is achieved by inspection of the learned compression schemes: the
variation transmitted during compression is composed of distinctions among
measurement values that are most relevant to the macroscale behavior. We focus
our analysis on two paradigmatic complex systems: a Boolean circuit and an
amorphous material undergoing plastic deformation. In both examples, specific
bits of entropy are identified out of the high entropy of the system state as
most related to macroscale behavior for insight about the connection between
micro- and macro- in the complex system. The identification of meaningful
variation in data, with the full generality brought by information theory, is
made practical for the study of complex systems.",http://arxiv.org/pdf/2307.04755v1
2307.06816v1,cs.LG,Data-driven Nonlinear Parametric Model Order Reduction Framework using Deep Hierarchical Variational Autoencoder,2023-07-10 02:44:53+00:00,"A data-driven parametric model order reduction (MOR) method using a deep
artificial neural network is proposed. The present network, which is the
least-squares hierarchical variational autoencoder (LSH-VAE), is capable of
performing nonlinear MOR for the parametric interpolation of a nonlinear
dynamic system with a significant number of degrees of freedom. LSH-VAE
exploits two major changes to the existing networks: a hierarchical deep
structure and a hybrid weighted, probabilistic loss function. The enhancements
result in a significantly improved accuracy and stability compared against the
conventional nonlinear MOR methods, autoencoder, and variational autoencoder.
Upon LSH-VAE, a parametric MOR framework is presented based on the spherically
linear interpolation of the latent manifold. The present framework is validated
and evaluated on three nonlinear and multiphysics dynamic systems. First, the
present framework is evaluated on the fluid-structure interaction benchmark
problem to assess its efficiency and accuracy. Then, a highly nonlinear
aeroelastic phenomenon, limit cycle oscillation, is analyzed. Finally, the
present framework is applied to a three-dimensional fluid flow to demonstrate
its capability of efficiently analyzing a significantly large number of degrees
of freedom. The performance of LSH-VAE is emphasized by comparing its results
against that of the widely used nonlinear MOR methods, convolutional
autoencoder, and $\beta$-VAE. The present framework exhibits a significantly
enhanced accuracy to the conventional methods while still exhibiting a large
speed-up factor.",http://arxiv.org/pdf/2307.06816v1
2307.04201v1,stat.ME,Bayesian estimation of the Kullback-Leibler divergence for categorical sytems using mixtures of Dirichlet priors,2023-07-09 15:11:49+00:00,"In many applications in biology, engineering and economics, identifying
similarities and differences between distributions of data from complex
processes requires comparing finite categorical samples of discrete counts.
Statistical divergences quantify the difference between two distributions.
However, their estimation is very difficult and empirical methods often fail,
especially when the samples are small. We develop a Bayesian estimator of the
Kullback-Leibler divergence between two probability distributions that makes
use of a mixture of Dirichlet priors on the distributions being compared. We
study the properties of the estimator on two examples: probabilities drawn from
Dirichlet distributions, and random strings of letters drawn from Markov
chains. We extend the approach to the squared Hellinger divergence. Both
estimators outperform other estimation techniques, with better results for data
with a large number of categories and for higher values of divergences.",http://arxiv.org/pdf/2307.04201v1
2307.04776v1,physics.data-an,A new Machine Learning-based method for identification of time-correlated events at tagged photon facilities,2023-07-09 14:29:16+00:00,"We present a new Machine Learning-based multivariate analysis method for the
selection of time-correlated hits in the tagging system and devices used to
detect particles in the final state at the bremsstrahlung-based tagged photon
facilities. This method can be applied instead of the widely used sampling and
subtraction of the time-uncorrelated background, in particular at experiments
aiming for high precision, where the subtraction of the time-uncorrelated
background leads to increased uncertainties. Moreover, the identification of
events with Machine Learning algorithms allows to preserve the information
about correlations of kinematic variables in the final state, which can be
advantageous for further phenomenological analyses of the experimental results.",http://arxiv.org/pdf/2307.04776v1
2307.04007v1,physics.data-an,Uncertainty components in profile likelihood fits,2023-07-08 16:29:49+00:00,"When a measurement of a physical quantity is reported, the total uncertainty
is usually decomposed into statistical and systematic uncertainties. This
decomposition is not only useful to understand the contributions to the total
uncertainty, but also to propagate these contributions in a subsequent
analysis, such as combinations or interpretation fits including results from
other measurements or experiments. In profile-likelihood fits, contributions of
systematic uncertainties are most often quantified using impacts, which are not
adequate for such applications. We discuss the difference between these impacts
and uncertainty components, and propose a simple method to determine the
latter.",http://arxiv.org/pdf/2307.04007v1
2307.03995v1,physics.data-an,Linear approximation to the statistical significance autocovariance matrix in the asymptotic regime,2023-07-08 15:36:52+00:00,"Approximating significance scans of searches for new particles in high-energy
physics experiments as Gaussian fields is a well-established way to estimate
the trials factors required to quantify global significances. We propose a
novel, highly efficient method to estimate the covariance matrix of such a
Gaussian field. The method is based on the linear approximation of statistical
fluctuations of the signal amplitude. For one-dimensional searches the upper
bound on the trials factor can then be calculated directly from the covariance
matrix. For higher dimensions, the Gaussian process described by this
covariance matrix may be sampled to calculate the trials factor directly. This
method also serves as the theoretical basis for a recent study of the trials
factor with an empirically constructed set of Asmiov-like background datasets.
We illustrate the method with studies of a $H \rightarrow \gamma \gamma$
inspired model that was used in the empirical paper.",http://arxiv.org/pdf/2307.03995v1
2307.03840v1,physics.soc-ph,Detecting periodic time scales in temporal networks,2023-07-07 21:19:19+00:00,"Temporal networks are commonly used to represent dynamical complex systems
like social networks, simultaneous firing of neurons, human mobility or public
transportation. Their dynamics may evolve on multiple time scales
characterising for instance periodic activity patterns or structural changes.
The detection of these time scales can be challenging from the direct
observation of simple dynamical network properties like the activity of nodes
or the density of links. Here we propose two new methods, which rely on already
established static representations of temporal networks, namely supra-adjacency
matrices and temporal event graphs. We define dissimilarity metrics extracted
from these representations and compute their Fourier Transform to effectively
identify dominant periodic time scales characterising the original temporal
network. We demonstrate our methods using synthetic and real-world data sets
describing various kinds of temporal networks. We find that while in all cases
the two methods outperform the reference measures, the supra-adjacency based
method identifies more easily periodic changes in network density, while the
temporal event graph based method is better suited to detect periodic changes
in the group structure of the network. Our methodology may provide insights
into different phenomena occurring at multiple time-scales in systems
represented by temporal networks.",http://arxiv.org/pdf/2307.03840v1
2307.03799v1,physics.flu-dyn,Uncertainty quantification for the squeeze flow of generalized Newtonian fluids,2023-07-07 18:54:31+00:00,"The calibration of rheological parameters in the modeling of complex flows of
non-Newtonian fluids can be a daunting task. In this paper we demonstrate how
the framework of Uncertainty Quantification (UQ) can be used to improve the
predictive capabilities of rheological models in such flow scenarios. For this
demonstration, we consider the squeeze flow of generalized Newtonian fluids. To
systematically study uncertainties, we have developed a tailored squeeze flow
setup, which we have used to perform experiments with glycerol and PVP
solution. To mimic these experiments, we have developed a three-region
truncated power law model, which can be evaluated semi-analytically. This
fast-to-evaluate model enables us to consider uncertainty propagation and
Bayesian inference using (Markov chain) Monte Carlo techniques. We demonstrate
that with prior information obtained from dedicated experiments - most
importantly rheological measurements - the truncated power law model can
adequately predict the experimental results. We observe that when the squeeze
flow experiments are incorporated in the analysis in the case of Bayesian
inference, this leads to an update of the prior information on the rheological
parameters, giving evidence of the need for recalibration in the considered
complex flow scenario. In the process of Bayesian inference we also obtain
information on quantities of interest that are not directly observable in the
experimental data, such as the spatial distribution of the three flow regimes.
In this way, besides improving the predictive capabilities of the model, the
uncertainty quantification framework enhances the insight into complex flow
scenarios.",http://arxiv.org/pdf/2307.03799v1
2307.03457v1,physics.geo-ph,Evaluating the incompleteness magnitude using an unbiased estimate of the $b$ value,2023-07-07 08:42:41+00:00,"The evaluation of the $b$ value of the Gutenberg-Richter (GR) law, for a
sample composed of $n$ earthquakes, presents a systematic positive bias $\delta
b$ which is proportional to $1/n$, as already observed by Ogata \& Yamashina
(1986). In this study we show how to incorporate in $\delta b$ the bias
introduced by deviations from the GR law. More precisely we show that $\delta
b$ is proportional to the square of the variability coefficient $CV$, defined
as the ratio between {the standard deviation of the magnitude distribution and
its mean value.} When the magnitude distribution follows the GR law $CV=1$ and
this allows us to introduce a new procedure, based on the dependence of $b$ on
$n$, which allows us to {identify} the incompleteness magnitude $m_c$ as the
threshold magnitude leading to $CV=1$. The method is tested on synthetic
catalogs and it is applied to estimate $m_c$ in Southern California, Japan and
New Zealand.",http://arxiv.org/pdf/2307.03457v1
2308.14549v1,q-bio.QM,Computational modelling of peritoneal dialysis: an overview,2023-08-28 13:08:35+00:00,"Peritoneal dialysis (PD) is becoming more popular as a result of a rising
interest in home dialysis, lower intrusion in social life and longer
preservation of residual kidney function. However, PD has several important
drawbacks: small solute clearance is relatively low compared to hemodialysis
and technique survival is limited. Application of continuous flow,
sorbent-based dialysate regeneration and novel glucose-sparing PD solutions are
some solutions proposed to address the limitations of PD. To optimize and
personalize current and novel PD therapies, patient peritoneal characteristics
interacting with PD techniques need to be studied together and separately as
they interplay. However, considering the multitude of parameters, it would be
difficult, expensive, and time consuming to optimize all parameter settings
only with the help of clinical trials. Mathematical modelling is an exciting
tool to dissect these interacting processes and comprehend PD techniques better
at a patient specific level. In this review, we look at the history of
computational PD models, explore the many ways a computational PD model can be
constructed and review the various existing PD models that can be used to
optimize and personalize PD treatment.",http://arxiv.org/pdf/2308.14549v1
2308.13891v1,cs.LG,Drug Interaction Vectors Neural Network: DrIVeNN,2023-08-26 14:24:41+00:00,"Polypharmacy, the concurrent use of multiple drugs to treat a single
condition, is common in patients managing multiple or complex conditions.
However, as more drugs are added to the treatment plan, the risk of adverse
drug events (ADEs) rises rapidly. Many serious ADEs associated with
polypharmacy only become known after the drugs are in use. It is impractical to
test every possible drug combination during clinical trials. This issue is
particularly prevalent among older adults with cardiovascular disease (CVD)
where polypharmacy and ADEs are commonly observed. In this research, our
primary objective was to identify key drug features to build and evaluate a
model for modeling polypharmacy ADEs. Our secondary objective was to assess our
model on a domain-specific case study. We developed a two-layer neural network
that incorporated drug features such as molecular structure, drug-protein
interactions, and mono drug side effects (DrIVeNN). We assessed DrIVeNN using
publicly available side effect databases and determined Principal Component
Analysis (PCA) with a variance threshold of 0.95 as the most effective feature
selection method. DrIVeNN performed moderately better than state-of-the-art
models like RESCAL, DEDICOM, DeepWalk, Decagon, DeepDDI, KGDDI, and KGNN in
terms of AUROC for the drug-drug interaction prediction task. We also conducted
a domain-specific case study centered on the treatment of cardiovascular
disease (CVD). When the best performing model architecture was applied to the
CVD treatment cohort, there was a significant increase in performance from the
general model. We observed an average AUROC for CVD drug pair prediction
increasing from 0.826 (general model) to 0.975 (CVD specific model). Our
findings indicate the strong potential of domain-specific models for improving
the accuracy of drug-drug interaction predictions.",http://arxiv.org/pdf/2308.13891v1
2308.13304v1,eess.IV,Bang and the Artefacts are Gone! Rapid Artefact Removal and Tissue Segmentation in Haematoxylin and Eosin Stained Biopsies,2023-08-25 11:04:35+00:00,"We present H&E Otsu thresholding, a scheme for rapidly detecting tissue in
whole-slide images (WSIs) that eliminates a wide range of undesirable artefacts
such as pen marks and scanning artefacts. Our method involves obtaining a
bid-modal representation of a low-magnification RGB overview image which
enables simple Otsu thresholding to separate tissue from background and
artefacts. We demonstrate our method on WSIs prepared from a wide range of
institutions and WSI digital scanners, each containing substantial artefacts
that cause other methods to fail. The beauty of our approach lies in its
simplicity: manipulating RGB colour space and using Otsu thresholding allows
for the rapid removal of artefacts and segmentation of tissue.",http://arxiv.org/pdf/2308.13304v1
2308.13182v1,cs.CV,Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon,2023-08-25 05:24:23+00:00,"With the advent of digital scanners and deep learning, diagnostic operations
may move from a microscope to a desktop. Hematoxylin and Eosin (H&E) staining
is one of the most frequently used stains for disease analysis, diagnosis, and
grading, but pathologists do need different immunohistochemical (IHC) stains to
analyze specific structures or cells. Obtaining all of these stains (H&E and
different IHCs) on a single specimen is a tedious and time-consuming task.
Consequently, virtual staining has emerged as an essential research direction.
Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for
synthesizing IHC stains from H&E images, and vice versa. Our method expressly
incorporates structural information in the form of edges (in addition to color
data) and employs attention modules exclusively in the decoder of the proposed
generator model. This integration enhances feature localization and preserves
contextual information during the generation process. In addition, a structural
loss is incorporated to ensure accurate structure alignment between the
generated and input markers. To demonstrate the efficacy of the proposed model,
experiments are conducted with two IHC markers emphasizing distinct structures
of glands in the colon: the nucleus of epithelial cells (CDX2) and the
cytoplasm (CK818). Quantitative metrics such as FID and SSIM are frequently
used for the analysis of generative models, but they do not correlate
explicitly with higher-quality virtual staining results. Therefore, we propose
two new quantitative metrics that correlate directly with the virtual staining
specificity of IHC markers.",http://arxiv.org/pdf/2308.13182v1
2308.13171v1,quant-ph,Q-Drug: a Framework to bring Drug Design into Quantum Space using Deep Learning,2023-08-25 04:26:02+00:00,"Optimizing the properties of molecules (materials or drugs) for stronger
toughness, lower toxicity, or better bioavailability has been a long-standing
challenge. In this context, we propose a molecular optimization framework
called Q-Drug (Quantum-inspired optimization algorithm for Drugs) that
leverages quantum-inspired algorithms to optimize molecules on discrete binary
domain variables. The framework begins by encoding the molecules into binary
embeddings using a discrete VAE. The binary embeddings are then used to
construct an Ising energy-like objective function, over which the
state-of-the-art quantum-inspired optimization algorithm is adopted to find the
optima. The binary embeddings corresponding to the optima are decoded to obtain
the optimized molecules. We have tested the framework for optimizing drug
molecule properties and have found that it outperforms other molecular
optimization methods, finding molecules with better properties in 1/20th to
1/10th of the time previously required. The framework can also be deployed
directly on various quantum computing equipment, such as laser pulses CIMs,
FPGA Ising Machines, and quantum computers based on quantum annealing, among
others. Our work demonstrates a new paradigm that leverages the advantages of
quantum computing and AI to solve practically useful problems.",http://arxiv.org/pdf/2308.13171v1
2308.13066v1,cs.LG,Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE,2023-08-24 20:22:22+00:00,"Variational autoencoder (VAE) is a popular method for drug discovery and
various architectures and pipelines have been proposed to improve its
performance. However, VAE approaches are known to suffer from poor manifold
recovery when the data lie on a low-dimensional manifold embedded in a higher
dimensional ambient space [Dai and Wipf, 2019]. The consequences of it in drug
discovery are somewhat under-explored. In this paper, we explore applying a
multi-stage VAE approach, that can improve manifold recovery on a synthetic
dataset, to the field of drug discovery. We experimentally evaluate our
multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability
to improve the property statistics of generated molecules substantially from
pre-existing methods without incorporating property predictors into the
training pipeline. We further fine-tune our models on two curated and much
smaller molecule datasets that target different proteins. Our experiments show
an increase in the number of active molecules generated by the multi-stage VAE
in comparison to their one-stage equivalent. For each of the two tasks, our
baselines include methods that use learned property predictors to incorporate
target metrics directly into the training objective and we discuss
complications that arise with this methodology.",http://arxiv.org/pdf/2308.13066v1
2308.13035v1,q-bio.QM,The intersection of video capsule endoscopy and artificial intelligence: addressing unique challenges using machine learning,2023-08-24 19:00:26+00:00,"Introduction: Technical burdens and time-intensive review processes limit the
practical utility of video capsule endoscopy (VCE). Artificial intelligence
(AI) is poised to address these limitations, but the intersection of AI and VCE
reveals challenges that must first be overcome. We identified five challenges
to address. Challenge #1: VCE data are stochastic and contains significant
artifact. Challenge #2: VCE interpretation is cost-intensive. Challenge #3: VCE
data are inherently imbalanced. Challenge #4: Existing VCE AIMLT are
computationally cumbersome. Challenge #5: Clinicians are hesitant to accept
AIMLT that cannot explain their process.
  Methods: An anatomic landmark detection model was used to test the
application of convolutional neural networks (CNNs) to the task of classifying
VCE data. We also created a tool that assists in expert annotation of VCE data.
We then created more elaborate models using different approaches including a
multi-frame approach, a CNN based on graph representation, and a few-shot
approach based on meta-learning.
  Results: When used on full-length VCE footage, CNNs accurately identified
anatomic landmarks (99.1%), with gradient weighted-class activation mapping
showing the parts of each frame that the CNN used to make its decision. The
graph CNN with weakly supervised learning (accuracy 89.9%, sensitivity of
91.1%), the few-shot model (accuracy 90.8%, precision 91.4%, sensitivity
90.9%), and the multi-frame model (accuracy 97.5%, precision 91.5%, sensitivity
94.8%) performed well. Discussion: Each of these five challenges is addressed,
in part, by one of our AI-based models. Our goal of producing high performance
using lightweight models that aim to improve clinician confidence was achieved.",http://arxiv.org/pdf/2308.13035v1
2308.12780v1,physics.bio-ph,The motility-matrix production switch in Bacillus subtilis -- a modeling perspective,2023-08-24 13:34:01+00:00,"Phenotype switching can be triggered by external stimuli and by intrinsic
stochasticity. Here, we focus on the motility-matrix production switch in
Bacillus subtilis. We use modeling to describe the SinR-SlrR bistable switch
its regulation by SinI, and to distinguish different sources of stochasticity.
Our simulations indicate that intrinsic fluctuations in the synthesis of SinI
are insufficient to drive spontaneous switching and suggest that switching is
triggered by upstream noise from the Spo0A phosphorelay.",http://arxiv.org/pdf/2308.12780v1
2308.12740v1,cs.AI,Human Comprehensible Active Learning of Genome-Scale Metabolic Networks,2023-08-24 12:42:00+00:00,"An important application of Synthetic Biology is the engineering of the host
cell system to yield useful products. However, an increase in the scale of the
host system leads to huge design space and requires a large number of
validation trials with high experimental costs. A comprehensible machine
learning approach that efficiently explores the hypothesis space and guides
experimental design is urgently needed for the Design-Build-Test-Learn (DBTL)
cycle of the host cell system. We introduce a novel machine learning framework
ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive
logical reasoning and actively learns from training examples. In contrast to
numerical models, ILP-iML1515 is built on comprehensible logical
representations of a genome-scale metabolic model and can update the model by
learning new logical structures from auxotrophic mutant trials. The ILP-iML1515
framework 1) allows high-throughput simulations and 2) actively selects
experiments that reduce the experimental cost of learning gene functions in
comparison to randomly selected experiments.",http://arxiv.org/pdf/2308.12740v1
2308.12416v1,eess.IV,Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach,2023-08-23 20:33:22+00:00,"Deep learning models have achieved state-of-the-art results in estimating
brain age, which is an important brain health biomarker, from magnetic
resonance (MR) images. However, most of these models only provide a global age
prediction, and rely on techniques, such as saliency maps to interpret their
results. These saliency maps highlight regions in the input image that were
significant for the model's predictions, but they are hard to be interpreted,
and saliency map values are not directly comparable across different samples.
In this work, we reframe the age prediction problem from MR images to an
image-to-image regression problem where we estimate the brain age for each
brain voxel in MR images. We compare voxel-wise age prediction models against
global age prediction models and their corresponding saliency maps. The results
indicate that voxel-wise age prediction models are more interpretable, since
they provide spatial information about the brain aging process, and they
benefit from being quantitative.",http://arxiv.org/pdf/2308.12416v1
2308.12224v1,q-bio.QM,Enhancing cardiovascular risk prediction through AI-enabled calcium-omics,2023-08-23 16:05:14+00:00,"Background. Coronary artery calcium (CAC) is a powerful predictor of major
adverse cardiovascular events (MACE). Traditional Agatston score simply sums
the calcium, albeit in a non-linear way, leaving room for improved
calcification assessments that will more fully capture the extent of disease.
  Objective. To determine if AI methods using detailed calcification features
(i.e., calcium-omics) can improve MACE prediction.
  Methods. We investigated additional features of calcification including
assessment of mass, volume, density, spatial distribution, territory, etc. We
used a Cox model with elastic-net regularization on 2457 CT calcium score
(CTCS) enriched for MACE events obtained from a large no-cost CLARIFY program
(ClinicalTri-als.gov Identifier: NCT04075162). We employed sampling techniques
to enhance model training. We also investigated Cox models with selected
features to identify explainable high-risk characteristics.
  Results. Our proposed calcium-omics model with modified synthetic down
sampling and up sampling gave C-index (80.5%/71.6%) and two-year AUC
(82.4%/74.8%) for (80:20, training/testing), respectively (sampling was applied
to the training set only). Results compared favorably to Agatston which gave
C-index (71.3%/70.3%) and AUC (71.8%/68.8%), respectively. Among calcium-omics
features, numbers of calcifications, LAD mass, and diffusivity (a measure of
spatial distribution) were important determinants of increased risk, with dense
calcification (>1000HU) associated with lower risk. The calcium-omics model
reclassified 63% of MACE patients to the high risk group in a held-out test.
The categorical net-reclassification index was NRI=0.153.
  Conclusions. AI analysis of coronary calcification can lead to improved
results as compared to Agatston scoring. Our findings suggest the utility of
calcium-omics in improved prediction of risk.",http://arxiv.org/pdf/2308.12224v1
2308.12325v1,q-bio.QM,Predicting Drug Solubility Using Different Machine Learning Methods -- Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network,2023-08-23 15:35:20+00:00,"Predicting the solubility of given molecules is an important task in the
pharmaceutical industry, and consequently this is a well-studied topic. In this
research, we revisited this problem with the advantage of modern computing
resources. We applied two machine learning models, a linear regression model
and a graph convolutional neural network model, on multiple experimental
datasets. Both methods can make reasonable predictions while the GCNN model had
the best performance. However, the current GCNN model is a black box, while
feature importance analysis from the linear regression model offers more
insights into the underlying chemical influences. Using the linear regression
model, we show how each functional group affects the overall solubility.
Ultimately, knowing how chemical structure influences chemical properties is
crucial when designing new drugs. Future work should aim to combine the high
performance of GCNNs with the interpretability of linear regression, unlocking
new advances in next generation high throughput screening.",http://arxiv.org/pdf/2308.12325v1
2308.12188v1,cs.LG,Development and external validation of a lung cancer risk estimation tool using gradient-boosting,2023-08-23 15:25:17+00:00,"Lung cancer is a significant cause of mortality worldwide, emphasizing the
importance of early detection for improved survival rates. In this study, we
propose a machine learning (ML) tool trained on data from the PLCO Cancer
Screening Trial and validated on the NLST to estimate the likelihood of lung
cancer occurrence within five years. The study utilized two datasets, the PLCO
(n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk
factors, clinical measurements, and outcomes related to lung cancer. Data
preprocessing involved removing patients who were not current or former smokers
and those who had died of causes unrelated to lung cancer. Additionally, a
focus was placed on mitigating bias caused by censored data. Feature selection,
hyper-parameter optimization, and model calibration were performed using
XGBoost, an ensemble learning algorithm that combines gradient boosting and
decision trees. The ML model was trained on the pre-processed PLCO dataset and
tested on the NLST dataset. The model incorporated features such as age,
gender, smoking history, medical diagnoses, and family history of lung cancer.
The model was well-calibrated (Brier score=0.044). ROC-AUC was 82% on the PLCO
dataset and 70% on the NLST dataset. PR-AUC was 29% and 11% respectively. When
compared to the USPSTF guidelines for lung cancer screening, our model provided
the same recall with a precision of 13.1% vs. 9.3% on the PLCO dataset and 3.2%
vs. 3.1% on the NLST dataset. The developed ML tool provides a freely available
web application for estimating the likelihood of developing lung cancer within
five years. By utilizing risk factors and clinical data, individuals can assess
their risk and make informed decisions regarding lung cancer screening. This
research contributes to the efforts in early detection and prevention
strategies, aiming to reduce lung cancer-related mortality rates.",http://arxiv.org/pdf/2308.12188v1
2308.11969v1,eess.IV,Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification,2023-08-23 07:30:16+00:00,"The burden of liver tumors is important, ranking as the fourth leading cause
of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation
of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is
performed to guide the treatment strategy. As this task is time-consuming,
needs high expertise and could be subject to inter-observer variability there
is a strong need for automatic tools. However, challenges arise from the lack
of available training data, as well as the high variability in terms of image
resolution and MRI sequence. In this work we propose to compare two different
pipelines based on anisotropic models to obtain the segmentation of the liver
and tumors. The first pipeline corresponds to a baseline multi-class model that
performs the simultaneous segmentation of the liver and tumor classes. In the
second approach, we train two distinct binary models, one segmenting the liver
only and the other the tumors. Our results show that both pipelines exhibit
different strengths and weaknesses. Moreover we propose an uncertainty
quantification strategy allowing the identification of potential false positive
tumor lesions. Both solutions were submitted to the MICCAI 2023 Atlas challenge
regarding liver and tumor segmentation.",http://arxiv.org/pdf/2308.11969v1
2308.11927v1,q-bio.QM,Recovering a Molecule's 3D Dynamics from Liquid-phase Electron Microscopy Movies,2023-08-23 05:26:27+00:00,"The dynamics of biomolecules are crucial for our understanding of their
functioning in living systems. However, current 3D imaging techniques, such as
cryogenic electron microscopy (cryo-EM), require freezing the sample, which
limits the observation of their conformational changes in real time. The
innovative liquid-phase electron microscopy (liquid-phase EM) technique allows
molecules to be placed in the native liquid environment, providing a unique
opportunity to observe their dynamics. In this paper, we propose TEMPOR, a
Temporal Electron MicroscoPy Object Reconstruction algorithm for liquid-phase
EM that leverages an implicit neural representation (INR) and a dynamical
variational auto-encoder (DVAE) to recover time series of molecular structures.
We demonstrate its advantages in recovering different motion dynamics from two
simulated datasets, 7bcq and Cas9. To our knowledge, our work is the first
attempt to directly recover 3D structures of a temporally-varying particle from
liquid-phase EM movies. It provides a promising new approach for studying
molecules' 3D dynamics in structural biology.",http://arxiv.org/pdf/2308.11927v1
2308.11846v1,nlin.PS,A Data-Driven Approach to Morphogenesis under Structural Instability,2023-08-23 00:51:43+00:00,"Morphological development into evolutionary patterns under structural
instability is ubiquitous in living systems and often of vital importance for
engineering structures. Here we propose a data-driven approach to understand
and predict their spatiotemporal complexities. A machine-learning framework is
proposed based on the physical modeling of morphogenesis triggered by internal
or external forcing. Digital libraries of structural patterns are constructed
from the simulation data, which are then used to recognize the abnormalities,
predict their development, and assist in risk assessment and prognosis. The
capabilities to identify the key bifurcation characteristics and predict the
history-dependent development from the global and local features are
demonstrated by examples of brain growth and aerospace structural design, which
offer guidelines for disease diagnosis/prognosis and instability-tolerant
design.",http://arxiv.org/pdf/2308.11846v1
2308.11773v1,cs.CL,Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model,2023-08-22 20:30:59+00:00,"Language use has been shown to correlate with depression, but large-scale
validation is needed. Traditional methods like clinic studies are expensive.
So, natural language processing has been employed on social media to predict
depression, but limitations remain-lack of validated labels, biased user
samples, and no context. Our study identified 29 topics in 3919
smartphone-collected speech recordings from 265 participants using the Whisper
tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal
to 10 were regarded as risk topics for depression: No Expectations, Sleep,
Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic
emergence and associations with depression, we compared behavioral (from
wearables) and linguistic characteristics across identified topics. The
correlation between topic shifts and changes in depression severity over time
was also investigated, indicating the importance of longitudinally monitoring
language use. We also tested the BERTopic model on a similar smaller dataset
(356 speech recordings from 57 participants), obtaining some consistent
results. In summary, our findings demonstrate specific speech topics may
indicate depression severity. The presented data-driven workflow provides a
practical approach to collecting and analyzing large-scale speech data from
real-world settings for digital health research.",http://arxiv.org/pdf/2308.11773v1
2308.11349v1,q-bio.QM,MulMarker: a GPT-assisted comprehensive framework for identifying potential multi-gene prognostic signatures,2023-08-22 10:54:04+00:00,"Prognostic signatures play an important role in clinical research, offering
insights into the potential health outcomes of patients and guiding therapeutic
decisions. Although single-gene prognostic biomarkers are valuable, multi-gene
prognostic signatures offer deeper insights into disease progression. In this
paper, we propose MulMarker, a framework that harnesses large language models
(LLMs, such as GPT) to identify and evaluate multi-gene prognostic signatures
across various diseases. MulMarker comprises three core modules: a GPT-driven
chatbot for addressing user queries, a module for identifying multi-gene
prognostic signatures, and a module for generating tailored reports. Using
MulMarker, we identified a cell cycle-related prognostic signature that
consists of CCNA1/2, CCNB1/2/3, CCNC, CCND1/2/3, CCNE1/2, CCNF, CCNG1/2, and
CCNH. When stratifying patients based on the prognostic signature, we found
that patients in the low-risk group have a higher survival rate than those in
the high-risk group. Overall, MulMarker offers an approach to the
identification and validation of potential multi-gene prognostic signatures. By
employing GPT to address user queries and generate tailored reports, it
underscores the potential of integrating cutting-edge Artificial Intelligence
(AI) solutions into prognostic research. We release the code of MulMarker at
https://github.com/Tina9/MulMarker.",http://arxiv.org/pdf/2308.11349v1
2308.11167v1,q-bio.QM,Predicting Dosage of Immunosuppressant Drugs After Kidney Transplantation Using Machine Learning,2023-08-22 03:52:29+00:00,"While kidney transplants are seen as the best treatment option for patients
with end-stage renal disease and kidney failure, the organ's health depends on
the dosage of immunosuppressant drugs post-transplantation. Due to the dosage
variance based on each patient's unique physiology, nephrologists face numerous
difficulties when determining the precise dosage needed for each patient.
Therefore, in this research we aim to devise a machine learning algorithm to
forecast the dosage of immunosuppressant drugs needed for different patients
after kidney transplantation. Utilizing a random forest algorithm, the devised
model is able to achieve accurate measurements for patient drug dosages.",http://arxiv.org/pdf/2308.11167v1
2308.11162v1,eess.IV,A Preliminary Investigation into Search and Matching for Tumour Discrimination in WHO Breast Taxonomy Using Deep Networks,2023-08-22 03:40:46+00:00,"Breast cancer is one of the most common cancers affecting women worldwide.
They include a group of malignant neoplasms with a variety of biological,
clinical, and histopathological characteristics. There are more than 35
different histological forms of breast lesions that can be classified and
diagnosed histologically according to cell morphology, growth, and architecture
patterns. Recently, deep learning, in the field of artificial intelligence, has
drawn a lot of attention for the computerized representation of medical images.
Searchable digital atlases can provide pathologists with patch matching tools
allowing them to search among evidently diagnosed and treated archival cases, a
technology that may be regarded as computational second opinion. In this study,
we indexed and analyzed the WHO breast taxonomy (Classification of Tumours 5th
Ed.) spanning 35 tumour types. We visualized all tumour types using deep
features extracted from a state-of-the-art deep learning model, pre-trained on
millions of diagnostic histopathology images from the TCGA repository.
Furthermore, we test the concept of a digital ""atlas"" as a reference for search
and matching with rare test cases. The patch similarity search within the WHO
breast taxonomy data reached over 88% accuracy when validating through
""majority vote"" and more than 91% accuracy when validating using top-n tumour
types. These results show for the first time that complex relationships among
common and rare breast lesions can be investigated using an indexed digital
archive.",http://arxiv.org/pdf/2308.11162v1
2308.11086v1,math.DS,Pushing coarse-grained models beyond the continuum limit using equation learning,2023-08-21 23:49:03+00:00,"Mathematical modelling of biological population dynamics often involves
proposing high fidelity discrete agent-based models that capture stochasticity
and individual-level processes. These models are often considered in
conjunction with an approximate coarse-grained differential equation that
captures population-level features only. These coarse-grained models are only
accurate in certain asymptotic parameter regimes, such as enforcing that the
time scale of individual motility far exceeds the time scale of birth/death
processes. When these coarse-grained models are accurate, the discrete model
still abides by conservation laws at the microscopic level, which implies that
there is some macroscopic conservation law that can describe the macroscopic
dynamics. In this work, we introduce an equation learning framework to find
accurate coarse-grained models when standard continuum limit approaches are
inaccurate. We demonstrate our approach using a discrete mechanical model of
epithelial tissues, considering a series of four case studies that illustrate
how we can learn macroscopic equations describing mechanical relaxation, cell
proliferation, and the equation governing the dynamics of the free boundary of
the tissue. While our presentation focuses on this biological application, our
approach is more broadly applicable across a range of scenarios where discrete
models are approximated by approximate continuum-limit descriptions. All code
and data to reproduce this work are available at
https://github.com/DanielVandH/StepwiseEQL.jl.",http://arxiv.org/pdf/2308.11086v1
2308.11041v1,stat.ME,Bayesian Prevalence Estimation from Pooled and Individual Data,2023-08-21 21:02:00+00:00,"Pooled and individual disease testing are common methods for determining the
population prevalences of diseases. Recently, researchers have used Monte Carlo
Markov Chain methods to estimate population prevalence from the combined
streams of these two types of testing data. We propose an analytical solution
for estimating population prevalence from combined individual and pooled binary
sampling data. We also use simulated sampling data to characterize these
posterior distributions under a variety of sampling conditions, including a
range of true prevalences, variable numbers of pooled and individual tests,
variable number of individual samples per pooled sample, and a range of values
for test sensitivity and specificity.",http://arxiv.org/pdf/2308.11041v1
2308.10917v1,q-bio.QM,PACS: Prediction and analysis of cancer subtypes from multi-omics data based on a multi-head attention mechanism model,2023-08-21 03:54:21+00:00,"Due to the high heterogeneity and clinical characteristics of cancer, there
are significant differences in multi-omic data and clinical characteristics
among different cancer subtypes. Therefore, accurate classification of cancer
subtypes can help doctors choose the most appropriate treatment options,
improve treatment outcomes, and provide more accurate patient survival
predictions. In this study, we propose a supervised multi-head attention
mechanism model (SMA) to classify cancer subtypes successfully. The attention
mechanism and feature sharing module of the SMA model can successfully learn
the global and local feature information of multi-omics data. Second, it
enriches the parameters of the model by deeply fusing multi-head attention
encoders from Siamese through the fusion module. Validated by extensive
experiments, the SMA model achieves the highest accuracy, F1 macroscopic, F1
weighted, and accurate classification of cancer subtypes in simulated,
single-cell, and cancer multiomics datasets compared to AE, CNN, and GNN-based
models. Therefore, we contribute to future research on multiomics data using
our attention-based approach.",http://arxiv.org/pdf/2308.10917v1
2308.10372v1,eess.IV,Developing a Machine Learning-Based Clinical Decision Support Tool for Uterine Tumor Imaging,2023-08-20 21:46:05+00:00,"Uterine leiomyosarcoma (LMS) is a rare but aggressive malignancy. On imaging,
it is difficult to differentiate LMS from, for example, degenerated leiomyoma
(LM), a prevalent but benign condition. We curated a data set of 115 axial
T2-weighted MRI images from 110 patients (mean [range] age=45 [17-81] years)
with UTs that included five different tumor types. These data were randomly
split stratifying on tumor volume into training (n=85) and test sets (n=30). An
independent second reader (reader 2) provided manual segmentations for all test
set images. To automate segmentation, we applied nnU-Net and explored the
effect of training set size on performance by randomly generating subsets with
25, 45, 65 and 85 training set images. We evaluated the ability of radiomic
features to distinguish between types of UT individually and when combined
through feature selection and machine learning. Using the entire training set
the mean [95% CI] fibroid DSC was measured as 0.87 [0.59-1.00] and the
agreement between the two readers was 0.89 [0.77-1.0] on the test set. When
classifying degenerated LM from LMS we achieve a test set F1-score of 0.80.
Classifying UTs based on radiomic features we identify classifiers achieving
F1-scores of 0.53 [0.45, 0.61] and 0.80 [0.80, 0.80] on the test set for the
benign versus malignant, and degenerated LM versus LMS tasks. We show that it
is possible to develop an automated method for 3D segmentation of the uterus
and UT that is close to human-level performance with fewer than 150 annotated
images. For distinguishing UT types, while we train models that merit further
investigation with additional data, reliable automatic differentiation of UTs
remains a challenge.",http://arxiv.org/pdf/2308.10372v1
2308.10302v1,q-bio.QM,Preserving Specificity in Federated Graph Learning for fMRI-based Neurological Disorder Identification,2023-08-20 15:55:45+00:00,"Resting-state functional magnetic resonance imaging (rs-fMRI) offers a
non-invasive approach to examining abnormal brain connectivity associated with
brain disorders. Graph neural network (GNN) gains popularity in fMRI
representation learning and brain disorder analysis with powerful graph
representation capabilities. Training a general GNN often necessitates a
large-scale dataset from multiple imaging centers/sites, but centralizing
multi-site data generally faces inherent challenges related to data privacy,
security, and storage burden. Federated Learning (FL) enables collaborative
model training without centralized multi-site fMRI data. Unfortunately,
previous FL approaches for fMRI analysis often ignore site-specificity,
including demographic factors such as age, gender, and education level. To this
end, we propose a specificity-aware federated graph learning (SFGL) framework
for rs-fMRI analysis and automated brain disorder identification, with a server
and multiple clients/sites for federated model aggregation and prediction. At
each client, our model consists of a shared and a personalized branch, where
parameters of the shared branch are sent to the server while those of the
personalized branch remain local. This can facilitate knowledge sharing among
sites and also helps preserve site specificity. In the shared branch, we employ
a spatio-temporal attention graph isomorphism network to learn dynamic fMRI
representations. In the personalized branch, we integrate vectorized
demographic information (i.e., age, gender, and education years) and functional
connectivity networks to preserve site-specific characteristics.
Representations generated by the two branches are then fused for
classification. Experimental results on two fMRI datasets with a total of 1,218
subjects suggest that SFGL outperforms several state-of-the-art approaches.",http://arxiv.org/pdf/2308.10302v1
2308.10275v1,q-bio.QM,SBSM-Pro: Support Bio-sequence Machine for Proteins,2023-08-20 14:10:50+00:00,"Proteins play a pivotal role in biological systems. The use of machine
learning algorithms for protein classification can assist and even guide
biological experiments, offering crucial insights for biotechnological
applications. We propose a support bio-sequence machine for proteins, a model
specifically designed for biological sequence classification. This model starts
with raw sequences and groups amino acids based on their physicochemical
properties. It incorporates sequence alignment to measure the similarities
between proteins and uses a novel MKL approach to integrate various types of
information, utilizing support vector machines for classification prediction.
The results indicate that our model demonstrates commendable performance across
10 datasets in terms of the identification of protein function and
posttranslational modification. This research not only showcases
state-of-the-art work in protein classification but also paves the way for new
directions in this domain, representing a beneficial endeavour in the
development of platforms tailored for biological sequence classification.
SBSM-Pro is available for access at http://lab.malab.cn/soft/SBSM-Pro/.",http://arxiv.org/pdf/2308.10275v1
2308.09852v1,cs.MA,SICO: Simulation for Infection Control Operations,2023-08-18 23:06:40+00:00,"In response to the COVID-19 pandemic and the potential threat of future
epidemics caused by novel viruses, we developed a flexible framework for
modeling disease intervention effects. This tool is intended to aid decision
makers at multiple levels as they compare possible responses to emerging
epidemiological threats for optimal control and reduction of harm. The
framework is specifically designed to be both scalable and modular, allowing it
to model a variety of population levels, viruses, testing methods and
strategies--including pooled testing--and intervention strategies. In this
paper, we provide an overview of this framework and examine the impact of
different intervention strategies and their impact on infection dynamics.",http://arxiv.org/pdf/2308.09852v1
2308.09312v1,stat.ML,Path Signatures for Seizure Forecasting,2023-08-18 05:19:18+00:00,"Forecasting the state of a system from an observed time series is the subject
of research in many domains, such as computational neuroscience. Here, the
prediction of epileptic seizures from brain measurements is an unresolved
problem. There are neither complete models describing underlying brain
dynamics, nor do individual patients exhibit a single seizure onset pattern,
which complicates the development of a `one-size-fits-all' solution. Based on a
longitudinal patient data set, we address the automated discovery and
quantification of statistical features (biomarkers) that can be used to
forecast seizures in a patient-specific way. We use existing and novel feature
extraction algorithms, in particular the path signature, a recent development
in time series analysis. Of particular interest is how this set of complex,
nonlinear features performs compared to simpler, linear features on this task.
Our inference is based on statistical classification algorithms with in-built
subset selection to discern time series with and without an impending seizure
while selecting only a small number of relevant features. This study may be
seen as a step towards a generalisable pattern recognition pipeline for time
series in a broader context.",http://arxiv.org/pdf/2308.09312v1
2308.09086v1,q-bio.QM,Embracing assay heterogeneity with neural processes for markedly improved bioactivity predictions,2023-08-17 16:26:58+00:00,"Predicting the bioactivity of a ligand is one of the hardest and most
important challenges in computer-aided drug discovery. Despite years of data
collection and curation efforts by research organizations worldwide,
bioactivity data remains sparse and heterogeneous, thus hampering efforts to
build predictive models that are accurate, transferable and robust. The
intrinsic variability of the experimental data is further compounded by data
aggregation practices that neglect heterogeneity to overcome sparsity. Here we
discuss the limitations of these practices and present a hierarchical
meta-learning framework that exploits the information synergy across disparate
assays by successfully accounting for assay heterogeneity. We show that the
model achieves a drastic improvement in affinity prediction across diverse
protein targets and assay types compared to conventional baselines. It can
quickly adapt to new target contexts using very few observations, thus enabling
large-scale virtual screening in early-phase drug discovery.",http://arxiv.org/pdf/2308.09086v1
2308.08978v1,cs.RO,Quantifying the biomimicry gap in biohybrid systems,2023-08-17 13:33:15+00:00,"Biohybrid systems in which robotic lures interact with animals have become
compelling tools for probing and identifying the mechanisms underlying
collective animal behavior. One key challenge lies in the transfer of social
interaction models from simulations to reality, using robotics to validate the
modeling hypotheses. This challenge arises in bridging what we term the
""biomimicry gap"", which is caused by imperfect robotic replicas, communication
cues and physics constrains not incorporated in the simulations that may elicit
unrealistic behavioral responses in animals. In this work, we used a biomimetic
lure of a rummy-nose tetra fish (Hemigrammus rhodostomus) and a neural network
(NN) model for generating biomimetic social interactions. Through experiments
with a biohybrid pair comprising a fish and the robotic lure, a pair of real
fish, and simulations of pairs of fish, we demonstrate that our biohybrid
system generates high-fidelity social interactions mirroring those of genuine
fish pairs. Our analyses highlight that: 1) the lure and NN maintain minimal
deviation in real-world interactions compared to simulations and fish-only
experiments, 2) our NN controls the robot efficiently in real-time, and 3) a
comprehensive validation is crucial to bridge the biomimicry gap, ensuring
realistic biohybrid systems.",http://arxiv.org/pdf/2308.08978v1
2308.08662v1,q-bio.PE,Evaluating the potential impacts of grey seal predation and fishery bycatch/discards on cod productivity on the Western Scotian Shelf and in the Bay of Fundy,2023-08-16 20:26:13+00:00,"The recovery of many groundfish stocks throughout the Northwest Atlantic has
been impeded by elevated natural (i.e., non-fishing) mortality (M) among
older/larger individuals. The causes of elevated mortality are not well known,
though predation by rapidly growing grey seal herds and unreported fishing are
thought to be possible drivers of mortality for Atlantic Cod (Gadus morhua) on
the Western Scotian Shelf and in the Bay of Fundy (known as ""4X5Y cod"") and in
nearby ecosystems. We developed a statistical catch-at-age model for 4X5Y cod
that accounted for both grey seal predation and estimated bycatch/discards to
evaluate the degree to which either of these factors may influence cod
mortality. The model was fit over a range of predation and discarding scenarios
to account for uncertainties and a lack of data for these processes. We found
that most cod M remained unaccounted for unless cod comprised a large
proportion (>0.45) of the grey seal diet by weight. If the reported bycatch
estimates are taken as accurate, then the magnitude of cod discards from
non-directed fisheries was minor, though these estimates are highly uncertain.",http://arxiv.org/pdf/2308.08662v1
2308.08618v1,q-bio.QM,"Modeling Biphasic, Non-Sigmoidal Dose-Response Relationships: Comparison of Brain-Cousens and Cedergreen Models for a Biochemical Dataset",2023-08-16 18:22:37+00:00,"Biphasic, non-sigmoidal dose-response relationships are frequently observed
in biochemistry and pharmacology, but they are not always analyzed with
appropriate statistical methods. Here, we examine curve fitting methods for
""hormetic"" dose-response relationships where low and high doses of an effector
produce opposite responses. We provide the full dataset used for modeling, and
we provide the code for analyzing the dataset in SAS using two established
mathematical models of hormesis, the Brain-Cousens model and the Cedergreen
model. We show how to obtain and interpret curve parameters such as the ED50
that arise from modeling, and we discuss how curve parameters might change in a
predictable manner when the conditions of the dose-response assay are altered.
In addition to modeling the raw dataset that we provide, we also model the
dataset after applying common normalization techniques, and we indicate how
this affects the parameters that are associated with the fit curves. The
Brain-Cousens and Cedergreen models that we used for curve fitting were
similarly effective at capturing quantitative information about the biphasic
dose-response relationships.",http://arxiv.org/pdf/2308.08618v1
2308.08578v1,q-bio.QM,PEvoLM: Protein Sequence Evolutionary Information Language Model,2023-08-16 06:46:28+00:00,"With the exponential increase of the protein sequence databases over time,
multiple-sequence alignment (MSA) methods, like PSI-BLAST, perform exhaustive
and time-consuming database search to retrieve evolutionary information. The
resulting position-specific scoring matrices (PSSMs) of such search engines
represent a crucial input to many machine learning (ML) models in the field of
bioinformatics and computational biology. A protein sequence is a collection of
contiguous tokens or characters called amino acids (AAs). The analogy to
natural language allowed us to exploit the recent advancements in the field of
Natural Language Processing (NLP) and therefore transfer NLP state-of-the-art
algorithms to bioinformatics. This research presents an Embedding Language
Model (ELMo), converting a protein sequence to a numerical vector
representation. While the original ELMo trained a 2-layer bidirectional Long
Short-Term Memory (LSTMs) network following a two-path architecture, one for
the forward and the second for the backward pass, by merging the idea of PSSMs
with the concept of transfer-learning, this work introduces a novel
bidirectional language model (bi-LM) with four times less free parameters and
using rather a single path for both passes. The model was trained not only on
predicting the next AA but also on the probability distribution of the next AA
derived from similar, yet different sequences as summarized in a PSSM,
simultaneously for multi-task learning, hence learning evolutionary information
of protein sequences as well. The network architecture and the pre-trained
model are made available as open source under the permissive MIT license on
GitHub at https://github.com/issararab/PEvoLM.",http://arxiv.org/pdf/2308.08578v1
2308.08129v1,cs.LG,Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?,2023-08-16 03:38:43+00:00,"The prediction of material properties plays a crucial role in the development
and discovery of materials in diverse applications, such as batteries,
semiconductors, catalysts, and pharmaceuticals. Recently, there has been a
growing interest in employing data-driven approaches by using machine learning
technologies, in combination with conventional theoretical calculations. In
material science, the prediction of unobserved values, commonly referred to as
extrapolation, is particularly critical for property prediction as it enables
researchers to gain insight into materials beyond the limits of available data.
However, even with the recent advancements in powerful machine learning models,
accurate extrapolation is still widely recognized as a significantly
challenging problem. On the other hand, self-supervised pretraining is a
machine learning technique where a model is first trained on unlabeled data
using relatively simple pretext tasks before being trained on labeled data for
target tasks. As self-supervised pretraining can effectively utilize material
data without observed property values, it has the potential to improve the
model's extrapolation ability. In this paper, we clarify how such
self-supervised pretraining can enhance extrapolation performance.We propose an
experimental framework for the demonstration and empirically reveal that while
models were unable to accurately extrapolate absolute property values,
self-supervised pretraining enables them to learn relative tendencies of
unobserved property values and improve extrapolation performance.",http://arxiv.org/pdf/2308.08129v1
2308.08112v1,eess.IV,A Comprehensive Overview of Computational Nuclei Segmentation Methods in Digital Pathology,2023-08-16 02:55:29+00:00,"In the cancer diagnosis pipeline, digital pathology plays an instrumental
role in the identification, staging, and grading of malignant areas on biopsy
tissue specimens. High resolution histology images are subject to high variance
in appearance, sourcing either from the acquisition devices or the H\&E
staining process. Nuclei segmentation is an important task, as it detects the
nuclei cells over background tissue and gives rise to the topology, size, and
count of nuclei which are determinant factors for cancer detection. Yet, it is
a fairly time consuming task for pathologists, with reportedly high
subjectivity. Computer Aided Diagnosis (CAD) tools empowered by modern
Artificial Intelligence (AI) models enable the automation of nuclei
segmentation. This can reduce the subjectivity in analysis and reading time.
This paper provides an extensive review, beginning from earlier works use
traditional image processing techniques and reaching up to modern approaches
following the Deep Learning (DL) paradigm. Our review also focuses on the weak
supervision aspect of the problem, motivated by the fact that annotated data is
scarce. At the end, the advantages of different models and types of supervision
are thoroughly discussed. Furthermore, we try to extrapolate and envision how
future research lines will potentially be, so as to minimize the need for
labeled data while maintaining high performance. Future methods should
emphasize efficient and explainable models with a transparent underlying
process so that physicians can trust their output.",http://arxiv.org/pdf/2308.08112v1
2308.06205v1,math.AT,Relational persistent homology for multispecies data with application to the tumor microenvironment,2023-08-11 16:09:21+00:00,"Topological data analysis (TDA) is an active field of mathematics for
quantifying shape in complex data. Standard methods in TDA such as persistent
homology (PH) are typically focused on the analysis of data consisting of a
single entity (e.g., cells or molecular species). However, state-of-the-art
data collection techniques now generate exquisitely detailed multispecies data,
prompting a need for methods that can examine and quantify the relations among
them. Such heterogeneous data types arise in many contexts, ranging from
biomedical imaging, geospatial analysis, to species ecology. Here, we propose
two methods for encoding spatial relations among different data types that are
based on Dowker complexes and Witness complexes. We apply the methods to
synthetic multispecies data of a tumor microenvironment and analyze topological
features that capture relations between different cell types, e.g., blood
vessels, macrophages, tumor cells, and necrotic cells. We demonstrate that
relational topological features can extract biological insight, including the
dominant immune cell phenotype (an important predictor of patient prognosis)
and the parameter regimes of a data-generating model. The methods provide a
quantitative perspective on the relational analysis of multispecies spatial
data, overcome the limits of traditional PH, and are readily computable.",http://arxiv.org/pdf/2308.06205v1
2308.06117v1,q-bio.QM,Molecular fingerprinting of biological nanoparticles with a label-free optofluidic platform,2023-08-11 13:10:53+00:00,"Label-free detecting multiple analytes in a high-throughput fashion has been
one of the long-sought goals in biosensing applications. Yet, for all-optical
approaches, interfacing state-of-the-art label-free techniques with
microfluidics tools that can process small volumes of sample with high
throughput, and with surface chemistry that grants analyte specificity, poses a
critical challenge to date. Here, we introduce an optofluidic platform that
brings together state-of-the-art digital holography with PDMS microfluidics by
using supported lipid bilayers as a surface chemistry building block to
integrate both technologies. Specifically, this platform fingerprints
heterogeneous biological nanoparticle populations via a multiplexed label-free
immunoaffinity assay with single particle sensitivity. Herein, we first
thoroughly characterise the robustness and performance of the platform, and
then apply it to profile four distinct ovarian cell-derived extracellular
vesicle populations over a panel of surface protein biomarkers, thus developing
a unique biomarker fingerprint for each cell line. We foresee that our approach
will find many applications where routine and multiplexed characterisation of
biological nanoparticles is required.",http://arxiv.org/pdf/2308.06117v1
2308.06296v2,eess.IV,Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review,2023-08-11 06:32:25+00:00,"Machine learning (ML) and deep learning (DL) models have been employed to
significantly improve analyses of medical imagery, with these approaches used
to enhance the accuracy of prediction and classification. Model predictions and
classifications assist diagnoses of various cancers and tumors. This review
presents an in-depth analysis of modern techniques applied within the domain of
medical image analysis for white blood cell classification. The methodologies
that use blood smear images, magnetic resonance imaging (MRI), X-rays, and
similar medical imaging domains are identified and discussed, with a detailed
analysis of ML/DL techniques applied to the classification of white blood cells
(WBCs) representing the primary focus of the review. The data utilized in this
research has been extracted from a collection of 136 primary papers that were
published between the years 2006 and 2023. The most widely used techniques and
best-performing white blood cell classification methods are identified. While
the use of ML and DL for white blood cell classification has concurrently
increased and improved in recent year, significant challenges remain - 1)
Availability of appropriate datasets remain the primary challenge, and may be
resolved using data augmentation techniques. 2) Medical training of researchers
is recommended to improve current understanding of white blood cell structure
and subsequent selection of appropriate classification models. 3) Advanced DL
networks including Generative Adversarial Networks, R-CNN, Fast R-CNN, and
faster R-CNN will likely be increasingly employed to supplement or replace
current techniques.",http://arxiv.org/pdf/2308.06296v2
2308.06294v1,q-bio.QM,Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT,2023-08-11 03:40:22+00:00,"We hypothesize that large language models (LLMs) based on the transformer
architecture can enable automated detection of clinical phenotype terms,
including terms not documented in the HPO. In this study, we developed two
types of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERT
as its pre-trained model, and PhenoGPT, a GPT-based model that can be
initialized from diverse GPT models, including open-source versions such as
GPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 and
GPT-3.5. We compared our methods with PhenoTagger, a recently developed HPO
recognition tool that combines rule-based and deep learning methods. We found
that our methods can extract more phenotype concepts, including novel ones not
characterized by HPO. We also performed case studies on biomedical literature
to illustrate how new phenotype information can be recognized and extracted. We
compared current BERT-based versus GPT-based models for phenotype tagging, in
multiple aspects including model architecture, memory usage, speed, accuracy,
and privacy protection. We also discussed the addition of a negation step and
an HPO normalization layer to the transformer models for improved HPO term
tagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discovery
of phenotype terms from clinical notes and biomedical literature, facilitating
automated downstream tasks to derive new biological insights on human diseases.",http://arxiv.org/pdf/2308.06294v1
2308.06292v1,q-bio.QM,The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences,2023-08-11 01:32:05+00:00,"A complete time-parameterized statistical model quantifying the divergent
evolution of protein structures in terms of the patterns of conservation of
their secondary structures is inferred from a large collection of protein 3D
structure alignments. This provides a better alternative to time-parameterized
sequence-based models of protein relatedness, that have clear limitations
dealing with twilight and midnight zones of sequence relationships. Since
protein structures are far more conserved due to the selection pressure
directly placed on their function, divergence time estimates can be more
accurate when inferred from structures. We use the Bayesian and
information-theoretic framework of Minimum Message Length to infer a
time-parameterized stochastic matrix (accounting for perturbed structural
states of related residues) and associated Dirichlet models (accounting for
insertions and deletions during the evolution of protein domains). These are
used in concert to estimate the Markov time of divergence of tertiary
structures, a task previously only possible using proxies (like RMSD). By
analyzing one million pairs of homologous structures, we yield a relationship
between the Markov divergence time of structures and of sequences. Using these
inferred models and the relationship between the divergence of sequences and
structures, we demonstrate a competitive performance in secondary structure
prediction against neural network architectures commonly employed for this
task. The source code and supplementary information are downloadable from
\url{http://lcb.infotech.monash.edu.au/sstsum}.",http://arxiv.org/pdf/2308.06292v1
2308.05864v1,eess.IV,The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions,2023-08-10 21:59:23+00:00,"Cell segmentation is a critical step for quantitative single-cell analysis in
microscopy images. Existing cell segmentation methods are often tailored to
specific modalities or require manual interventions to specify hyperparameters
in different experimental settings. Here, we present a multi-modality cell
segmentation benchmark, comprising over 1500 labeled images derived from more
than 50 diverse biological experiments. The top participants developed a
Transformer-based deep-learning algorithm that not only exceeds existing
methods, but can also be applied to diverse microscopy images across imaging
platforms and tissue types without manual parameter adjustments. This benchmark
and the improved algorithm offer promising avenues for more accurate and
versatile cell analysis in microscopy imaging.",http://arxiv.org/pdf/2308.05864v1
2308.06288v1,q-bio.QM,Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology,2023-08-10 17:16:52+00:00,"Podocytes, specialized epithelial cells that envelop the glomerular
capillaries, play a pivotal role in maintaining renal health. The current
description and quantification of features on pathology slides are limited,
prompting the need for innovative solutions to comprehensively assess diverse
phenotypic attributes within Whole Slide Images (WSIs). In particular,
understanding the morphological characteristics of podocytes, terminally
differentiated glomerular epithelial cells, is crucial for studying glomerular
injury. This paper introduces the Spatial Pathomics Toolkit (SPT) and applies
it to podocyte pathomics. The SPT consists of three main components: (1)
instance object segmentation, enabling precise identification of podocyte
nuclei; (2) pathomics feature generation, extracting a comprehensive array of
quantitative features from the identified nuclei; and (3) robust statistical
analyses, facilitating a comprehensive exploration of spatial relationships
between morphological and spatial transcriptomics features.The SPT successfully
extracted and analyzed morphological and textural features from podocyte
nuclei, revealing a multitude of podocyte morphomic features through
statistical analysis. Additionally, we demonstrated the SPT's ability to
unravel spatial information inherent to podocyte distribution, shedding light
on spatial patterns associated with glomerular injury. By disseminating the
SPT, our goal is to provide the research community with a powerful and
user-friendly resource that advances cellular spatial pathomics in renal
pathology. The implementation and its complete source code of the toolkit are
made openly accessible at https://github.com/hrlblab/spatial_pathomics.",http://arxiv.org/pdf/2308.06288v1
2308.05536v1,q-bio.QM,A coupled-mechanisms modelling framework for neurodegeneration,2023-08-10 12:34:25+00:00,"Computational models of neurodegeneration aim to emulate the evolving pattern
of pathology in the brain during neurodegenerative disease, such as Alzheimer's
disease. Previous studies have made specific choices on the mechanisms of
pathology production and diffusion, or assume that all the subjects lie on the
same disease progression trajectory. However, the complexity and heterogeneity
of neurodegenerative pathology suggests that multiple mechanisms may contribute
synergistically with complex interactions, meanwhile the degree of contribution
of each mechanism may vary among individuals. We thus put forward a
coupled-mechanisms modelling framework which non-linearly combines the
network-topology-informed pathology appearance with the process of pathology
spreading within a dynamic modelling system. We account for the heterogeneity
of disease by fitting the model at the individual level, allowing the
epicenters and rate of progression to vary among subjects. We construct a
Bayesian model selection framework to account for feature importance and
parameter uncertainty. This provides a combination of mechanisms that best
explains the observations for each individual from the ADNI dataset. With the
obtained distribution of mechanism importance for each subject, we are able to
identify subgroups of patients sharing similar combinations of apparent
mechanisms.",http://arxiv.org/pdf/2308.05536v1
2308.05527v1,q-bio.QM,UQSA -- An R-Package for Uncertainty Quantification and Sensitivity Analysis for Biochemical Reaction Network Models,2023-08-10 12:12:03+00:00,"We present an R-package developed for modeling of biochemical reaction
networks, uncertainty quantification (UQ) and sensitivity analysis (SA).
Estimating parameters and quantifying their uncertainty (and resulting
prediction uncertainty), is required for data-driven systems biology modeling.
Sampling methods need to be efficient when confronted with high-dimensional,
correlated parameter distributions. We have developed the UQSA package to be
fast for this problem class and work well with other tools for modelling. We
aim for simplicity, and part of that is our use of the SBtab format for the
unified storage of model and data. Our tool-set is modular enough, that parts
can be replaced. We use intermediate formats that are not hidden from the user
to make this feasible. UQ is performed through Markov chain Monte Carlo (MCMC)
sampling in an Approximate Bayesian Computation (ABC) setting. This can be
followed by a variance-decomposition based global sensitivity analysis. If
needed, complex parameter distributions can be described, evaluated, and
sampled from, with the help of Vine-copulas that are available in R. This
approach is especially useful when new experimental data become available, and
a previously calibrated model needs to be updated.
  Implementation: R is a high-level language and allows the use of
sophisticated statistical methods. The ode solver we used is written in C
(gsl_odeiv2, interface to R is ours). We use the SBtab tabular format for the
model description, as well as the data and an event system to be able to model
inputs frequently encountered in systems biology and neuroscience. The code has
been tested on one node with 256 cores of a computing cluster, but smaller
examples are included in the repository that can be run on a laptop.
  Source code: https://github.com/icpm-kth/uqsa",http://arxiv.org/pdf/2308.05527v1
2308.05777v1,q-bio.QM,PoseBusters: AI-based docking methods fail to generate physically valid poses or generalise to novel sequences,2023-08-10 11:28:48+00:00,"The last few years have seen the development of numerous deep learning-based
protein-ligand docking methods. They offer huge promise in terms of speed and
accuracy. However, despite claims of state-of-the-art performance in terms of
crystallographic root-mean-square deviation (RMSD), upon closer inspection, it
has become apparent that they often produce physically implausible molecular
structures. It is therefore not sufficient to evaluate these methods solely by
RMSD to a native binding mode. It is vital, particularly for deep
learning-based methods, that they are also evaluated on steric and energetic
criteria. We present PoseBusters, a Python package that performs a series of
standard quality checks using the well-established cheminformatics toolkit
RDKit. Only methods that both pass these checks and predict native-like binding
modes should be classed as having ``state-of-the-art'' performance. We use
PoseBusters to compare five deep learning-based docking methods (DeepDock,
DiffDock, EquiBind, TankBind, and Uni-Mol) and two well-established standard
docking methods (AutoDock Vina and CCDC Gold) with and without an additional
post-prediction energy minimisation step using a molecular mechanics force
field. We show that both in terms of physical plausibility and the ability to
generalise to examples that are distinct from the training data, no deep
learning-based method yet outperforms classical docking tools. In addition, we
find that molecular mechanics force fields contain docking-relevant physics
missing from deep-learning methods. PoseBusters allows practitioners to assess
docking and molecular generation methods and may inspire new inductive biases
still required to improve deep learning-based methods, which will help drive
the development of more accurate and more realistic predictions.",http://arxiv.org/pdf/2308.05777v1
2308.05256v1,cs.SI,Social Network Analysis and Validation of an Agent-Based Model,2023-08-09 23:32:06+00:00,"Agent-based models (ABMs) simulate the formation and evolution of social
processes at a fundamental level by decoupling agent behavior from global
observations. In the case where ABM networks evolve over time as a result of
(or in conjunction with) agent states, there is a need for understanding the
relationship between the dynamic processes and network structure. Social
networks provide a natural set of tools for understanding the emergent
relationships of these systems. This work examines the utility of a collection
of network comparison methods for the purpose of tracking network changes in an
ABM over time or between model parameters. Among the techniques examined is a
novel graph pseudometric based on heat content asymptotics, which have been
shown to distinguish many isospectral graphs which are not isomorphic.
Additionally, we establish the use of observations about real-world networks
from network science (e.g. fat-tailed degree distribution, small-world
property) for ABM validation in the case where empirical population data is
unavailable. These methods are all demonstrated on systematic perturbations of
an original model simulating the formation of friendships in a population of
20,000 agents in Cincinnati, OH.",http://arxiv.org/pdf/2308.05256v1
2308.04978v1,cs.LG,Transferable Models for Bioacoustics with Human Language Supervision,2023-08-09 14:22:18+00:00,"Passive acoustic monitoring offers a scalable, non-invasive method for
tracking global biodiversity and anthropogenic impacts on species. Although
deep learning has become a vital tool for processing this data, current models
are inflexible, typically cover only a handful of species, and are limited by
data scarcity. In this work, we propose BioLingual, a new model for
bioacoustics based on contrastive language-audio pretraining. We first
aggregate bioacoustic archives into a language-audio dataset, called
AnimalSpeak, with over a million audio-caption pairs holding information on
species, vocalization context, and animal behavior. After training on this
dataset to connect language and audio representations, our model can identify
over a thousand species' calls across taxa, complete bioacoustic tasks
zero-shot, and retrieve animal vocalization recordings from natural text
queries. When fine-tuned, BioLingual sets a new state-of-the-art on nine tasks
in the Benchmark of Animal Sounds. Given its broad taxa coverage and ability to
be flexibly queried in human language, we believe this model opens new
paradigms in ecological monitoring and research, including free-text search on
the world's acoustic monitoring archives. We open-source our models, dataset,
and code.",http://arxiv.org/pdf/2308.04978v1
2308.04956v1,eess.IV,ACE-HetEM for ab initio Heterogenous Cryo-EM 3D Reconstruction,2023-08-09 13:41:30+00:00,"Due to the extremely low signal-to-noise ratio (SNR) and unknown poses
(projection angles and image translation) in cryo-EM experiments,
reconstructing 3D structures from 2D images is very challenging. On top of
these challenges, heterogeneous cryo-EM reconstruction also has an additional
requirement: conformation classification. An emerging solution to this problem
is called amortized inference, implemented using the autoencoder architecture
or its variants. Instead of searching for the correct
image-to-pose/conformation mapping for every image in the dataset as in
non-amortized methods, amortized inference only needs to train an encoder that
maps images to appropriate latent spaces representing poses or conformations.
Unfortunately, standard amortized-inference-based methods with entangled latent
spaces have difficulty learning the distribution of conformations and poses
from cryo-EM images. In this paper, we propose an unsupervised deep learning
architecture called ""ACE-HetEM"" based on amortized inference. To explicitly
enforce the disentanglement of conformation classifications and pose
estimations, we designed two alternating training tasks in our method:
image-to-image task and pose-to-pose task. Results on simulated datasets show
that ACE-HetEM has comparable accuracy in pose estimation and produces even
better reconstruction resolution than non-amortized methods. Furthermore, we
show that ACE-HetEM is also applicable to real experimental datasets.",http://arxiv.org/pdf/2308.04956v1
2308.05131v1,q-bio.QM,"A low-cost, user-friendly rheo-optical compression assay to measure mechanical properties of cell spheroids in standard cell culture plates",2023-08-09 12:40:58+00:00,"The mechanical characterization of cell spheroids, one of the most widely
used 3D biology models in vitro, is a hotspot of current research on the role
played by the mechanical response of cells and tissues. The techniques proposed
so far in the literature, while providing important scientific insights,
require a specialized equipment and technical skills which are not usually
available in cell biology facilities. Here, we present an innovative
rheo-optical compression assay based on microscopy glass coverslips as the load
applied to cell spheroids in standard cell culture plates and on image
acquisition with an optical microscope or even a smartphone equipped with
adequate magnification lenses. Mechanical properties can be simply obtained by
correlating the applied load to the deformation of cell spheroids measured by
image analysis. The low-cost, user-friendly features of the proposed technique
can boost mechanobiology research making it easily affordable to any biomedical
lab equipped with cell culture facilities.",http://arxiv.org/pdf/2308.05131v1
2308.05125v1,q-bio.MN,Two Novel Approaches to Detect Community: A Case Study of Omicron Lineage Variants PPI Network,2023-08-09 03:51:20+00:00,"The capacity to identify and analyze protein-protein interactions, along with
their internal modular organization, plays a crucial role in comprehending the
intricate mechanisms underlying biological processes at the molecular level. We
can learn a lot about the structure and dynamics of these interactions by using
network analysis. We can improve our understanding of the biological roots of
disease pathogenesis by recognizing network communities. This knowledge, in
turn, holds significant potential for driving advancements in drug discovery
and facilitating personalized medicine approaches for disease treatment. In
this study, we aimed to uncover the communities within the variant B.1.1.529
(Omicron virus) using two proposed novel algorithm (ABCDE and ALCDE) and four
widely recognized algorithms: Girvan-Newman, Louvain, Leiden, and Label
Propagation algorithm. Each of these algorithms has established prominence in
the field and offers unique perspectives on identifying communities within
complex networks. We also compare the networks by the global properties,
statistic summary, subgraph count, graphlet and validate by the modulaity. By
employing these approaches, we sought to gain deeper insights into the
structural organization and interconnections present within the Omicron virus
network.",http://arxiv.org/pdf/2308.05125v1
2308.04650v1,cs.LG,Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals,2023-08-09 01:30:07+00:00,"Heart failure is a debilitating condition that affects millions of people
worldwide and has a significant impact on their quality of life and mortality
rates. An objective assessment of cardiac pressures remains an important method
for the diagnosis and treatment prognostication for patients with heart
failure. Although cardiac catheterization is the gold standard for estimating
central hemodynamic pressures, it is an invasive procedure that carries
inherent risks, making it a potentially dangerous procedure for some patients.
Approaches that leverage non-invasive signals - such as electrocardiogram (ECG)
- have the promise to make the routine estimation of cardiac pressures feasible
in both inpatient and outpatient settings. Prior models trained to estimate
intracardiac pressures (e.g., mean pulmonary capillary wedge pressure (mPCWP))
in a supervised fashion have shown good discriminatory ability but have been
limited to the labeled dataset from the heart failure cohort. To address this
issue and build a robust representation, we apply deep metric learning (DML)
and propose a novel self-supervised DML with distance-based mining that
improves the performance of a model with limited labels. We use a dataset that
contains over 5.4 million ECGs without concomitant central pressure labels to
pre-train a self-supervised DML model which showed improved classification of
elevated mPCWP compared to self-supervised contrastive baselines. Additionally,
the supervised DML model that is using ECGs with access to 8,172 mPCWP labels
demonstrated significantly better performance on the mPCWP regression task
compared to the supervised baseline. Moreover, our data suggest that DML yields
models that are performant across patient subgroups, even when some patient
subgroups are under-represented in the dataset. Our code is available at
https://github.com/mandiehyewon/ssldml",http://arxiv.org/pdf/2308.04650v1
2308.04610v1,q-bio.QM,"MicroBundleCompute: Automated segmentation, tracking, and analysis of subdomain deformation in cardiac microbundles",2023-08-08 22:27:45+00:00,"Advancing human induced pluripotent stem cell derived cardiomyocyte
(hiPSC-CM) technology will lead to significant progress ranging from disease
modeling, to drug discovery, to regenerative tissue engineering. Yet, alongside
these potential opportunities comes a critical challenge: attaining mature
hiPSC-CM tissues. At present, there are multiple techniques to promote maturity
of hiPSC-CMs including physical platforms and cell culture protocols. However,
when it comes to making quantitative comparisons of functional behavior, there
are limited options for reliably and reproducibly computing functional metrics
that are suitable for direct cross-system comparison. In addition, the current
standard functional metrics obtained from time-lapse images of cardiac
microbundle contraction reported in the field (i.e., post forces, average
tissue stress) do not take full advantage of the available information present
in these data (i.e., full-field tissue displacements and strains). Thus, we
present ""MicroBundleCompute,"" a computational framework for automatic
quantification of morphology-based mechanical metrics from movies of cardiac
microbundles. Briefly, this computational framework offers tools for automatic
tissue segmentation, tracking, and analysis of brightfield and phase contrast
movies of beating cardiac microbundles. It is straightforward to implement,
requires little to no parameter tuning, and runs quickly on a personal
computer. In this paper, we describe the methods underlying this computational
framework, show the results of our extensive validation studies, and
demonstrate the utility of exploring heterogeneous tissue deformations and
strains as functional metrics. With this manuscript, we disseminate
""MicroBundleCompute"" as an open-source computational tool with the aim of
making automated quantitative analysis of beating cardiac microbundles more
accessible to the community.",http://arxiv.org/pdf/2308.04610v1
2308.05122v1,q-bio.QM,Copy Number Variation Informs fMRI-based Prediction of Autism Spectrum Disorder,2023-08-08 19:53:43+00:00,"The multifactorial etiology of autism spectrum disorder (ASD) suggests that
its study would benefit greatly from multimodal approaches that combine data
from widely varying platforms, e.g., neuroimaging, genetics, and clinical
characterization. Prior neuroimaging-genetic analyses often apply naive feature
concatenation approaches in data-driven work or use the findings from one
modality to guide posthoc analysis of another, missing the opportunity to
analyze the paired multimodal data in a truly unified approach. In this paper,
we develop a more integrative model for combining genetic, demographic, and
neuroimaging data. Inspired by the influence of genotype on phenotype, we
propose using an attention-based approach where the genetic data guides
attention to neuroimaging features of importance for model prediction. The
genetic data is derived from copy number variation parameters, while the
neuroimaging data is from functional magnetic resonance imaging. We evaluate
the proposed approach on ASD classification and severity prediction tasks,
using a sex-balanced dataset of 228 ASD and typically developing subjects in a
10-fold cross-validation framework. We demonstrate that our attention-based
model combining genetic information, demographic data, and functional magnetic
resonance imaging results in superior prediction performance compared to other
multimodal approaches.",http://arxiv.org/pdf/2308.05122v1
2308.04387v1,q-bio.QM,Harnessing Artificial Intelligence To Reduce Phototoxicity in Live Imaging,2023-08-08 16:41:30+00:00,"Fluorescence microscopy, widely used in the study of living cells, tissues,
and organisms, often faces the challenge of photodamage. This is primarily
caused by the interaction between light and biochemical components during the
imaging process, leading to compromised accuracy and reliability of biological
results. Methods necessitating extended high-intensity illumination, such as
super-resolution microscopy or thick sample imaging, are particularly
susceptible to this issue. As part of the solution to these problems, advanced
imaging approaches involving artificial intelligence (AI) have been developed.
Here we underscore the necessity of establishing constraints to maintain
light-induced damage at levels that permit cells to sustain their live
behaviour. From this perspective, data-driven live-cell imaging bears
significant potential in aiding the development of AI-enhanced
photodamage-aware microscopy. These technologies could streamline precise
observations of natural biological dynamics while minimising phototoxicity
risks.",http://arxiv.org/pdf/2308.04387v1
2308.04478v1,q-bio.QM,EasyMergeR: an interactive Shiny application to manipulate multiple XLSX files of multiple sheets,2023-08-08 15:08:55+00:00,"The integration of sequencing data with clinical information is a widely
accepted strategy in bioinformatics and health informatics. Despite advanced
databases and sophisticated tools for processing omics data, challenges remain
in handling the raw clinical data (typically in XLSX format with multiple
sheets inside), either exported from health information system (HIS) or
manually collected by investigators. This is particularly difficult for
time-constrained medical staff with little or no programming background, and it
is typically the first bottleneck in many clinical-oriented studies. To fill
this gap, we developed EasyMergeR, a simple, user-friendly, code-free R Shiny
application that allows interactive manipulation of multiple XLSX files with
multiple sheets and provides basic data manipulation capabilities based on the
tidyverse and other handy R packages.",http://arxiv.org/pdf/2308.04478v1
2308.04244v1,cs.SD,Auditory Attention Decoding with Task-Related Multi-View Contrastive Learning,2023-08-08 13:17:37+00:00,"The human brain can easily focus on one speaker and suppress others in
scenarios such as a cocktail party. Recently, researchers found that auditory
attention can be decoded from the electroencephalogram (EEG) data. However,
most existing deep learning methods are difficult to use prior knowledge of
different views (that is attended speech and EEG are task-related views) and
extract an unsatisfactory representation. Inspired by Broadbent's filter model,
we decode auditory attention in a multi-view paradigm and extract the most
relevant and important information utilizing the missing view. Specifically, we
propose an auditory attention decoding (AAD) method based on multi-view VAE
with task-related multi-view contrastive (TMC) learning. Employing TMC learning
in multi-view VAE can utilize the missing view to accumulate prior knowledge of
different views into the fusion of representation, and extract the approximate
task-related representation. We examine our method on two popular AAD datasets,
and demonstrate the superiority of our method by comparing it to the
state-of-the-art method.",http://arxiv.org/pdf/2308.04244v1
2308.05115v2,q-bio.QM,PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer,2023-08-08 07:50:38+00:00,"Phosphorylation is central to numerous fundamental cellular processes,
influencing the onset and progression of a variety of diseases. The correct
identification of these phosphorylation sites is of great importance to unravel
the intricate molecular mechanisms within cells and during viral infections,
potentially leading to the discovery of new therapeutic targets. In this study,
we introduce PTransIPs, a novel deep learning model for the identification of
phosphorylation sites. PTransIPs treat amino acids within protein sequences as
words, extracting unique encodings based on their type and sequential position.
The model also incorporates embeddings from large pretrained protein models as
additional data inputs. PTransIPS is further trained on a combination model of
convolutional neural network with residual connections and Transformer model
equipped with multi-head attention mechanisms. At last, the model outputs
classification results through a fully connected layer. The results of
independent testing reveal that PTransIPs outperforms existing
state-of-the-art(SOTA) methods, achieving AUROCs of 0.9232 and 0.9660 for
identifying phosphorylated S/T and Y sites respectively. In addition, ablation
studies prove that pretrained model embeddings contribute to the performance of
PTransIPs. Furthermore, PTransIPs has interpretable amino acid preference,
visible training process and shows generalizability on other bioactivity
classification tasks. To facilitate usage, our code and data are publicly
accessible at \url{https://github.com/StatXzy7/PTransIPs}.",http://arxiv.org/pdf/2308.05115v2
2308.03928v1,cs.LG,Optimizing the switching operation in monoclonal antibody production: Economic MPC and reinforcement learning,2023-08-07 22:12:48+00:00,"Monoclonal antibodies (mAbs) have emerged as indispensable assets in
medicine, and are currently at the forefront of biopharmaceutical product
development. However, the growing market demand and the substantial doses
required for mAb clinical treatments necessitate significant progress in its
large-scale production. Most of the processes for industrial mAb production
rely on batch operations, which result in significant downtime. The shift
towards a fully continuous and integrated manufacturing process holds the
potential to boost product yield and quality, while eliminating the extra
expenses associated with storing intermediate products. The integrated
continuous mAb production process can be divided into the upstream and
downstream processes. One crucial aspect that ensures the continuity of the
integrated process is the switching of the capture columns, which are typically
chromatography columns operated in a fed-batch manner downstream. Due to the
discrete nature of the switching operation, advanced process control algorithms
such as economic MPC (EMPC) are computationally difficult to implement. This is
because an integer nonlinear program (INLP) needs to be solved online at each
sampling time. This paper introduces two computationally-efficient approaches
for EMPC implementation, namely, a sigmoid function approximation approach and
a rectified linear unit (ReLU) approximation approach. It also explores the
application of deep reinforcement learning (DRL). These three methods are
compared to the traditional switching approach which is based on a 1% product
breakthrough rule and which involves no optimization.",http://arxiv.org/pdf/2308.03928v1
2308.03714v1,q-bio.PE,The role of APOBEC3-induced mutations in the differential evolution of monkeypox virus,2023-08-07 16:35:27+00:00,"Recent studies show that newly sampled monkeypox virus (MPXV) genomes exhibit
mutations consistent with Apolipoprotein B mRNA Editing Catalytic
Polypeptide-like3 (APOBEC3)-mediated editing, compared to MPXV genomes
collected earlier. It is unclear whether these single nucleotide polymorphisms
(SNPs) result from APOBEC3-induced editing or are a consequence of genetic
drift within one or more MPXV animal reservoirs. We develop a simple method
based on a generalization of the General-Time-Reversible (GTR) model to show
that the observed SNPs are likely the result of APOBEC3-induced editing. The
statistical features allow us to extract lineage information and estimate
evolutionary events.",http://arxiv.org/pdf/2308.03714v1
2308.03654v1,cs.CV,FFF: Fragments-Guided Flexible Fitting for Building Complete Protein Structures,2023-08-07 15:10:21+00:00,"Cryo-electron microscopy (cryo-EM) is a technique for reconstructing the
3-dimensional (3D) structure of biomolecules (especially large protein
complexes and molecular assemblies). As the resolution increases to the
near-atomic scale, building protein structures de novo from cryo-EM maps
becomes possible. Recently, recognition-based de novo building methods have
shown the potential to streamline this process. However, it cannot build a
complete structure due to the low signal-to-noise ratio (SNR) problem. At the
same time, AlphaFold has led to a great breakthrough in predicting protein
structures. This has inspired us to combine fragment recognition and structure
prediction methods to build a complete structure. In this paper, we propose a
new method named FFF that bridges protein structure prediction and protein
structure recognition with flexible fitting. First, a multi-level recognition
network is used to capture various structural features from the input 3D
cryo-EM map. Next, protein structural fragments are generated using pseudo
peptide vectors and a protein sequence alignment method based on these
extracted features. Finally, a complete structural model is constructed using
the predicted protein fragments via flexible fitting. Based on our benchmark
tests, FFF outperforms the baseline methods for building complete protein
structures.",http://arxiv.org/pdf/2308.03654v1
2308.03278v1,q-bio.QM,Key Gene Mining in Transcriptional Regulation for Specific Biological Processes with Small Sample Sizes Using Multi-network pipeline Transformer,2023-08-07 03:37:38+00:00,"Gene mining is an important topic in the field of life sciences, but
traditional machine learning methods cannot consider the regulatory
relationships between genes. Deep learning methods perform poorly in small
sample sizes. This study proposed a deep learning method, called
TransGeneSelector, that can mine critical regulatory genes involved in certain
life processes using a small-sample transcriptome dataset. The method combines
a WGAN-GP data augmentation network, a sample filtering network, and a
Transformer classifier network, which successfully classified the state
(germinating or dry seeds) of Arabidopsis thaliana seed in a dataset of 79
samples, showing performance comparable to that of Random Forests. Further,
through the use of SHapley Additive exPlanations method, TransGeneSelector
successfully mined genes involved in seed germination. Through the construction
of gene regulatory networks and the enrichment analysis of KEGG, as well as
RT-qPCR quantitative analysis, it was confirmed that these genes are at a more
upstream regulatory level than those Random Forests mined, and the top 11 genes
that were uniquely mined by TransGeneSelector were found to be related to the
KAI2 signaling pathway, which is of great regulatory importance for
germination-related genes. This study provides a practical tool for life
science researchers to mine key genes from transcriptome data.",http://arxiv.org/pdf/2308.03278v1
2308.03175v1,cs.LG,Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience,2023-08-06 18:05:39+00:00,"Machine learning (ML) has shown great promise for revolutionizing a number of
areas, including healthcare. However, it is also facing a reproducibility
crisis, especially in medicine. ML models that are carefully constructed from
and evaluated on a training set might not generalize well on data from
different patient populations or acquisition instrument settings and protocols.
We tackle this problem in the context of neuroimaging of Alzheimer's disease
(AD), schizophrenia (SZ) and brain aging. We develop a weighted empirical risk
minimization approach that optimally combines data from a source group, e.g.,
subjects are stratified by attributes such as sex, age group, race and clinical
cohort to make predictions on a target group, e.g., other sex, age group, etc.
using a small fraction (10%) of data from the target group. We apply this
method to multi-source data of 15,363 individuals from 20 neuroimaging studies
to build ML models for diagnosis of AD and SZ, and estimation of brain age. We
found that this approach achieves substantially better accuracy than existing
domain adaptation techniques: it obtains area under curve greater than 0.95 for
AD classification, area under curve greater than 0.7 for SZ classification and
mean absolute error less than 5 years for brain age prediction on all target
groups, achieving robustness to variations of scanners, protocols, and
demographic or clinical characteristics. In some cases, it is even better than
training on all data from the target group, because it leverages the diversity
and size of a larger training set. We also demonstrate the utility of our
models for prognostic tasks such as predicting disease progression in
individuals with mild cognitive impairment. Critically, our brain age
prediction models lead to new clinical insights regarding correlations with
neurophysiological tests.",http://arxiv.org/pdf/2308.03175v1
2308.03887v1,eess.IV,Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach,2023-08-04 15:57:28+00:00,"The accurate tracking of live cells using video microscopy recordings remains
a challenging task for popular state-of-the-art image processing based object
tracking methods. In recent years, several existing and new applications have
attempted to integrate deep-learning based frameworks for this task, but most
of them still heavily rely on consecutive frame based tracking embedded in
their architecture or other premises that hinder generalized learning. To
address this issue, we aimed to develop a new deep-learning based tracking
method that relies solely on the assumption that cells can be tracked based on
their spatio-temporal neighborhood, without restricting it to consecutive
frames. The proposed method has the additional benefit that the motion patterns
of the cells can be learned completely by the predictor without any prior
assumptions, and it has the potential to handle a large number of video frames
with heavy artifacts. The efficacy of the proposed method is demonstrated
through multiple biologically motivated validation strategies and compared
against several state-of-the-art cell tracking methods.",http://arxiv.org/pdf/2308.03887v1
2308.02363v1,eess.IV,Brain MRI Segmentation using Template-Based Training and Visual Perception Augmentation,2023-08-04 14:53:20+00:00,"Deep learning models usually require sufficient training data to achieve high
accuracy, but obtaining labeled data can be time-consuming and labor-intensive.
Here we introduce a template-based training method to train a 3D U-Net model
from scratch using only one population-averaged brain MRI template and its
associated segmentation label. The process incorporated visual perception
augmentation to enhance the model's robustness in handling diverse image inputs
and mitigating overfitting. Leveraging this approach, we trained 3D U-Net
models for mouse, rat, marmoset, rhesus, and human brain MRI to achieve
segmentation tasks such as skull-stripping, brain segmentation, and tissue
probability mapping. This tool effectively addresses the limited availability
of training data and holds significant potential for expanding deep learning
applications in image analysis, providing researchers with a unified solution
to train deep neural networks with only one image sample.",http://arxiv.org/pdf/2308.02363v1
2308.02589v1,q-bio.QM,An Effective Hydrodynamic Description of Marching Locusts,2023-08-03 18:56:58+00:00,"A fundamental question in complex systems is how to relate interactions
between individual components (""microscopic description"") to the global
properties of the system (""macroscopic description""). Another fundamental
question is whether such a macroscopic description exists at all and how well
it describes the large-scale properties. Here, we address these questions using
as a canonical example of a self-organizing complex system - the collective
motion of desert locusts. One of the world's most devastating insect plagues
begins when flightless juvenile locusts form ""marching bands"". Moving through
semiarid habitats in the search for food, these bands display remarkable
coordinated motion. We investigated how well physical models can describe the
flow of locusts within a band. For this, we filmed locusts within marching
bands during an outbreak in Kenya and automatically tracked all individuals
passing through the camera frame. We first analysed the spatial topology of
nearest neighbors and found individuals to be isotropically distributed.
Despite this apparent randomness, a local order was observed in regions of high
density with a clear second neighbor peak in the radial distribution function,
akin to an ordered fluid. Furthermore, reconstructing individual locust
trajectories revealed a highly-aligned movement, consistent with the
one-dimensional version of the Toner-Tu equations, which are a generalization
of the Navier-Stokes equations for fluids, used to describe the equivalent
macroscopic fluid properties of active particles. Using this effective Toner-Tu
equation, which relates the gradient of the pressure to the acceleration, we
show that the effective ""pressure"" of locusts increases as a linear function of
density in segments with highest polarization. Our study thus demonstrates an
effective hydrodynamic description of flow dynamics in plague locust swarms.",http://arxiv.org/pdf/2308.02589v1
2308.01982v1,eess.IV,"Predicting Ki67, ER, PR, and HER2 Statuses from H&E-stained Breast Cancer Images",2023-08-03 18:31:18+00:00,"Despite the advances in machine learning and digital pathology, it is not yet
clear if machine learning methods can accurately predict molecular information
merely from histomorphology. In a quest to answer this question, we built a
large-scale dataset (185538 images) with reliable measurements for Ki67, ER,
PR, and HER2 statuses. The dataset is composed of mirrored images of H\&E and
corresponding images of immunohistochemistry (IHC) assays (Ki67, ER, PR, and
HER2. These images are mirrored through registration. To increase reliability,
individual pairs were inspected and discarded if artifacts were present (tissue
folding, bubbles, etc). Measurements for Ki67, ER and PR were determined by
calculating H-Score from image analysis. HER2 measurement is based on binary
classification: 0 and 1+ (IHC scores representing a negative subset) vs 3+ (IHC
score positive subset). Cases with IHC equivocal score (2+) were excluded. We
show that a standard ViT-based pipeline can achieve prediction performances
around 90% in terms of Area Under the Curve (AUC) when trained with a proper
labeling protocol. Finally, we shed light on the ability of the trained
classifiers to localize relevant regions, which encourages future work to
improve the localizations. Our proposed dataset is publicly available:
https://ihc4bc.github.io/",http://arxiv.org/pdf/2308.01982v1
2308.01839v1,q-bio.QM,Is your data alignable? Principled and interpretable alignability testing and integration of single-cell data,2023-08-03 16:04:14+00:00,"Single-cell data integration can provide a comprehensive molecular view of
cells, and many algorithms have been developed to remove unwanted technical or
biological variations and integrate heterogeneous single-cell datasets. Despite
their wide usage, existing methods suffer from several fundamental limitations.
In particular, we lack a rigorous statistical test for whether two
high-dimensional single-cell datasets are alignable (and therefore should even
be aligned). Moreover, popular methods can substantially distort the data
during alignment, making the aligned data and downstream analysis difficult to
interpret. To overcome these limitations, we present a spectral manifold
alignment and inference (SMAI) framework, which enables principled and
interpretable alignability testing and structure-preserving integration of
single-cell data. SMAI provides a statistical test to robustly determine the
alignability between datasets to avoid misleading inference, and is justified
by high-dimensional statistical theory. On a diverse range of real and
simulated benchmark datasets, it outperforms commonly used alignment methods.
Moreover, we show that SMAI improves various downstream analyses such as
identification of differentially expressed genes and imputation of single-cell
spatial transcriptomics, providing further biological insights. SMAI's
interpretability also enables quantification and a deeper understanding of the
sources of technical confounders in single-cell data.",http://arxiv.org/pdf/2308.01839v1
2308.12354v1,q-bio.BM,Machine Learning Small Molecule Properties in Drug Discovery,2023-08-02 22:18:41+00:00,"Machine learning (ML) is a promising approach for predicting small molecule
properties in drug discovery. Here, we provide a comprehensive overview of
various ML methods introduced for this purpose in recent years. We review a
wide range of properties, including binding affinities, solubility, and ADMET
(Absorption, Distribution, Metabolism, Excretion, and Toxicity). We discuss
existing popular datasets and molecular descriptors and embeddings, such as
chemical fingerprints and graph-based neural networks. We highlight also
challenges of predicting and optimizing multiple properties during hit-to-lead
and lead optimization stages of drug discovery and explore briefly possible
multi-objective optimization techniques that can be used to balance diverse
properties while optimizing lead candidates. Finally, techniques to provide an
understanding of model predictions, especially for critical decision-making in
drug discovery are assessed. Overall, this review provides insights into the
landscape of ML models for small molecule property predictions in drug
discovery. So far, there are multiple diverse approaches, but their
performances are often comparable. Neural networks, while more flexible, do not
always outperform simpler models. This shows that the availability of
high-quality training data remains crucial for training accurate models and
there is a need for standardized benchmarks, additional performance metrics,
and best practices to enable richer comparisons between the different
techniques and models that can shed a better light on the differences between
the many techniques.",http://arxiv.org/pdf/2308.12354v1
2308.01458v1,q-bio.QM,Semi-supervised Cooperative Learning for Multiomics Data Fusion,2023-08-02 22:18:14+00:00,"Multiomics data fusion integrates diverse data modalities, ranging from
transcriptomics to proteomics, to gain a comprehensive understanding of
biological systems and enhance predictions on outcomes of interest related to
disease phenotypes and treatment responses. Cooperative learning, a recently
proposed method, unifies the commonly-used fusion approaches, including early
and late fusion, and offers a systematic framework for leveraging the shared
underlying relationships across omics to strengthen signals. However, the
challenge of acquiring large-scale labeled data remains, and there are cases
where multiomics data are available but in the absence of annotated labels. To
harness the potential of unlabeled multiomcis data, we introduce
semi-supervised cooperative learning. By utilizing an ""agreement penalty"", our
method incorporates the additional unlabeled data in the learning process and
achieves consistently superior predictive performance on simulated data and a
real multiomics study of aging. It offers an effective solution to multiomics
data fusion in settings with both labeled and unlabeled data and maximizes the
utility of available data resources, with the potential of significantly
improving predictive models for diagnostics and therapeutics in an increasingly
multiomics world.",http://arxiv.org/pdf/2308.01458v1
2308.01451v1,q-bio.NC,Identifiability in Functional Connectivity May Unintentionally Inflate Prediction Results,2023-08-02 21:59:42+00:00,"Functional magnetic resonance (fMRI) is an invaluable tool in studying
cognitive processes in vivo. Many recent studies use functional connectivity
(FC), partial correlation connectivity (PC), or fMRI-derived brain networks to
predict phenotypes with results that sometimes cannot be replicated. At the
same time, FC can be used to identify the same subject from different scans
with great accuracy. In this paper, we show a method by which one can
unknowingly inflate classification results from 61% accuracy to 86% accuracy by
treating longitudinal or contemporaneous scans of the same subject as
independent data points. Using the UK Biobank dataset, we find one can achieve
the same level of variance explained with 50 training subjects by exploiting
identifiability as with 10,000 training subjects without double-dipping. We
replicate this effect in four different datasets: the UK Biobank (UKB), the
Philadelphia Neurodevelopmental Cohort (PNC), the Bipolar and Schizophrenia
Network for Intermediate Phenotypes (BSNIP), and an OpenNeuro Fibromyalgia
dataset (Fibro). The unintentional improvement ranges between 7% and 25% in the
four datasets. Additionally, we find that by using dynamic functional
connectivity (dFC), one can apply this method even when one is limited to a
single scan per subject. One major problem is that features such as ROIs or
connectivities that are reported alongside inflated results may confuse future
work. This article hopes to shed light on how even minor pipeline anomalies may
lead to unexpectedly superb results.",http://arxiv.org/pdf/2308.01451v1
2308.01431v1,q-bio.QM,Using Single Molecule Imaging to Explore Intracellular Heterogeneity,2023-08-02 21:03:01+00:00,"Despite more than 100 years of study, it is unclear if the movement of
proteins inside the cell is best described as a mosh pit or an exquisitely
choreographed dance. Recent studies suggest the latter. Local interactions
induce molecular condensates such as liquid-liquid phase separations (LLPSs) or
non-liquid, functionally significant molecular aggregates, including synaptic
densities, nucleoli, and Amyloid fibrils. Molecular condensates trigger
intracellular signaling and drive processes ranging from gene expression to
cell division. However, the descriptions of condensates tend to be qualitative
and correlative. Here, we indicate how single-molecule imaging and analyses can
be applied to quantify condensates. We discuss the pros and cons of different
techniques for measuring differences between transient molecular behaviors
inside and outside condensates. Finally, we offer suggestions for how imaging
and analyses from different time and space regimes can be combined to identify
molecular behaviors indicative of condensates within the dynamic high-density
intracellular environment.",http://arxiv.org/pdf/2308.01431v1
2308.01362v1,q-bio.QM,Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE,2023-08-02 18:08:27+00:00,"While tumor dynamic modeling has been widely applied to support the
development of oncology drugs, there remains a need to increase predictivity,
enable personalized therapy, and improve decision-making. We propose the use of
Tumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network to
enable model discovery from longitudinal tumor size data. We show that TDNODE
overcomes a key limitation of existing models in its ability to make unbiased
predictions from truncated data. The encoder-decoder architecture is designed
to express an underlying dynamical law which possesses the fundamental property
of generalized homogeneity with respect to time. Thus, the modeling formalism
enables the encoder output to be interpreted as kinetic rate metrics, with
inverse time as the physical unit. We show that the generated metrics can be
used to predict patients' overall survival (OS) with high accuracy. The
proposed modeling formalism provides a principled way to integrate multimodal
dynamical datasets in oncology disease modeling.",http://arxiv.org/pdf/2308.01362v1
2308.01328v1,eess.IV,A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models,2023-08-02 17:05:36+00:00,"Determining lymphoma subtypes is a crucial step for better patients treatment
targeting to potentially increase their survival chances. In this context, the
existing gold standard diagnosis method, which is based on gene expression
technology, is highly expensive and time-consuming making difficult its
accessibility. Although alternative diagnosis methods based on IHC
(immunohistochemistry) technologies exist (recommended by the WHO), they still
suffer from similar limitations and are less accurate. WSI (Whole Slide Image)
analysis by deep learning models showed promising new directions for cancer
diagnosis that would be cheaper and faster than existing alternative methods.
In this work, we propose a vision transformer-based framework for
distinguishing DLBCL (Diffuse Large B-Cell Lymphoma) cancer subtypes from
high-resolution WSIs. To this end, we propose a multi-modal architecture to
train a classifier model from various WSI modalities. We then exploit this
model through a knowledge distillation mechanism for efficiently driving the
learning of a mono-modal classifier. Our experimental study conducted on a
dataset of 157 patients shows the promising performance of our mono-modal
classification model, outperforming six recent methods from the
state-of-the-art dedicated for cancer classification. Moreover, the power-law
curve, estimated on our experimental data, shows that our classification model
requires a reasonable number of additional patients for its training to
potentially reach identical diagnosis accuracy as IHC technologies.",http://arxiv.org/pdf/2308.01328v1
2308.00986v1,cond-mat.stat-mech,Ranking species in complex ecosystems through nestedness maximization,2023-08-02 07:40:39+00:00,"Identifying the rank of species in a social or ecological network is a
difficult task, since the rank of each species is invariably determined by
complex interactions stipulated with other species. Simply put, the rank of a
species is a function of the ranks of all other species through the adjacency
matrix of the network. A common system of ranking is to order species in such a
way that their neighbours form maximally nested sets, a problem called nested
maximization problem (NMP). Here we show that the NMP can be formulated as an
instance of the Quadratic Assignment Problem, one of the most important
combinatorial optimization problem widely studied in computer science,
economics, and operations research. We tackle the problem by Statistical
Physics techniques: we derive a set of self-consistent nonlinear equations
whose fixed point represents the optimal rankings of species in an arbitrary
bipartite mutualistic network, which generalize the Fitness-Complexity
equations widely used in the field of economic complexity. Furthermore, we
present an efficient algorithm to solve the NMP that outperforms
state-of-the-art network-based metrics and genetic algorithms. Eventually, our
theoretical framework may be easily generalized to study the relationship
between ranking and network structure beyond pairwise interactions, e.g. in
higher-order networks.",http://arxiv.org/pdf/2308.00986v1
2308.00699v1,quant-ph,Efficient Quantum Counting and Quantum Content-Addressable Memory for DNA similarity,2023-08-01 17:59:02+00:00,"We present QCAM, a quantum analogue of Content-Addressable Memory (CAM),
useful for finding matches in two sequences of bit-strings. Our QCAM
implementation takes advantage of Grover's search algorithm and proposes a
highly-optimized quantum circuit implementation of the QCAM oracle. Our circuit
construction uses the parallel uniformly controlled rotation gates, which were
used in previous work to generate QBArt encodings. These circuits have a high
degree of quantum parallelism which reduces their critical depth. The optimal
number of repetitions of the Grover iterator used in QCAM depends on the number
of true matches and hence is input dependent. We additionally propose a
hardware-efficient implementation of the quantum counting algorithm (HEQC) that
can infer the optimal number of Grover iterations from the measurement of a
single observable. We demonstrate the QCAM application for computing the
Jaccard similarity between two sets of k-mers obtained from two DNA sequences.",http://arxiv.org/pdf/2308.00699v1
2308.00176v1,cs.LG,A Flow Artist for High-Dimensional Cellular Data,2023-07-31 22:14:42+00:00,"We consider the problem of embedding point cloud data sampled from an
underlying manifold with an associated flow or velocity. Such data arises in
many contexts where static snapshots of dynamic entities are measured,
including in high-throughput biology such as single-cell transcriptomics.
Existing embedding techniques either do not utilize velocity information or
embed the coordinates and velocities independently, i.e., they either impose
velocities on top of an existing point embedding or embed points within a
prescribed vector field. Here we present FlowArtist, a neural network that
embeds points while jointly learning a vector field around the points. The
combination allows FlowArtist to better separate and visualize
velocity-informed structures. Our results, on toy datasets and single-cell RNA
velocity data, illustrate the value of utilizing coordinate and velocity
information in tandem for embedding and visualizing high-dimensional data.",http://arxiv.org/pdf/2308.00176v1
2307.16374v1,stat.ME,LASSO extension: using the number of non-zero coefficients to test the global model hypothesis,2023-07-31 02:38:34+00:00,"In this paper, we propose a test procedure based on the LASSO methodology to
test the global null hypothesis of no dependence between a response variable
and $p$ predictors, where $n$ observations with $n < p$ are available. The
proposed procedure is similar to the F-test for a linear model, which evaluates
significance based on the ratio of explained to unexplained variance. However,
the F-test is not suitable for models where $p \geq n$. This limitation is due
to the fact that when $p \geq n$, the unexplained variance is zero and thus the
F-statistic can no longer be calculated. In contrast, the proposed extension of
the LASSO methodology overcomes this limitation by using the number of non-zero
coefficients in the LASSO model as a test statistic after suitably specifying
the regularization parameter. The method allows reliable analysis of
high-dimensional datasets with as few as $n = 40$ observations. The performance
of the method is tested by means of a power study.",http://arxiv.org/pdf/2307.16374v1
2307.16352v2,q-bio.QM,Semi-Quantitative Group Testing for Efficient and Accurate qPCR Screening of Pathogens with a Wide Range of Loads,2023-07-31 00:18:18+00:00,"Pathogenic infections pose a significant threat to global health, affecting
millions of people every year and presenting substantial challenges to
healthcare systems worldwide. Efficient and timely testing plays a critical
role in disease control and transmission prevention. Group testing is a
well-established method for reducing the number of tests needed to screen large
populations when the disease prevalence is low. However, it does not fully
utilize the quantitative information provided by qPCR methods, nor is it able
to accommodate a wide range of pathogen loads. To address these issues, we
introduce a novel adaptive semi-quantitative group testing (SQGT) scheme to
efficiently screen populations via two-stage qPCR testing. The SQGT method
quantizes cycle threshold ($Ct$) values into multiple bins, leveraging the
information from the first stage of screening to improve the detection
sensitivity. Dynamic $Ct$ threshold adjustments mitigate dilution effects and
enhance test accuracy. Comparisons with traditional binary outcome GT methods
show that SQGT reduces the number of tests by $24$% while maintaining a
negligible false negative rate.",http://arxiv.org/pdf/2307.16352v2
2307.16182v1,q-bio.QM,Redundancy-aware unsupervised rankings for collections of gene sets,2023-07-30 09:39:42+00:00,"The biological roles of gene sets are used to group them into collections.
These collections are often characterized by being high-dimensional,
overlapping, and redundant families of sets, thus precluding a straightforward
interpretation and study of their content. Bioinformatics looked for solutions
to reduce their dimension or increase their intepretability. One possibility
lies in aggregating overlapping gene sets to create larger pathways, but the
modified biological pathways are hardly biologically justifiable. We propose to
use importance scores to rank the pathways in the collections studying the
context from a set covering perspective. The proposed Shapley values-based
scores consider the distribution of the singletons and the size of the sets in
the families; Furthermore, a trick allows us to circumvent the usual
exponential complexity of Shapley values' computation. Finally, we address the
challenge of including a redundancy awareness in the obtained rankings where,
in our case, sets are redundant if they show prominent intersections.
  The rankings can be used to reduce the dimension of collections of gene sets,
such that they show lower redundancy and still a high coverage of the genes. We
further investigate the impact of our selection on Gene Sets Enrichment
Analysis. The proposed method shows a practical utility in bioinformatics to
increase the interpretability of the collections of gene sets and a step
forward to include redundancy into Shapley values computations.",http://arxiv.org/pdf/2307.16182v1
2307.16167v1,q-bio.QM,Quantitative modeling and simulation of biochemical processes in the human body,2023-07-30 08:25:53+00:00,"We present a whole-body model of human metabolism that utilizes a system of
organs and blood vessels to simulate the enzymatic reactions. The model focuses
on key organs, including the brain, heart and lungs, liver, gut, and kidney, as
well as muscle and adipose tissue. The model equations are formulated using
stoichiometry and Michaelis-Menten kinetics to describe the enzymatic
reactions. We demonstrate how the model can be used to simulate the effects of
prolonged fasting and intermittent fasting on selected metabolite
concentrations and glucose flux. Furthermore, by simulating intermittent
fasting the effect on the carbohydrate, the protein and the lipid storage is
examined. We propose this method as a simple and intuitive approach for
modeling the human metabolism, which is general, systematic and easy to
incorporate. This could have potential applications in PK/PD drug development
and in understanding metabolic disorders.",http://arxiv.org/pdf/2307.16167v1
2307.15857v1,q-bio.QM,Parameter identifiability in PDE models of fluorescence recovery after photobleaching,2023-07-29 01:21:02+00:00,"Identifying unique parameters for mathematical models describing biological
data can be challenging and often impossible. Parameter identifiability for
partial differential equations models in cell biology is especially difficult
given that many established \textit{in vivo} measurements of protein dynamics
average out the spatial dimensions. Here, we are motivated by recent
experiments on the binding dynamics of the RNA-binding protein PTBP3 based on
fluorescence recovery after photobleaching (FRAP) measurements in RNP granules
of frog oocytes. We consider a simple reaction-diffusion model of the protein
dynamics, and show the limitations of current methods of structural and
practical parameter identifiability for this model and data. We propose a
pipeline for assessing parameter identifiability and for learning parameter
combinations based on re-parameterization and profile likelihoods analysis. We
show that this method is able to recover parameter combinations for synthetic
FRAP datasets and investigate its application to real experimental data.",http://arxiv.org/pdf/2307.15857v1
2307.15719v1,cs.LG,Identifying acute illness phenotypes via deep temporal interpolation and clustering network on physiologic signatures,2023-07-27 21:05:23+00:00,"Initial hours of hospital admission impact clinical trajectory, but early
clinical decisions often suffer due to data paucity. With clustering analysis
for vital signs within six hours of admission, patient phenotypes with distinct
pathophysiological signatures and outcomes may support early clinical
decisions. We created a single-center, longitudinal EHR dataset for 75,762
adults admitted to a tertiary care center for 6+ hours. We proposed a deep
temporal interpolation and clustering network to extract latent representations
from sparse, irregularly sampled vital sign data and derived distinct patient
phenotypes in a training cohort (n=41,502). Model and hyper-parameters were
chosen based on a validation cohort (n=17,415). Test cohort (n=16,845) was used
to analyze reproducibility and correlation with biomarkers. The training,
validation, and testing cohorts had similar distributions of age (54-55 yrs),
sex (55% female), race, comorbidities, and illness severity. Four clusters were
identified. Phenotype A (18%) had most comorbid disease with higher rate of
prolonged respiratory insufficiency, acute kidney injury, sepsis, and
three-year mortality. Phenotypes B (33%) and C (31%) had diffuse patterns of
mild organ dysfunction. Phenotype B had favorable short-term outcomes but
second-highest three-year mortality. Phenotype C had favorable clinical
outcomes. Phenotype D (17%) had early/persistent hypotension, high rate of
early surgery, and substantial biomarker rate of inflammation but second-lowest
three-year mortality. After comparing phenotypes' SOFA scores, clustering
results did not simply repeat other acuity assessments. In a heterogeneous
cohort, four phenotypes with distinct categories of disease and outcomes were
identified by a deep temporal interpolation and clustering network. This tool
may impact triage decisions and clinical decision-support under time
constraints.",http://arxiv.org/pdf/2307.15719v1
2307.14929v1,q-bio.QM,The channel capacity of the ribosome,2023-07-27 15:19:14+00:00,"Translation is one of the most fundamental processes in the biological cell.
Because of the central role that translation plays across all domains of life,
the enzyme that carries out this process, the ribosome, is required to process
information with high accuracy. This accuracy often approaches values near
unity experimentally. In this paper, we model the ribosome as an information
channel and demonstrate mathematically that this biological machine has
information-processing capabilities that have not been recognized previously.
In particular, we calculate bounds on the ribosome's theoretical Shannon
capacity and numerically approximate this capacity. Finally, by incorporating
estimates on the ribosome's operation time, we show that the ribosome operates
at speeds safely below its capacity, allowing the ribosome to process
information with an arbitrary degree of error. Our results show that the
ribosome achieves a high accuracy in line with purely information-theoretic
means.",http://arxiv.org/pdf/2307.14929v1
2307.14907v1,eess.IV,Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples,2023-07-27 14:48:02+00:00,"Human tissue and its constituent cells form a microenvironment that is
fundamentally three-dimensional (3D). However, the standard-of-care in
pathologic diagnosis involves selecting a few two-dimensional (2D) sections for
microscopic evaluation, risking sampling bias and misdiagnosis. Diverse methods
for capturing 3D tissue morphologies have been developed, but they have yet had
little translation to clinical practice; manual and computational evaluations
of such large 3D data have so far been impractical and/or unable to provide
patient-level clinical insights. Here we present Modality-Agnostic Multiple
instance learning for volumetric Block Analysis (MAMBA), a deep-learning-based
platform for processing 3D tissue images from diverse imaging modalities and
predicting patient outcomes. Archived prostate cancer specimens were imaged
with open-top light-sheet microscopy or microcomputed tomography and the
resulting 3D datasets were used to train risk-stratification networks based on
5-year biochemical recurrence outcomes via MAMBA. With the 3D block-based
approach, MAMBA achieves an area under the receiver operating characteristic
curve (AUC) of 0.86 and 0.74, superior to 2D traditional single-slice-based
prognostication (AUC of 0.79 and 0.57), suggesting superior prognostication
with 3D morphological features. Further analyses reveal that the incorporation
of greater tissue volume improves prognostic performance and mitigates risk
prediction variability from sampling bias, suggesting the value of capturing
larger extents of heterogeneous 3D morphology. With the rapid growth and
adoption of 3D spatial biology and pathology techniques by researchers and
clinicians, MAMBA provides a general and efficient framework for 3D weakly
supervised learning for clinical decision support and can help to reveal novel
3D morphological biomarkers for prognosis and therapeutic response.",http://arxiv.org/pdf/2307.14907v1
2307.14581v1,q-bio.QM,Explainable Techniques for Analyzing Flow Cytometry Cell Transformers,2023-07-27 02:03:52+00:00,"Explainability for Deep Learning Models is especially important for clinical
applications, where decisions of automated systems have far-reaching
consequences.
  While various post-hoc explainable methods, such as attention visualization
and saliency maps, already exist for common data modalities, including natural
language and images, little work has been done to adapt them to the modality of
Flow CytoMetry (FCM) data.
  In this work, we evaluate the usage of a transformer architecture called
ReluFormer that ease attention visualization as well as we propose a gradient-
and an attention-based visualization technique tailored for FCM. We
qualitatively evaluate the visualization techniques for cell classification and
polygon regression on pediatric Acute Lymphoblastic Leukemia (ALL) FCM samples.
The results outline the model's decision process and demonstrate how to utilize
the proposed techniques to inspect the trained model. The gradient-based
visualization not only identifies cells that are most significant for a
particular prediction but also indicates the directions in the FCM feature
space in which changes have the most impact on the prediction. The attention
visualization provides insights on the transformer's decision process when
handling FCM data. We show that different attention heads specialize by
attending to different biologically meaningful sub-populations in the data,
even though the model retrieved solely supervised binary classification signals
during training.",http://arxiv.org/pdf/2307.14581v1
2307.14537v1,q-bio.QM,Measuring 3D tree imbalance of plant models using graph-theoretical approaches,2023-07-26 23:03:15+00:00,"Imbalance in the 3D structure of plants can be an important indicator of
insufficient light or nutrient supply, as well as excessive wind, (formerly
present) physical barriers, neighbor or storm damage. It can also be a simple
means to detect certain illnesses, since some diseases like the apple
proliferation disease, an infection with the barley yellow dwarf virus or plant
canker can cause abnormal growth, like \enquote{witches' brooms} or burls,
resulting in a deviating 3D plant architecture. However, quantifying imbalance
of plant growth is not an easy task, and it requires a mathematically sound 3D
model of plants to which imbalance indices can be applied. Current models of
plants are often based on stacked cylinders or voxel matrices and do not allow
for measuring the degree of 3D imbalance in the branching structure of the
whole plant.
  On the other hand, various imbalance indices are readily available for
so-called graph-theoretical trees and are frequently used in areas like
phylogenetics and computer science. While only some basic ideas of these
indices can be transferred to the 3D setting, graph-theoretical trees are a
logical foundation for 3D plant models that allow for elegant and natural
imbalance measures.
  In this manuscript, our aim is thus threefold: We first present a new
graph-theoretical 3D model of plants and discuss desirable properties of
imbalance measures in the 3D setting. We then introduce and analyze eight
different imbalance indices and their properties. Thirdly, we illustrate all
our findings using a data set of 63 bush beans. Moreover, we implemented all
our indices in the publicly available \textsf{R}-software package
\textsf{treeDbalance} accompanying this manuscript.",http://arxiv.org/pdf/2307.14537v1
2307.14436v3,eess.IV,Phenotype-preserving metric design for high-content image reconstruction by generative inpainting,2023-07-26 18:13:16+00:00,"In the past decades, automated high-content microscopy demonstrated its
ability to deliver large quantities of image-based data powering the
versatility of phenotypic drug screening and systems biology applications.
However, as the sizes of image-based datasets grew, it became infeasible for
humans to control, avoid and overcome the presence of imaging and sample
preparation artefacts in the images. While novel techniques like machine
learning and deep learning may address these shortcomings through generative
image inpainting, when applied to sensitive research data this may come at the
cost of undesired image manipulation. Undesired manipulation may be caused by
phenomena such as neural hallucinations, to which some artificial neural
networks are prone. To address this, here we evaluate the state-of-the-art
inpainting methods for image restoration in a high-content fluorescence
microscopy dataset of cultured cells with labelled nuclei. We show that
architectures like DeepFill V2 and Edge Connect can faithfully restore
microscopy images upon fine-tuning with relatively little data. Our results
demonstrate that the area of the region to be restored is of higher importance
than shape. Furthermore, to control for the quality of restoration, we propose
a novel phenotype-preserving metric design strategy. In this strategy, the size
and count of the restored biological phenotypes like cell nuclei are quantified
to penalise undesirable manipulation. We argue that the design principles of
our approach may also generalise to other applications.",http://arxiv.org/pdf/2307.14436v3
2307.14099v1,q-bio.NC,Investigating structural and functional aspects of the brain's criticality in stroke,2023-07-26 10:58:53+00:00,"This paper addresses the question of the brain's critical dynamics after an
injury such as a stroke. It is hypothesized that the healthy brain operates
near a phase transition (critical point), which provides optimal conditions for
information transmission and responses to inputs. If structural damage could
cause the critical point to disappear and thus make self-organized criticality
unachievable, it would offer the theoretical explanation for the post-stroke
impairment of brain function. In our contribution, however, we demonstrate
using network models of the brain, that the dynamics remain critical even after
a stroke. In cases where the average size of the second-largest cluster of
active nodes, which is one of the commonly used indicators of criticality,
shows an anomalous behavior, it results from the loss of integrity of the
network, quantifiable within graph theory, and not from genuine non-critical
dynamics. We propose a new simple model of an artificial stroke that explains
this anomaly. The proposed interpretation of the results is confirmed by an
analysis of real connectomes acquired from post-stroke patients and a control
group. The results presented refer to neurobiological data; however, the
conclusions reached apply to a broad class of complex systems that admit a
critical state.",http://arxiv.org/pdf/2307.14099v1
2307.14396v1,q-bio.QM,Biological Modelling with Nonlocal Advection Diffusion Equations,2023-07-26 10:39:20+00:00,"The employment of nonlocal PDE models to describe biological aggregation and
other phenomena has gained considerable traction in recent years. For cell
populations, these methods grant a means of accommodating essential elements
such as cell adhesion, critical to the development and structure of tissues.
For animals, they can be used to describe how the nearby presence of
conspecifics and/or heterospecifics influence movement behaviour. In this
review, we will focus on classes of biological movement models in which the
advective (or directed) component to motion is governed by an integral term
that accounts for how the surrounding distribution(s) of the population(s)
impact on a member's movement. We recount the fundamental motivation for these
models: the intrinsic capacity of cell populations to self-organise and
spatially sort within tissues; the wide-ranging tendency of animals towards
spatial structuring, from the formations of herds and swarms to territorial
segregation. We examine the derivation of these models from an individual
level, illustrating in the process methods that allow models to be connected to
data. We explore a growing analytical literature, including methods of
stability and bifurcation analysis, and existence results. We conclude with a
short section that lays out some future challenges and connections to the
modelling of sociological phenomena including opinion dynamics.",http://arxiv.org/pdf/2307.14396v1
2307.14025v1,cs.LG,Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification,2023-07-26 08:14:18+00:00,"Diagnosing rare anemia disorders using microscopic images is challenging for
skilled specialists and machine-learning methods alike. Due to thousands of
disease-relevant cells in a single blood sample, this constitutes a complex
multiple-instance learning (MIL) problem. While the spatial neighborhood of red
blood cells is not meaningful per se, the topology, i.e., the geometry of blood
samples as a whole, contains informative features to remedy typical MIL issues,
such as vanishing gradients and overfitting when training on limited data. We
thus develop a topology-based approach that extracts multi-scale topological
features from bags of single red blood cell images. The topological features
are used to regularize the model, enforcing the preservation of characteristic
topological properties of the data. Applied to a dataset of 71 patients
suffering from rare anemia disorders with 521 microscopic images of red blood
cells, our experiments show that topological regularization is an effective
method that leads to more than 3% performance improvements for the automated
classification of rare anemia disorders based on single-cell images. This is
the first approach that uses topological properties for regularizing the MIL
process.",http://arxiv.org/pdf/2307.14025v1
2307.13918v2,stat.ML,Simulation-based Inference for Cardiovascular Models,2023-07-26 02:34:57+00:00,"Over the past decades, hemodynamics simulators have steadily evolved and have
become tools of choice for studying cardiovascular systems in-silico. While
such tools are routinely used to simulate whole-body hemodynamics from
physiological parameters, solving the corresponding inverse problem of mapping
waveforms back to plausible physiological parameters remains both promising and
challenging. Motivated by advances in simulation-based inference (SBI), we cast
this inverse problem as statistical inference. In contrast to alternative
approaches, SBI provides \textit{posterior distributions} for the parameters of
interest, providing a \textit{multi-dimensional} representation of uncertainty
for \textit{individual} measurements. We showcase this ability by performing an
in-silico uncertainty analysis of five biomarkers of clinical interest
comparing several measurement modalities. Beyond the corroboration of known
facts, such as the feasibility of estimating heart rate, our study highlights
the potential of estimating new biomarkers from standard-of-care measurements.
SBI reveals practically relevant findings that cannot be captured by standard
sensitivity analyses, such as the existence of sub-populations for which
parameter estimation exhibits distinct uncertainty regimes. Finally, we study
the gap between in-vivo and in-silico with the MIMIC-III waveform database and
critically discuss how cardiovascular simulations can inform real-world data
analysis.",http://arxiv.org/pdf/2307.13918v2
2307.14376v1,q-bio.QM,Variability in the local and global composition of human T-cell receptor repertoires during thymic development across cell types and individuals,2023-07-25 17:17:47+00:00,"The adaptive immune response relies on T cells that combine phenotypic
specialization with diversity of T cell receptors (TCRs) to recognize a wide
range of pathogens. TCRs are acquired and selected during T cell maturation in
the thymus. Characterizing TCR repertoires across individuals and T cell
maturation stages is important for better understanding adaptive immune
responses and for developing new diagnostics and therapies. Analyzing a dataset
of human TCR repertoires from thymocyte subsets, we find that the variability
between individuals generated during the TCR V(D)J recombination is maintained
through all stages of T cell maturation and differentiation. The
inter-individual variability of repertoires of the same cell type is of
comparable magnitude to the variability across cell types within the same
individual. To zoom in on smaller scales than whole repertoires, we defined a
distance measuring the relative overlap of locally similar sequences in
repertoires. We find that the whole repertoire models correctly predict local
similarity networks, suggesting a lack of forbidden T cell receptor sequences.
The local measure correlates well with distances calculated using whole
repertoire traits and carries information about cell types.",http://arxiv.org/pdf/2307.14376v1
2307.13646v1,cs.CV,"QuickQual: Lightweight, convenient retinal image quality scoring with off-the-shelf pretrained models",2023-07-25 16:55:13+00:00,"Image quality remains a key problem for both traditional and deep learning
(DL)-based approaches to retinal image analysis, but identifying poor quality
images can be time consuming and subjective. Thus, automated methods for
retinal image quality scoring (RIQS) are needed. The current state-of-the-art
is MCFNet, composed of three Densenet121 backbones each operating in a
different colour space. MCFNet, and the EyeQ dataset released by the same
authors, was a huge step forward for RIQS. We present QuickQual, a simple
approach to RIQS, consisting of a single off-the-shelf ImageNet-pretrained
Densenet121 backbone plus a Support Vector Machine (SVM). QuickQual performs
very well, setting a new state-of-the-art for EyeQ (Accuracy: 88.50% vs 88.00%
for MCFNet; AUC: 0.9687 vs 0.9588). This suggests that RIQS can be solved with
generic perceptual features learned on natural images, as opposed to requiring
DL models trained on large amounts of fundus images. Additionally, we propose a
Fixed Prior linearisation scheme, that converts EyeQ from a 3-way
classification to a continuous logistic regression task. For this task, we
present a second model, QuickQual MEga Minified Estimator (QuickQual-MEME),
that consists of only 10 parameters on top of an off-the-shelf Densenet121 and
can distinguish between gradable and ungradable images with an accuracy of
89.18% (AUC: 0.9537). Code and model are available on GitHub:
https://github.com/justinengelmann/QuickQual . QuickQual is so lightweight,
that the entire inference code (and even the parameters for QuickQual-MEME) is
already contained in this paper.",http://arxiv.org/pdf/2307.13646v1
2307.14372v1,q-bio.QM,Microbial Engineering to Mitigate Methane Emissions in Ruminant Livestock -- A Review,2023-07-25 14:09:26+00:00,"The most recent and promising strategies for mitigating methane emissions in
ruminants are reviewed highlighting the potential of reductive acetogenesis as
a viable alternative to methanogenesis. The emergence of Clustered Regularly
Interspaced Short Palindromic Repeats (CRISPR) technology, and its exceptional
precision in genome editing, further enhances the prospects of exploring this
avenue. Indeed, research in ruminant methane mitigation has been extensive, and
over the years has resulted in the development of a wide variety of mitigation
strategies. There is no doubt that the concepts of meat alternatives like
lab-meat, microbial proteins and plant proteins may produce equivalent
emissions. Reducing methane intensity through breeding and diet has been
limited by our inability to phenotype ruminants in a high-throughput manner and
the intensification of feed-food competition. Although chemical inhibitors have
demonstrated effectiveness in manipulating the rumen microbiota to reduce net
emissions, their success is constrained in terms of duration and feasibility in
grazing system. Progress in making acetogenesis the dominant hydrogen sink in
the rumen has been hampered by the thermodynamic cost of the reaction and the
limited hydrogen levels in the rumen environment. We believe that CRISPR may
allow the dominance of acetogenesis by converting methanogens into acetogens.
We propose Methanobrevibacter ruminantium to be targeted because it is the
dominant methane producer in the rumen.",http://arxiv.org/pdf/2307.14372v1
2307.14367v1,q-bio.QM,Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers,2023-07-25 09:35:43+00:00,"The complex nature of big biological systems pushed some scientists to
classify its understanding under the inconceivable missions. Different leveled
challenges complicated this task, one of is the prediction of a protein's
function. In recent years, significant progress has been made in this field
through the development of various machine learning approaches. However, most
existing methods formulate the task as a multi-classification problem, i.e
assigning predefined labels to proteins. In this work, we propose a novel
approach, \textbf{Prot2Text}, which predicts a protein function's in a free
text style, moving beyond the conventional binary or categorical
classifications. By combining Graph Neural Networks(GNNs) and Large Language
Models(LLMs), in an encoder-decoder framework, our model effectively integrates
diverse data types including proteins' sequences, structures, and textual
annotations. This multimodal approach allows for a holistic representation of
proteins' functions, enabling the generation of detailed and accurate
descriptions. To evaluate our model, we extracted a multimodal protein dataset
from SwissProt, and demonstrate empirically the effectiveness of Prot2Text.
These results highlight the transformative impact of multimodal models,
specifically the fusion of GNNs and LLMs, empowering researchers with powerful
tools for more accurate prediction of proteins' functions. The code, the models
and a demo will be publicly released.",http://arxiv.org/pdf/2307.14367v1
2307.13288v1,cs.DB,A Generic Framework for Hidden Markov Models on Biomedical Data,2023-07-25 06:50:23+00:00,"Background: Biomedical data are usually collections of longitudinal data
assessed at certain points in time. Clinical observations assess the presences
and severity of symptoms, which are the basis for description and modeling of
disease progression. Deciphering potential underlying unknowns solely from the
distinct observation would substantially improve the understanding of
pathological cascades. Hidden Markov Models (HMMs) have been successfully
applied to the processing of possibly noisy continuous signals. The aim was to
improve the application HMMs to multivariate time-series of categorically
distributed data. Here, we used HHMs to study prediction of the loss of free
walking ability as one major clinical deterioration in the most common
autosomal dominantly inherited ataxia disorder worldwide. We used HHMs to
investigate the prediction of loss of the ability to walk freely, representing
a major clinical deterioration in the most common autosomal-dominant inherited
ataxia disorder worldwide.
  Results: We present a prediction pipeline which processes data paired with a
configuration file, enabling to construct, validate and query a fully
parameterized HMM-based model. In particular, we provide a theoretical and
practical framework for multivariate time-series inference based on HMMs that
includes constructing multiple HMMs, each to predict a particular observable
variable. Our analysis is done on random data, but also on biomedical data
based on Spinocerebellar ataxia type 3 disease.
  Conclusions: HHMs are a promising approach to study biomedical data that
naturally are represented as multivariate time-series. Our implementation of a
HHMs framework is publicly available and can easily be adapted for further
applications.",http://arxiv.org/pdf/2307.13288v1
2307.13275v1,cs.LG,Curvature-based Transformer for Molecular Property Prediction,2023-07-25 06:13:01+00:00,"The prediction of molecular properties is one of the most important and
challenging tasks in the field of artificial intelligence-based drug design.
Among the current mainstream methods, the most commonly used feature
representation for training DNN models is based on SMILES and molecular graphs,
although these methods are concise and effective, they also limit the ability
to capture spatial information. In this work, we propose Curvature-based
Transformer to improve the ability of Graph Transformer neural network models
to extract structural information on molecular graph data by introducing
Discretization of Ricci Curvature. To embed the curvature in the model, we add
the curvature information of the graph as positional Encoding to the node
features during the attention-score calculation. This method can introduce
curvature information from graph data without changing the original network
architecture, and it has the potential to be extended to other models. We
performed experiments on chemical molecular datasets including PCQM4M-LST,
MoleculeNet and compared with models such as Uni-Mol, Graphormer, and the
results show that this method can achieve the state-of-the-art results. It is
proved that the discretized Ricci curvature also reflects the structural and
functional relationship while describing the local geometry of the graph
molecular data.",http://arxiv.org/pdf/2307.13275v1
2307.14361v2,q-bio.QM,"A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe",2023-07-24 21:01:46+00:00,"This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and
GloVe to classify gene mutations using Kaggle's Personalized Medicine:
Redefining Cancer Treatment dataset. The results were compared against
well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and
their LSTM ensembles. Our model outperformed all other models in terms of
accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it
also needed less training time, resulting in a perfect combination of
performance and efficiency. This study demonstrates the utility of ensemble
models for difficult tasks such as gene mutation classification.",http://arxiv.org/pdf/2307.14361v2
2308.14671v1,stat.ME,A generalized Bayesian stochastic block model for microbiome community detection,2023-08-28 15:57:59+00:00,"Advances in next-generation sequencing technology have enabled the
high-throughput profiling of metagenomes and accelerated the microbiome study.
Recently, there has been a rise in quantitative studies that aim to decipher
the microbiome co-occurrence network and its underlying community structure
based on metagenomic sequence data. Uncovering the complex microbiome community
structure is essential to understanding the role of the microbiome in disease
progression and susceptibility. Taxonomic abundance data generated from
metagenomic sequencing technologies are high-dimensional and compositional,
suffering from uneven sampling depth, over-dispersion, and zero-inflation.
These characteristics often challenge the reliability of the current methods
for microbiome community detection. To this end, we propose a Bayesian
stochastic block model to study the microbiome co-occurrence network based on
the recently developed modified centered-log ratio transformation tailored for
microbiome data analysis. Our model allows us to incorporate taxonomic tree
information using a Markov random field prior. The model parameters are jointly
inferred by using Markov chain Monte Carlo sampling techniques. Our simulation
study showed that the proposed approach performs better than competing methods
even when taxonomic tree information is non-informative. We applied our
approach to a real urinary microbiome dataset from postmenopausal women, the
first time the urinary microbiome co-occurrence network structure has been
studied. In summary, this statistical methodology provides a new tool for
facilitating advanced microbiome studies.",http://arxiv.org/pdf/2308.14671v1
2308.14106v1,stat.CO,Diffusion Schrdinger Bridges for Bayesian Computation,2023-08-27 13:22:55+00:00,"Denoising diffusion models are a novel class of generative models that have
recently become extremely popular in machine learning. In this paper, we
describe how such ideas can also be used to sample from posterior distributions
and, more generally, any target distribution whose density is known up to a
normalizing constant. The key idea is to consider a forward ``noising''
diffusion initialized at the target distribution which ``transports'' this
latter to a normal distribution for long diffusion times. The time-reversal of
this process, the ``denoising'' diffusion, thus ``transports'' the normal
distribution to the target distribution and can be approximated so as to sample
from the target. To accelerate simulation, we show how one can introduce and
approximate a Schr\""{o}dinger bridge between these two distributions, i.e. a
diffusion which transports the normal to the target in finite time.",http://arxiv.org/pdf/2308.14106v1
2308.13928v1,stat.ME,A flexible Bayesian tool for CoDa mixed models: logistic-normal distribution with Dirichlet covariance,2023-08-26 18:02:15+00:00,"Compositional Data Analysis (CoDa) has gained popularity in recent years.
This type of data consists of values from disjoint categories that sum up to a
constant. Both Dirichlet regression and logistic-normal regression have become
popular as CoDa analysis methods. However, fitting this kind of multivariate
models presents challenges, especially when structured random effects are
included in the model, such as temporal or spatial effects.
  To overcome these challenges, we propose the logistic-normal Dirichlet Model
(LNDM). We seamlessly incorporate this approach into the \textbf{R-INLA}
package, facilitating model fitting, model and model predicting within the
framework of Latent Gaussian Models (LGMs). Moreover, we explore metrics like
Deviance Information Criteria (DIC), Watanabe Akaike information criterion
(WAIC), and cross-validation measure conditional predictive ordinate (CPO) for
model selection in \textbf{R-INLA} for CoDa.
  Illustrating LNDM through a simple simulated example and with an ecological
case study on \textit{Arabidopsis thaliana} in the Iberian Peninsula, we
underscore its potential as an effective tool for managing CoDa and large CoDa
databases.",http://arxiv.org/pdf/2308.13928v1
2308.13630v1,stat.ME,Degrees of Freedom: Search Cost and Self-consistency,2023-08-25 18:55:10+00:00,"Model degrees of freedom ($\df$) is a fundamental concept in statistics
because it quantifies the flexibility of a fitting procedure and is
indispensable in model selection. The $\df$ is often intuitively equated with
the number of independent variables in the fitting procedure. But for adaptive
regressions that perform variable selection (e.g., the best subset
regressions), the model $\df$ is larger than the number of selected variables.
The excess part has been defined as the \emph{search degrees of freedom}
($\sdf$) to account for model selection. However, this definition is limited
since it does not consider fitting procedures in augmented space, such as
splines and regression trees; and it does not use the same fitting procedure
for $\sdf$ and $\df$. For example, the lasso's $\sdf$ is defined through the
\emph{relaxed} lasso's $\df$ instead of the lasso's $\df$.
  Here we propose a \emph{modified search degrees of freedom} ($\msdf$) to
directly account for the cost of searching in the original or augmented space.
Since many fitting procedures can be characterized by a linear operator, we
define the search cost as the effort to determine such a linear operator. When
we construct a linear operator for the lasso via the iterative ridge
regression, $\msdf$ offers a new perspective for its search cost. For some
complex procedures such as the multivariate adaptive regression splines (MARS),
the search cost needs to be pre-determined to serve as a tuning parameter for
the procedure itself, but it might be inaccurate. To investigate the inaccurate
pre-determined search cost, we develop two concepts, \emph{nominal} $\df$ and
\emph{actual} $\df$, and formulate a property named \emph{self-consistency}
when there is no gap between the \emph{nominal} $\df$ and the \emph{actual}
$\df$.",http://arxiv.org/pdf/2308.13630v1
2308.13564v1,econ.EM,SGMM: Stochastic Approximation to Generalized Method of Moments,2023-08-25 00:22:45+00:00,"We introduce a new class of algorithms, Stochastic Generalized Method of
Moments (SGMM), for estimation and inference on (overidentified) moment
restriction models. Our SGMM is a novel stochastic approximation alternative to
the popular Hansen (1982) (offline) GMM, and offers fast and scalable
implementation with the ability to handle streaming datasets in real time. We
establish the almost sure convergence, and the (functional) central limit
theorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we
propose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that
can be seamlessly integrated within the SGMM framework. Extensive Monte Carlo
simulations show that as the sample size increases, the SGMM matches the
standard (offline) GMM in terms of estimation accuracy and gains over
computational efficiency, indicating its practical value for both large-scale
and online datasets. We demonstrate the efficacy of our approach by a proof of
concept using two well known empirical examples with large sample sizes.",http://arxiv.org/pdf/2308.13564v1
2308.13068v1,cs.LG,Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology,2023-08-24 20:24:12+00:00,"Multivariate Time Series (MVTS) anomaly detection is a long-standing and
challenging research topic that has attracted tremendous research effort from
both industry and academia recently. However, a careful study of the literature
makes us realize that 1) the community is active but not as organized as other
sibling machine learning communities such as Computer Vision (CV) and Natural
Language Processing (NLP), and 2) most proposed solutions are evaluated using
either inappropriate or highly flawed protocols, with an apparent lack of
scientific foundation. So flawed is one very popular protocol, the so-called
\pa protocol, that a random guess can be shown to systematically outperform
\emph{all} algorithms developed so far. In this paper, we review and evaluate
many recent algorithms using more robust protocols and discuss how a normally
good protocol may have weaknesses in the context of MVTS anomaly detection and
how to mitigate them. We also share our concerns about benchmark datasets,
experiment design and evaluation methodology we observe in many works.
Furthermore, we propose a simple, yet challenging, baseline algorithm based on
Principal Components Analysis (PCA) that surprisingly outperforms many recent
Deep Learning (DL) based approaches on popular benchmark datasets. The main
objective of this work is to stimulate more effort towards important aspects of
the research such as data, experiment design, evaluation methodology and result
interpretability, instead of putting the highest weight on the design of
increasingly more complex and ""fancier"" algorithms.",http://arxiv.org/pdf/2308.13068v1
2308.13033v1,stat.CO,"A Strength and Sparsity Preserving Algorithm for Generating Weighted, Directed Networks with Predetermined Assortativity",2023-08-24 18:59:20+00:00,"Degree-preserving rewiring is a widely used technique for generating
unweighted networks with given assortativity, but for weighted networks, it is
unclear how an analog would preserve the strengths and other critical network
features such as sparsity level. This study introduces a novel approach for
rewiring weighted networks to achieve desired directed assortativity. The
method utilizes a mixed integer programming framework to establish a target
network with predetermined assortativity coefficients, followed by an efficient
rewiring algorithm termed ""strength and sparsity preserving rewiring"" (SSPR).
SSPR retains the node strength distributions and network sparsity after
rewiring. It is also possible to accommodate additional properties like edge
weight distribution with extra computational cost. The optimization scheme can
be used to determine feasible assortativity ranges for an initial network. The
effectiveness of the proposed SSPR algorithm is demonstrated through its
application to two classes of popular network models.",http://arxiv.org/pdf/2308.13033v1
2308.12944v1,quant-ph,Parallel-in-time quantum simulation via Page and Wootters quantum time,2023-08-24 17:32:41+00:00,"In the past few decades, researchers have created a veritable zoo of quantum
algorithm by drawing inspiration from classical computing, information theory,
and even from physical phenomena. Here we present quantum algorithms for
parallel-in-time simulations that are inspired by the Page and Wooters
formalism. In this framework, and thus in our algorithms, the classical
time-variable of quantum mechanics is promoted to the quantum realm by
introducing a Hilbert space of ""clock"" qubits which are then entangled with the
""system"" qubits. We show that our algorithms can compute temporal properties
over $N$ different times of many-body systems by only using $\log(N)$ clock
qubits. As such, we achieve an exponential trade-off between time and spatial
complexities. In addition, we rigorously prove that the entanglement created
between the system qubits and the clock qubits has operational meaning, as it
encodes valuable information about the system's dynamics. We also provide a
circuit depth estimation of all the protocols, showing an exponential advantage
in computation times over traditional sequential in time algorithms. In
particular, for the case when the dynamics are determined by the Aubry-Andre
model, we present a hybrid method for which our algorithms have a depth that
only scales as $\mathcal{O}(\log(N)n)$. As a by product we can relate the
previous schemes to the problem of equilibration of an isolated quantum system,
thus indicating that our framework enable a new dimension for studying
dynamical properties of many-body systems.",http://arxiv.org/pdf/2308.12944v1
2308.12470v1,stat.ME,Scalable Estimation of Multinomial Response Models with Uncertain Consideration Sets,2023-08-23 23:48:47+00:00,"A standard assumption in the fitting of unordered multinomial response models
for J mutually exclusive nominal categories, on cross-sectional or longitudinal
data, is that the responses arise from the same set of J categories between
subjects. However, when responses measure a choice made by the subject, it is
more appropriate to assume that the distribution of multinomial responses is
conditioned on a subject-specific consideration set, where this consideration
set is drawn from the power set of {1,2,...,J}. Because the cardinality of this
power set is exponential in J, estimation is infeasible in general. In this
paper, we provide an approach to overcoming this problem. A key step in the
approach is a probability model over consideration sets, based on a general
representation of probability distributions on contingency tables. Although the
support of this distribution is exponentially large, the posterior distribution
over consideration sets given parameters is typically sparse, and is easily
sampled as part of an MCMC scheme that iterates sampling of subject-specific
consideration sets given parameters, followed by parameters given consideration
sets. The effectiveness of the procedure is documented in simulated
longitudinal data sets with J=100 categories and real data from the cereal
market with J=73 brands.",http://arxiv.org/pdf/2308.12470v1
2308.11495v1,stat.AP,Evaluating the accuracy of Gaussian approximations in VSWIR imaging spectroscopy retrievals,2023-08-22 15:17:33+00:00,"The joint retrieval of surface reflectances and atmospheric parameters in
VSWIR imaging spectroscopy is a computationally challenging high-dimensional
problem. Using NASA's Surface Biology and Geology mission as the motivational
context, the uncertainty associated with the retrievals is crucial for further
application of the retrieved results for environmental applications. Although
Markov chain Monte Carlo (MCMC) is a Bayesian method ideal for uncertainty
quantification, the full-dimensional implementation of MCMC for the retrieval
is computationally intractable.
  In this work, we developed a block Metropolis MCMC algorithm for the
high-dimensional VSWIR surface reflectance retrieval that leverages the
structure of the forward radiative transfer model to enable tractable fully
Bayesian computation. We use the posterior distribution from this MCMC
algorithm to assess the limitations of optimal estimation, the state-of-the-art
Bayesian algorithm in operational retrievals which is more computationally
efficient but uses a Gaussian approximation to characterize the posterior.
Analyzing the differences in the posterior computed by each method, the MCMC
algorithm was shown to give more physically sensible results and reveals the
non-Gaussian structure of the posterior, specifically in the atmospheric
aerosol optical depth parameter and the low-wavelength surface reflectances.",http://arxiv.org/pdf/2308.11495v1
2308.10839v1,cs.CV,Vision Transformer Pruning Via Matrix Decomposition,2023-08-21 16:40:51+00:00,"This is a further development of Vision Transformer Pruning via matrix
decomposition. The purpose of the Vision Transformer Pruning is to prune the
dimension of the linear projection of the dataset by learning their associated
importance score in order to reduce the storage, run-time memory, and
computational demands. In this paper we further reduce dimension and complexity
of the linear projection by implementing and comparing several matrix
decomposition methods while preserving the generated important features. We end
up selected the Singular Value Decomposition as the method to achieve our goal
by comparing the original accuracy scores in the original Github repository and
the accuracy scores of using those matrix decomposition methods, including
Singular Value Decomposition, four versions of QR Decomposition, and LU
factorization.",http://arxiv.org/pdf/2308.10839v1
2308.10823v1,stat.ME,Simulation Experiments as a Causal Problem,2023-08-21 16:14:16+00:00,"Simulation methods are among the most ubiquitous methodological tools in
statistical science. In particular, statisticians often is simulation to
explore properties of statistical functionals in models for which developed
statistical theory is insufficient or to assess finite sample properties of
theoretical results. We show that the design of simulation experiments can be
viewed from the perspective of causal intervention on a data generating
mechanism. We then demonstrate the use of causal tools and frameworks in this
context. Our perspective is agnostic to the particular domain of the simulation
experiment which increases the potential impact of our proposed approach. In
this paper, we consider two illustrative examples. First, we re-examine a
predictive machine learning example from a popular textbook designed to assess
the relationship between mean function complexity and the mean-squared error.
Second, we discuss a traditional causal inference method problem, simulating
the effect of unmeasured confounding on estimation, specifically to illustrate
bias amplification. In both cases, applying causal principles and using
graphical models with parameters and distributions as nodes in the spirit of
influence diagrams can 1) make precise which estimand the simulation targets ,
2) suggest modifications to better attain the simulation goals, and 3) provide
scaffolding to discuss performance criteria for a particular simulation design.",http://arxiv.org/pdf/2308.10823v1
2308.11659v1,cs.LG,An engine to simulate insurance fraud network data,2023-08-21 13:14:00+00:00,"Traditionally, the detection of fraudulent insurance claims relies on
business rules and expert judgement which makes it a time-consuming and
expensive process (\'Oskarsd\'ottir et al., 2022). Consequently, researchers
have been examining ways to develop efficient and accurate analytic strategies
to flag suspicious claims. Feeding learning methods with features engineered
from the social network of parties involved in a claim is a particularly
promising strategy (see for example Van Vlasselaer et al. (2016); Tumminello et
al. (2023)). When developing a fraud detection model, however, we are
confronted with several challenges. The uncommon nature of fraud, for example,
creates a high class imbalance which complicates the development of well
performing analytic classification models. In addition, only a small number of
claims are investigated and get a label, which results in a large corpus of
unlabeled data. Yet another challenge is the lack of publicly available data.
This hinders not only the development of new methods, but also the validation
of existing techniques. We therefore design a simulation machine that is
engineered to create synthetic data with a network structure and available
covariates similar to the real life insurance fraud data set analyzed in
\'Oskarsd\'ottir et al. (2022). Further, the user has control over several
data-generating mechanisms. We can specify the total number of policyholders
and parties, the desired level of imbalance and the (effect size of the)
features in the fraud generating model. As such, the simulation engine enables
researchers and practitioners to examine several methodological challenges as
well as to test their (development strategy of) insurance fraud detection
models in a range of different settings. Moreover, large synthetic data sets
can be generated to evaluate the predictive performance of (advanced) machine
learning techniques.",http://arxiv.org/pdf/2308.11659v1
2308.10505v1,cs.LG,A Clustering Algorithm to Organize Satellite Hotspot Data for the Purpose of Tracking Bushfires Remotely,2023-08-21 06:45:58+00:00,"This paper proposes a spatiotemporal clustering algorithm and its
implementation in the R package spotoroo. This work is motivated by the
catastrophic bushfires in Australia throughout the summer of 2019-2020 and made
possible by the availability of satellite hotspot data. The algorithm is
inspired by two existing spatiotemporal clustering algorithms but makes
enhancements to cluster points spatially in conjunction with their movement
across consecutive time periods. It also allows for the adjustment of key
parameters, if required, for different locations and satellite data sources.
Bushfire data from Victoria, Australia, is used to illustrate the algorithm and
its use within the package.",http://arxiv.org/pdf/2308.10505v1
2308.10346v1,stat.ME,An Exact Sampler for Inference after Polyhedral Model Selection,2023-08-20 19:39:17+00:00,"Inference after model selection presents computational challenges when
dealing with intractable conditional distributions. Markov chain Monte Carlo
(MCMC) is a common method for sampling from these distributions, but its slow
convergence often limits its practicality. In this work, we introduce a method
tailored for selective inference in cases where the selection event can be
characterized by a polyhedron. The method transforms the variables constrained
by a polyhedron into variables within a unit cube, allowing for efficient
sampling using conventional numerical integration techniques. Compared to MCMC,
the proposed sampling method is highly accurate and equipped with an error
estimate. Additionally, we introduce an approach to use a single batch of
samples for hypothesis testing and confidence interval construction across
multiple parameters, reducing the need for repetitive sampling. Furthermore,
our method facilitates fast and precise computation of the maximum likelihood
estimator based on the selection-adjusted likelihood, enhancing the reliability
of MLE-based inference. Numerical results demonstrate the superior performance
of the proposed method compared to alternative approaches for selective
inference.",http://arxiv.org/pdf/2308.10346v1
2308.10231v1,stat.ME,Static and Dynamic BART for Rank-Order Data,2023-08-20 11:26:28+00:00,"Ranking lists are often provided at regular time intervals by one or multiple
rankers in a range of applications, including sports, marketing, and politics.
Most popular methods for rank-order data postulate a linear specification for
the latent scores, which determine the observed ranks, and ignore the temporal
dependence of the ranking lists. To address these issues, novel nonparametric
static (ROBART) and autoregressive (ARROBART) models are introduced, with
latent scores defined as nonlinear Bayesian additive regression tree functions
of covariates. To make inferences in the dynamic ARROBART model, closed-form
filtering, predictive, and smoothing distributions for the latent time-varying
scores are derived. These results are applied in a Gibbs sampler with data
augmentation for posterior inference. The proposed methods are shown to
outperform existing competitors in simulation studies, and the advantages of
the dynamic model are demonstrated by forecasts of weekly pollster rankings of
NCAA football teams.",http://arxiv.org/pdf/2308.10231v1
2308.10113v1,stat.ML,Modeling Random Networks with Heterogeneous Reciprocity,2023-08-19 21:21:25+00:00,"Reciprocity, or the tendency of individuals to mirror behavior, is a key
measure that describes information exchange in a social network. Users in
social networks tend to engage in different levels of reciprocal behavior.
Differences in such behavior may indicate the existence of communities that
reciprocate links at varying rates. In this paper, we develop methodology to
model the diverse reciprocal behavior in growing social networks. In
particular, we present a preferential attachment model with heterogeneous
reciprocity that imitates the attraction users have for popular users, plus the
heterogeneous nature by which they reciprocate links. We compare Bayesian and
frequentist model fitting techniques for large networks, as well as
computationally efficient variational alternatives. Cases where the number of
communities are known and unknown are both considered. We apply the presented
methods to the analysis of a Facebook wallpost network where users have
non-uniform reciprocal behavior patterns. The fitted model captures the
heavy-tailed nature of the empirical degree distributions in the Facebook data
and identifies multiple groups of users that differ in their tendency to reply
to and receive responses to wallposts.",http://arxiv.org/pdf/2308.10113v1
2308.09769v1,stat.CO,Pigeons.jl: Distributed Sampling From Intractable Distributions,2023-08-18 18:47:17+00:00,"We introduce a software package, Pigeons.jl, that provides a way to leverage
distributed computation to obtain samples from complicated probability
distributions, such as multimodal posteriors arising in Bayesian inference and
high-dimensional distributions in statistical mechanics. Pigeons.jl provides
simple APIs to perform such computations single-threaded, multi-threaded,
and/or distributed over thousands of MPI-communicating machines. In addition,
Pigeons.jl guarantees a property that we call strong parallelism invariance:
the output for a given seed is identical irrespective of the number of threads
and processes, which is crucial for scientific reproducibility and software
validation. We describe the key features of Pigeons.jl and the approach taken
to implement a distributed and randomized algorithm that satisfies strong
parallelism invariance.",http://arxiv.org/pdf/2308.09769v1
2308.09463v2,stat.CO,Fixed-Point Algorithms for Solving the Critical Value and Upper Tail Quantile of Kuiper's Statistics,2023-08-18 11:00:48+00:00,"Kuiper's statistic is a good measure for the difference of ideal distribution
and empirical distribution in the goodness-of-fit test. However, it is a
challenging problem to solve the critical value and upper tail quantile, or
simply Kuiper pair, of Kuiper's statistics due to the difficulties of solving
the nonlinear equation and reasonable approximation of infinite series. The
pioneering work by Kuiper just provided the key ideas and few numerical tables
created from the upper tail probability $\alpha$ and sample capacity $n$, which
limited its propagation and possible applications in various fields since there
are infinite configurations for the parameters $\alpha$ and $n$. In this work,
the contributions lie in two perspectives: firstly, the second order
approximation for the infinite series of the cumulative distribution of the
critical value is used to achieve higher precision; secondly, the principles
and fixed-point algorithms for solving the Kuiper pair are presented with
details. The algorithms are verified and validated by comparing with the table
provided by Kuiper. The methods and algorithms proposed are enlightening and
worthy of introducing to the college students, computer programmers, engineers,
experimental psychologists and so on.",http://arxiv.org/pdf/2308.09463v2
2308.10871v1,stat.CO,FunQuant: A R package to perform quantization in the context of rare events and time-consuming simulations,2023-08-18 08:34:45+00:00,"Quantization summarizes continuous distributions by calculating a discrete
approximation. Among the widely adopted methods for data quantization is
Lloyd's algorithm, which partitions the space into Vorono\""i cells, that can be
seen as clusters, and constructs a discrete distribution based on their
centroids and probabilistic masses. Lloyd's algorithm estimates the optimal
centroids in a minimal expected distance sense, but this approach poses
significant challenges in scenarios where data evaluation is costly, and
relates to rare events. Then, the single cluster associated to no event takes
the majority of the probability mass. In this context, a metamodel is required
and adapted sampling methods are necessary to increase the precision of the
computations on the rare clusters.",http://arxiv.org/pdf/2308.10871v1
2308.09102v1,cs.CE,Universal and Automatic Elbow Detection for Learning the Effective Number of Components in Model Selection Problems,2023-08-17 17:02:53+00:00,"We design a Universal Automatic Elbow Detector (UAED) for deciding the
effective number of components in model selection problems. The relationship
with the information criteria widely employed in the literature is also
discussed. The proposed UAED does not require the knowledge of a likelihood
function and can be easily applied in diverse applications, such as regression
and classification, feature and/or order selection, clustering, and dimension
reduction. Several experiments involving synthetic and real data show the
advantages of the proposed scheme with benchmark techniques in the literature.",http://arxiv.org/pdf/2308.09102v1
2308.09060v1,stat.CO,TraitLab: a Matlab package for fitting and simulating binary tree-like data,2023-08-17 15:47:56+00:00,"TraitLab is a software package for simulating, fitting and analysing
tree-like binary data under a stochastic Dollo model of evolution. The model
also allows for rate heterogeneity through catastrophes, evolutionary events
where many traits are simultaneously lost while new ones arise, and borrowing,
whereby traits transfer laterally between species as well as through ancestral
relationships. The core of the package is a Markov chain Monte Carlo (MCMC)
sampling algorithm that enables the user to sample from the Bayesian joint
posterior distribution for tree topologies, clade and root ages, and the trait
loss, catastrophe and borrowing rates for a given data set. Data can be
simulated according to the fitted Dollo model or according to a number of
generalized models that allow for heterogeneity in the trait loss rate, biases
in the data collection process and borrowing of traits between lineages.
Coupled pairs of Markov chains can be used to diagnose MCMC mixing and
convergence and to debias MCMC estimators. The raw data, MCMC run output, and
model fit can be inspected using a number of useful graphical and analytical
tools provided within the package or imported into other popular analysis
programs. TraitLab is freely available and runs within the Matlab computing
environment with its Statistics and Machine Learning toolbox, no other
additional toolboxes are required.",http://arxiv.org/pdf/2308.09060v1
2308.09009v1,econ.EM,Closed-form approximations of moments and densities of continuous-time Markov models,2023-08-17 14:25:39+00:00,"This paper develops power series expansions of a general class of moment
functions, including transition densities and option prices, of continuous-time
Markov processes, including jump--diffusions. The proposed expansions extend
the ones in Kristensen and Mele (2011) to cover general Markov processes. We
demonstrate that the class of expansions nests the transition density and
option price expansions developed in Yang, Chen, and Wan (2019) and Wan and
Yang (2021) as special cases, thereby connecting seemingly different ideas in a
unified framework. We show how the general expansion can be implemented for
fully general jump--diffusion models. We provide a new theory for the validity
of the expansions which shows that series expansions are not guaranteed to
converge as more terms are added in general. Thus, these methods should be used
with caution. At the same time, the numerical studies in this paper demonstrate
good performance of the proposed implementation in practice when a small number
of terms are included.",http://arxiv.org/pdf/2308.09009v1
2308.08852v1,math.OC,Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm,2023-08-17 08:24:28+00:00,"Graphical models have exhibited their performance in numerous tasks ranging
from biological analysis to recommender systems. However, graphical models with
hub nodes are computationally difficult to fit, particularly when the dimension
of the data is large. To efficiently estimate the hub graphical models, we
introduce a two-phase algorithm. The proposed algorithm first generates a good
initial point via a dual alternating direction method of multipliers (ADMM),
and then warm starts a semismooth Newton (SSN) based augmented Lagrangian
method (ALM) to compute a solution that is accurate enough for practical tasks.
The sparsity structure of the generalized Jacobian ensures that the algorithm
can obtain a nice solution very efficiently. Comprehensive experiments on both
synthetic data and real data show that it obviously outperforms the existing
state-of-the-art algorithms. In particular, in some high dimensional tasks, it
can save more than 70\% of the execution time, meanwhile still achieves a
high-quality estimation.",http://arxiv.org/pdf/2308.08852v1
2308.08035v1,math.NA,Gain coefficients for scrambled Halton points,2023-08-15 20:50:59+00:00,"Randomized quasi-Monte Carlo, via certain scramblings of digital nets,
produces unbiased estimates of
$\int_{[0,1]^d}f(\boldsymbol{x})\,\mathrm{d}\boldsymbol{x}$ with a variance
that is $o(1/n)$ for any $f\in L^2[0,1]^d$. It also satisfies some
non-asymptotic bounds where the variance is no larger than some $\Gamma<\infty$
times the ordinary Monte Carlo variance. For scrambled Sobol' points, this
quantity $\Gamma$ grows exponentially in $d$. For scrambled Faure points,
$\Gamma \leqslant \exp(1)\doteq 2.718$ in any dimension, but those points are
awkward to use for large $d$. This paper shows that certain scramblings of
Halton sequences have gains below an explicit bound that is $O(\log d)$ but not
$O( (\log d)^{1-\epsilon})$ for any $\epsilon>0$ as $d\to\infty$. For
$6\leqslant d\leqslant 10^6$, the upper bound on the gain coefficient is never
larger than $3/2+\log(d/2)$.",http://arxiv.org/pdf/2308.08035v1
2308.07896v2,stat.ML,SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation,2023-08-15 17:37:44+00:00,"Diffusion probabilistic models (DPMs) are a powerful class of generative
models known for their ability to generate high-fidelity image samples. A major
challenge in the implementation of DPMs is the slow sampling process. In this
work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a
score-based exact solution paradigm for the diffusion ODEs corresponding to the
sampling process of DPMs, which introduces a new perspective on developing
numerical algorithms for solving diffusion ODEs. To achieve an efficient
sampler, we propose a recursive derivative estimation (RDE) method to reduce
the estimation error. With our proposed solution paradigm and RDE method, we
propose the score-integrand solver with the convergence order guarantee as
efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver
attains state-of-the-art (SOTA) sampling performance with a limited number of
score function evaluations (NFE) on both discrete-time and continuous-time DPMs
in comparison to existing training-free sampling algorithms. Such as, we
achieve $3.48$ FID with $12$ NFE and $2.42$ FID with $20$ NFE for
continuous-time DPMs on CIFAR10, respectively. Different from other samplers,
SciRE-Solver has the promising potential to surpass the FIDs achieved in the
original papers of some pre-trained models with a small NFEs. For example, we
reach SOTA value of $2.40$ FID with $100$ NFE for continuous-time DPM and of
$3.15$ FID with $84$ NFE for discrete-time DPM on CIFAR-10, as well as of
$2.17$ ($2.02$) FID with $18$ ($50$) NFE for discrete-time DPM on CelebA
64$\times$64.",http://arxiv.org/pdf/2308.07896v2
2308.07673v1,cs.CV,A Review of Adversarial Attacks in Computer Vision,2023-08-15 09:43:10+00:00,"Deep neural networks have been widely used in various downstream tasks,
especially those safety-critical scenario such as autonomous driving, but deep
networks are often threatened by adversarial samples. Such adversarial attacks
can be invisible to human eyes, but can lead to DNN misclassification, and
often exhibits transferability between deep learning and machine learning
models and real-world achievability. Adversarial attacks can be divided into
white-box attacks, for which the attacker knows the parameters and gradient of
the model, and black-box attacks, for the latter, the attacker can only obtain
the input and output of the model. In terms of the attacker's purpose, it can
be divided into targeted attacks and non-targeted attacks, which means that the
attacker wants the model to misclassify the original sample into the specified
class, which is more practical, while the non-targeted attack just needs to
make the model misclassify the sample. The black box setting is a scenario we
will encounter in practice.",http://arxiv.org/pdf/2308.07673v1
2308.07607v1,math.OC,Quantile Optimization via Multiple Timescale Local Search for Black-box Functions,2023-08-15 07:34:00+00:00,"We consider quantile optimization of black-box functions that are estimated
with noise. We propose two new iterative three-timescale local search
algorithms. The first algorithm uses an appropriately modified
finite-difference-based gradient estimator that requires $2d$ + 1 samples of
the black-box function per iteration of the algorithm, where $d$ is the number
of decision variables (dimension of the input vector). For higher-dimensional
problems, this algorithm may not be practical if the black-box function
estimates are expensive. The second algorithm employs a
simultaneous-perturbation-based gradient estimator that uses only three samples
for each iteration regardless of problem dimension. Under appropriate
conditions, we show the almost sure convergence of both algorithms. In
addition, for the class of strongly convex functions, we further establish
their (finite-time) convergence rate through a novel fixed-point argument.
Simulation experiments indicate that the algorithms work well on a variety of
test problems and compare well with recently proposed alternative methods.",http://arxiv.org/pdf/2308.07607v1
2308.07523v1,stat.ML,Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System,2023-08-15 01:25:35+00:00,"This research introduces the Deep Operator Network (DeepONet) as a robust
surrogate modeling method within the context of digital twin (DT) systems for
nuclear engineering. With the increasing importance of nuclear energy as a
carbon-neutral solution, adopting DT technology has become crucial to enhancing
operational efficiencies, safety, and predictive capabilities in nuclear
engineering applications. DeepONet exhibits remarkable prediction accuracy,
outperforming traditional ML methods. Through extensive benchmarking and
evaluation, this study showcases the scalability and computational efficiency
of DeepONet in solving a challenging particle transport problem. By taking
functions as input data and constructing the operator $G$ from training data,
DeepONet can handle diverse and complex scenarios effectively. However, the
application of DeepONet also reveals challenges related to optimal sensor
placement and model evaluation, critical aspects of real-world implementation.
Addressing these challenges will further enhance the method's practicality and
reliability. Overall, DeepONet presents a promising and transformative tool for
nuclear engineering research and applications. Its accurate prediction and
computational efficiency capabilities can revolutionize DT systems, advancing
nuclear engineering research. This study marks an important step towards
harnessing the power of surrogate modeling techniques in critical engineering
domains.",http://arxiv.org/pdf/2308.07523v1
2308.07410v1,stat.CO,reslife: Residual Lifetime Analysis Tool in R,2023-08-14 18:57:11+00:00,"Mean residual lifetime is an important measure utilized in various fields,
including pharmaceutical companies, manufacturing companies, and insurance
companies for survival analysis. However, the computation of mean residual
lifetime can be laborious and challenging. To address this issue, the R package
reslife has been developed, which enables efficient calculation of mean
residual lifetime based on closed-form solution in a user-friendly manner.
reslife offers the capability to utilize either the results of a flexsurv
regression or user-provided parameters to compute mean residual lifetime.
Furthermore, there are options to return median and percentile residual
lifetime. If the user chooses to use the outputs of a flexsurv regression,
there is an option to input a data frame with unobserved data. In this article,
we present reslife, explain its underlying mathematical principles, illustrate
its functioning, and provide examples on how to utilize the package. The aim is
to facilitate the use of mean residual lifetime, making it more accessible and
efficient for practitioners in various disciplines, particularly those involved
in survival analysis within the pharmaceutical industry.",http://arxiv.org/pdf/2308.07410v1
2308.07196v1,cond-mat.stat-mech,Nonequilibrium phase transition of a one dimensional system reaches the absorbing state by two different ways,2023-08-14 15:05:10+00:00,"We study the nonequilibrium phase transitions from the absorbing phase to the
active phase for the model of disease spreading
(Susceptible-Infected-Refractory-Susceptible (SIRS)) on a regular one
dimensional lattice. In this model, particles of three species (S, I and R) on
a lattice react as follows: $S+I\rightarrow 2I$ with probability $\lambda$,
$I\rightarrow R$ after infection time $\tau_I$ and $R\rightarrow I$ after
recovery time $\tau_R$. In the case of $\tau_R>\tau_I$, this model has been
found to has two critical thresholds separate the active phase from absorbing
phases \cite{ali1}. The first critical threshold $\lambda_{c1}$ is
corresponding to a low infection probability and second critical threshold
$\lambda_{c2}$ is corresponding to a high infection probability. At the first
critical threshold $\lambda_{c1}$, our Monte Carlo simulations of this model
suggest the phase transition to be of directed percolation class (DP). However,
at the second critical threshold $\lambda_{c2}$ we observe that, the system
becomes so sensitive to initial values conditions which suggests the phase
transition to be discontinuous transition. We confirm this result using order
parameter quasistationary probability distribution and finite-size analysis for
this model at $\lambda_{c2}$. Additionally, the typical space-time evolution of
this model at $\lambda_{c2}$ shows that, the spreading of active particles are
compact in a behavior which remind us the spreading behavior in the compact
directed percolation.14",http://arxiv.org/pdf/2308.07196v1
2308.07176v1,stat.CO,Perfect simulation from unbiased simulation,2023-08-14 14:35:11+00:00,"We show that any application of the technique of unbiased simulation becomes
perfect simulation when coalescence of the two coupled Markov chains can be
practically assured in advance. This happens when a fixed number of iterations
is high enough that the probability of needing any more to achieve coalescence
is negligible; we suggest a value of $10^{-20}$. This finding enormously
increases the range of problems for which perfect simulation, which exactly
follows the target distribution, can be implemented. We design a new algorithm
to make practical use of the high number of iterations by producing extra
perfect sample points with little extra computational effort, at a cost of a
small, controllable amount of serial correlation within sample sets of about 20
points. Different sample sets remain completely independent. The algorithm
includes maximal coupling for continuous processes, to bring together chains
that are already close. We illustrate the methodology on a simple, two-state
Markov chain and on standard normal distributions up to 20 dimensions. Our
technical formulation involves a nonzero probability, which can be made
arbitrarily small, that a single perfect sample point may have its place taken
by a ""string"" of many points which are assigned weights, each equal to $\pm 1$,
that sum to~$1$. A point with a weight of $-1$ is a ""hole"", which is an object
that can be cancelled by an equivalent point that has the same value but
opposite weight $+1$.",http://arxiv.org/pdf/2308.07176v1
2308.06845v1,stat.CO,csSampling: An R Package for Bayesian Models for Complex Survey Data,2023-08-13 20:49:49+00:00,"We present csSampling, an R package for estimation of Bayesian models for
data collected from complex survey samples. csSampling combines functionality
from the probabilistic programming language Stan (via the rstan and brms R
packages) and the handling of complex survey data from the survey R package.
Under this approach, the user creates a survey-weighted model in brms or
provides a custom weighted model via rstan. Survey design information is
provided via the svydesign function of the survey package. The cs_sampling
function of csSampling estimates the weighted stan model and provides an
asymptotic covariance correction for model mis-specification due to using
survey sampling weights as plug-in values in the likelihood. This is often
known as a ``design effect'' which is the ratio between the variance from a
complex survey sample and a simple random sample of the same size. The
resulting adjusted posterior draws can then be used for the usual Bayesian
inference while also achieving frequentist properties of asymptotic consistency
and correct uncertainty (e.g. coverage).",http://arxiv.org/pdf/2308.06845v1
2308.05916v1,cs.CY,An Exploration of Mars Colonization with Agent-Based Modeling,2023-08-11 02:52:46+00:00,"Establishing a human settlement on Mars is an incredibly complex engineering
problem. The inhospitable nature of the Martian environment requires any
habitat to be largely self-sustaining. Beyond mining a few basic minerals and
water, the colonizers will be dependent on Earth resupply and replenishment of
necessities via technological means, i.e., splitting Martian water into oxygen
for breathing and hydrogen for fuel. Beyond the technical and engineering
challenges, future colonists will also face psychological and human behavior
challenges. Our goal is to better understand the behavioral and psychological
interactions of future Martian colonists through an Agent-Based Modeling (ABM
simulation) approach. We seek to identify areas of consideration for planning a
colony as well as propose a minimum initial population size required to create
a stable colony. Accounting for engineering and technological limitations, we
draw on research regarding high performing teams in isolated and high stress
environments (ex: submarines, Arctic exploration, ISS, war) to include the 4
basic personality types within the ABM. Interactions between agents with
different psychological profiles are modeled at the individual level, while
global events such as accidents or delays in Earth resupply affect the colony
as a whole. From our multiple simulations and scenarios (up to 28 Earth years),
we found that an initial population of 22 was the minimum required to maintain
a viable colony size over the long run. We also found that the agreeable
personality type was the one more likely to survive. We find, contrary to other
literature, that the minimum number of people with all personality types that
can lead to a sustainable settlement is in the tens and not hundreds.",http://arxiv.org/pdf/2308.05916v1
2308.05816v1,stat.CO,Snowballing Nested Sampling,2023-08-10 18:33:19+00:00,"A new way to run nested sampling, combined with realistic MCMC proposals to
generate new live points, is presented. Nested sampling is run with a fixed
number of MCMC steps. Subsequently, snowballing nested sampling extends the run
to more and more live points. This stabilizes MCMC proposals over time, and
leads to pleasant properties, including that the number of live points and
number of MCMC steps do not have to be calibrated, that the evidence and
posterior approximation improves as more compute is added and can be diagnosed
with convergence diagnostics from the MCMC literature. Snowballing nested
sampling converges to a ``perfect'' nested sampling run with infinite number of
MCMC steps.",http://arxiv.org/pdf/2308.05816v1
2308.05812v1,stat.CO,Exploring the Efficacy of Statistical and Deep Learning Methods for Large Spatial Datasets: A Case Study,2023-08-10 18:13:31+00:00,"Increasingly large and complex spatial datasets pose massive inferential
challenges due to high computational and storage costs. Our study is motivated
by the KAUST Competition on Large Spatial Datasets 2023, which tasked
participants with estimating spatial covariance-related parameters and
predicting values at testing sites, along with uncertainty estimates. We
compared various statistical and deep learning approaches through
cross-validation and ultimately selected the Vecchia approximation technique
for model fitting. To overcome the constraints in the R package GpGp, which
lacked support for fitting zero-mean Gaussian processes and direct uncertainty
estimation-two things that are necessary for the competition, we developed
additional \texttt{R} functions. Besides, we implemented certain
subsampling-based approximations and parametric smoothing for skewed sampling
distributions of the estimators. Our team DesiBoys secured victory in two out
of four sub-competitions, validating the effectiveness of our proposed
strategies. Moreover, we extended our evaluation to a large real spatial
satellite-derived dataset on total precipitable water, where we compared the
predictive performances of different models using multiple diagnostics.",http://arxiv.org/pdf/2308.05812v1
2308.05738v1,stat.CO,Continuous and Atlas-free Analysis of Brain Structural Connectivity,2023-08-10 17:57:15+00:00,"Brain structural networks are often represented as discrete adjacency
matrices with elements summarizing the connectivity between pairs of regions of
interest (ROIs). These ROIs are typically determined a-priori using a brain
atlas. The choice of atlas is often arbitrary and can lead to a loss of
important connectivity information at the sub-ROI level. This work introduces
an atlas-free framework that overcomes these issues by modeling brain
connectivity using smooth random functions. In particular, we assume that the
observed pattern of white matter fiber tract endpoints is driven by a latent
random function defined over a product manifold domain. To facilitate
statistical analysis of these high dimensional functional data objects, we
develop a novel algorithm to construct a data-driven reduced-rank function
space that offers a desirable trade-off between computational complexity and
flexibility. Using real data from the Human Connectome Project, we show that
our method outperforms state-of-the-art approaches that use the traditional
atlas-based structural connectivity representation on a variety of connectivity
analysis tasks. We further demonstrate how our method can be used to detect
localized regions and connectivity patterns associated with group differences.",http://arxiv.org/pdf/2308.05738v1
2308.05564v1,econ.EM,Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns,2023-08-10 13:24:45+00:00,"Large skew-t factor copula models are attractive for the modeling of
financial data because they allow for asymmetric and extreme tail dependence.
We show that the copula implicit in the skew-t distribution of Azzalini and
Capitanio (2003) allows for a higher level of pairwise asymmetric dependence
than two popular alternative skew-t copulas. Estimation of this copula in high
dimensions is challenging, and we propose a fast and accurate Bayesian
variational inference (VI) approach to do so. The method uses a conditionally
Gaussian generative representation of the skew-t distribution to define an
augmented posterior that can be approximated accurately. A fast stochastic
gradient ascent algorithm is used to solve the variational optimization. The
new methodology is used to estimate copula models for intraday returns from
2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity
in asymmetric dependence over equity pairs, in addition to the variability in
pairwise correlations. We show that intraday predictive densities from the
skew-t copula are more accurate than from some other copula models, while
portfolio selection strategies based on the estimated pairwise tail
dependencies improve performance relative to the benchmark index.",http://arxiv.org/pdf/2308.05564v1
2308.05373v1,math.ST,Conditional Independence Testing for Discrete Distributions: Beyond $^2$- and $G$-tests,2023-08-10 06:41:10+00:00,"This paper is concerned with the problem of conditional independence testing
for discrete data. In recent years, researchers have shed new light on this
fundamental problem, emphasizing finite-sample optimality. The non-asymptotic
viewpoint adapted in these works has led to novel conditional independence
tests that enjoy certain optimality under various regimes. Despite their
attractive theoretical properties, the considered tests are not necessarily
practical, relying on a Poissonization trick and unspecified constants in their
critical values. In this work, we attempt to bridge the gap between theory and
practice by reproving optimality without Poissonization and calibrating tests
using Monte Carlo permutations. Along the way, we also prove that classical
asymptotic $\chi^2$- and $G$-tests are notably sub-optimal in a
high-dimensional regime, which justifies the demand for new tools. Our
theoretical results are complemented by experiments on both simulated and
real-world datasets. Accompanying this paper is an R package UCI that
implements the proposed tests.",http://arxiv.org/pdf/2308.05373v1
2308.05002v1,math.ST,Deficiency bounds for the multivariate inverse hypergeometric distribution,2023-08-09 15:00:53+00:00,"The multivariate inverse hypergeometric (MIH) distribution is an extension of
the negative multinomial (NM) model that accounts for sampling without
replacement in a finite population. Even though most studies on longitudinal
count data with a specific number of `failures' occur in a finite setting, the
NM model is typically chosen over the more accurate MIH model. This raises the
question: How much information is lost when inferring with the approximate NM
model instead of the true MIH model? The loss is quantified by a measure called
deficiency in statistics. In this paper, asymptotic bounds for the deficiencies
between MIH and NM experiments are derived, as well as between MIH and the
corresponding multivariate normal experiments with the same mean-covariance
structure. The findings are supported by a local approximation for the
log-ratio of the MIH and NM probability mass functions, and by Hellinger
distance bounds.",http://arxiv.org/pdf/2308.05002v1
2308.04970v1,stat.CO,Fitting Concentric Elliptical Shapes Under General Model,2023-08-09 14:10:17+00:00,"The problem of fitting concentric ellipses is a vital problem in image
processing, pattern recognition, and astronomy. Several methods have been
developed but all address very special cases. In this paper, this problem has
been investigated under a more general setting, and two estimators for
estimating the parameters have been proposed. Since both estimators are
obtained iterative fashion, several numerical schemes are investigated and the
best initial guess is determined. Furthermore, the constraint Cram\'{e} Rao
lower bound for this problem is derived and it is compared with the variance of
each estimator. Finally, our theory is assessed and validated by a series of
numerical experiments on both real and synthetic data.",http://arxiv.org/pdf/2308.04970v1
2308.03565v2,cs.CL,Topological Interpretations of GPT-3,2023-08-07 13:16:42+00:00,"This is an experiential study of investigating a consistent method for
deriving the correlation between sentence vector and semantic meaning of a
sentence. We first used three state-of-the-art word/sentence embedding methods
including GPT-3, Word2Vec, and Sentence-BERT, to embed plain text sentence
strings into high dimensional spaces. Then we compute the pairwise distance
between any possible combination of two sentence vectors in an embedding space
and map them into a matrix. Based on each distance matrix, we compute the
correlation of distances of a sentence vector with respect to the other
sentence vectors in an embedding space. Then we compute the correlation of each
pair of the distance matrices. We observed correlations of the same sentence in
different embedding spaces and correlations of different sentences in the same
embedding space. These observations are consistent with our hypothesis and take
us to the next stage.",http://arxiv.org/pdf/2308.03565v2
2308.03100v1,math.ST,Asymptotic comparison of negative multinomial and multivariate normal experiments,2023-08-06 12:36:34+00:00,"This note presents a refined local approximation for the logarithm of the
ratio between the negative multinomial probability mass function and a
multivariate normal density, both having the same mean-covariance structure.
This approximation, which is derived using Stirling's formula and a meticulous
treatment of Taylor expansions, yields an upper bound on the Hellinger distance
between the jittered negative multinomial distribution and the corresponding
multivariate normal distribution. Upper bounds on the Le Cam distance between
negative multinomial and multivariate normal experiments ensue.",http://arxiv.org/pdf/2308.03100v1
2308.02344v1,math.ST,Learning Networks from Gaussian Graphical Models and Gaussian Free Fields,2023-08-04 14:18:39+00:00,"We investigate the problem of estimating the structure of a weighted network
from repeated measurements of a Gaussian Graphical Model (GGM) on the network.
In this vein, we consider GGMs whose covariance structures align with the
geometry of the weighted network on which they are based. Such GGMs have been
of longstanding interest in statistical physics, and are referred to as the
Gaussian Free Field (GFF). In recent years, they have attracted considerable
interest in the machine learning and theoretical computer science. In this
work, we propose a novel estimator for the weighted network (equivalently, its
Laplacian) from repeated measurements of a GFF on the network, based on the
Fourier analytic properties of the Gaussian distribution. In this pursuit, our
approach exploits complex-valued statistics constructed from observed data,
that are of interest on their own right. We demonstrate the effectiveness of
our estimator with concrete recovery guarantees and bounds on the required
sample complexity. In particular, we show that the proposed statistic achieves
the parametric rate of estimation for fixed network size. In the setting of
networks growing with sample size, our results show that for Erdos-Renyi random
graphs $G(d,p)$ above the connectivity threshold, we demonstrate that network
recovery takes place with high probability as soon as the sample size $n$
satisfies $n \gg d^4 \log d \cdot p^{-2}$.",http://arxiv.org/pdf/2308.02344v1
2308.02293v2,stat.ME,A stochastic optimization approach to train non-linear neural networks with a higher-order variation regularization,2023-08-04 12:57:13+00:00,"While highly expressive parametric models including deep neural networks have
an advantage to model complicated concepts, training such highly non-linear
models is known to yield a high risk of notorious overfitting. To address this
issue, this study considers a $(k,q)$th order variation regularization
($(k,q)$-VR), which is defined as the $q$th-powered integral of the absolute
$k$th order derivative of the parametric models to be trained; penalizing the
$(k,q)$-VR is expected to yield a smoother function, which is expected to avoid
overfitting. Particularly, $(k,q)$-VR encompasses the conventional
(general-order) total variation with $q=1$. While the $(k,q)$-VR terms applied
to general parametric models are computationally intractable due to the
integration, this study provides a stochastic optimization algorithm, that can
efficiently train general models with the $(k,q)$-VR without conducting
explicit numerical integration. The proposed approach can be applied to the
training of even deep neural networks whose structure is arbitrary, as it can
be implemented by only a simple stochastic gradient descent algorithm and
automatic differentiation. Our numerical experiments demonstrate that the
neural networks trained with the $(k,q)$-VR terms are more ``resilient'' than
those with the conventional parameter regularization. The proposed algorithm
also can be extended to the physics-informed training of neural networks
(PINNs).",http://arxiv.org/pdf/2308.02293v2
2308.02596v2,physics.soc-ph,Revisiting small-world network models: Exploring technical realizations and the equivalence of the Newman-Watts and Harary models,2023-08-04 01:08:55+00:00,"We address the relatively less known facts on the equivalence and technical
realizations surrounding two network models showing the ""small-world"" property,
namely the Newman-Watts and the Harary models. We provide the most accurate (in
terms of faithfulness to the original literature) versions of these models to
clarify the deviation from them existing in their variants adopted in one of
the most popular network analysis packages. The difference in technical
realizations of those models could be conceived as minor details, but we
discover significantly notable changes caused by the possibly inadvertent
modification. For the Harary model, the stochasticity in the original
formulation allows a much wider range of the clustering coefficient and the
average shortest path length. For the Newman-Watts model, due to the
drastically different degree distributions, the clustering coefficient can also
be affected, which is verified by our higher-order analytic derivation. During
the process, we discover the equivalence of the Newman-Watts (better known in
the network science or physics community) and the Harary (better known in the
graph theory or mathematics community) models under a specific condition of
restricted parity in variables, which would bridge the two relatively
independently developed models in different fields. Our result highlights the
importance of each detailed step in constructing network models and the
possibility of deeply related models, even if they might initially appear
distinct in terms of the time period or the academic disciplines from which
they emerged.",http://arxiv.org/pdf/2308.02596v2
2308.01796v1,stat.CO,Greedy Matroid Algorithm And Computational Persistent Homology,2023-08-03 14:50:54+00:00,"An important problem in computational topology is to calculate the homology
of a space from samples. In this work, we develop a statistical approach to
this problem by calculating the expected rank of an induced map on homology
from a sub-sample to the full space. We develop a greedy matroid algorithm for
finding an optimal basis for the image of the induced map, and investigate the
relationship between this algorithm and the probability of sampling vectors in
the image of the induced map.",http://arxiv.org/pdf/2308.01796v1
2308.01417v1,math.OC,Subgradient Langevin Methods for Sampling from Non-smooth Potentials,2023-08-02 20:30:42+00:00,"This paper is concerned with sampling from probability distributions $\pi$ on
$\mathbb{R}^d$ admitting a density of the form $\pi(x) \propto e^{-U(x)}$,
where $U(x)=F(x)+G(Kx)$ with $K$ being a linear operator and $G$ being
non-differentiable. Two different methods are proposed, both employing a
subgradient step with respect to $G\circ K$, but, depending on the regularity
of $F$, either an explicit or an implicit gradient step with respect to $F$ can
be implemented. For both methods, non-asymptotic convergence proofs are
provided, with improved convergence results for more regular $F$. Further,
numerical experiments are conducted for simple 2D examples, illustrating the
convergence rates, and for examples of Bayesian imaging, showing the practical
feasibility of the proposed methods for high dimensional data.",http://arxiv.org/pdf/2308.01417v1
2308.10831v1,q-bio.NC,"Excitatory/Inhibitory Balance Emerges as a Key Factor for RBN Performance, Overriding Attractor Dynamics",2023-08-02 17:41:58+00:00,"Reservoir computing provides a time and cost-efficient alternative to
traditional learning methods.Critical regimes, known as the ""edge of chaos,""
have been found to optimize computational performance in binary neural
networks. However, little attention has been devoted to studying
reservoir-to-reservoir variability when investigating the link between
connectivity, dynamics, and performance. As physical reservoir computers become
more prevalent, developing a systematic approach to network design is crucial.
In this article, we examine Random Boolean Networks (RBNs) and demonstrate that
specific distribution parameters can lead to diverse dynamics near critical
points. We identify distinct dynamical attractors and quantify their
statistics, revealing that most reservoirs possess a dominant attractor. We
then evaluate performance in two challenging tasks, memorization and
prediction, and find that a positive excitatory balance produces a critical
point with higher memory performance. In comparison, a negative inhibitory
balance delivers another critical point with better prediction performance.
Interestingly, we show that the intrinsic attractor dynamics have little
influence on performance in either case.",http://arxiv.org/pdf/2308.10831v1
2308.00869v1,stat.ME,Adaptive MCMC for Bayesian variable selection in generalised linear models and survival models,2023-08-01 22:38:23+00:00,"Developing an efficient computational scheme for high-dimensional Bayesian
variable selection in generalised linear models and survival models has always
been a challenging problem due to the absence of closed-form solutions for the
marginal likelihood. The RJMCMC approach can be employed to samples model and
coefficients jointly, but effective design of the transdimensional jumps of
RJMCMC can be challenge, making it hard to implement. Alternatively, the
marginal likelihood can be derived using data-augmentation scheme e.g.
Polya-gamma data argumentation for logistic regression) or through other
estimation methods. However, suitable data-augmentation schemes are not
available for every generalised linear and survival models, and using
estimations such as Laplace approximation or correlated pseudo-marginal to
derive marginal likelihood within a locally informed proposal can be
computationally expensive in the ""large n, large p"" settings. In this paper,
three main contributions are presented. Firstly, we present an extended
Point-wise implementation of Adaptive Random Neighbourhood Informed proposal
(PARNI) to efficiently sample models directly from the marginal posterior
distribution in both generalised linear models and survival models. Secondly,
in the light of the approximate Laplace approximation, we also describe an
efficient and accurate estimation method for the marginal likelihood which
involves adaptive parameters. Additionally, we describe a new method to adapt
the algorithmic tuning parameters of the PARNI proposal by replacing the
Rao-Blackwellised estimates with the combination of a warm-start estimate and
an ergodic average. We present numerous numerical results from simulated data
and 8 high-dimensional gene fine mapping data-sets to showcase the efficiency
of the novel PARNI proposal compared to the baseline add-delete-swap proposal.",http://arxiv.org/pdf/2308.00869v1
2308.00679v1,math.NA,Sharp Taylor Polynomial Enclosures in One Dimension,2023-08-01 17:32:12+00:00,"It is often useful to have polynomial upper or lower bounds on a
one-dimensional function that are valid over a finite interval, called a trust
region. A classical way to produce polynomial bounds of degree $k$ involves
bounding the range of the $k$th derivative over the trust region, but this
produces suboptimal bounds. We improve on this by deriving sharp polynomial
upper and lower bounds for a wide variety of one-dimensional functions. We
further show that sharp bounds of degree $k$ are at least $k+1$ times tighter
than those produced by the classical method, asymptotically as the width of the
trust region approaches zero. We discuss how these sharp bounds can be used in
majorization-minimization optimization, among other applications.",http://arxiv.org/pdf/2308.00679v1
2308.00251v1,stat.ML,Best-Subset Selection in Generalized Linear Models: A Fast and Consistent Algorithm via Splicing Technique,2023-08-01 03:11:31+00:00,"In high-dimensional generalized linear models, it is crucial to identify a
sparse model that adequately accounts for response variation. Although the best
subset section has been widely regarded as the Holy Grail of problems of this
type, achieving either computational efficiency or statistical guarantees is
challenging. In this article, we intend to surmount this obstacle by utilizing
a fast algorithm to select the best subset with high certainty. We proposed and
illustrated an algorithm for best subset recovery in regularity conditions.
Under mild conditions, the computational complexity of our algorithm scales
polynomially with sample size and dimension. In addition to demonstrating the
statistical properties of our method, extensive numerical experiments reveal
that it outperforms existing methods for variable selection and coefficient
estimation. The runtime analysis shows that our implementation achieves
approximately a fourfold speedup compared to popular variable selection
toolkits like glmnet and ncvreg.",http://arxiv.org/pdf/2308.00251v1
2308.00190v1,math.OC,Universal Majorization-Minimization Algorithms,2023-07-31 23:01:54+00:00,"Majorization-minimization (MM) is a family of optimization methods that
iteratively reduce a loss by minimizing a locally-tight upper bound, called a
majorizer. Traditionally, majorizers were derived by hand, and MM was only
applicable to a small number of well-studied problems. We present optimizers
that instead derive majorizers automatically, using a recent generalization of
Taylor mode automatic differentiation. These universal MM optimizers can be
applied to arbitrary problems and converge from any starting point, with no
hyperparameter tuning.",http://arxiv.org/pdf/2308.00190v1
2307.16485v1,math.ST,Parameter Inference for Degenerate Diffusion Processes,2023-07-31 08:30:43+00:00,"We study parametric inference for hypo-elliptic Stochastic Differential
Equations (SDEs). Existing research focuses on a particular class of
hypo-elliptic SDEs, with components split into `rough'/`smooth' and noise from
rough components propagating directly onto smooth ones, but some critical model
classes arising in applications have yet to be explored. We aim to cover this
gap, thus analyse the highly degenerate class of SDEs, where components split
into further sub-groups. Such models include e.g.~the notable case of
generalised Langevin equations. We propose a tailored time-discretisation
scheme and provide asymptotic results supporting our scheme in the context of
high-frequency, full observations. The proposed discretisation scheme is
applicable in much more general data regimes and is shown to overcome biases
via simulation studies also in the practical case when only a smooth component
is observed. Joint consideration of our study for highly degenerate SDEs and
existing research provides a general `recipe' for the development of
time-discretisation schemes to be used within statistical methods for general
classes of hypo-elliptic SDEs.",http://arxiv.org/pdf/2307.16485v1
2307.15424v2,cs.LG,"Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis",2023-07-28 09:17:03+00:00,"This article provides a comprehensive synthesis of the recent developments in
synthetic data generation via deep generative models, focusing on tabular
datasets. We specifically outline the importance of synthetic data generation
in the context of privacy-sensitive data. Additionally, we highlight the
advantages of using deep generative models over other methods and provide a
detailed explanation of the underlying concepts, including unsupervised
learning, neural networks, and generative models. The paper covers the
challenges and considerations involved in using deep generative models for
tabular datasets, such as data normalization, privacy concerns, and model
evaluation. This review provides a valuable resource for researchers and
practitioners interested in synthetic data generation and its applications.",http://arxiv.org/pdf/2307.15424v2
2307.14973v1,stat.ME,Insufficient Gibbs Sampling,2023-07-27 16:09:19+00:00,"In some applied scenarios, the availability of complete data is restricted,
often due to privacy concerns, and only aggregated, robust and inefficient
statistics derived from the data are accessible. These robust statistics are
not sufficient, but they demonstrate reduced sensitivity to outliers and offer
enhanced data protection due to their higher breakdown point. In this article,
operating within a parametric framework, we propose a method to sample from the
posterior distribution of parameters conditioned on different robust and
inefficient statistics: specifically, the pairs (median, MAD) or (median, IQR),
or one or more quantiles. Leveraging a Gibbs sampler and the simulation of
latent augmented data, our approach facilitates simulation according to the
posterior distribution of parameters belonging to specific families of
distributions. We demonstrate its applicability on the Gaussian, Cauchy, and
translated Weibull families.",http://arxiv.org/pdf/2307.14973v1
2307.14642v1,stat.ML,Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?,2023-07-27 06:32:43+00:00,"We prove that black-box variational inference (BBVI) with control variates,
particularly the sticking-the-landing (STL) estimator, converges at a geometric
(traditionally called ""linear"") rate under perfect variational family
specification. In particular, we prove a quadratic bound on the gradient
variance of the STL estimator, one which encompasses misspecified variational
families. Combined with previous works on the quadratic variance condition,
this directly implies convergence of BBVI with the use of projected stochastic
gradient descent. We also improve existing analysis on the regular closed-form
entropy gradient estimators, which enables comparison against the STL estimator
and provides explicit non-asymptotic complexity guarantees for both.",http://arxiv.org/pdf/2307.14642v1
2307.14160v1,stat.ME,On the application of Gaussian graphical models to paired data problems,2023-07-26 12:40:09+00:00,"Gaussian graphical models are nowadays commonly applied to the comparison of
groups sharing the same variables, by jointy learning their independence
structures. We consider the case where there are exactly two dependent groups
and the association structure is represented by a family of coloured Gaussian
graphical models suited to deal with paired data problems. To learn the two
dependent graphs, together with their across-graph association structure, we
implement a fused graphical lasso penalty. We carry out a comprehensive
analysis of this approach, with special attention to the role played by some
relevant submodel classes. In this way, we provide a broad set of tools for the
application of Gaussian graphical models to paired data problems. These include
results useful for the specification of penalty values in order to obtain a
path of lasso solutions and an ADMM algorithm that solves the fused graphical
lasso optimization problem. Finally, we present an application of our method to
cancer genomics where it is of interest to compare cancer cells with a control
sample from histologically normal tissues adjacent to the tumor. All the
methods described in this article are implemented in the $\texttt{R}$ package
$\texttt{pdglasso}$ availabe at: https://github.com/savranciati/pdglasso.",http://arxiv.org/pdf/2307.14160v1
2307.12862v1,cs.SI,Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs),2023-07-24 15:02:03+00:00,"Statistical analysis of social networks provides valuable insights into
complex network interactions across various scientific disciplines. However,
accurate modeling of networks remains challenging due to the heavy
computational burden and the need to account for observed network dependencies.
Exponential Random Graph Models (ERGMs) have emerged as a promising technique
used in social network modeling to capture network dependencies by
incorporating endogenous variables. Nevertheless, using ERGMs poses multiple
challenges, including the occurrence of ERGM degeneracy, which generates
unrealistic and meaningless network structures. To address these challenges and
enhance the modeling of collaboration networks, we propose and test a novel
approach that focuses on endogenous variable selection within ERGMs. Our method
aims to overcome the computational burden and improve the accommodation of
observed network dependencies, thereby facilitating more accurate and
meaningful interpretations of network phenomena in various scientific fields.
We conduct empirical testing and rigorous analysis to contribute to the
advancement of statistical techniques and offer practical insights for network
analysis.",http://arxiv.org/pdf/2307.12862v1
2307.12832v2,math.ST,More Power by using Fewer Permutations,2023-07-24 14:31:38+00:00,"We consider testing invariance of a distribution under an algebraic group of
transformations, which includes permutations. In this context, it is commonly
believed that one should strive to construct a test based on the entire group.
We find that one can sometimes obtain dramatically more power by replacing the
entire group with a tiny subgroup. Surprisingly, this allows us to obtain much
more power at a much lower computational cost. We examine this finding in the
popular group invariance-based Westfall & Young MaxT multiple testing method.
Studying the relative efficiency in a Gaussian location model, we find the
power gain to be largest in high-dimensional settings.",http://arxiv.org/pdf/2307.12832v2
2307.12438v2,stat.CO,Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices,2023-07-23 21:46:55+00:00,"We introduce a multifidelity estimator of covariance matrices formulated as
the solution to a regression problem on the manifold of symmetric positive
definite matrices. The estimator is positive definite by construction, and the
Mahalanobis distance minimized to obtain it possesses properties which enable
practical computation. We show that our manifold regression multifidelity
(MRMF) covariance estimator is a maximum likelihood estimator under a certain
error model on manifold tangent space. More broadly, we show that our
Riemannian regression framework encompasses existing multifidelity covariance
estimators constructed from control variates. We demonstrate via numerical
examples that our estimator can provide significant decreases, up to one order
of magnitude, in squared estimation error relative to both single-fidelity and
other multifidelity covariance estimators. Furthermore, preservation of
positive definiteness ensures that our estimator is compatible with downstream
tasks, such as data assimilation and metric learning, in which this property is
essential.",http://arxiv.org/pdf/2307.12438v2
2307.11857v1,econ.EM,Scenario Sampling for Large Supermodular Games,2023-07-21 18:51:32+00:00,"This paper introduces a simulation algorithm for evaluating the
log-likelihood function of a large supermodular binary-action game. Covered
examples include (certain types of) peer effect, technology adoption, strategic
network formation, and multi-market entry games. More generally, the algorithm
facilitates simulated maximum likelihood (SML) estimation of games with large
numbers of players, $T$, and/or many binary actions per player, $M$ (e.g.,
games with tens of thousands of strategic actions, $TM=O(10^4)$). In such cases
the likelihood of the observed pure strategy combination is typically (i) very
small and (ii) a $TM$-fold integral who region of integration has a complicated
geometry. Direct numerical integration, as well as accept-reject Monte Carlo
integration, are computationally impractical in such settings. In contrast, we
introduce a novel importance sampling algorithm which allows for accurate
likelihood simulation with modest numbers of simulation draws.",http://arxiv.org/pdf/2307.11857v1
2307.11682v1,stat.ME,Longitudinal Data Clustering with a Copula Kernel Mixture Model,2023-07-21 16:36:34+00:00,"Many common clustering methods cannot be used for clustering multivariate
longitudinal data in cases where variables exhibit high autocorrelations. In
this article, a copula kernel mixture model (CKMM) is proposed for clustering
data of this type. The CKMM is a finite mixture model which decomposes each
mixture component's joint density function into its copula and marginal
distribution functions. In this decomposition, the Gaussian copula is used due
to its mathematical tractability and Gaussian kernel functions are used to
estimate the marginal distributions. A generalized expectation-maximization
algorithm is used to estimate the model parameters. The performance of the
proposed model is assessed in a simulation study and on two real datasets. The
proposed model is shown to have effective performance in comparison to standard
methods, such as K-means with dynamic time warping clustering and latent growth
models.",http://arxiv.org/pdf/2307.11682v1
2307.11648v1,stat.CO,Sparse Cholesky factorization by greedy conditional selection,2023-07-21 15:27:17+00:00,"Dense kernel matrices resulting from pairwise evaluations of a kernel
function arise naturally in machine learning and statistics. Previous work in
constructing sparse approximate inverse Cholesky factors of such matrices by
minimizing Kullback-Leibler divergence recovers the Vecchia approximation for
Gaussian processes. These methods rely only on the geometry of the evaluation
points to construct the sparsity pattern. In this work, we instead construct
the sparsity pattern by leveraging a greedy selection algorithm that maximizes
mutual information with target points, conditional on all points previously
selected. For selecting $k$ points out of $N$, the naive time complexity is
$\mathcal{O}(N k^4)$, but by maintaining a partial Cholesky factor we reduce
this to $\mathcal{O}(N k^2)$. Furthermore, for multiple ($m$) targets we
achieve a time complexity of $\mathcal{O}(N k^2 + N m^2 + m^3)$, which is
maintained in the setting of aggregated Cholesky factorization where a selected
point need not condition every target. We apply the selection algorithm to
image classification and recovery of sparse Cholesky factors. By minimizing
Kullback-Leibler divergence, we apply the algorithm to Cholesky factorization,
Gaussian process regression, and preconditioning with the conjugate gradient,
improving over $k$-nearest neighbors selection.",http://arxiv.org/pdf/2307.11648v1
2307.11553v1,stat.CO,Adaptively switching between a particle marginal Metropolis-Hastings and a particle Gibbs kernel in SMC$^2$,2023-07-21 12:59:25+00:00,"Sequential Monte Carlo squared (SMC$^2$; Chopin et al., 2012) methods can be
used to sample from the exact posterior distribution of intractable likelihood
state space models. These methods are the SMC analogue to particle Markov chain
Monte Carlo (MCMC; Andrieu et al., 2010) and rely on particle MCMC kernels to
mutate the particles at each iteration. Two options for the particle MCMC
kernels are particle marginal Metropolis-Hastings (PMMH) and particle Gibbs
(PG). We introduce a method to adaptively select the particle MCMC kernel at
each iteration of SMC$^2$, with a particular focus on switching between a PMMH
and PG kernel. The resulting method can significantly improve the efficiency of
SMC$^2$ compared to using a fixed particle MCMC kernel throughout the
algorithm. Code for our methods is available at
https://github.com/imkebotha/kernel_switching_smc2.",http://arxiv.org/pdf/2307.11553v1
2307.11255v1,stat.ME,A Framework for Statistical Inference via Randomized Algorithms,2023-07-20 22:11:42+00:00,"Randomized algorithms, such as randomized sketching or projections, are a
promising approach to ease the computational burden in analyzing large
datasets. However, randomized algorithms also produce non-deterministic
outputs, leading to the problem of evaluating their accuracy. In this paper, we
develop a statistical inference framework for quantifying the uncertainty of
the outputs of randomized algorithms. We develop appropriate statistical
methods -- sub-randomization, multi-run plug-in and multi-run aggregation
inference -- by using multiple runs of the same randomized algorithm, or by
estimating the unknown parameters of the limiting distribution. As an example,
we develop methods for statistical inference for least squares parameters via
random sketching using matrices with i.i.d.entries, or uniform partial
orthogonal matrices. For this, we characterize the limiting distribution of
estimators obtained via sketch-and-solve as well as partial sketching methods.
The analysis of i.i.d. sketches uses a trigonometric interpolation argument to
establish a differential equation for the limiting expected characteristic
function and find the dependence on the kurtosis of the entries of the
sketching matrix. The results are supported via a broad range of simulations.",http://arxiv.org/pdf/2307.11255v1
2307.10694v1,econ.EM,PySDTest: a Python Package for Stochastic Dominance Tests,2023-07-20 08:37:20+00:00,"We introduce PySDTest, a Python package for statistical tests of stochastic
dominance. PySDTest can implement the testing procedures of Barrett and Donald
(2003), Linton et al. (2005), Linton et al. (2010), Donald and Hsu (2016), and
their extensions. PySDTest provides several options to compute the critical
values including bootstrap, subsampling, and numerical delta methods. In
addition, PySDTest allows various notions of the stochastic dominance
hypothesis, including stochastic maximality among multiple prospects and
prospect dominance. We briefly give an overview of the concepts of stochastic
dominance and testing methods. We then provide a practical guidance for using
PySDTest. For an empirical illustration, we apply PySDTest to the portfolio
choice problem between the daily returns of Bitcoin and S&P 500 index. We find
that the S&P 500 index returns second-order stochastically dominate the Bitcoin
returns.",http://arxiv.org/pdf/2307.10694v1
2307.10673v1,stat.CO,Sparse model-based clustering of three-way data via lasso-type penalties,2023-07-20 07:55:03+00:00,"Mixtures of matrix Gaussian distributions provide a probabilistic framework
for clustering continuous matrix-variate data, which are becoming increasingly
prevalent in various fields. Despite its widespread adoption and successful
application, this approach suffers from over-parameterization issues, making it
less suitable even for matrix-variate data of moderate size. To overcome this
drawback, we introduce a sparse model-based clustering approach for three-way
data. Our approach assumes that the matrix mixture parameters are sparse and
have different degree of sparsity across clusters, allowing to induce parsimony
in a flexible manner. Estimation of the model relies on the maximization of a
penalized likelihood, with specifically tailored group and graphical lasso
penalties. These penalties enable the selection of the most informative
features for clustering three-way data where variables are recorded over
multiple occasions and allow to capture cluster-specific association
structures. The proposed methodology is tested extensively on synthetic data
and its validity is demonstrated in application to time-dependent crime
patterns in different US cities.",http://arxiv.org/pdf/2307.10673v1
2307.10462v1,stat.ME,Lasso and elastic nets by orthants,2023-07-19 21:15:49+00:00,"We propose a new method for computing the lasso path, using the fact that the
Manhattan norm of the coefficient vector is linear over every orthant of the
parameter space. We use simple calculus and present an algorithm in which the
lasso path is series of orthant moves. Our proposal gives the same results as
standard literature, with the advantage of neat interpretation of results and
explicit lasso formul{\ae}. We extend this proposal to elastic nets and obtain
explicit, exact formul{\ae} for the elastic net path, and with a simple change,
our lasso algorithm can be used for elastic nets. We present computational
examples and provide simple R prototype code.",http://arxiv.org/pdf/2307.10462v1
2307.10436v1,stat.ML,A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks,2023-07-19 20:00:00+00:00,"Deep Learners (DLs) are the state-of-art predictive mechanism with
applications in many fields requiring complex high dimensional data processing.
Although conventional DLs get trained via gradient descent with
back-propagation, Kalman Filter (KF)-based techniques that do not need gradient
computation have been developed to approximate DLs. We propose a multi-arm
extension of a KF-based DL approximator that can mimic DL when the sample size
is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman
Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking
that becomes relevant when the training sample has an unequal-size feature set.
Our proposed technique can approximate Long Short-term Memory (LSTM) Networks
and attach uncertainty to the predictions obtained from these LSTMs with
desirable coverage. We demonstrate how MEnKF-ANN can ""adequately"" approximate
an LSTM network trained to classify what carbohydrate substrates are digested
and utilized by a microbiome sample whose genomic sequences consist of
polysaccharide utilization loci (PULs) and their encoded genes.",http://arxiv.org/pdf/2307.10436v1
2307.10411v1,stat.CO,Stop Simulating! Efficient Computation of Tournament Winning Probabilities,2023-07-19 18:42:34+00:00,"In the run-up to any major sports tournament, winning probabilities of
participants are publicized for engagement and betting purposes. These are
generally based on simulating the tournament tens of thousands of times by
sampling from single-match outcome models. We show that, by virtue of the
tournament schedule, exact computation of winning probabilties can be
substantially faster than their approximation through simulation. This notably
applies to the 2022 and 2023 FIFA World Cup Finals, and is independent of the
model used for individual match outcomes.",http://arxiv.org/pdf/2307.10411v1
2307.10099v1,math.ST,Memory Efficient And Minimax Distribution Estimation Under Wasserstein Distance Using Bayesian Histograms,2023-07-19 16:13:20+00:00,"We study Bayesian histograms for distribution estimation on $[0,1]^d$ under
the Wasserstein $W_v, 1 \leq v < \infty$ distance in the i.i.d sampling regime.
We newly show that when $d < 2v$, histograms possess a special \textit{memory
efficiency} property, whereby in reference to the sample size $n$, order
$n^{d/2v}$ bins are needed to obtain minimax rate optimality. This result holds
for the posterior mean histogram and with respect to posterior contraction:
under the class of Borel probability measures and some classes of smooth
densities. The attained memory footprint overcomes existing minimax optimal
procedures by a polynomial factor in $n$; for example an $n^{1 - d/2v}$ factor
reduction in the footprint when compared to the empirical measure, a minimax
estimator in the Borel probability measure class. Additionally constructing
both the posterior mean histogram and the posterior itself can be done
super--linearly in $n$. Due to the popularity of the $W_1,W_2$ metrics and the
coverage provided by the $d < 2v$ case, our results are of most practical
interest in the $(d=1,v =1,2), (d=2,v=2), (d=3,v=2)$ settings and we provide
simulations demonstrating the theory in several of these instances.",http://arxiv.org/pdf/2307.10099v1
2307.10065v1,stat.ME,Entropy regularization in probabilistic clustering,2023-07-19 15:36:40+00:00,"Bayesian nonparametric mixture models are widely used to cluster
observations. However, one major drawback of the approach is that the estimated
partition often presents unbalanced clusters' frequencies with only a few
dominating clusters and a large number of sparsely-populated ones. This feature
translates into results that are often uninterpretable unless we accept to
ignore a relevant number of observations and clusters. Interpreting the
posterior distribution as penalized likelihood, we show how the unbalance can
be explained as a direct consequence of the cost functions involved in
estimating the partition. In light of our findings, we propose a novel Bayesian
estimator of the clustering configuration. The proposed estimator is equivalent
to a post-processing procedure that reduces the number of sparsely-populated
clusters and enhances interpretability. The procedure takes the form of
entropy-regularization of the Bayesian estimate. While being computationally
convenient with respect to alternative strategies, it is also theoretically
justified as a correction to the Bayesian loss function used for point
estimation and, as such, can be applied to any posterior distribution of
clusters, regardless of the specific model used.",http://arxiv.org/pdf/2307.10065v1
2307.09341v1,stat.CO,Adaptively Optimised Adaptive Importance Samplers,2023-07-18 15:26:42+00:00,"We introduce a new class of adaptive importance samplers leveraging adaptive
optimisation tools, which we term AdaOAIS. We build on Optimised Adaptive
Importance Samplers (OAIS), a class of techniques that adapt proposals to
improve the mean-squared error of the importance sampling estimators by
parameterising the proposal and optimising the $\chi^2$-divergence between the
target and the proposal. We show that a naive implementation of OAIS using
stochastic gradient descent may lead to unstable estimators despite its
convergence guarantees. To remedy this shortcoming, we instead propose to use
adaptive optimisers (such as AdaGrad and Adam) to improve the stability of the
OAIS. We provide convergence results for AdaOAIS in a similar manner to OAIS.
We also provide empirical demonstration on a variety of examples and show that
AdaOAIS lead to stable importance sampling estimators in practice.",http://arxiv.org/pdf/2307.09341v1
2307.08997v1,stat.ME,Deterministic Objective Bayesian Analysis for Spatial Models,2023-07-18 06:31:19+00:00,"Berger et al. (2001) and Ren et al. (2012) derived noninformative priors for
Gaussian process models of spatially correlated data using the reference prior
approach (Berger, Bernardo, 1991). The priors have good statistical properties
and provide a basis for objective Bayesian analysis (Berger, 2006). Using a
trust-region algorithm for optimization with exact equations for posterior
derivatives and an adaptive sparse grid at Chebyshev nodes, this paper develops
deterministic algorithms for fully Bayesian prediction and inference with the
priors. Implementations of the algorithms are available at
https://github.com/rnburn/bbai.",http://arxiv.org/pdf/2307.08997v1
2307.08609v1,math.ST,"Overlapping Batch Confidence Intervals on Statistical Functionals Constructed from Time Series: Application to Quantiles, Optimization, and Estimation",2023-07-17 16:21:48+00:00,"We propose a general purpose confidence interval procedure (CIP) for
statistical functionals constructed using data from a stationary time series.
The procedures we propose are based on derived distribution-free analogues of
the $\chi^2$ and Student's $t$ random variables for the statistical functional
context, and hence apply in a wide variety of settings including quantile
estimation, gradient estimation, M-estimation, CVAR-estimation, and arrival
process rate estimation, apart from more traditional statistical settings. Like
the method of subsampling, we use overlapping batches of time series data to
estimate the underlying variance parameter; unlike subsampling and the
bootstrap, however, we assume that the implied point estimator of the
statistical functional obeys a central limit theorem (CLT) to help identify the
weak asymptotics (called OB-x limits, x=I,II,III) of batched Studentized
statistics. The OB-x limits, certain functionals of the Wiener process
parameterized by the size of the batches and the extent of their overlap, form
the essential machinery for characterizing dependence, and consequently the
correctness of the proposed CIPs. The message from extensive numerical
experimentation is that in settings where a functional CLT on the point
estimator is in effect, using \emph{large overlapping batches} alongside OB-x
critical values yields confidence intervals that are often of significantly
higher quality than those obtained from more generic methods like subsampling
or the bootstrap. We illustrate using examples from CVaR estimation, ARMA
parameter estimation, and NHPP rate estimation; R and MATLAB code for OB-x
critical values is available at~\texttt{web.ics.purdue.edu/~pasupath/}.",http://arxiv.org/pdf/2307.08609v1
2307.08138v1,eess.IV,Neural Orientation Distribution Fields for Estimation and Uncertainty Quantification in Diffusion MRI,2023-07-16 19:34:28+00:00,"Inferring brain connectivity and structure \textit{in-vivo} requires accurate
estimation of the orientation distribution function (ODF), which encodes key
local tissue properties. However, estimating the ODF from diffusion MRI (dMRI)
signals is a challenging inverse problem due to obstacles such as significant
noise, high-dimensional parameter spaces, and sparse angular measurements. In
this paper, we address these challenges by proposing a novel deep-learning
based methodology for continuous estimation and uncertainty quantification of
the spatially varying ODF field. We use a neural field (NF) to parameterize a
random series representation of the latent ODFs, implicitly modeling the often
ignored but valuable spatial correlation structures in the data, and thereby
improving efficiency in sparse and noisy regimes. An analytic approximation to
the posterior predictive distribution is derived which can be used to quantify
the uncertainty in the ODF estimate at any spatial location, avoiding the need
for expensive resampling-based approaches that are typically employed for this
purpose. We present empirical evaluations on both synthetic and real in-vivo
diffusion data, demonstrating the advantages of our method over existing
approaches.",http://arxiv.org/pdf/2307.08138v1
2307.07923v1,eess.SY,Modeling Physical Activity Impact on Glucose Dynamics in People with Type 1 Diabetes for a Fully Automated Artificial Pancreas,2023-07-16 02:01:08+00:00,"In this paper, models of the blood glucose (BG) dynamics in people with Type
1 diabetes (T1D) in response to moderate intensity aerobic activity are derived
from physiology-based first principles and system identification experiments.
We show that by enhancing insulin-dependent glucose utilization by the tissues
in two phases, a rapid short-term increase in insulin-independent glucose
clearance and augmented glucose uptake, and a long-term sustained increase
sensitivity to insulin action, a metabolic model able to reproduce the effects
of activity on glucose disposal is obtained. Second, a control-oriented
transfer function model is proposed to predict the BG response to an exercise
bout modeled as a step change in heart rate (HR). Results comparing model
predictions with actual patients data collected in a series of experimental
sessions including physical activity (PA) are presented. The findings will
contribute to the design of a fully automated closed-loop for improved glucose
control in conditions of daily life for people with T1D.",http://arxiv.org/pdf/2307.07923v1
2307.07342v2,stat.ME,Bounded-memory adjusted scores estimation in generalized linear models with large data sets,2023-07-14 13:43:25+00:00,"The widespread use of maximum Jeffreys'-prior penalized likelihood in
binomial-response generalized linear models, and in logistic regression, in
particular, are supported by the results of Kosmidis and Firth (2021,
Biometrika), who show that the resulting estimates are also always
finite-valued, even in cases where the maximum likelihood estimates are not,
which is a practical issue regardless of the size of the data set. In logistic
regression, the implied adjusted score equations are formally bias-reducing in
asymptotic frameworks with a fixed number of parameters and appear to deliver a
substantial reduction in the persistent bias of the maximum likelihood
estimator in high-dimensional settings where the number of parameters grows
asymptotically linearly and slower than the number of observations. In this
work, we develop and present two new variants of iteratively reweighted least
squares for estimating generalized linear models with adjusted score equations
for mean bias reduction and maximization of the likelihood penalized by a
positive power of the Jeffreys-prior penalty, which eliminate the requirement
of storing $O(n)$ quantities in memory, and can operate with data sets that
exceed computer memory or even hard drive capacity. We achieve that through
incremental QR decompositions, which enable IWLS iterations to have access only
to data chunks of predetermined size. We assess the procedures through a
real-data application with millions of observations, and in high-dimensional
logistic regression, where a large-scale simulation experiment produces
concrete evidence for the existence of a simple adjustment to the maximum
Jeffreys'-penalized likelihood estimates that delivers high accuracy in terms
of signal recovery even in cases where estimates from ML and other
recently-proposed corrective methods do not exist.",http://arxiv.org/pdf/2307.07342v2
2307.07042v1,stat.ME,Bayesian Analysis of Beta Autoregressive Moving Average Models,2023-07-13 19:58:28+00:00,"This work presents a Bayesian approach for the estimation of Beta
Autoregressive Moving Average ($\beta$ARMA) models. We discuss standard choice
for the prior distributions and employ a Hamiltonian Monte Carlo algorithm to
sample from the posterior. We propose a method to approach the problem of unit
roots in the model's systematic component. We then present a series of Monte
Carlo simulations to evaluate the performance of this Bayesian approach. In
addition to parameter estimation, we evaluate the proposed approach to verify
the presence of unit roots in the model's systematic component and study prior
sensitivity. An empirical application is presented to exemplify the usefulness
of the method. In the application, we compare the fitted Bayesian and
frequentist approaches in terms of their out-of-sample forecasting
capabilities.",http://arxiv.org/pdf/2307.07042v1
2307.07005v1,stat.CO,Fast Bayesian Record Linkage for Streaming Data Contexts,2023-07-13 18:08:26+00:00,"Record linkage is the task of combining records from multiple files which
refer to overlapping sets of entities when there is no unique identifying
field. In streaming record linkage, files arrive sequentially in time and
estimates of links are updated after the arrival of each file. This problem
arises in settings such as longitudinal surveys, electronic health records, and
online events databases, among others. The challenge in streaming record
linkage is to efficiently update parameter estimates as new data arrives. We
approach the problem from a Bayesian perspective with estimates in the form of
posterior samples of parameters and present methods for updating link estimates
after the arrival of a new file that are faster than fitting a joint model with
each new data file. In this paper, we generalize a two-file Bayesian
Fellegi-Sunter model to the multi-file case and propose two methods to perform
streaming updates. We examine the effect of prior distribution on the resulting
linkage accuracy as well as the computational trade-offs between the methods
when compared to a Gibbs sampler through simulated and real-world survey panel
data. We achieve near-equivalent posterior inference at a small fraction of the
compute time.",http://arxiv.org/pdf/2307.07005v1
2307.06957v1,stat.ML,Embracing the chaos: analysis and diagnosis of numerical instability in variational flows,2023-07-12 23:13:10+00:00,"In this paper, we investigate the impact of numerical instability on the
reliability of sampling, density evaluation, and evidence lower bound (ELBO)
estimation in variational flows. We first empirically demonstrate that common
flows can exhibit a catastrophic accumulation of error: the numerical flow map
deviates significantly from the exact map -- which affects sampling -- and the
numerical inverse flow map does not accurately recover the initial input --
which affects density and ELBO computations. Surprisingly though, we find that
results produced by flows are often accurate enough for applications despite
the presence of serious numerical instability. In this work, we treat
variational flows as dynamical systems, and leverage shadowing theory to
elucidate this behavior via theoretical guarantees on the error of sampling,
density evaluation, and ELBO estimation. Finally, we develop and empirically
test a diagnostic procedure that can be used to validate results produced by
numerically unstable flows in practice.",http://arxiv.org/pdf/2307.06957v1
2307.06424v1,stat.ME,Robust scalable initialization for Bayesian variational inference with multi-modal Laplace approximations,2023-07-12 19:30:04+00:00,"For predictive modeling relying on Bayesian inversion, fully independent, or
``mean-field'', Gaussian distributions are often used as approximate
probability density functions in variational inference since the number of
variational parameters is twice the number of unknown model parameters. The
resulting diagonal covariance structure coupled with unimodal behavior can be
too restrictive when dealing with highly non-Gaussian behavior, including
multimodality. High-fidelity surrogate posteriors in the form of Gaussian
mixtures can capture any distribution to an arbitrary degree of accuracy while
maintaining some analytical tractability. Variational inference with Gaussian
mixtures with full-covariance structures suffers from a quadratic growth in
variational parameters with the number of model parameters. Coupled with the
existence of multiple local minima due to nonconvex trends in the loss
functions often associated with variational inference, these challenges
motivate the need for robust initialization procedures to improve the
performance and scalability of variational inference with mixture models.
  In this work, we propose a method for constructing an initial Gaussian
mixture model approximation that can be used to warm-start the iterative
solvers for variational inference. The procedure begins with an optimization
stage in model parameter space in which local gradient-based optimization,
globalized through multistart, is used to determine a set of local maxima,
which we take to approximate the mixture component centers. Around each mode, a
local Gaussian approximation is constructed via the Laplace method. Finally,
the mixture weights are determined through constrained least squares
regression. Robustness and scalability are demonstrated using synthetic tests.
The methodology is applied to an inversion problem in structural dynamics
involving unknown viscous damping coefficients.",http://arxiv.org/pdf/2307.06424v1
2307.06024v2,stat.CO,balance -- a Python package for balancing biased data samples,2023-07-12 09:09:49+00:00,"Surveys are an important research tool, providing unique measurements on
subjective experiences such as sentiment and opinions that cannot be measured
by other means. However, because survey data is collected from a self-selected
group of participants, directly inferring insights from it to a population of
interest, or training ML models on such data, can lead to erroneous estimates
or under-performing models. In this paper we present balance, an open-source
Python package by Meta, offering a simple workflow for analyzing and adjusting
biased data samples with respect to a population of interest.
  The balance workflow includes three steps: understanding the initial bias in
the data relative to a target we would like to infer, adjusting the data to
correct for the bias by producing weights for each unit in the sample based on
propensity scores, and evaluating the final biases and the variance inflation
after applying the fitted weights. The package provides a simple API that can
be used by researchers and data scientists from a wide range of fields on a
variety of data. The paper provides the relevant context, methodological
background, and presents the package's API.",http://arxiv.org/pdf/2307.06024v2
2307.05234v1,stat.ME,CR-Lasso: Robust cellwise regularized sparse regression,2023-07-11 13:08:28+00:00,"Cellwise contamination remains a challenging problem for data scientists,
particularly in research fields that require the selection of sparse features.
Traditional robust methods may not be feasible nor efficient in dealing with
such contaminated datasets. We propose CR-Lasso, a robust Lasso-type cellwise
regularization procedure that performs feature selection in the presence of
cellwise outliers by minimising a regression loss and cell deviation measure
simultaneously. To evaluate the approach, we conduct empirical studies
comparing its selection and prediction performance with several sparse
regression methods. We show that CR-Lasso is competitive under the settings
considered. We illustrate the effectiveness of the proposed method on real data
through an analysis of a bone mineral density dataset.",http://arxiv.org/pdf/2307.05234v1
2307.05149v1,math.NA,Multi-index Importance Sampling for McKean-Vlasov Stochastic Differential Equation,2023-07-11 10:11:22+00:00,"This work introduces a novel approach that combines the multi-index Monte
Carlo (MC) method with importance sampling (IS) to estimate rare event
quantities expressed as an expectation of a smooth observable of solutions to a
broad class of McKean-Vlasov stochastic differential equations. We extend the
double loop Monte Carlo (DLMC) estimator, previously introduced in our works
(Ben Rached et al., 2022a,b), to the multi-index setting. We formulate a new
multi-index DLMC estimator and conduct a comprehensive cost-error analysis,
leading to improved complexity results. To address rare events, an importance
sampling scheme is applied using stochastic optimal control of the single level
DLMC estimator. This combination of IS and multi-index DLMC not only reduces
computational complexity by two orders but also significantly decreases the
associated constant compared to vanilla MC. The effectiveness of the proposed
multi-index DLMC estimator is demonstrated using the Kuramoto model from
statistical physics. The results confirm a reduced complexity from
$\mathcal{O}(\mathrm{TOL}_{\mathrm{r}}^{-4})$ for the single level DLMC
estimator (Ben Rached et al., 2022a) to
$\mathcal{O}(\mathrm{TOL}_{\mathrm{r}}^{-2} (\log
\mathrm{TOL}_{\mathrm{r}}^{-1})^2)$ for the considered example, while ensuring
accurate estimation of rare event quantities within the prescribed relative
error tolerance $\mathrm{TOL}_\mathrm{r}$.",http://arxiv.org/pdf/2307.05149v1
2307.04944v1,stat.ME,Linear mixed models for complex survey data: implementing and evaluating pairwise likelihood,2023-07-11 00:20:22+00:00,"As complex-survey data becomes more widely used in health and social-science
research, there is increasing interest in fitting a wider range of regression
models. We describe an implementation of two-level linear mixed models in R
using the pairwise composite likelihood approach of Rao and co-workers. We
discuss the computational efficiency of pairwise composite likelihood and
compare the estimator to the existing stagewise pseudolikelihood estimator in
simulations and in data from the PISA educational survey.",http://arxiv.org/pdf/2307.04944v1
2307.04675v2,cs.LG,LINFA: a Python library for variational inference with normalizing flow and annealing,2023-07-10 16:21:05+00:00,"Variational inference is an increasingly popular method in statistics and
machine learning for approximating probability distributions. We developed
LINFA (Library for Inference with Normalizing Flow and Annealing), a Python
library for variational inference to accommodate computationally expensive
models and difficult-to-sample distributions with dependent parameters. We
discuss the theoretical background, capabilities, and performance of LINFA in
various benchmarks. LINFA is publicly available on GitHub at
https://github.com/desResLab/LINFA.",http://arxiv.org/pdf/2307.04675v2
2307.05558v1,stat.CO,From Estimation to Sampling for Bayesian Linear Regression with Spike-and-Slab Prior,2023-07-09 16:03:41+00:00,"We consider Bayesian linear regression with sparsity-inducing prior and
design efficient sampling algorithms leveraging posterior contraction
properties. A quasi-likelihood with Gaussian spike-and-slab (that is favorable
both statistically and computationally) is investigated and two algorithms
based on Gibbs sampling and Stochastic Localization are analyzed, both under
the same (quite natural) statistical assumptions that also enable valid
inference on the sparse planted signal. The benefit of the Stochastic
Localization sampler is particularly prominent for data matrix that is not
well-designed.",http://arxiv.org/pdf/2307.05558v1
2307.04102v1,stat.ML,A generative flow for conditional sampling via optimal transport,2023-07-09 05:36:26+00:00,"Sampling conditional distributions is a fundamental task for Bayesian
inference and density estimation. Generative models, such as normalizing flows
and generative adversarial networks, characterize conditional distributions by
learning a transport map that pushes forward a simple reference (e.g., a
standard Gaussian) to a target distribution. While these approaches
successfully describe many non-Gaussian problems, their performance is often
limited by parametric bias and the reliability of gradient-based (adversarial)
optimizers to learn these transformations. This work proposes a non-parametric
generative model that iteratively maps reference samples to the target. The
model uses block-triangular transport maps, whose components are shown to
characterize conditionals of the target distribution. These maps arise from
solving an optimal transport problem with a weighted $L^2$ cost function,
thereby extending the data-driven approach in [Trigila and Tabak, 2016] for
conditional sampling. The proposed approach is demonstrated on a two
dimensional example and on a parameter inference problem involving nonlinear
ODEs.",http://arxiv.org/pdf/2307.04102v1
2307.06279v1,stat.CO,SpreadNUTS -- Moderate Dynamic Extension of Paths for No-U-Turn Sampling & Partitioning Visited Regions,2023-07-09 05:00:25+00:00,"Markov chain Monte Carlo (MCMC) methods have existed for a long time and the
field is well-explored. The purpose of MCMC methods is to approximate a
distribution through repeated sampling; most MCMC algorithms exhibit
asymptotically optimal behavior in that they converge to the true distribution
at the limit. However, what differentiates these algorithms are their practical
convergence guarantees and efficiency. While a sampler may eventually
approximate a distribution well, because it is used in the real world it is
necessary that the point at which the sampler yields a good estimate of the
distribution is reachable in a reasonable amount of time. Similarly, if it is
computationally difficult or intractable to produce good samples from a
distribution for use in estimation, then there is no real-world utility
afforded by the sampler. Thus, most MCMC methods these days focus on improving
efficiency and speeding up convergence. However, many MCMC algorithms suffer
from random walk behavior and often only mitigate such behavior as outright
erasing random walks is difficult. Hamiltonian Monte Carlo (HMC) is a class of
MCMC methods that theoretically exhibit no random walk behavior because of
properties related to Hamiltonian dynamics. This paper introduces modifications
to a specific HMC algorithm known as the no-U-turn sampler (NUTS) that aims to
explore the sample space faster than NUTS, yielding a sampler that has faster
convergence to the true distribution than NUTS.",http://arxiv.org/pdf/2307.06279v1
2307.03922v2,stat.CO,The Polytope of Optimal Approximate Designs: Extending the Selection of Efficient Experiments,2023-07-08 07:28:34+00:00,"Consider the problem of constructing an experimental design that is optimal
for a given statistical model with respect to a chosen criterion. To address
this problem, the literature typically provides a single solution; sometimes,
however, there exists a rich set of optimal designs. The knowledge of this set
allows the experimenter to select an optimal design based on its secondary
properties.
  In this paper, we show that the set of all optimal approximate designs often
corresponds to a polytope. The polytope can be fully represented by the set of
its vertices, which we call vertex optimal designs. We prove that these optimal
designs possess unique characteristics, such as small supports, and suggest
their potential applications. We also demonstrate that for a variety of models,
the vertex optimal designs can be computed using rational arithmetic with
perfect accuracy. Consequently, we enumerate all vertex optimal designs for
several standard multifactor regression models.",http://arxiv.org/pdf/2307.03922v2
2307.03460v1,stat.CO,On the convergence of dynamic implementations of Hamiltonian Monte Carlo and No U-Turn Samplers,2023-07-07 08:44:33+00:00,"There is substantial empirical evidence about the success of dynamic
implementations of Hamiltonian Monte Carlo (HMC), such as the No U-Turn Sampler
(NUTS), in many challenging inference problems but theoretical results about
their behavior are scarce. The aim of this paper is to fill this gap. More
precisely, we consider a general class of MCMC algorithms we call dynamic HMC.
We show that this general framework encompasses NUTS as a particular case,
implying the invariance of the target distribution as a by-product. Second, we
establish conditions under which NUTS is irreducible and aperiodic and as a
corrolary ergodic. Under conditions similar to the ones existing for HMC, we
also show that NUTS is geometrically ergodic. Finally, we improve existing
convergence results for HMC showing that this method is ergodic without any
boundedness condition on the stepsize and the number of leapfrog steps, in the
case where the target is a perturbation of a Gaussian distribution.",http://arxiv.org/pdf/2307.03460v1
2307.03428v1,stat.CO,Revisiting the Two-Filter Formula for Smoothing for State-Space Models,2023-07-07 07:19:39+00:00,"Smoothing algorithms for state-space models, i.e., fixed-interval smoothing,
fixed-lag smoothing, and two-filter formula for smoothing, are examined using
real examples. For linear and Gaussian state-space models, it is observed that
similar posterior distributions can be obtained by properly defining the
inverse filter. In the case of linear non-Gaussian state-space models, it is
shown that Gaussian-sum smoothing is possible even for relatively high
dimensional state-space model with Gaussian-mixture noise inputs by properly
setting the inverse filter. The two-filter formula is also applicable for
particle filter, but better results are obtained with fixed lag smoothing or
with the average of forward and backward fixed lag smoothers.",http://arxiv.org/pdf/2307.03428v1
2307.03210v1,cs.LG,Sparse Graphical Linear Dynamical Systems,2023-07-06 14:10:02+00:00,"Time-series datasets are central in numerous fields of science and
engineering, such as biomedicine, Earth observation, and network analysis.
Extensive research exists on state-space models (SSMs), which are powerful
mathematical tools that allow for probabilistic and interpretable learning on
time series. Estimating the model parameters in SSMs is arguably one of the
most complicated tasks, and the inclusion of prior knowledge is known to both
ease the interpretation but also to complicate the inferential tasks. Very
recent works have attempted to incorporate a graphical perspective on some of
those model parameters, but they present notable limitations that this work
addresses. More generally, existing graphical modeling tools are designed to
incorporate either static information, focusing on statistical dependencies
among independent random variables (e.g., graphical Lasso approach), or dynamic
information, emphasizing causal relationships among time series samples (e.g.,
graphical Granger approaches). However, there are no joint approaches combining
static and dynamic graphical modeling within the context of SSMs. This work
proposes a novel approach to fill this gap by introducing a joint graphical
modeling framework that bridges the static graphical Lasso model and a
causal-based graphical approach for the linear-Gaussian SSM. We present DGLASSO
(Dynamic Graphical Lasso), a new inference method within this framework that
implements an efficient block alternating majorization-minimization algorithm.
The algorithm's convergence is established by departing from modern tools from
nonlinear analysis. Experimental validation on synthetic and real weather
variability data showcases the effectiveness of the proposed model and
inference algorithm.",http://arxiv.org/pdf/2307.03210v1
