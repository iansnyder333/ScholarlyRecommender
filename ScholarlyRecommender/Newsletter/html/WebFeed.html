<!DOCTYPE html>
    <html>
    <body style="background-color: #FFFFFF; color: #A2A2F5">
    
    <div class="feed-item" style="border: 1px solid #ccc;
    padding: 15px;
    margin: 15px;
    border-radius: 8px;">
    <h2 class="title" style=" font-size: 24px;
    font-weight: bold;
    margin-bottom: 5px; color: #262730;">YaRN: Efficient Context Window Extension of Large Language Models</h2>
    <h4 class="author" style="font-size: 14px;
    font-weight: bold;
    margin-bottom: 10px; color:#262730;">Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole</h4>
    <div class="metadata" style="font-size: 14px;
    color: #262730;
    margin-bottom: 10px;">
        <span class="id">Entry Id: 2309.00071v1</span> | 
        <span class="category">cs.CL</span> | 
        <span class="published">Published on 08-31-2023</span>
    </div>
    <div class="abstract" style="font-size: 16px;
    margin-bottom: 10px; color: #262730;">
        Rotary Position Embeddings (RoPE) have been shown to effectively encode
positional information in transformer-based language models. However, these
models fail to generalize past the sequence length they were trained on. We
present YaRN (Yet another RoPE extensioN method), a compute-efficient method to
extend the context window of such models, requiring 10x less tokens and 2.5x
less training steps than previous methods. Using YaRN, we show that LLaMA
models can effectively utilize and extrapolat...
    </div>
    <a href="http://arxiv.org/pdf/2309.00071v1" target="_blank" style="display: inline-block;
    background-color: #A2A2F5;
    color: white;
    padding: 8px 16px;
    border-radius: 4px;
    text-decoration: none;">Read More</a>
    </div>
    
    <div class="feed-item" style="border: 1px solid #ccc;
    padding: 15px;
    margin: 15px;
    border-radius: 8px;">
    <h2 class="title" style=" font-size: 24px;
    font-weight: bold;
    margin-bottom: 5px; color: #262730;">LLaSM: Large Language and Speech Model</h2>
    <h4 class="author" style="font-size: 14px;
    font-weight: bold;
    margin-bottom: 10px; color:#262730;">Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, Yemin Shi</h4>
    <div class="metadata" style="font-size: 14px;
    color: #262730;
    margin-bottom: 10px;">
        <span class="id">Entry Id: 2308.15930v1</span> | 
        <span class="category">cs.CL</span> | 
        <span class="published">Published on 08-30-2023</span>
    </div>
    <div class="abstract" style="font-size: 16px;
    margin-bottom: 10px; color: #262730;">
        Multi-modal large language models have garnered significant interest
recently. Though, most of the works focus on vision-language multi-modal models
providing strong capabilities in following vision-and-language instructions.
However, we claim that speech is also an important modality through which
humans interact with the world. Hence, it is crucial for a general-purpose
assistant to be able to follow multi-modal speech-and-language instructions. In
this work, we propose Large Language and Spee...
    </div>
    <a href="http://arxiv.org/pdf/2308.15930v1" target="_blank" style="display: inline-block;
    background-color: #A2A2F5;
    color: white;
    padding: 8px 16px;
    border-radius: 4px;
    text-decoration: none;">Read More</a>
    </div>
    
    <div class="feed-item" style="border: 1px solid #ccc;
    padding: 15px;
    margin: 15px;
    border-radius: 8px;">
    <h2 class="title" style=" font-size: 24px;
    font-weight: bold;
    margin-bottom: 5px; color: #262730;">Large Language Models as Data Preprocessors</h2>
    <h4 class="author" style="font-size: 14px;
    font-weight: bold;
    margin-bottom: 10px; color:#262730;">Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada</h4>
    <div class="metadata" style="font-size: 14px;
    color: #262730;
    margin-bottom: 10px;">
        <span class="id">Entry Id: 2308.16361v1</span> | 
        <span class="category">cs.AI</span> | 
        <span class="published">Published on 08-30-2023</span>
    </div>
    <div class="abstract" style="font-size: 16px;
    margin-bottom: 10px; color: #262730;">
        Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's
LLaMA variants, have marked a significant advancement in artificial
intelligence. Trained on vast amounts of text data, LLMs are capable of
understanding and generating human-like text across a diverse range of topics.
This study expands on the applications of LLMs, exploring their potential in
data preprocessing, a critical stage in data mining and analytics applications.
We delve into the applicability of state-of-the-art...
    </div>
    <a href="http://arxiv.org/pdf/2308.16361v1" target="_blank" style="display: inline-block;
    background-color: #A2A2F5;
    color: white;
    padding: 8px 16px;
    border-radius: 4px;
    text-decoration: none;">Read More</a>
    </div>
    
    <div class="feed-item" style="border: 1px solid #ccc;
    padding: 15px;
    margin: 15px;
    border-radius: 8px;">
    <h2 class="title" style=" font-size: 24px;
    font-weight: bold;
    margin-bottom: 5px; color: #262730;">LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models</h2>
    <h4 class="author" style="font-size: 14px;
    font-weight: bold;
    margin-bottom: 10px; color:#262730;">Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, Sinong Wang</h4>
    <div class="metadata" style="font-size: 14px;
    color: #262730;
    margin-bottom: 10px;">
        <span class="id">Entry Id: 2308.16137v1</span> | 
        <span class="category">cs.CL</span> | 
        <span class="published">Published on 08-30-2023</span>
    </div>
    <div class="abstract" style="font-size: 16px;
    margin-bottom: 10px; color: #262730;">
        In recent years, there have been remarkable advancements in the performance
of Transformer-based Large Language Models (LLMs) across various domains. As
these LLMs are deployed for increasingly complex tasks, they often face the
needs to conduct longer reasoning processes or understanding larger contexts.
In these situations, the length generalization failure of LLMs on long
sequences become more prominent. Most pre-training schemes truncate training
sequences to a fixed length (such as 2048 for...
    </div>
    <a href="http://arxiv.org/pdf/2308.16137v1" target="_blank" style="display: inline-block;
    background-color: #A2A2F5;
    color: white;
    padding: 8px 16px;
    border-radius: 4px;
    text-decoration: none;">Read More</a>
    </div>
    
    <div class="feed-item" style="border: 1px solid #ccc;
    padding: 15px;
    margin: 15px;
    border-radius: 8px;">
    <h2 class="title" style=" font-size: 24px;
    font-weight: bold;
    margin-bottom: 5px; color: #262730;">WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model</h2>
    <h4 class="author" style="font-size: 14px;
    font-weight: bold;
    margin-bottom: 10px; color:#262730;">Tianyu Wang, Yifan Li, Haitao Lin, Xiangyang Xue, Yanwei Fu</h4>
    <div class="metadata" style="font-size: 14px;
    color: #262730;
    margin-bottom: 10px;">
        <span class="id">Entry Id: 2308.15962v2</span> | 
        <span class="category">cs.RO</span> | 
        <span class="published">Published on 08-30-2023</span>
    </div>
    <div class="abstract" style="font-size: 16px;
    margin-bottom: 10px; color: #262730;">
        Enabling robots to understand language instructions and react accordingly to
visual perception has been a long-standing goal in the robotics research
community. Achieving this goal requires cutting-edge advances in natural
language processing, computer vision, and robotics engineering. Thus, this
paper mainly investigates the potential of integrating the most recent Large
Language Models (LLMs) and existing visual grounding and robotic grasping
system to enhance the effectiveness of the human-ro...
    </div>
    <a href="http://arxiv.org/pdf/2308.15962v2" target="_blank" style="display: inline-block;
    background-color: #A2A2F5;
    color: white;
    padding: 8px 16px;
    border-radius: 4px;
    text-decoration: none;">Read More</a>
    </div>
     </body>
    </html>