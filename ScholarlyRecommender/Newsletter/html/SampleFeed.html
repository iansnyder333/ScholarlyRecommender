<!DOCTYPE html>
    <html>
    <head>
        <link rel="stylesheet" href="style.css">
    </head>
    <body>
    <div class="feed-item">
    <h2 class="title">AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics</h2>
    <h4 class="author">Vahid Ghafouri, Vibhor Agarwal, Yong Zhang, Nishanth Sastry, Jose Such, Guillermo Suarez-Tangil</h4>
    <div class="metadata">
        <span class="id">Entry Id: 2308.14608v1</span> | 
        <span class="category">cs.LG</span> | 
        <span class="published">Published on 08-28-2023</span>
    </div>
    <div class="abstract">
        The introduction of ChatGPT and the subsequent improvement of Large Language
Models (LLMs) have prompted more and more individuals to turn to the use of
ChatBots, both for information and assistance with decision-making. However,
the information the user is after is often not formulated by these ChatBots
objectively enough to be provided with a definite, globally accepted answer.
  Controversial topics, such as "religion", "gender identity", "freedom of
speech", and "equality", among others, can...
    </div>
    <a href="http://arxiv.org/pdf/2308.14608v1" target="_blank">Read More</a>
    </div>
    
    <div class="feed-item">
    <h2 class="title">Gender bias and stereotypes in Large Language Models</h2>
    <h4 class="author">Hadas Kotek, Rikker Dockum, David Q. Sun</h4>
    <div class="metadata">
        <span class="id">Entry Id: 2308.14921v1</span> | 
        <span class="category">cs.CL</span> | 
        <span class="published">Published on 08-28-2023</span>
    </div>
    <div class="abstract">
        Large Language Models (LLMs) have made substantial progress in the past
several months, shattering state-of-the-art benchmarks in many domains. This
paper investigates LLMs' behavior with respect to gender stereotypes, a known
issue for prior models. We use a simple paradigm to test the presence of gender
bias, building on but differing from WinoBias, a commonly used gender bias
dataset, which is likely to be included in the training data of current LLMs.
We test four recently published LLMs and...
    </div>
    <a href="http://arxiv.org/pdf/2308.14921v1" target="_blank">Read More</a>
    </div>
    
    <div class="feed-item">
    <h2 class="title">Natlog: Embedding Logic Programming into the Python Deep-Learning Ecosystem</h2>
    <h4 class="author">Paul Tarau</h4>
    <div class="metadata">
        <span class="id">Entry Id: 2308.15890v1</span> | 
        <span class="category">cs.AI</span> | 
        <span class="published">Published on 08-30-2023</span>
    </div>
    <div class="abstract">
        Driven by expressiveness commonalities of Python and our Python-based
embedded logic-based language Natlog, we design high-level interaction patterns
between equivalent language constructs and data types on the two sides.
  By directly connecting generators and backtracking, nested tuples and terms,
coroutines and first-class logic engines, reflection and meta-interpretation,
we enable logic-based language constructs to access the full power of the
Python ecosystem.
  We show the effectiveness o...
    </div>
    <a href="http://arxiv.org/pdf/2308.15890v1" target="_blank">Read More</a>
    </div>
    
    <div class="feed-item">
    <h2 class="title">Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks</h2>
    <h4 class="author">Payal Mohapatra, Akash Pandey, Yueyuan Sui, Qi Zhu</h4>
    <div class="metadata">
        <span class="id">Entry Id: 2308.14359v2</span> | 
        <span class="category">cs.AI</span> | 
        <span class="published">Published on 08-28-2023</span>
    </div>
    <div class="abstract">
        Human emotion understanding is pivotal in making conversational technology
mainstream. We view speech emotion understanding as a perception task which is
a more realistic setting. With varying contexts (languages, demographics, etc.)
different share of people perceive the same speech segment as a non-unanimous
emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics
ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset
of multilingual speakers and multi-...
    </div>
    <a href="http://arxiv.org/pdf/2308.14359v2" target="_blank">Read More</a>
    </div>
    
    <div class="feed-item">
    <h2 class="title">Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models</h2>
    <h4 class="author">Hritik Bansal, John Dang, Aditya Grover</h4>
    <div class="metadata">
        <span class="id">Entry Id: 2308.15812v1</span> | 
        <span class="category">cs.LG</span> | 
        <span class="published">Published on 08-30-2023</span>
    </div>
    <div class="abstract">
        Aligning large language models (LLMs) with human values and intents
critically involves the use of human or AI feedback. While dense feedback
annotations are expensive to acquire and integrate, sparse feedback presents a
structural design choice between ratings (e.g., score Response A on a scale of
1-7) and rankings (e.g., is Response A better than Response B?). In this work,
we analyze the effect of this design choice for the alignment and evaluation of
LLMs. We uncover an inconsistency problem...
    </div>
    <a href="http://arxiv.org/pdf/2308.15812v1" target="_blank">Read More</a>
    </div>
     </body>
    </html>