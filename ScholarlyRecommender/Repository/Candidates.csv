Id,Category,Title,Published,Abstract,URL
2308.16185v1,cs.RO,Learning Vision-based Pursuit-Evasion Robot Policies,2023-08-30 17:59:05+00:00,"Learning strategic robot behavior -- like that required in pursuit-evasion
interactions -- under real-world constraints is extremely challenging. It
requires exploiting the dynamics of the interaction, and planning through both
physical state and latent intent uncertainty. In this paper, we transform this
intractable problem into a supervised learning problem, where a
fully-observable robot policy generates supervision for a partially-observable
one. We find that the quality of the supervision signal for the
partially-observable pursuer policy depends on two key factors: the balance of
diversity and optimality of the evader's behavior and the strength of the
modeling assumptions in the fully-observable policy. We deploy our policy on a
physical quadruped robot with an RGB-D camera on pursuit-evasion interactions
in the wild. Despite all the challenges, the sensing constraints bring about
creativity: the robot is pushed to gather information when uncertain, predict
intent from noisy measurements, and anticipate in order to intercept. Project
webpage: https://abajcsy.github.io/vision-based-pursuit/",http://arxiv.org/pdf/2308.16185v1
2308.16175v1,cs.CL,Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment,2023-08-30 17:53:25+00:00,"We introduce BSDetector, a method for detecting bad and speculative answers
from a pretrained Large Language Model by estimating a numeric confidence score
for any output it generated. Our uncertainty quantification technique works for
any LLM accessible only via a black-box API, and combines intrinsic and
extrinsic assessments of confidence into a single trustworthiness estimate for
any LLM response to a given prompt. Our method is extremely general and can
applied to all of the best LLMs available today (whose training data remains
unknown). By expending a bit of extra computation, users of any LLM API can now
get the same response as they would ordinarily, as well as a confidence
estimate that caution when not to trust this response. Experiments on both
closed and open-form Question-Answer benchmarks reveal that BSDetector more
accurately identifies incorrect LLM responses than alternative uncertainty
estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple
responses from the LLM and considering the one with the highest confidence
score, we can additionally obtain more accurate responses from the same LLM,
without any extra training steps.",http://arxiv.org/pdf/2308.16175v1
2308.16157v1,cs.LO,"Algebraic, Topological, and Mereological Foundations of Existential Granules",2023-08-30 17:22:11+00:00,"In this research, new concepts of existential granules that determine
themselves are invented, and are characterized from algebraic, topological, and
mereological perspectives. Existential granules are those that determine
themselves initially, and interact with their environment subsequently.
Examples of the concept, such as those of granular balls, though inadequately
defined, algorithmically established, and insufficiently theorized in earlier
works by others, are already used in applications of rough sets and soft
computing. It is shown that they fit into multiple theoretical frameworks
(axiomatic, adaptive, and others) of granular computing. The characterization
is intended for algorithm development, application to classification problems
and possible mathematical foundations of generalizations of the approach.
Additionally, many open problems are posed and directions provided.",http://arxiv.org/pdf/2308.16157v1
2308.16149v1,cs.CL,Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,2023-08-30 17:07:17+00:00,"We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric
foundation and instruction-tuned open generative large language models (LLMs).
The models are based on the GPT-3 decoder-only architecture and are pretrained
on a mixture of Arabic and English texts, including source code in various
programming languages. With 13 billion parameters, they demonstrate better
knowledge and reasoning capabilities in Arabic than any existing open Arabic
and multilingual models by a sizable margin, based on extensive evaluation.
Moreover, the models are competitive in English compared to English-centric
open models of similar size, despite being trained on much less English data.
We provide a detailed description of the training, the tuning, the safety
alignment, and the evaluation of the models. We release two open versions of
the model -- the foundation Jais model, and an instruction-tuned Jais-chat
variant -- with the aim of promoting research on Arabic LLMs. Available at
https://huggingface.co/inception-mbzuai/jais-13b-chat",http://arxiv.org/pdf/2308.16149v1
2308.16137v1,cs.CL,LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models,2023-08-30 16:47:51+00:00,"In recent years, there have been remarkable advancements in the performance
of Transformer-based Large Language Models (LLMs) across various domains. As
these LLMs are deployed for increasingly complex tasks, they often face the
needs to conduct longer reasoning processes or understanding larger contexts.
In these situations, the length generalization failure of LLMs on long
sequences become more prominent. Most pre-training schemes truncate training
sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to
generate fluent texts, let alone carry out downstream tasks, after longer
contexts, even with relative positional encoding which is designed to cope with
this problem. Common solutions such as finetuning on longer corpora often
involves daunting hardware and time costs and requires careful training process
design. To more efficiently leverage the generation capacity of existing LLMs,
we theoretically and empirically investigate the main out-of-distribution (OOD)
factors contributing to this problem. Inspired by this diagnosis, we propose a
simple yet effective solution for on-the-fly length generalization,
LM-Infinite, which involves only a $\Lambda$-shaped attention mask and a
distance limit while requiring no parameter updates or learning. We find it
applicable to a variety of LLMs using relative-position encoding methods.
LM-Infinite is computational efficient with $O(n)$ time and space, and
demonstrates consistent fluency and generation quality to as long as 32k tokens
on ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup. On downstream
task such as passkey retrieval, it continues to work on inputs much longer than
training lengths where vanilla models fail immediately.",http://arxiv.org/pdf/2308.16137v1
2308.16126v1,cs.CV,CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a Novel Metric,2023-08-30 16:23:07+00:00,"Detecting visually similar images is a particularly useful attribute to look
to when calculating product recommendations. Embedding similarity, which
utilizes pre-trained computer vision models to extract high-level image
features, has demonstrated remarkable efficacy in identifying images with
similar compositions. However, there is a lack of methods for evaluating the
embeddings generated by these models, as conventional loss and performance
metrics do not adequately capture their performance in image similarity search
tasks.
  In this paper, we evaluate the viability of the image embeddings from
numerous pre-trained computer vision models using a novel approach named
CorrEmbed. Our approach computes the correlation between distances in image
embeddings and distances in human-generated tag vectors. We extensively
evaluate numerous pre-trained Torchvision models using this metric, revealing
an intuitive relationship of linear scaling between ImageNet1k accuracy scores
and tag-correlation scores. Importantly, our method also identifies deviations
from this pattern, providing insights into how different models capture
high-level image features.
  By offering a robust performance evaluation of these pre-trained models,
CorrEmbed serves as a valuable tool for researchers and practitioners seeking
to develop effective, data-driven approaches to similar item recommendations in
fashion retail.",http://arxiv.org/pdf/2308.16126v1
2308.16118v1,cs.CL,Response: Emergent analogical reasoning in large language models,2023-08-30 16:17:26+00:00,"In their recent Nature Human Behaviour paper, ""Emergent analogical reasoning
in large language models,"" (Webb, Holyoak, and Lu, 2023) the authors argue that
""large language models such as GPT-3 have acquired an emergent ability to find
zero-shot solutions to a broad range of analogy problems."" In this response, we
provide counterexamples of the letter string analogies. In our tests, GPT-3
fails to solve even the easiest variants of the problems presented in the
original paper. Zero-shot reasoning is an extraordinary claim that requires
extraordinary evidence. We do not see that evidence in our experiments. To
strengthen claims of humanlike reasoning such as zero-shot reasoning, it is
important that the field develop approaches that rule out data memorization.",http://arxiv.org/pdf/2308.16118v1
2308.16113v1,cs.LG,survex: an R package for explaining machine learning survival models,2023-08-30 16:14:20+00:00,"Due to their flexibility and superior performance, machine learning models
frequently complement and outperform traditional statistical survival models.
However, their widespread adoption is hindered by a lack of user-friendly tools
to explain their internal operations and prediction rationales. To tackle this
issue, we introduce the survex R package, which provides a cohesive framework
for explaining any survival model by applying explainable artificial
intelligence techniques. The capabilities of the proposed software encompass
understanding and diagnosing survival models, which can lead to their
improvement. By revealing insights into the decision-making process, such as
variable effects and importances, survex enables the assessment of model
reliability and the detection of biases. Thus, transparency and responsibility
may be promoted in sensitive areas, such as biomedical research and healthcare
applications.",http://arxiv.org/pdf/2308.16113v1
2308.16109v1,cs.CL,Grandma Karl is 27 years old -- research agenda for pseudonymization of research data,2023-08-30 16:04:54+00:00,"Accessibility of research data is critical for advances in many research
fields, but textual data often cannot be shared due to the personal and
sensitive information which it contains, e.g names or political opinions.
General Data Protection Regulation (GDPR) suggests pseudonymization as a
solution to secure open access to research data, but we need to learn more
about pseudonymization as an approach before adopting it for manipulation of
research data. This paper outlines a research agenda within pseudonymization,
namely need of studies into the effects of pseudonymization on unstructured
data in relation to e.g. readability and language assessment, as well as the
effectiveness of pseudonymization as a way of protecting writer identity, while
also exploring different ways of developing context-sensitive algorithms for
detection, labelling and replacement of personal information in unstructured
data. The recently granted project on pseudonymization Grandma Karl is 27 years
old addresses exactly those challenges.",http://arxiv.org/pdf/2308.16109v1
2308.16089v1,cs.LG,Application of Zone Method based Machine Learning and Physics-Informed Neural Networks in Reheating Furnaces,2023-08-30 15:26:35+00:00,"Despite the high economic relevance of Foundation Industries, certain
components like Reheating furnaces within their manufacturing chain are
energy-intensive. Notable energy consumption reduction could be obtained by
reducing the overall heating time in furnaces. Computer-integrated Machine
Learning (ML) and Artificial Intelligence (AI) powered control systems in
furnaces could be enablers in achieving the Net-Zero goals in Foundation
Industries for sustainable manufacturing.
  In this work, due to the infeasibility of achieving good quality data in
scenarios like reheating furnaces, classical Hottel's zone method based
computational model has been used to generate data for ML and Deep Learning
(DL) based model training via regression. It should be noted that the zone
method provides an elegant way to model the physical phenomenon of Radiative
Heat Transfer (RHT), the dominating heat transfer mechanism in high-temperature
processes inside heating furnaces. Using this data, an extensive comparison
among a wide range of state-of-the-art, representative ML and DL methods has
been made against their temperature prediction performances in varying furnace
environments. Owing to their holistic balance among inference times and model
performance, DL stands out among its counterparts. To further enhance the
Out-Of-Distribution (OOD) generalization capability of the trained DL models,
we propose a Physics-Informed Neural Network (PINN) by incorporating prior
physical knowledge using a set of novel Energy-Balance regularizers. Our setup
is a generic framework, is geometry-agnostic of the 3D structure of the
underlying furnace, and as such could accommodate any standard ML regression
model, to serve as a Digital Twin of the underlying physical processes, for
transitioning Foundation Industries towards Industry 4.0.",http://arxiv.org/pdf/2308.16089v1
2308.16075v1,cs.CL,Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages,2023-08-30 14:52:14+00:00,"The study investigates the effectiveness of utilizing multimodal information
in Neural Machine Translation (NMT). While prior research focused on using
multimodal data in low-resource scenarios, this study examines how image
features impact translation when added to a large-scale, pre-trained unimodal
NMT system. Surprisingly, the study finds that images might be redundant in
this context. Additionally, the research introduces synthetic noise to assess
whether images help the model deal with textual noise. Multimodal models
slightly outperform text-only models in noisy settings, even with random
images. The study's experiments translate from English to Hindi, Bengali, and
Malayalam, outperforming state-of-the-art benchmarks significantly.
Interestingly, the effect of visual context varies with source text noise: no
visual context works best for non-noisy translations, cropped image features
are optimal for low noise, and full image features work better in high-noise
scenarios. This sheds light on the role of visual context, especially in noisy
settings, opening up a new research direction for Noisy Neural Machine
Translation in multimodal setups. The research emphasizes the importance of
combining visual and textual information for improved translation in various
environments.",http://arxiv.org/pdf/2308.16075v1
2308.16071v1,cs.CV,Semantic Image Synthesis via Class-Adaptive Cross-Attention,2023-08-30 14:49:34+00:00,"In semantic image synthesis, the state of the art is dominated by methods
that use spatially-adaptive normalization layers, which allow for excellent
visual generation quality and editing versatility. Granted their efficacy,
recent research efforts have focused toward finer-grained local style control
and multi-modal generation. By construction though, such layers tend to
overlook global image statistics leading to unconvincing local style editing
and causing global inconsistencies such as color or illumination distribution
shifts. Also, the semantic layout is required for mapping styles in the
generator, putting a strict alignment constraint over the features. In
response, we designed a novel architecture where cross-attention layers are
used in place of de-normalization ones for conditioning the image generation.
Our model inherits the advantages of both solutions, retaining state-of-the-art
reconstruction quality, as well as improved global and local style transfer.
Code and models available at https://github.com/TFonta/CA2SIS.",http://arxiv.org/pdf/2308.16071v1
2308.16067v1,cs.LG,Consensus of state of the art mortality prediction models: From all-cause mortality to sudden death prediction,2023-08-30 14:44:04+00:00,"Worldwide, many millions of people die suddenly and unexpectedly each year,
either with or without a prior history of cardiovascular disease. Such events
are sparse (once in a lifetime), many victims will not have had prior
investigations for cardiac disease and many different definitions of sudden
death exist. Accordingly, sudden death is hard to predict.
  This analysis used NHS Electronic Health Records (EHRs) for people aged
$\geq$50 years living in the Greater Glasgow and Clyde (GG\&C) region in 2010
(n = 380,000) to try to overcome these challenges. We investigated whether
medical history, blood tests, prescription of medicines, and hospitalisations
might, in combination, predict a heightened risk of sudden death.
  We compared the performance of models trained to predict either sudden death
or all-cause mortality. We built six models for each outcome of interest: three
taken from state-of-the-art research (BEHRT, Deepr and Deep Patient), and three
of our own creation. We trained these using two different data representations:
a language-based representation, and a sparse temporal matrix.
  We used global interpretability to understand the most important features of
each model, and compare how much agreement there was amongst models using Rank
Biased Overlap. It is challenging to account for correlated variables without
increasing the complexity of the interpretability technique. We overcame this
by clustering features into groups and comparing the most important groups for
each model. We found the agreement between models to be much higher when
accounting for correlated variables.
  Our analysis emphasises the challenge of predicting sudden death and
emphasises the need for better understanding and interpretation of machine
learning models applied to healthcare applications.",http://arxiv.org/pdf/2308.16067v1
2308.16060v1,cs.CL,Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,2023-08-30 14:33:25+00:00,"We present Text-to-OverpassQL, a task designed to facilitate a natural
language interface for querying geodata from OpenStreetMap (OSM). The Overpass
Query Language (OverpassQL) allows users to formulate complex database queries
and is widely adopted in the OSM ecosystem. Generating Overpass queries from
natural language input serves multiple use-cases. It enables novice users to
utilize OverpassQL without prior knowledge, assists experienced users with
crafting advanced queries, and enables tool-augmented large language models to
access information stored in the OSM database. In order to assess the
performance of current sequence generation models on this task, we propose
OverpassNL, a dataset of 8,352 queries with corresponding natural language
inputs. We further introduce task specific evaluation metrics and ground the
evaluation of the Text-to-OverpassQL task by executing the queries against the
OSM database. We establish strong baselines by finetuning sequence-to-sequence
models and adapting large language models with in-context examples. The
detailed evaluation reveals strengths and weaknesses of the considered learning
strategies, laying the foundations for further research into the
Text-to-OverpassQL task.",http://arxiv.org/pdf/2308.16060v1
2308.16055v1,cs.CL,AsyncET: Asynchronous Learning for Knowledge Graph Entity Typing with Auxiliary Relations,2023-08-30 14:24:16+00:00,"Knowledge graph entity typing (KGET) is a task to predict the missing entity
types in knowledge graphs (KG). Previously, KG embedding (KGE) methods tried to
solve the KGET task by introducing an auxiliary relation, 'hasType', to model
the relationship between entities and their types. However, a single auxiliary
relation has limited expressiveness for diverse entity-type patterns. We
improve the expressiveness of KGE methods by introducing multiple auxiliary
relations in this work. Similar entity types are grouped to reduce the number
of auxiliary relations and improve their capability to model entity-type
patterns with different granularities. With the presence of multiple auxiliary
relations, we propose a method adopting an Asynchronous learning scheme for
Entity Typing, named AsyncET, which updates the entity and type embeddings
alternatively to keep the learned entity embedding up-to-date and informative
for entity type prediction. Experiments are conducted on two commonly used KGET
datasets to show that the performance of KGE methods on the KGET task can be
substantially improved by the proposed multiple auxiliary relations and
asynchronous embedding learning. Furthermore, our method has a significant
advantage over state-of-the-art methods in model sizes and time complexity.",http://arxiv.org/pdf/2308.16055v1
2308.16008v1,cs.RO,EnsembleFollower: A Hybrid Car-Following Framework Based On Reinforcement Learning and Hierarchical Planning,2023-08-30 12:55:02+00:00,"Car-following models have made significant contributions to our understanding
of longitudinal driving behavior. However, they often exhibit limited accuracy
and flexibility, as they cannot fully capture the complexity inherent in
car-following processes, or may falter in unseen scenarios due to their
reliance on confined driving skills present in training data. It is worth
noting that each car-following model possesses its own strengths and weaknesses
depending on specific driving scenarios. Therefore, we propose
EnsembleFollower, a hierarchical planning framework for achieving advanced
human-like car-following. The EnsembleFollower framework involves a high-level
Reinforcement Learning-based agent responsible for judiciously managing
multiple low-level car-following models according to the current state, either
by selecting an appropriate low-level model to perform an action or by
allocating different weights across all low-level components. Moreover, we
propose a jerk-constrained kinematic model for more convincing car-following
simulations. We evaluate the proposed method based on real-world driving data
from the HighD dataset. The experimental results illustrate that
EnsembleFollower yields improved accuracy of human-like behavior and achieves
effectiveness in combining hybrid models, demonstrating that our proposed
framework can handle diverse car-following conditions by leveraging the
strengths of various low-level models.",http://arxiv.org/pdf/2308.16008v1
2308.15991v1,cs.RO,DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous Driving,2023-08-30 12:24:30+00:00,"Autonomous driving systems are always built on motion-related modules such as
the planner and the controller. An accurate and robust trajectory tracking
method is indispensable for these motion-related modules as a primitive
routine. Current methods often make strong assumptions about the model such as
the context and the dynamics, which are not robust enough to deal with the
changing scenarios in a real-world system. In this paper, we propose a Deep
Reinforcement Learning (DRL)-based trajectory tracking method for the
motion-related modules in autonomous driving systems. The representation
learning ability of DL and the exploration nature of RL bring strong robustness
and improve accuracy. Meanwhile, it enhances versatility by running the
trajectory tracking in a model-free and data-driven manner. Through extensive
experiments, we demonstrate both the efficiency and effectiveness of our method
compared to current methods.",http://arxiv.org/pdf/2308.15991v1
2308.15987v1,cs.CL,FPTQ: Fine-grained Post-Training Quantization for Large Language Models,2023-08-30 12:18:18+00:00,"In the era of large-scale language models, the substantial parameter size
poses significant challenges for deployment. Being a prevalent compression
technique, quantization has emerged as the mainstream practice to tackle this
issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and
activations in such bit widths). In this study, we propose a novel W4A8
post-training quantization method for the available open-sourced LLMs, which
combines the advantages of both two recipes. Therefore, we can leverage the
benefit in the I/O utilization of 4-bit weight quantization and the
acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces
notorious performance degradation. As a remedy, we involve layerwise activation
quantization strategies which feature a novel logarithmic equalization for most
intractable layers, and we combine them with fine-grained weight quantization.
Without whistles and bells, we eliminate the necessity for further fine-tuning
and obtain the state-of-the-art W4A8 quantized performance on BLOOM, LLaMA, and
LLaMA-2 on standard benchmarks. We confirm that the W4A8 quantization is
achievable for the deployment of large language models, fostering their
wide-spreading real-world applications.",http://arxiv.org/pdf/2308.15987v1
2308.15985v1,cs.AI,Vision-Based Traffic Accident Detection and Anticipation: A Survey,2023-08-30 12:13:41+00:00,"Traffic accident detection and anticipation is an obstinate road safety
problem and painstaking efforts have been devoted. With the rapid growth of
video data, Vision-based Traffic Accident Detection and Anticipation (named
Vision-TAD and Vision-TAA) become the last one-mile problem for safe driving
and surveillance safety. However, the long-tailed, unbalanced, highly dynamic,
complex, and uncertain properties of traffic accidents form the
Out-of-Distribution (OOD) feature for Vision-TAD and Vision-TAA. Current AI
development may focus on these OOD but important problems. What has been done
for Vision-TAD and Vision-TAA? What direction we should focus on in the future
for this problem? A comprehensive survey is important. We present the first
survey on Vision-TAD in the deep learning era and the first-ever survey for
Vision-TAA. The pros and cons of each research prototype are discussed in
detail during the investigation. In addition, we also provide a critical review
of 31 publicly available benchmarks and related evaluation metrics. Through
this survey, we want to spawn new insights and open possible trends for
Vision-TAD and Vision-TAA tasks.",http://arxiv.org/pdf/2308.15985v1
2308.15975v1,cs.RO,RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation,2023-08-30 11:57:04+00:00,"For robots to be useful outside labs and specialized factories we need a way
to teach them new useful behaviors quickly. Current approaches lack either the
generality to onboard new tasks without task-specific engineering, or else lack
the data-efficiency to do so in an amount of time that enables practical use.
In this work we explore dense tracking as a representational vehicle to allow
faster and more general learning from demonstration. Our approach utilizes
Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration,
and parameterize a low-level controller to reproduce this motion across changes
in the scene configuration. We show this results in robust robot policies that
can solve complex object-arrangement tasks such as shape-matching, stacking,
and even full path-following tasks such as applying glue and sticking objects
together, all from demonstrations that can be collected in minutes.",http://arxiv.org/pdf/2308.15975v1
2308.15969v1,cs.AI,Iterative Reward Shaping using Human Feedback for Correcting Reward Misspecification,2023-08-30 11:45:40+00:00,"A well-defined reward function is crucial for successful training of an
reinforcement learning (RL) agent. However, defining a suitable reward function
is a notoriously challenging task, especially in complex, multi-objective
environments. Developers often have to resort to starting with an initial,
potentially misspecified reward function, and iteratively adjusting its
parameters, based on observed learned behavior. In this work, we aim to
automate this process by proposing ITERS, an iterative reward shaping approach
using human feedback for mitigating the effects of a misspecified reward
function. Our approach allows the user to provide trajectory-level feedback on
agent's behavior during training, which can be integrated as a reward shaping
signal in the following training iteration. We also allow the user to provide
explanations of their feedback, which are used to augment the feedback and
reduce user effort and feedback frequency. We evaluate ITERS in three
environments and show that it can successfully correct misspecified reward
functions.",http://arxiv.org/pdf/2308.15969v1
2308.15965v1,cs.AI,Review of Parameter Tuning Methods for Nature-Inspired Algorithms,2023-08-30 11:41:39+00:00,"Almost all optimization algorithms have algorithm-dependent parameters, and
the setting of such parameter values can largely influence the behaviour of the
algorithm under consideration. Thus, proper parameter tuning should be carried
out to ensure the algorithm used for optimization may perform well and can be
sufficiently robust for solving different types of optimization problems. This
chapter reviews some of the main methods for parameter tuning and then
highlights the important issues concerning the latest development in parameter
tuning. A few open problems are also discussed with some recommendations for
future research.",http://arxiv.org/pdf/2308.15965v1
2308.15962v1,cs.RO,WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model,2023-08-30 11:35:21+00:00,"Enabling robots to understand language instructions and react accordingly to
visual perception has been a long-standing goal in the robotics research
community. Achieving this goal requires cutting-edge advances in natural
language processing, computer vision, and robotics engineering. Thus, this
paper mainly investigates the potential of integrating the most recent Large
Language Models (LLMs) and existing visual grounding and robotic grasping
system to enhance the effectiveness of the human-robot interaction. We
introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language
model) as an example of this integration. The system utilizes the LLM of
ChatGPT to summarize the preference object of the users as a target instruction
via the multi-round interactive dialogue. The target instruction is then
forwarded to a visual grounding system for object pose and size estimation,
following which the robot grasps the object accordingly. We deploy this
LLM-empowered system on the physical robot to provide a more user-friendly
interface for the instruction-guided grasping task. The further experimental
results on various real-world scenarios demonstrated the feasibility and
efficacy of our proposed framework.",http://arxiv.org/pdf/2308.15962v1
2308.15926v1,cs.AI,IDVT: Interest-aware Denoising and View-guided Tuning for Social Recommendation,2023-08-30 10:03:55+00:00,"In the information age, recommendation systems are vital for efficiently
filtering information and identifying user preferences. Online social platforms
have enriched these systems by providing valuable auxiliary information.
Socially connected users are assumed to share similar preferences, enhancing
recommendation accuracy and addressing cold start issues. However, empirical
findings challenge the assumption, revealing that certain social connections
can actually harm system performance. Our statistical analysis indicates a
significant amount of noise in the social network, where many socially
connected users do not share common interests. To address this issue, we
propose an innovative \underline{I}nterest-aware \underline{D}enoising and
\underline{V}iew-guided \underline{T}uning (IDVT) method for the social
recommendation. The first ID part effectively denoises social connections.
Specifically, the denoising process considers both social network structure and
user interaction interests in a global view. Moreover, in this global view, we
also integrate denoised social information (social domain) into the propagation
of the user-item interactions (collaborative domain) and aggregate user
representations from two domains using a gating mechanism. To tackle potential
user interest loss and enhance model robustness within the global view, our
second VT part introduces two additional views (local view and dropout-enhanced
view) for fine-tuning user representations in the global view through
contrastive learning. Extensive evaluations on real-world datasets with varying
noise ratios demonstrate the superiority of IDVT over state-of-the-art social
recommendation methods.",http://arxiv.org/pdf/2308.15926v1
2308.15911v1,cs.LG,Cyclophobic Reinforcement Learning,2023-08-30 09:38:44+00:00,"In environments with sparse rewards, finding a good inductive bias for
exploration is crucial to the agent's success. However, there are two competing
goals: novelty search and systematic exploration. While existing approaches
such as curiosity-driven exploration find novelty, they sometimes do not
systematically explore the whole state space, akin to depth-first-search vs
breadth-first-search. In this paper, we propose a new intrinsic reward that is
cyclophobic, i.e., it does not reward novelty, but punishes redundancy by
avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of
hierarchical representations based on the agent's cropped observations we are
able to achieve excellent results in the MiniGrid and MiniHack environments.
Both are particularly hard, as they require complex interactions with different
objects in order to be solved. Detailed comparisons with previous approaches
and thorough ablation studies show that our newly proposed cyclophobic
reinforcement learning is more sample efficient than other state of the art
methods in a variety of tasks.",http://arxiv.org/pdf/2308.15911v1
2308.15906v1,cs.CY,Is the U.S. Legal System Ready for AI's Challenges to Human Values?,2023-08-30 09:19:06+00:00,"Our interdisciplinary study investigates how effectively U.S. laws confront
the challenges posed by Generative AI to human values. Through an analysis of
diverse hypothetical scenarios crafted during an expert workshop, we have
identified notable gaps and uncertainties within the existing legal framework
regarding the protection of fundamental values, such as autonomy, privacy,
dignity, diversity, equality, and physical/mental well-being. Constitutional
and civil rights, it appears, may not provide sufficient protection against
AI-generated discriminatory outputs. Furthermore, even if we exclude the
liability shield provided by Section 230, proving causation for defamation and
product liability claims is a challenging endeavor due to the intricate and
opaque nature of AI systems. To address the unique and unforeseeable threats
posed by Generative AI, we advocate for legal frameworks that evolve to
recognize new threat and provide proactive, auditable guidelines to industry
stakeholders. Addressing these issues requires deep interdisciplinary
collaborations to identify harms, values, and mitigation strategies.",http://arxiv.org/pdf/2308.15906v1
2308.15905v1,quant-ph,Thermodynamic Computing via Autonomous Quantum Thermal Machines,2023-08-30 09:15:41+00:00,"We develop a physics-based model for classical computation based on
autonomous quantum thermal machines. These machines consist of few interacting
quantum bits (qubits) connected to several environments at different
temperatures. Heat flows through the machine are here exploited for computing.
The process starts by setting the temperatures of the environments according to
the logical input. The machine evolves, eventually reaching a non-equilibrium
steady state, from which the output of the computation can be determined via
the temperature of an auxilliary finite-size reservoir. Such a machine, which
we term a ""thermodynamic neuron"", can implement any linearly-separable
function, and we discuss explicitly the cases of NOT, 3-majority and NOR gates.
In turn, we show that a network of thermodynamic neurons can perform any
desired function. We discuss the close connection between our model and
artificial neurons (perceptrons), and argue that our model provides an
alternative physics-based analogue implementation of neural networks, and more
generally a platform for thermodynamic computing.",http://arxiv.org/pdf/2308.15905v1
2308.15901v1,cs.AI,Explainable Answer-set Programming,2023-08-30 09:09:57+00:00,"The interest in explainability in artificial intelligence (AI) is growing
vastly due to the near ubiquitous state of AI in our lives and the increasing
complexity of AI systems. Answer-set Programming (ASP) is used in many areas,
among them are industrial optimisation, knowledge management or life sciences,
and thus of great interest in the context of explainability. To ensure the
successful application of ASP as a problem-solving paradigm in the future, it
is thus crucial to investigate explanations for ASP solutions. Such an
explanation generally tries to give an answer to the question of why something
is, respectively is not, part of the decision produced or solution to the
formulated problem. Although several explanation approaches for ASP exist,
almost all of them lack support for certain language features that are used in
practice. Most notably, this encompasses the various ASP extensions that have
been developed in the recent years to enable reasoning over theories, external
computations, or neural networks. This project aims to fill some of these gaps
and contribute to the state of the art in explainable ASP. We tackle this by
extending the language support of existing approaches but also by the
development of novel explanation formalisms, like contrastive explanations.",http://arxiv.org/pdf/2308.15901v1
2308.15899v1,cs.AI,Beyond Traditional Neural Networks: Toward adding Reasoning and Learning Capabilities through Computational Logic Techniques,2023-08-30 09:09:42+00:00,"Deep Learning (DL) models have become popular for solving complex problems,
but they have limitations such as the need for high-quality training data, lack
of transparency, and robustness issues. Neuro-Symbolic AI has emerged as a
promising approach combining the strengths of neural networks and symbolic
reasoning. Symbolic knowledge injection (SKI) techniques are a popular method
to incorporate symbolic knowledge into sub-symbolic systems. This work proposes
solutions to improve the knowledge injection process and integrate elements of
ML and logic into multi-agent systems (MAS).",http://arxiv.org/pdf/2308.15899v1
2308.15898v1,cs.AI,An xAI Approach for Data-to-Text Processing with ASP,2023-08-30 09:09:09+00:00,"The generation of natural language text from data series gained renewed
interest among AI research goals. Not surprisingly, the few proposals in the
state of the art are based on training some system, in order to produce a text
that describes and that is coherent to the data provided as input. Main
challenges of such approaches are the proper identification of ""what"" to say
(the key descriptive elements to be addressed in the data) and ""how"" to say:
the correspondence and accuracy between data and text, the presence of
contradictions/redundancy in the text, the control of the amount of synthesis.
  This paper presents a framework that is compliant with xAI requirements. In
particular we model ASP/Python programs that enable an explicit control of
accuracy errors and amount of synthesis, with proven optimal solutions. The
text description is hierarchically organized, in a top-down structure where
text is enriched with further details, according to logic rules. The generation
of natural language descriptions' structure is also managed by logic rules.",http://arxiv.org/pdf/2308.15898v1
2308.15897v1,cs.AI,Nemo: First Glimpse of a New Rule Engine,2023-08-30 09:08:28+00:00,"This system demonstration presents Nemo, a new logic programming engine with
a focus on reliability and performance. Nemo is built for data-centric analytic
computations, modelled in a fully declarative Datalog dialect. Its scalability
for these tasks matches or exceeds that of leading Datalog systems. We
demonstrate uses in reasoning with knowledge graphs and ontologies with 10^5 to
10^8 input facts, all on a laptop. Nemo is written in Rust and available as a
free and open source tool.",http://arxiv.org/pdf/2308.15897v1
2308.15895v1,cs.AI,Assessing Drivers' Situation Awareness in Semi-Autonomous Vehicles: ASP based Characterisations of Driving Dynamics for Modelling Scene Interpretation and Projection,2023-08-30 09:07:49+00:00,"Semi-autonomous driving, as it is already available today and will eventually
become even more accessible, implies the need for driver and automation system
to reliably work together in order to ensure safe driving. A particular
challenge in this endeavour are situations in which the vehicle's automation is
no longer able to drive and is thus requesting the human to take over. In these
situations the driver has to quickly build awareness for the traffic situation
to be able to take over control and safely drive the car. Within this context
we present a software and hardware framework to asses how aware the driver is
about the situation and to provide human-centred assistance to help in building
situation awareness. The framework is developed as a modular system within the
Robot Operating System (ROS) with modules for sensing the environment and the
driver state, modelling the driver's situation awareness, and for guiding the
driver's attention using specialized Human Machine Interfaces (HMIs).
  A particular focus of this paper is on an Answer Set Programming (ASP) based
approach for modelling and reasoning about the driver's interpretation and
projection of the scene. This is based on scene data, as well as eye-tracking
data reflecting the scene elements observed by the driver. We present the
overall application and discuss the role of semantic reasoning and modelling
cognitive functions based on logic programming in such applications.
Furthermore we present the ASP approach for interpretation and projection of
the driver's situation awareness and its integration within the overall system
in the context of a real-world use-case in simulated as well as in real
driving.",http://arxiv.org/pdf/2308.15895v1
2308.15892v1,cs.AI,A Logic Programming Approach to Global Logistics in a Co-Design Environment,2023-08-30 09:06:34+00:00,"In a co-design environment changes need to be integrated quickly and in an
automated manner. This paper considers the challenge of creating and optimizing
a global logistics system for the construction of a passenger aircraft within a
co-design approach with respect to key performance indicators (like cost, time
or resilience). The product in question is an aircraft, comprised of multiple
components, manufactured at multiple sites worldwide. The goal is to find an
optimal way to build the aircraft taking into consideration the requirements
for its industrial system. The main motivation for approaching this challenge
is to develop the industrial system in tandem with the product and making it
more resilient against unforeseen events, reducing the risks of bottlenecks in
the supply chain. This risk reduction ensures continued efficiency and
operational success. To address this challenging and complex task we have
chosen Answer Set Programming (ASP) as the modeling language, formalizing the
relevant requirements of the investigated industrial system. The approach
presented in this paper covers three main aspects: the extraction of the
relevant information from a knowledge graph, the translation into logic
programs and the computation of existing configurations guided by optimization
criteria. Finally we visualize the results for an effortless evaluation of
these models. Internal results seem promising and yielded several new research
questions for future improvements of the discussed use case.",http://arxiv.org/pdf/2308.15892v1
2308.15891v1,cs.AI,Understanding ProbLog as Probabilistic Argumentation,2023-08-30 09:05:32+00:00,"ProbLog is a popular probabilistic logic programming language/tool, widely
used for applications requiring to deal with inherent uncertainties in
structured domains. In this paper we study connections between ProbLog and a
variant of another well-known formalism combining symbolic reasoning and
reasoning under uncertainty, i.e. probabilistic argumentation. Specifically, we
show that ProbLog is an instance of a form of Probabilistic Abstract
Argumentation (PAA) that builds upon Assumption-Based Argumentation (ABA). The
connections pave the way towards equipping ProbLog with alternative semantics,
inherited from PAA/PABA, as well as obtaining novel argumentation semantics for
PAA/PABA, leveraging on prior connections between ProbLog and argumentation.
Further, the connections pave the way towards novel forms of argumentative
explanations for ProbLog's outputs.",http://arxiv.org/pdf/2308.15891v1
2308.15890v1,cs.AI,Natlog: Embedding Logic Programming into the Python Deep-Learning Ecosystem,2023-08-30 09:05:13+00:00,"Driven by expressiveness commonalities of Python and our Python-based
embedded logic-based language Natlog, we design high-level interaction patterns
between equivalent language constructs and data types on the two sides.
  By directly connecting generators and backtracking, nested tuples and terms,
coroutines and first-class logic engines, reflection and meta-interpretation,
we enable logic-based language constructs to access the full power of the
Python ecosystem.
  We show the effectiveness of our design via Natlog apps working as
orchestrators for JAX and Pytorch pipelines and as DCG-driven GPT3 and DALL.E
prompt generators.
  Keyphrases: embedding of logic programming in the Python ecosystem,
high-level inter-paradigm data exchanges, coroutining with logic engines,
logic-based neuro-symbolic computing, logic grammars as prompt-generators for
Large Language Models, logic-based neural network configuration and training.",http://arxiv.org/pdf/2308.15890v1
2308.15888v1,cs.LO,Generalizing Level Ranking Constraints for Monotone and Convex Aggregates,2023-08-30 09:04:39+00:00,"In answer set programming (ASP), answer sets capture solutions to search
problems of interest and thus the efficient computation of answer sets is of
utmost importance. One viable implementation strategy is provided by
translation-based ASP where logic programs are translated into other KR
formalisms such as Boolean satisfiability (SAT), SAT modulo theories (SMT), and
mixed-integer programming (MIP). Consequently, existing solvers can be
harnessed for the computation of answer sets. Many of the existing translations
rely on program completion and level rankings to capture the minimality of
answer sets and default negation properly. In this work, we take level ranking
constraints into reconsideration, aiming at their generalizations to cover
aggregate-based extensions of ASP in more systematic way. By applying a number
of program transformations, ranking constraints can be rewritten in a general
form that preserves the structure of monotone and convex aggregates and thus
offers a uniform basis for their incorporation into translation-based ASP. The
results open up new possibilities for the implementation of translators and
solver pipelines in practice.",http://arxiv.org/pdf/2308.15888v1
2308.15887v1,cs.AI,On the Potential of CLIP for Compositional Logical Reasoning,2023-08-30 09:04:24+00:00,"In this paper we explore the possibility of using OpenAI's CLIP to perform
logically coherent grounded visual reasoning. To that end, we formalize our
terms and give a geometric analysis of how embeddings in CLIP's latent space
would need to be configured in order for the system to be logically coherent.
Our main conclusion is that, as usually configured, CLIP cannot perform such
reasoning.",http://arxiv.org/pdf/2308.15887v1
2308.15883v1,cs.LO,"""Would life be more interesting if I were in AI?"" Answering Counterfactuals based on Probabilistic Inductive Logic Programming",2023-08-30 09:03:45+00:00,"Probabilistic logic programs are logic programs where some facts hold with a
specified probability. Here, we investigate these programs with a causal
framework that allows counterfactual queries. Learning the program structure
from observational data is usually done through heuristic search relying on
statistical tests. However, these statistical tests lack information about the
causal mechanism generating the data, which makes it unfeasible to use the
resulting programs for counterfactual reasoning. To address this, we propose a
language fragment that allows reconstructing a program from its induced
distribution. This further enables us to learn programs supporting
counterfactual queries.",http://arxiv.org/pdf/2308.15883v1
2308.15879v1,cs.AI,Explanations for Answer Set Programming,2023-08-30 09:03:07+00:00,"The paper presents an enhancement of xASP, a system that generates
explanation graphs for Answer Set Programming (ASP). Different from xASP, the
new system, xASP2, supports different clingo constructs like the choice rules,
the constraints, and the aggregates such as #sum, #min. This work formalizes
and presents an explainable artificial intelligence system for a broad fragment
of ASP, capable of shrinking as much as possible the set of assumptions and
presenting explanations in terms of directed acyclic graphs.",http://arxiv.org/pdf/2308.15879v1
2308.15877v1,cs.AI,ABA Learning via ASP,2023-08-30 09:02:29+00:00,"Recently, ABA Learning has been proposed as a form of symbolic machine
learning for drawing Assumption-Based Argumentation frameworks from background
knowledge and positive and negative examples. We propose a novel method for
implementing ABA Learning using Answer Set Programming as a way to help guide
Rote Learning and generalisation in ABA Learning.",http://arxiv.org/pdf/2308.15877v1
2308.15870v1,cs.LO,Deontic Paradoxes in ASP with Weak Constraints,2023-08-30 08:56:54+00:00,"The rise of powerful AI technology for a range of applications that are
sensitive to legal, social, and ethical norms demands decision-making support
in presence of norms and regulations. Normative reasoning is the realm of
deontic logics, that are challenged by well-known benchmark problems (deontic
paradoxes), and lack efficient computational tools. In this paper, we use
Answer Set Programming (ASP) for addressing these shortcomings and showcase how
to encode and resolve several well-known deontic paradoxes utilizing weak
constraints. By abstracting and generalizing this encoding, we present a
methodology for translating normative systems in ASP with weak constraints.
This methodology is applied to ""ethical"" versions of Pac-man, where we obtain a
comparable performance with related works, but ethically preferable results.",http://arxiv.org/pdf/2308.15870v1
2308.15865v1,cs.LO,On the Independencies Hidden in the Structure of a Probabilistic Logic Program,2023-08-30 08:55:55+00:00,"Pearl and Verma developed d-separation as a widely used graphical criterion
to reason about the conditional independencies that are implied by the causal
structure of a Bayesian network. As acyclic ground probabilistic logic programs
correspond to Bayesian networks on their dependency graph, we can compute
conditional independencies from d-separation in the latter.
  In the present paper, we generalize the reasoning above to the non-ground
case. First, we abstract the notion of a probabilistic logic program away from
external databases and probabilities to obtain so-called program structures. We
then present a correct meta-interpreter that decides whether a certain
conditional independence statement is implied by a program structure on a given
external database. Finally, we give a fragment of program structures for which
we obtain a completeness statement of our conditional independence oracle. We
close with an experimental evaluation of our approach revealing that our
meta-interpreter performs significantly faster than checking the definition of
independence using exact inference in ProbLog 2.",http://arxiv.org/pdf/2308.15865v1
2308.15863v1,cs.AI,Inductive Learning of Declarative Domain-Specific Heuristics for ASP,2023-08-30 08:55:17+00:00,"Domain-specific heuristics are a crucial technique for the efficient solving
of problems that are large or computationally hard. Answer Set Programming
(ASP) systems support declarative specifications of domain-specific heuristics
to improve solving performance. However, such heuristics must be invented
manually so far. Inventing domain-specific heuristics for answer-set programs
requires expertise with the domain under consideration and familiarity with ASP
syntax, semantics, and solving technology. The process of inventing useful
heuristics would highly profit from automatic support. This paper presents a
novel approach to the automatic learning of such heuristics. We use Inductive
Logic Programming (ILP) to learn declarative domain-specific heuristics from
examples stemming from (near-)optimal answer sets of small but representative
problem instances. Our experimental results indicate that the learned
heuristics can improve solving performance and solution quality when solving
larger, harder instances of the same problem.",http://arxiv.org/pdf/2308.15863v1
2308.15854v1,cs.CV,Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models,2023-08-30 08:40:15+00:00,"Denoising diffusion models have shown outstanding performance in image
editing. Existing works tend to use either image-guided methods, which provide
a visual reference but lack control over semantic coherence, or text-guided
methods, which ensure faithfulness to text guidance but lack visual quality. To
address the problem, we propose the Zero-shot Inversion Process (ZIP), a
framework that injects a fusion of generated visual reference and text guidance
into the semantic latent space of a \textit{frozen} pre-trained diffusion
model. Only using a tiny neural network, the proposed ZIP produces diverse
content and attributes under the intuitive control of the text prompt.
Moreover, ZIP shows remarkable robustness for both in-domain and out-of-domain
attribute manipulation on real images. We perform detailed experiments on
various benchmark datasets. Compared to state-of-the-art methods, ZIP produces
images of equivalent quality while providing a realistic editing effect.",http://arxiv.org/pdf/2308.15854v1
2308.15840v1,cs.LG,MSGNN: Multi-scale Spatio-temporal Graph Neural Network for Epidemic Forecasting,2023-08-30 08:21:56+00:00,"Infectious disease forecasting has been a key focus and proved to be crucial
in controlling epidemic. A recent trend is to develop forecast-ing models based
on graph neural networks (GNNs). However, existing GNN-based methods suffer
from two key limitations: (1) Current models broaden receptive fields by
scaling the depth of GNNs, which is insuffi-cient to preserve the semantics of
long-range connectivity between distant but epidemic related areas. (2)
Previous approaches model epidemics within single spatial scale, while ignoring
the multi-scale epidemic pat-terns derived from different scales. To address
these deficiencies, we devise the Multi-scale Spatio-temporal Graph Neural
Network (MSGNN) based on an innovative multi-scale view. To be specific, in the
proposed MSGNN model, we first devise a novel graph learning module, which
directly captures long-range connectivity from trans-regional epidemic signals
and integrates them into a multi-scale graph. Based on the learned multi-scale
graph, we utilize a newly designed graph convolution module to exploit
multi-scale epidemic patterns. This module allows us to facilitate multi-scale
epidemic modeling by mining both scale-shared and scale-specific pat-terns.
Experimental results on forecasting new cases of COVID-19 in United State
demonstrate the superiority of our method over state-of-arts. Further analyses
and visualization also show that MSGNN offers not only accurate, but also
robust and interpretable forecasting result.",http://arxiv.org/pdf/2308.15840v1
2308.15833v1,cs.AI,Depth analysis of battery performance based on a data-driven approach,2023-08-30 08:15:27+00:00,"Capacity attenuation is one of the most intractable issues in the current of
application of the cells. The disintegration mechanism is well known to be very
complex across the system. It is a great challenge to fully comprehend this
process and predict the process accurately. Thus, the machine learning (ML)
technology is employed to predict the specific capacity change of the cell
throughout the cycle and grasp this intricate procedure. Different from the
previous work, according to the WOA-ELM model proposed in this work (R2 =
0.9999871), the key factors affecting the specific capacity of the battery are
determined, and the defects in the machine learning black box are overcome by
the interpretable model. Their connection with the structural damage of
electrode materials and battery failure during battery cycling is
comprehensively explained, revealing their essentiality to battery performance,
which is conducive to superior research on contemporary batteries and
modification.",http://arxiv.org/pdf/2308.15833v1
2308.15829v1,cs.CV,Early Detection of Red Palm Weevil Infestations using Deep Learning Classification of Acoustic Signals,2023-08-30 08:09:40+00:00,"The Red Palm Weevil (RPW), also known as the palm weevil, is considered among
the world's most damaging insect pests of palms. Current detection techniques
include the detection of symptoms of RPW using visual or sound inspection and
chemical detection of volatile signatures generated by infested palm trees.
However, efficient detection of RPW diseases at an early stage is considered
one of the most challenging issues for cultivating date palms. In this paper,
an efficient approach to the early detection of RPW is proposed. The proposed
approach is based on RPW sound activities being recorded and analyzed. The
first step involves the conversion of sound data into images based on a
selected set of features. The second step involves the combination of images
from the same sound file but computed by different features into a single
image. The third step involves the application of different Deep Learning (DL)
techniques to classify resulting images into two classes: infested and not
infested. Experimental results show good performances of the proposed approach
for RPW detection using different DL techniques, namely MobileNetV2,
ResNet50V2, ResNet152V2, VGG16, VGG19, DenseNet121, DenseNet201, Xception, and
InceptionV3. The proposed approach outperformed existing techniques for public
datasets.",http://arxiv.org/pdf/2308.15829v1
2308.15821v1,cs.LG,Federated Two Stage Decoupling With Adaptive Personalization Layers,2023-08-30 07:46:32+00:00,"Federated learning has gained significant attention due to its groundbreaking
ability to enable distributed learning while maintaining privacy constraints.
However, as a consequence of data heterogeneity among decentralized devices, it
inherently experiences significant learning degradation and slow convergence
speed. Therefore, it is natural to employ the concept of clustering homogeneous
clients into the same group, allowing only the model weights within each group
to be aggregated. While most existing clustered federated learning methods
employ either model gradients or inference outputs as metrics for client
partitioning, with the goal of grouping similar devices together, may still
have heterogeneity within each cluster. Moreover, there is a scarcity of
research exploring the underlying reasons for determining the appropriate
timing for clustering, resulting in the common practice of assigning each
client to its own individual cluster, particularly in the context of highly non
independent and identically distributed (Non-IID) data. In this paper, we
introduce a two-stage decoupling federated learning algorithm with adaptive
personalization layers named FedTSDP, where client clustering is performed
twice according to inference outputs and model weights, respectively. Hopkins
amended sampling is adopted to determine the appropriate timing for clustering
and the sampling weight of public unlabeled data. In addition, a simple yet
effective approach is developed to adaptively adjust the personalization layers
based on varying degrees of data skew. Experimental results show that our
proposed method has reliable performance on both IID and non-IID scenarios.",http://arxiv.org/pdf/2308.15821v1
2308.15819v1,cs.AI,SharpSAT-TD in Model Counting Competitions 2021-2023,2023-08-30 07:43:12+00:00,"We describe SharpSAT-TD, our submission to the unweighted and weighted tracks
of the Model Counting Competition in 2021-2023, which has won in total $6$
first places in different tracks of the competition. SharpSAT-TD is based on
SharpSAT [Thurley, SAT 2006], with the primary novel modification being the use
of tree decompositions in the variable selection heuristic as introduced by the
authors in [CP 2021]. Unlike the version of SharpSAT-TD evaluated in [CP 2021],
the current version that is available in https://github.com/Laakeri/sharpsat-td
features also other significant modifications compared to the original
SharpSAT, for example, a new preprocessor.",http://arxiv.org/pdf/2308.15819v1
2308.15812v1,cs.LG,Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models,2023-08-30 07:35:32+00:00,"Aligning large language models (LLMs) with human values and intents
critically involves the use of human or AI feedback. While dense feedback
annotations are expensive to acquire and integrate, sparse feedback presents a
structural design choice between ratings (e.g., score Response A on a scale of
1-7) and rankings (e.g., is Response A better than Response B?). In this work,
we analyze the effect of this design choice for the alignment and evaluation of
LLMs. We uncover an inconsistency problem wherein the preferences inferred from
ratings and rankings significantly disagree 60% for both human and AI
annotators. Our subsequent analysis identifies various facets of annotator
biases that explain this phenomena, such as human annotators would rate denser
responses higher while preferring accuracy during pairwise judgments. To our
surprise, we also observe that the choice of feedback protocol also has a
significant effect on the evaluation of aligned LLMs. In particular, we find
that LLMs that leverage rankings data for alignment (say model X) are preferred
over those that leverage ratings data (say model Y), with a rank-based
evaluation protocol (is X/Y's response better than reference response?) but not
with a rating-based evaluation protocol (score Rank X/Y's response on a scale
of 1-7). Our findings thus shed light on critical gaps in methods for
evaluating the real-world utility of language models and their strong
dependence on the feedback protocol used for alignment. Our code and data are
available at https://github.com/Hritikbansal/sparse_feedback.",http://arxiv.org/pdf/2308.15812v1
2308.15802v1,cs.AI,Benchmarking Robustness and Generalization in Multi-Agent Systems: A Case Study on Neural MMO,2023-08-30 07:16:11+00:00,"We present the results of the second Neural MMO challenge, hosted at IJCAI
2022, which received 1600+ submissions. This competition targets robustness and
generalization in multi-agent systems: participants train teams of agents to
complete a multi-task objective against opponents not seen during training. The
competition combines relatively complex environment design with large numbers
of agents in the environment. The top submissions demonstrate strong success on
this task using mostly standard reinforcement learning (RL) methods combined
with domain-specific engineering. We summarize the competition design and
results and suggest that, as an academic community, competitions may be a
powerful approach to solving hard problems and establishing a solid benchmark
for algorithms. We will open-source our benchmark including the environment
wrapper, baselines, a visualization tool, and selected policies for further
research.",http://arxiv.org/pdf/2308.15802v1
2308.15786v1,cs.LG,FedCiR: Client-Invariant Representation Learning for Federated Non-IID Features,2023-08-30 06:36:32+00:00,"Federated learning (FL) is a distributed learning paradigm that maximizes the
potential of data-driven models for edge devices without sharing their raw
data. However, devices often have non-independent and identically distributed
(non-IID) data, meaning their local data distributions can vary significantly.
The heterogeneity in input data distributions across devices, commonly referred
to as the feature shift problem, can adversely impact the training convergence
and accuracy of the global model. To analyze the intrinsic causes of the
feature shift problem, we develop a generalization error bound in FL, which
motivates us to propose FedCiR, a client-invariant representation learning
framework that enables clients to extract informative and client-invariant
features. Specifically, we improve the mutual information term between
representations and labels to encourage representations to carry essential
classification knowledge, and diminish the mutual information term between the
client set and representations conditioned on labels to promote representations
of clients to be client-invariant. We further incorporate two regularizers into
the FL framework to bound the mutual information terms with an approximate
global representation distribution to compensate for the absence of the
ground-truth global representation distribution, thus achieving informative and
client-invariant feature extraction. To achieve global representation
distribution approximation, we propose a data-free mechanism performed by the
server without compromising privacy. Extensive experiments demonstrate the
effectiveness of our approach in achieving client-invariant representation
learning and solving the data heterogeneity issue.",http://arxiv.org/pdf/2308.15786v1
2308.15742v1,cs.SD,ASTER: Automatic Speech Recognition System Accessibility Testing for Stutterers,2023-08-30 03:46:52+00:00,"The popularity of automatic speech recognition (ASR) systems nowadays leads
to an increasing need for improving their accessibility. Handling stuttering
speech is an important feature for accessible ASR systems. To improve the
accessibility of ASR systems for stutterers, we need to expose and analyze the
failures of ASR systems on stuttering speech. The speech datasets recorded from
stutterers are not diverse enough to expose most of the failures. Furthermore,
these datasets lack ground truth information about the non-stuttered text,
rendering them unsuitable as comprehensive test suites. Therefore, a
methodology for generating stuttering speech as test inputs to test and analyze
the performance of ASR systems is needed. However, generating valid test inputs
in this scenario is challenging. The reason is that although the generated test
inputs should mimic how stutterers speak, they should also be diverse enough to
trigger more failures. To address the challenge, we propose ASTER, a technique
for automatically testing the accessibility of ASR systems. ASTER can generate
valid test cases by injecting five different types of stuttering. The generated
test cases can both simulate realistic stuttering speech and expose failures in
ASR systems. Moreover, ASTER can further enhance the quality of the test cases
with a multi-objective optimization-based seed updating algorithm. We
implemented ASTER as a framework and evaluated it on four open-source ASR
models and three commercial ASR systems. We conduct a comprehensive evaluation
of ASTER and find that it significantly increases the word error rate, match
error rate, and word information loss in the evaluated ASR systems.
Additionally, our user study demonstrates that the generated stuttering audio
is indistinguishable from real-world stuttering audio clips.",http://arxiv.org/pdf/2308.15742v1
2308.15734v1,cs.LG,Efficient and Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search,2023-08-30 03:21:45+00:00,"Graph neural networks (GNNs) are powerful tools for performing data science
tasks in various domains. Although we use GNNs in wide application scenarios,
it is a laborious task for researchers and practitioners to design/select
optimal GNN rchitectures in diverse graphs. To save human efforts and
computational costs, graph neural architecture search (Graph NAS) has been used
to search for a sub-optimal GNN architecture that combines existing components.
However, there are no existing Graph NAS methods that satisfy explainability,
efficiency, and adaptability to various graphs. Therefore, we propose an
efficient and explainable Graph NAS method, called ExGNAS, which consists of
(i) a simple search space that can adapt to various graphs and (ii) a search
algorithm that makes the decision process explainable. The search space
includes only fundamental functions that can handle homophilic and heterophilic
graphs. The search algorithm efficiently searches for the best GNN architecture
via Monte-Carlo tree search without neural models. The combination of our
search space and algorithm achieves finding accurate GNN models and the
important functions within the search space. We comprehensively evaluate our
method compared with twelve hand-crafted GNN architectures and three Graph NAS
methods in four graphs. Our experimental results show that ExGNAS increases AUC
up to 3.6 and reduces run time up to 78\% compared with the state-of-the-art
Graph NAS methods. Furthermore, we show ExGNAS is effective in analyzing the
difference between GNN architectures in homophilic and heterophilic graphs.",http://arxiv.org/pdf/2308.15734v1
2308.15726v1,cs.SD,AGS: An Dataset and Taxonomy for Domestic Scene Sound Event Recognition,2023-08-30 03:03:47+00:00,"Environmental sound scene and sound event recognition is important for the
recognition of suspicious events in indoor and outdoor environments (such as
nurseries, smart homes, nursing homes, etc.) and is a fundamental task involved
in many audio surveillance applications. In particular, there is no public
common data set for the research field of sound event recognition for the data
set of the indoor environmental sound scene. Therefore, this paper proposes a
data set (called as AGS) for the home environment sound. This data set
considers various types of overlapping audio in the scene, background noise.
Moreover, based on the proposed data set, this paper compares and analyzes the
advanced methods for sound event recognition, and then illustrates the
reliability of the data set proposed in this paper, and studies the challenges
raised by the new data set. Our proposed AGS and the source code of the
corresponding baselines at https://github.com/taolunzu11/AGS .",http://arxiv.org/pdf/2308.15726v1
2308.15720v1,cs.LG,Surrogate-based Autotuning for Randomized Sketching Algorithms in Regression Problems,2023-08-30 02:50:54+00:00,"Algorithms from Randomized Numerical Linear Algebra (RandNLA) are known to be
effective in handling high-dimensional computational problems, providing
high-quality empirical performance as well as strong probabilistic guarantees.
However, their practical application is complicated by the fact that the user
needs to set various algorithm-specific tuning parameters which are different
than those used in traditional NLA. This paper demonstrates how a
surrogate-based autotuning approach can be used to address fundamental problems
of parameter selection in RandNLA algorithms. In particular, we provide a
detailed investigation of surrogate-based autotuning for
sketch-and-precondition (SAP) based randomized least squares methods, which
have been one of the great success stories in modern RandNLA. Empirical results
show that our surrogate-based autotuning approach can achieve near-optimal
performance with much less tuning cost than a random search (up to about 4x
fewer trials of different parameter configurations). Moreover, while our
experiments focus on least squares, our results demonstrate a general-purpose
autotuning pipeline applicable to any kind of RandNLA algorithm.",http://arxiv.org/pdf/2308.15720v1
2308.15711v1,cs.CL,Optimizing Factual Accuracy in Text Generation through Dynamic Knowledge Selection,2023-08-30 02:22:40+00:00,"Language models (LMs) have revolutionized the way we interact with
information, but they often generate nonfactual text, raising concerns about
their reliability. Previous methods use external knowledge as references for
text generation to enhance factuality but often struggle with the knowledge
mix-up(e.g., entity mismatch) of irrelevant references. Besides,as the length
of the output text grows, the randomness of sampling can escalate,
detrimentally impacting the factual accuracy of the generated text. In this
paper, we present DKGen, which divide the text generation process into an
iterative process. In each iteration, DKGen takes the input query, the
previously generated text and a subset of the reference passages as input to
generate short text. During the process, the subset is dynamically selected
from the full passage set based on their relevance to the previously generated
text and the query, largely eliminating the irrelevant references from input.
To further enhance DKGen's ability to correctly use these external knowledge,
DKGen distills the relevance order of reference passages to the cross-attention
distribution of decoder. We train and evaluate DKGen on a large-scale benchmark
dataset. Experiment results show that DKGen outperforms all baseline models.",http://arxiv.org/pdf/2308.15711v1
2308.15710v1,cs.AI,Speech Wikimedia: A 77 Language Multilingual Speech Dataset,2023-08-30 02:14:49+00:00,"The Speech Wikimedia Dataset is a publicly available compilation of audio
with transcriptions extracted from Wikimedia Commons. It includes 1780 hours
(195 GB) of CC-BY-SA licensed transcribed speech from a diverse set of
scenarios and speakers, in 77 different languages. Each audio file has one or
more transcriptions in different languages, making this dataset suitable for
training speech recognition, speech translation, and machine translation
models.",http://arxiv.org/pdf/2308.15710v1
2308.15704v1,cs.AI,Towards a Rigorous Analysis of Mutual Information in Contrastive Learning,2023-08-30 01:59:42+00:00,"Contrastive learning has emerged as a cornerstone in recent achievements of
unsupervised representation learning. Its primary paradigm involves an instance
discrimination task with a mutual information loss. The loss is known as
InfoNCE and it has yielded vital insights into contrastive learning through the
lens of mutual information analysis. However, the estimation of mutual
information can prove challenging, creating a gap between the elegance of its
mathematical foundation and the complexity of its estimation. As a result,
drawing rigorous insights or conclusions from mutual information analysis
becomes intricate. In this study, we introduce three novel methods and a few
related theorems, aimed at enhancing the rigor of mutual information analysis.
Despite their simplicity, these methods can carry substantial utility.
Leveraging these approaches, we reassess three instances of contrastive
learning analysis, illustrating their capacity to facilitate deeper
comprehension or to rectify pre-existing misconceptions. Specifically, we
investigate small batch size, mutual information as a measure, and the InfoMin
principle.",http://arxiv.org/pdf/2308.15704v1
2308.15700v1,cs.HC,Training Towards Critical Use: Learning to Situate AI Predictions Relative to Human Knowledge,2023-08-30 01:54:31+00:00,"A growing body of research has explored how to support humans in making
better use of AI-based decision support, including via training and onboarding.
Existing research has focused on decision-making tasks where it is possible to
evaluate ""appropriate reliance"" by comparing each decision against a ground
truth label that cleanly maps to both the AI's predictive target and the human
decision-maker's goals. However, this assumption does not hold in many
real-world settings where AI tools are deployed today (e.g., social work,
criminal justice, and healthcare). In this paper, we introduce a
process-oriented notion of appropriate reliance called critical use that
centers the human's ability to situate AI predictions against knowledge that is
uniquely available to them but unavailable to the AI model. To explore how
training can support critical use, we conduct a randomized online experiment in
a complex social decision-making setting: child maltreatment screening. We find
that, by providing participants with accelerated, low-stakes opportunities to
practice AI-assisted decision-making in this setting, novices came to exhibit
patterns of disagreement with AI that resemble those of experienced workers. A
qualitative examination of participants' explanations for their AI-assisted
decisions revealed that they drew upon qualitative case narratives, to which
the AI model did not have access, to learn when (not) to rely on AI
predictions. Our findings open new questions for the study and design of
training for real-world AI-assisted decision-making.",http://arxiv.org/pdf/2308.15700v1
2308.15690v1,cs.CV,CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts,2023-08-30 01:14:32+00:00,"We present 'CongNaMul', a comprehensive dataset designed for various tasks in
soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate
tasks such as image classification, semantic segmentation, decomposition, and
measurement of length and weight. The classification task provides four classes
to determine the quality of soybean sprouts: normal, broken, spotted, and
broken and spotted, for the development of AI-aided automatic quality
inspection technology. For semantic segmentation, images with varying
complexity, from single sprout images to images with multiple sprouts, along
with human-labelled mask images, are included. The label has 4 different
classes: background, head, body, tail. The dataset also provides images and
masks for the image decomposition task, including two separate sprout images
and their combined form. Lastly, 5 physical features of sprouts (head length,
body length, body thickness, tail length, weight) are provided for image-based
measurement tasks. This dataset is expected to be a valuable resource for a
wide range of research and applications in the advanced analysis of images of
soybean sprouts. Also, we hope that this dataset can assist researchers
studying classification, semantic segmentation, decomposition, and physical
feature measurement in other industrial fields, in evaluating their models. The
dataset is available at the authors' repository. (https://bhban.kr/data)",http://arxiv.org/pdf/2308.15690v1
2308.15684v1,cs.RO,Interactively Robot Action Planning with Uncertainty Analysis and Active Questioning by Large Language Model,2023-08-30 00:54:44+00:00,"The application of the Large Language Model (LLM) to robot action planning
has been actively studied. The instructions given to the LLM by natural
language may include ambiguity and lack of information depending on the task
context. It is possible to adjust the output of LLM by making the instruction
input more detailed; however, the design cost is high. In this paper, we
propose the interactive robot action planning method that allows the LLM to
analyze and gather missing information by asking questions to humans. The
method can minimize the design cost of generating precise robot instructions.
We demonstrated the effectiveness of our method through concrete examples in
cooking tasks. However, our experiments also revealed challenges in robot
action planning with LLM, such as asking unimportant questions and assuming
crucial information without asking. Shedding light on these issues provides
valuable insights for future research on utilizing LLM for robotics.",http://arxiv.org/pdf/2308.15684v1
2308.15670v1,cs.CV,Multimodal Foundation Models For Echocardiogram Interpretation,2023-08-29 23:45:54+00:00,"Multimodal deep learning foundation models can learn the relationship between
images and text. In the context of medical imaging, mapping images to language
concepts reflects the clinical task of diagnostic image interpretation, however
current general-purpose foundation models do not perform well in this context
because their training corpus have limited medical text and images. To address
this challenge and account for the range of cardiac physiology, we leverage
1,032,975 cardiac ultrasound videos and corresponding expert interpretations to
develop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP
displays strong zero-shot (not explicitly trained) performance in cardiac
function assessment (external validation left ventricular ejection fraction
mean absolute error (MAE) of 7.1%) and identification of implanted intracardiac
devices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and
artificial heart valves). We also developed a long-context variant (EchoCLIP-R)
with a custom echocardiography report text tokenizer which can accurately
identify unique patients across multiple videos (AUC of 0.86), identify
clinical changes such as orthotopic heart transplants (AUC of 0.79) or cardiac
surgery (AUC 0.77), and enable robust image-to-text search (mean cross-modal
retrieval rank in the top 1% of candidate text reports). These emergent
capabilities can be used for preliminary assessment and summarization of
echocardiographic findings.",http://arxiv.org/pdf/2308.15670v1
2308.15656v1,cs.RO,Deep Reinforcement Learning Based Framework for Mobile Energy Disseminator Dispatching to Charge On-the-Road Electric Vehicles,2023-08-29 22:23:52+00:00,"The exponential growth of electric vehicles (EVs) presents novel challenges
in preserving battery health and in addressing the persistent problem of
vehicle range anxiety. To address these concerns, wireless charging,
particularly, Mobile Energy Disseminators (MEDs) have emerged as a promising
solution. The MED is mounted behind a large vehicle and charges all
participating EVs within a radius upstream of it. Unfortuantely, during such
V2V charging, the MED and EVs inadvertently form platoons, thereby occupying
multiple lanes and impairing overall corridor travel efficiency. In addition,
constrained budgets for MED deployment necessitate the development of an
effective dispatching strategy to determine optimal timing and locations for
introducing the MEDs into traffic. This paper proposes a deep reinforcement
learning (DRL) based methodology to develop a vehicle dispatching framework. In
the first component of the framework, we develop a realistic reinforcement
learning environment termed ""ChargingEnv"" which incorporates a reliable
charging simulation system that accounts for common practical issues in
wireless charging deployment, specifically, the charging panel misalignment.
The second component, the Proximal-Policy Optimization (PPO) agent, is trained
to control MED dispatching through continuous interactions with ChargingEnv.
Numerical experiments were carried out to demonstrate the demonstrate the
efficacy of the proposed MED deployment decision processor. The experiment
results suggest that the proposed model can significantly enhance EV travel
range while efficiently deploying a optimal number of MEDs. The proposed model
is found to be not only practical in its applicability but also has promises of
real-world effectiveness. The proposed model can help travelers to maximize EV
range and help road agencies or private-sector vendors to manage the deployment
of MEDs efficiently.",http://arxiv.org/pdf/2308.15656v1
2308.15647v1,cs.LG,A General Recipe for Automated Machine Learning in Practice,2023-08-29 21:49:28+00:00,"Automated Machine Learning (AutoML) is an area of research that focuses on
developing methods to generate machine learning models automatically. The idea
of being able to build machine learning models with very little human
intervention represents a great opportunity for the practice of applied machine
learning. However, there is very little information on how to design an AutoML
system in practice. Most of the research focuses on the problems facing
optimization algorithms and leaves out the details of how that would be done in
practice. In this paper, we propose a frame of reference for building general
AutoML systems. Through a narrative review of the main approaches in the area,
our main idea is to distill the fundamental concepts in order to support them
in a single design. Finally, we discuss some open problems related to the
application of AutoML for future research.",http://arxiv.org/pdf/2308.15647v1
2308.15645v1,cs.PL,AskIt: Unified Programming Interface for Programming with Large Language Models,2023-08-29 21:44:27+00:00,"In the evolving landscape of software development, Large Language Models
(LLMs) exhibit a unique phenomenon known as emergent abilities, demonstrating
adeptness across numerous tasks, from text summarization to code generation.
While these abilities open up novel avenues in software design and crafting,
their incorporation presents substantial challenges. Developers grapple with
decisions surrounding the direct embedding of LLMs within applications versus
employing them for code generation. Moreover, effective prompt design becomes a
critical concern, given the necessity of data extraction from natural language
outputs. To address these intricacies, this paper introduces AskIt, a
domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies
LLM integration, offering type-guided output control, template-based function
definitions, and a unified interface that diminishes the distinction between
LLM-based code generation and application integration. Furthermore, through
Programming by Example (PBE), AskIt harnesses the power of few-shot learning at
the programming language level. Our evaluations underscore AskIt's potency.
Across 50 tasks, AskIt generated concise prompts for the given tasks, achieving
a 16.14% reduction in prompt length relative to benchmarks. Additionally, by
enabling the transition from direct LLM application usage to function
generation, AskIt achieved significant speedups, as observed in our GSM8K
benchmark experiments. Through these advancements, AskIt streamlines the
integration of LLMs in software development, offering a more efficient,
versatile approach for leveraging emergent abilities. The implementations of
AskIt in TypeScript and Python are available at
https://github.com/katsumiok/ts-askit and https://github.com/katsumiok/pyaskit,
respectively.",http://arxiv.org/pdf/2308.15645v1
2308.15639v1,cs.LG,Hyperbolic Convolutional Neural Networks,2023-08-29 21:20:16+00:00,"Deep Learning is mostly responsible for the surge of interest in Artificial
Intelligence in the last decade. So far, deep learning researchers have been
particularly successful in the domain of image processing, where Convolutional
Neural Networks are used. Although excelling at image classification,
Convolutional Neural Networks are quite naive in that no inductive bias is set
on the embedding space for images. Similar flaws are also exhibited by another
type of Convolutional Networks - Graph Convolutional Neural Networks. However,
using non-Euclidean space for embedding data might result in more robust and
explainable models. One example of such a non-Euclidean space is hyperbolic
space. Hyperbolic spaces are particularly useful due to their ability to fit
more data in a low-dimensional space and tree-likeliness properties. These
attractive properties have been previously used in multiple papers which
indicated that they are beneficial for building hierarchical embeddings using
shallow models and, recently, using MLPs and RNNs.
  However, no papers have yet suggested a general approach to using Hyperbolic
Convolutional Neural Networks for structured data processing, although these
are the most common examples of data used. Therefore, the goal of this work is
to devise a general recipe for building Hyperbolic Convolutional Neural
Networks. We hypothesize that ability of hyperbolic space to capture hierarchy
in the data would lead to better performance. This ability should be
particularly useful in cases where data has a tree-like structure. Since this
is the case for many existing datasets \citep{wordnet, imagenet, fb15k}, we
argue that such a model would be advantageous both in terms of applications and
future research prospects.",http://arxiv.org/pdf/2308.15639v1
2308.15620v1,cs.AI,Intelligent System for Assessing University Student Personality Development and Career Readiness,2023-08-29 20:32:58+00:00,"While academic metrics such as transcripts and GPA are commonly used to
evaluate students' knowledge acquisition, there is a lack of comprehensive
metrics to measure their preparedness for the challenges of post-graduation
life. This research paper explores the impact of various factors on university
students' readiness for change and transition, with a focus on their
preparedness for careers. The methodology employed in this study involves
designing a survey based on Paul J. Mayer's ""The Balance Wheel"" to capture
students' sentiments on various life aspects, including satisfaction with the
educational process and expectations of salary. The collected data from a KBTU
student survey (n=47) were processed through machine learning models: Linear
Regression, Support Vector Regression (SVR), Random Forest Regression.
Subsequently, an intelligent system was built using these models and fuzzy
sets. The system is capable of evaluating graduates' readiness for their future
careers and demonstrates a high predictive power. The findings of this research
have practical implications for educational institutions. Such an intelligent
system can serve as a valuable tool for universities to assess and enhance
students' preparedness for post-graduation challenges. By recognizing the
factors contributing to students' readiness for change, universities can refine
curricula and processes to better prepare students for their career journeys.",http://arxiv.org/pdf/2308.15620v1
2308.15609v1,cs.LG,InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning,2023-08-29 20:02:24+00:00,"One-Shot Neural Architecture Search (NAS) algorithms often rely on training a
hardware agnostic super-network for a domain specific task. Optimal
sub-networks are then extracted from the trained super-network for different
hardware platforms. However, training super-networks from scratch can be
extremely time consuming and compute intensive especially for large models that
rely on a two-stage training process of pre-training and fine-tuning. State of
the art pre-trained models are available for a wide range of tasks, but their
large sizes significantly limits their applicability on various hardware
platforms. We propose InstaTune, a method that leverages off-the-shelf
pre-trained weights for large models and generates a super-network during the
fine-tuning stage. InstaTune has multiple benefits. Firstly, since the process
happens during fine-tuning, it minimizes the overall time and compute resources
required for NAS. Secondly, the sub-networks extracted are optimized for the
target task, unlike prior work that optimizes on the pre-training objective.
Finally, InstaTune is easy to ""plug and play"" in existing frameworks. By using
multi-objective evolutionary search algorithms along with lightly trained
predictors, we find Pareto-optimal sub-networks that outperform their
respective baselines across different performance objectives such as accuracy
and MACs. Specifically, we demonstrate that our approach performs well across
both unimodal (ViT and BERT) and multi-modal (BEiT-3) transformer based
architectures.",http://arxiv.org/pdf/2308.15609v1
2308.15594v1,cs.LG,Can transformers learn the greatest common divisor?,2023-08-29 19:38:41+00:00,"I investigate the capability of small transformers to compute the greatest
common divisor (GCD) of two positive integers. When the training distribution
and the representation base are carefully chosen, models achieve 98% accuracy
and correctly predict 91 of the 100 first GCD. Model predictions are
deterministic and fully interpretable. During training, the models learn to
cluster input pairs with the same GCD, and classify them by their divisors.
Basic models, trained from uniform operands encoded on small bases, only
compute a handful of GCD (up to 38 out of 100): the products of divisors of the
base. Longer training and larger bases allow some models to ""grok"" small prime
GCD. Training from log-uniform operands boosts performance to 73 correct GCD,
and balancing the training distribution of GCD, from inverse square to
log-uniform, to 91 GCD. Training models from a uniform distribution of GCD
breaks the deterministic model behavior.",http://arxiv.org/pdf/2308.15594v1
2308.15568v1,cs.AI,Over-Squashing in Graph Neural Networks: A Comprehensive survey,2023-08-29 18:46:15+00:00,"Graph Neural Networks (GNNs) have emerged as a revolutionary paradigm in the
realm of machine learning, offering a transformative approach to dissect
intricate relationships inherent in graph-structured data. The foundational
architecture of most GNNs involves the dissemination of information through
message aggregation and transformation among interconnected nodes, a mechanism
that has demonstrated remarkable efficacy across diverse applications
encompassing node classification, link prediction, and recommendation systems.
Nonetheless, their potential prowess encounters a restraint intrinsic to
scenarios necessitating extensive contextual insights. In certain contexts,
accurate predictions hinge not only upon a node's immediate local surroundings
but also on interactions spanning far-reaching domains. This intricate demand
for long-range information dissemination exposes a pivotal challenge recognized
as ""over-squashing,"" wherein the fidelity of information flow from distant
nodes becomes distorted. This phenomenon significantly curtails the efficiency
of message-passing mechanisms, particularly for tasks reliant on intricate
long-distance interactions. In this comprehensive article, we illuminate the
prevalent constraint of over-squashing pervading GNNs. Our exploration entails
a meticulous exposition of the ongoing efforts by researchers to improve the
ramifications posed by this limitation. Through systematic elucidation, we
delve into strategies, methodologies, and innovations proposed thus far, all
aimed at mitigating the detriments of over-squashing. By shedding light on this
intricately woven issue, we aim to contribute to a nuanced understanding of the
challenges within the GNN landscape and the evolving solutions designed to
surmount them.",http://arxiv.org/pdf/2308.15568v1
2308.15560v1,physics.ao-ph,WeatherBench 2: A benchmark for the next generation of data-driven global weather models,2023-08-29 18:32:08+00:00,"WeatherBench 2 is an update to the global, medium-range (1-14 day) weather
forecasting benchmark proposed by Rasp et al. (2020), designed with the aim to
accelerate progress in data-driven weather modeling. WeatherBench 2 consists of
an open-source evaluation framework, publicly available training, ground truth
and baseline data as well as a continuously updated website with the latest
metrics and state-of-the-art models:
https://sites.research.google/weatherbench. This paper describes the design
principles of the evaluation framework and presents results for current
state-of-the-art physical and data-driven weather models. The metrics are based
on established practices for evaluating weather forecasts at leading
operational weather centers. We define a set of headline scores to provide an
overview of model performance. In addition, we also discuss caveats in the
current evaluation setup and challenges for the future of data-driven weather
forecasting.",http://arxiv.org/pdf/2308.15560v1
2308.15550v1,cs.LG,Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning,2023-08-29 18:17:35+00:00,"This paper proposes an algorithm that aims to improve generalization for
reinforcement learning agents by removing overfitting to confounding features.
Our approach consists of a max-min game theoretic objective. A generator
transfers the style of observation during reinforcement learning. An additional
goal of the generator is to perturb the observation, which maximizes the
agent's probability of taking a different action. In contrast, a policy network
updates its parameters to minimize the effect of such perturbations, thus
staying robust while maximizing the expected future reward. Based on this
setup, we propose a practical deep reinforcement learning algorithm,
Adversarial Robust Policy Optimization (ARPO), to find a robust policy that
generalizes to unseen environments. We evaluate our approach on Procgen and
Distracting Control Suite for generalization and sample efficiency.
Empirically, ARPO shows improved performance compared to a few baseline
algorithms, including data augmentation.",http://arxiv.org/pdf/2308.15550v1
2308.15474v1,cs.CV,A General-Purpose Self-Supervised Model for Computational Pathology,2023-08-29 17:52:10+00:00,"Tissue phenotyping is a fundamental computational pathology (CPath) task in
learning objective characterizations of histopathologic biomarkers in anatomic
pathology. However, whole-slide imaging (WSI) poses a complex computer vision
problem in which the large-scale image resolutions of WSIs and the enormous
diversity of morphological phenotypes preclude large-scale data annotation.
Current efforts have proposed using pretrained image encoders with either
transfer learning from natural image datasets or self-supervised pretraining on
publicly-available histopathology datasets, but have not been extensively
developed and evaluated across diverse tissue types at scale. We introduce UNI,
a general-purpose self-supervised model for pathology, pretrained using over
100 million tissue patches from over 100,000 diagnostic haematoxylin and
eosin-stained WSIs across 20 major tissue types, and evaluated on 33
representative CPath clinical tasks in CPath of varying diagnostic
difficulties. In addition to outperforming previous state-of-the-art models, we
demonstrate new modeling capabilities in CPath such as resolution-agnostic
tissue classification, slide classification using few-shot class prototypes,
and disease subtyping generalization in classifying up to 108 cancer types in
the OncoTree code classification system. UNI advances unsupervised
representation learning at scale in CPath in terms of both pretraining data and
downstream evaluation, enabling data-efficient AI models that can generalize
and transfer to a gamut of diagnostically-challenging tasks and clinical
workflows in anatomic pathology.",http://arxiv.org/pdf/2308.15474v1
2308.15469v1,cs.CV,Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer's Disease Prediction,2023-08-29 17:48:33+00:00,"Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD)
datasets contain valuable tabular data including AD biomarkers and clinical
assessments. Existing computer vision approaches struggle to utilize this
additional information. To address these needs, we propose a generalizable
framework for multimodal contrastive learning of image data and tabular data, a
novel tabular attention module for amplifying and ranking salient features in
tables, and the application of these techniques onto Alzheimer's disease
prediction. Experimental evaulations demonstrate the strength of our framework
by detecting Alzheimer's disease (AD) from over 882 MR image slices from the
ADNI database. We take advantage of the high interpretability of tabular data
and our novel tabular attention approach and through attribution of the
attention scores for each row of the table, we note and rank the most
predominant features. Results show that the model is capable of an accuracy of
over 83.8%, almost a 10% increase from previous state of the art.",http://arxiv.org/pdf/2308.15469v1
2308.15464v1,cs.LG,A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios,2023-08-29 17:44:02+00:00,"Spatiotemporal graph neural networks have achieved state-of-the-art
performance in traffic forecasting. However, they often struggle to forecast
congestion accurately due to the limitations of traditional loss functions.
While accurate forecasting of regular traffic conditions is crucial, a reliable
AI system must also accurately forecast congestion scenarios to maintain safe
and efficient transportation. In this paper, we explore various loss functions
inspired by heavy tail analysis and imbalanced classification problems to
address this issue. We evaluate the efficacy of these loss functions in
forecasting traffic speed, with an emphasis on congestion scenarios. Through
extensive experiments on real-world traffic datasets, we discovered that when
optimizing for Mean Absolute Error (MAE), the MAE-Focal Loss function stands
out as the most effective. When optimizing Mean Squared Error (MSE), Gumbel
Loss proves to be the superior choice. These choices effectively forecast
traffic congestion events without compromising the accuracy of regular traffic
speed forecasts. This research enhances deep learning models' capabilities in
forecasting sudden speed changes due to congestion and underscores the need for
more research in this direction. By elevating the accuracy of congestion
forecasting, we advocate for AI systems that are reliable, secure, and
resilient in practical traffic management scenarios.",http://arxiv.org/pdf/2308.15464v1
2308.15459v1,cs.CL,ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer,2023-08-29 17:36:02+00:00,"Textual style transfer is the task of transforming stylistic properties of
text while preserving meaning. Target ""styles"" can be defined in numerous ways,
ranging from single attributes (e.g, formality) to authorship (e.g,
Shakespeare). Previous unsupervised style-transfer approaches generally rely on
significant amounts of labeled data for only a fixed set of styles or require
large language models. In contrast, we introduce a novel diffusion-based
framework for general-purpose style transfer that can be flexibly adapted to
arbitrary target styles at inference time. Our parameter-efficient approach,
ParaGuide, leverages paraphrase-conditioned diffusion models alongside
gradient-based guidance from both off-the-shelf classifiers and strong existing
style embedders to transform the style of text while preserving semantic
information. We validate the method on the Enron Email Corpus, with both human
and automatic evaluations, and find that it outperforms strong baselines on
formality, sentiment, and even authorship style transfer.",http://arxiv.org/pdf/2308.15459v1
2308.15457v1,cs.LG,From SMOTE to Mixup for Deep Imbalanced Classification,2023-08-29 17:31:26+00:00,"Given imbalanced data, it is hard to train a good classifier using deep
learning because of the poor generalization of minority classes. Traditionally,
the well-known synthetic minority oversampling technique (SMOTE) for data
augmentation, a data mining approach for imbalanced learning, has been used to
improve this generalization. However, it is unclear whether SMOTE also benefits
deep learning. In this work, we study why the original SMOTE is insufficient
for deep learning, and enhance SMOTE using soft labels. Connecting the
resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to
a unified framework that puts traditional and modern data augmentation
techniques under the same umbrella. A careful study within this framework shows
that Mixup improves generalization by implicitly achieving uneven margins
between majority and minority classes. We then propose a novel margin-aware
Mixup technique that more explicitly achieves uneven margins. Extensive
experimental results demonstrate that our proposed technique yields
state-of-the-art performance on deep imbalanced classification while achieving
superior performance on extremely imbalanced data. The code is open-sourced in
our developed package https://github.com/ntucllab/imbalanced-DL to foster
future research in this direction.",http://arxiv.org/pdf/2308.15457v1
2308.15452v1,cs.CL,When Do Program-of-Thoughts Work for Reasoning?,2023-08-29 17:22:39+00:00,"The reasoning capabilities of Large Language Models (LLMs) play a pivotal
role in the realm of embodied artificial intelligence. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.",http://arxiv.org/pdf/2308.15452v1
2308.15514v1,cs.AI,International Governance of Civilian AI: A Jurisdictional Certification Approach,2023-08-29 16:43:59+00:00,"This report describes trade-offs in the design of international governance
arrangements for civilian artificial intelligence (AI) and presents one
approach in detail. This approach represents the extension of a standards,
licensing, and liability regime to the global level. We propose that states
establish an International AI Organization (IAIO) to certify state
jurisdictions (not firms or AI projects) for compliance with international
oversight standards. States can give force to these international standards by
adopting regulations prohibiting the import of goods whose supply chains embody
AI from non-IAIO-certified jurisdictions. This borrows attributes from models
of existing international organizations, such as the International Civilian
Aviation Organization (ICAO), the International Maritime Organization (IMO),
and the Financial Action Task Force (FATF). States can also adopt multilateral
controls on the export of AI product inputs, such as specialized hardware, to
non-certified jurisdictions. Indeed, both the import and export standards could
be required for certification. As international actors reach consensus on risks
of and minimum standards for advanced AI, a jurisdictional certification regime
could mitigate a broad range of potential harms, including threats to public
safety.",http://arxiv.org/pdf/2308.15514v1
2308.15427v1,cs.CV,Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction,2023-08-29 16:33:16+00:00,"High-Definition (HD) maps play a crucial role in autonomous driving systems.
Recent methods have attempted to construct HD maps in real-time based on
information obtained from vehicle onboard sensors. However, the performance of
these methods is significantly susceptible to the environment surrounding the
vehicle due to the inherent limitation of onboard sensors, such as weak
capacity for long-range detection. In this study, we demonstrate that
supplementing onboard sensors with satellite maps can enhance the performance
of HD map construction methods, leveraging the broad coverage capability of
satellite maps. For the purpose of further research, we release the satellite
map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose
a hierarchical fusion module that enables better fusion of satellite maps
information with existing methods. Specifically, we design an attention mask
based on segmentation and distance, applying the cross-attention mechanism to
fuse onboard Bird's Eye View (BEV) features and satellite features in
feature-level fusion. An alignment module is introduced before concatenation in
BEV-level fusion to mitigate the impact of misalignment between the two
features. The experimental results on the augmented nuScenes dataset showcase
the seamless integration of our module into three existing HD map construction
methods. It notably enhances their performance in both HD map semantic
segmentation and instance detection tasks.",http://arxiv.org/pdf/2308.15427v1
2308.15513v1,cs.LG,Tuning the perplexity for and computing sampling-based t-SNE embeddings,2023-08-29 16:24:11+00:00,"Widely used pipelines for the analysis of high-dimensional data utilize
two-dimensional visualizations. These are created, e.g., via t-distributed
stochastic neighbor embedding (t-SNE). When it comes to large data sets,
applying these visualization techniques creates suboptimal embeddings, as the
hyperparameters are not suitable for large data. Cranking up these parameters
usually does not work as the computations become too expensive for practical
workflows. In this paper, we argue that a sampling-based embedding approach can
circumvent these problems. We show that hyperparameters must be chosen
carefully, depending on the sampling rate and the intended final embedding.
Further, we show how this approach speeds up the computation and increases the
quality of the embeddings.",http://arxiv.org/pdf/2308.15513v1
2308.15397v1,cs.CV,Color Aesthetics: Fuzzy based User-driven Method for Harmony and Preference Prediction,2023-08-29 15:56:38+00:00,"Color is the most important intrinsic sensory feature that has a powerful
impact on product sales. Color is even responsible for raising the aesthetic
senses in our brains. Account for individual differences is crucial in color
aesthetics. It requires user-driven mechanisms for various e-commerce
applications. We propose a method for quantitative evaluation of all types of
perceptual responses to color(s): distinct color preference, color harmony, and
color combination preference. Preference for color schemes can be predicted by
combining preferences for the basic colors and ratings of color harmony.
Harmonious pallets are extracted from big data set using comparison algorithms
based on fuzzy similarity and grouping. The proposed model results in useful
predictions of harmony and preference of multicolored images. For example, in
the context of apparel coordination, it allows predicting a preference for a
look based on clothing colors. Our approach differs from standard aesthetic
models, since in accounts for a personal variation. In addition, it can process
not only lower-order color pairs, but also groups of several colors.",http://arxiv.org/pdf/2308.15397v1
2308.15394v1,cs.AI,Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System,2023-08-29 15:48:49+00:00,"This paper develops a Decentralized Multi-Agent Reinforcement Learning
(Dec-MARL) method to solve the SoC balancing problem in the distributed energy
storage system (DESS). First, the SoC balancing problem is formulated into a
finite Markov decision process with action constraints derived from demand
balance, which can be solved by Dec-MARL. Specifically, the first-order average
consensus algorithm is utilized to expand the observations of the DESS state in
a fully-decentralized way, and the initial actions (i.e., output power) are
decided by the agents (i.e., energy storage units) according to these
observations. In order to get the final actions in the allowable range, a
counterfactual demand balance algorithm is proposed to balance the total demand
and the initial actions. Next, the agents execute the final actions and get
local rewards from the environment, and the DESS steps into the next state.
Finally, through the first-order average consensus algorithm, the agents get
the average reward and the expended observation of the next state for later
training. By the above procedure, Dec-MARL reveals outstanding performance in a
fully-decentralized system without any expert experience or constructing any
complicated model. Besides, it is flexible and can be extended to other
decentralized multi-agent systems straightforwardly. Extensive simulations have
validated the effectiveness and efficiency of Dec-MARL.",http://arxiv.org/pdf/2308.15394v1
2308.15390v1,cs.AI,Bayesian Integration of Information Using Top-Down Modulated WTA Networks,2023-08-29 15:33:51+00:00,"Winner Take All (WTA) circuits a type of Spiking Neural Networks (SNN) have
been suggested as facilitating the brain's ability to process information in a
Bayesian manner. Research has shown that WTA circuits are capable of
approximating hierarchical Bayesian models via Expectation Maximization (EM).
So far, research in this direction has focused on bottom up processes. This is
contrary to neuroscientific evidence that shows that, besides bottom up
processes, top down processes too play a key role in information processing by
the human brain. Several functions ascribed to top down processes include
direction of attention, adjusting for expectations, facilitation of encoding
and recall of learned information, and imagery. This paper explores whether WTA
circuits are suitable for further integrating information represented in
separate WTA networks. Furthermore, it explores whether, and under what
circumstances, top down processes can improve WTA network performance with
respect to inference and learning. The results show that WTA circuits are
capable of integrating the probabilistic information represented by other WTA
networks, and that top down processes can improve a WTA network's inference and
learning performance. Notably, it is able to do this according to key
neuromorphic principles, making it ideal for low-latency and energy efficient
implementation on neuromorphic hardware.",http://arxiv.org/pdf/2308.15390v1
2308.15368v1,cs.RO,RED: A Systematic Real-Time Scheduling Approach for Robotic Environmental Dynamics,2023-08-29 15:04:08+00:00,"Intelligent robots are designed to effectively navigate dynamic and
unpredictable environments laden with moving mechanical elements and objects.
Such environment-induced dynamics, including moving obstacles, can readily
alter the computational demand (e.g., the creation of new tasks) and the
structure of workloads (e.g., precedence constraints among tasks) during
runtime, thereby adversely affecting overall system performance. This challenge
is amplified when multi-task inference is expected on robots operating under
stringent resource and real-time constraints. To address such a challenge, we
introduce RED, a systematic real-time scheduling approach designed to support
multi-task deep neural network workloads in resource-limited robotic systems.
It is designed to adaptively manage the Robotic Environmental Dynamics (RED)
while adhering to real-time constraints. At the core of RED lies a
deadline-based scheduler that employs an intermediate deadline assignment
policy, effectively managing to change workloads and asynchronous inference
prompted by complex, unpredictable environments. This scheduling framework also
facilitates the flexible deployment of MIMONet (multi-input multi-output neural
networks), which are commonly utilized in multi-tasking robotic systems to
circumvent memory bottlenecks. Building on this scheduling framework, RED
recognizes and leverages a unique characteristic of MIMONet: its weight-shared
architecture. To further accommodate and exploit this feature, RED devises a
novel and effective workload refinement and reconstruction process. This
process ensures the scheduling framework's compatibility with MIMONet and
maximizes efficiency.",http://arxiv.org/pdf/2308.15368v1
2308.15367v1,cs.CV,Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation,2023-08-29 15:03:05+00:00,"Federated learning (FL) emerges as a decentralized learning framework which
trains models from multiple distributed clients without sharing their data to
preserve privacy. Recently, large-scale pre-trained models (e.g., Vision
Transformer) have shown a strong capability of deriving robust representations.
However, the data heterogeneity among clients, the limited computation
resources, and the communication bandwidth restrict the deployment of
large-scale models in FL frameworks. To leverage robust representations from
large-scale models while enabling efficient model personalization for
heterogeneous clients, we propose a novel personalized FL framework of
client-specific Prompt Generation (pFedPG), which learns to deploy a
personalized prompt generator at the server for producing client-specific
visual prompts that efficiently adapts frozen backbones to local data
distributions. Our proposed framework jointly optimizes the stages of
personalized prompt adaptation locally and personalized prompt generation
globally. The former aims to train visual prompts that adapt foundation models
to each client, while the latter observes local optimization directions to
generate personalized prompts for all clients. Through extensive experiments on
benchmark datasets, we show that our pFedPG is favorable against
state-of-the-art personalized FL methods under various types of data
heterogeneity, allowing computation and communication efficient model
personalization.",http://arxiv.org/pdf/2308.15367v1
2308.15357v1,cs.RO,Ego-Motion Estimation and Dynamic Motion Separation from 3D Point Clouds for Accumulating Data and Improving 3D Object Detection,2023-08-29 14:53:16+00:00,"New 3+1D high-resolution radar sensors are gaining importance for 3D object
detection in the automotive domain due to their relative affordability and
improved detection compared to classic low-resolution radar sensors. One
limitation of high-resolution radar sensors, compared to lidar sensors, is the
sparsity of the generated point cloud. This sparsity could be partially
overcome by accumulating radar point clouds of subsequent time steps. This
contribution analyzes limitations of accumulating radar point clouds on the
View-of-Delft dataset. By employing different ego-motion estimation approaches,
the dataset's inherent constraints, and possible solutions are analyzed.
Additionally, a learning-based instance motion estimation approach is deployed
to investigate the influence of dynamic motion on the accumulated point cloud
for object detection. Experiments document an improved object detection
performance by applying an ego-motion estimation and dynamic motion correction
approach.",http://arxiv.org/pdf/2308.15357v1
2308.15346v1,cs.CV,Enhancing Mobile Face Anti-Spoofing: A Robust Framework for Diverse Attack Types under Screen Flash,2023-08-29 14:41:40+00:00,"Face anti-spoofing (FAS) is crucial for securing face recognition systems.
However, existing FAS methods with handcrafted binary or pixel-wise labels have
limitations due to diverse presentation attacks (PAs). In this paper, we
propose an attack type robust face anti-spoofing framework under light flash,
called ATR-FAS. Due to imaging differences caused by various attack types,
traditional FAS methods based on single binary classification network may
result in excessive intra-class distance of spoof faces, leading to a challenge
of decision boundary learning. Therefore, we employed multiple networks to
reconstruct multi-frame depth maps as auxiliary supervision, and each network
experts in one type of attack. A dual gate module (DGM) consisting of a type
gate and a frame-attention gate is introduced, which perform attack type
recognition and multi-frame attention generation, respectively. The outputs of
DGM are utilized as weight to mix the result of multiple expert networks. The
multi-experts mixture enables ATR-FAS to generate spoof-differentiated depth
maps, and stably detects spoof faces without being affected by different types
of PAs. Moreover, we design a differential normalization procedure to convert
original flash frames into differential frames. This simple but effective
processing enhances the details in flash frames, aiding in the generation of
depth maps. To verify the effectiveness of our framework, we collected a
large-scale dataset containing 12,660 live and spoof videos with diverse PAs
under dynamic flash from the smartphone screen. Extensive experiments
illustrate that the proposed ATR-FAS significantly outperforms existing
state-of-the-art methods. The code and dataset will be available at
https://github.com/Chaochao-Lin/ATR-FAS.",http://arxiv.org/pdf/2308.15346v1
2308.15339v1,cs.AI,"AI Framework for Early Diagnosis of Coronary Artery Disease: An Integration of Borderline SMOTE, Autoencoders and Convolutional Neural Networks Approach",2023-08-29 14:33:38+00:00,"The accuracy of coronary artery disease (CAD) diagnosis is dependent on a
variety of factors, including demographic, symptom, and medical examination,
ECG, and echocardiography data, among others. In this context, artificial
intelligence (AI) can help clinicians identify high-risk patients early in the
diagnostic process, by synthesizing information from multiple factors. To this
aim, Machine Learning algorithms are used to classify patients based on their
CAD disease risk. In this study, we contribute to this research filed by
developing a methodology for balancing and augmenting data for more accurate
prediction when the data is imbalanced and the sample size is small. The
methodology can be used in a variety of other situations, particularly when
data collection is expensive and the sample size is small. The experimental
results revealed that the average accuracy of our proposed method for CAD
prediction was 95.36, and was higher than random forest (RF), decision tree
(DT), support vector machine (SVM), logistic regression (LR), and artificial
neural network (ANN).",http://arxiv.org/pdf/2308.15339v1
2308.15334v1,cs.CY,A Framework for Responsible Development of Automated Student Feedback with Generative AI,2023-08-29 14:29:57+00:00,"Providing rich feedback to students is essential for supporting student
learning. Recent advances in generative AI, particularly within large language
modelling (LLM), provide the opportunity to deliver repeatable, scalable and
instant automatically generated feedback to students, making abundant a
previously scarce and expensive learning resource. Such an approach is feasible
from a technical perspective due to these recent advances in Artificial
Intelligence (AI) and Natural Language Processing (NLP); while the potential
upside is a strong motivator, doing so introduces a range of potential ethical
issues that must be considered as we apply these technologies. The
attractiveness of AI systems is that they can effectively automate the most
mundane tasks; but this risks introducing a ""tyranny of the majority"", where
the needs of minorities in the long tail are overlooked because they are
difficult to automate.
  Developing machine learning models that can generate valuable and authentic
feedback requires the input of human domain experts. The choices we make in
capturing this expertise -- whose, which, when, and how -- will have
significant consequences for the nature of the resulting feedback. How we
maintain our models will affect how that feedback remains relevant given
temporal changes in context, theory, and prior learning profiles of student
cohorts. These questions are important from an ethical perspective; but they
are also important from an operational perspective. Unless they can be
answered, our AI generated systems will lack the trust necessary for them to be
useful features in the contemporary learning environment.
  This article will outline the frontiers of automated feedback, identify the
ethical issues involved in the provision of automated feedback and present a
framework to assist academics to develop such systems responsibly.",http://arxiv.org/pdf/2308.15334v1
2308.15324v1,cs.AI,FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models,2023-08-29 14:20:17+00:00,"Leveraging ``chain-of-thought (CoT)'' reasoning to elicit rapid and precise
responses from large language models (LLMs) is rapidly attracting research
interest. A notable challenge here is how to design or select optimal prompts.
The process of prompt selection relies on trial and error, involving continuous
adjustments and combinations of input prompts by users based on the
corresponding new responses generated from LLMs. Furthermore, minimal research
has been conducted to explore how LLMs employ the mathematical problem-solving
capabilities learned from user interactions to address issues in narrative
writing. To improve interpretability and explore the balance principle between
generality and personalization under a multi-domain CoT prompt selection
scenario, we propose the Federated Logic rule learning approach (FedLogic). We
introduce a theoretical formalization and interactive emulation of the
multi-domain CoT prompt selection dilemma in the context of federated LLMs. We
cast the problem of joint probability modeling as a bilevel program, where the
CoT prompt selection intricacy can be likened to a fuzzy score-based rule
selection with the LLMs function as rule generators. FedLogic solves this
problem through variational expectation maximization (V-EM). In addition, we
incorporate two KL-divergence constraints within this probabilistic modeling
framework to surmount the intricacies of managing extensive search spaces and
accomplishing cross-domain personalization of CoTs. To the best of our
knowledge, FedLogic is the first interpretable and principled federated
multi-domain CoT prompt selection approach for LLMs.",http://arxiv.org/pdf/2308.15324v1
2308.15321v2,cs.LG,Elucidating the Exposure Bias in Diffusion Models,2023-08-29 14:16:09+00:00,"Diffusion models have demonstrated impressive generative capabilities, but
their 'exposure bias' problem, described as the input mismatch between training
and sampling, lacks in-depth exploration. In this paper, we systematically
investigate the exposure bias problem in diffusion models by first analytically
modelling the sampling distribution, based on which we then attribute the
prediction error at each sampling step as the root cause of the exposure bias
issue. Furthermore, we discuss potential solutions to this issue and propose an
intuitive metric for it. Along with the elucidation of exposure bias, we
propose a simple, yet effective, training-free method called Epsilon Scaling to
alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the
sampling trajectory closer to the vector field learned in the training phase by
scaling down the network output (Epsilon), mitigating the input mismatch
between training and sampling. Experiments on various diffusion frameworks
(ADM, DDPM/DDIM, LDM), unconditional and conditional settings, and
deterministic vs. stochastic sampling verify the effectiveness of our method.",http://arxiv.org/pdf/2308.15321v2
2308.15308v1,cs.LG,On-Device Learning with Binary Neural Networks,2023-08-29 13:48:35+00:00,"Existing Continual Learning (CL) solutions only partially address the
constraints on power, memory and computation of the deep learning models when
deployed on low-power embedded CPUs. In this paper, we propose a CL solution
that embraces the recent advancements in CL field and the efficiency of the
Binary Neural Networks (BNN), that use 1-bit for weights and activations to
efficiently execute deep learning models. We propose a hybrid quantization of
CWR* (an effective CL approach) that considers differently forward and backward
pass in order to retain more precision during gradient update step and at the
same time minimizing the latency overhead. The choice of a binary network as
backbone is essential to meet the constraints of low power devices and, to the
best of authors' knowledge, this is the first attempt to prove on-device
learning with BNN. The experimental validation carried out confirms the
validity and the suitability of the proposed method.",http://arxiv.org/pdf/2308.15308v1
2308.15298v1,cs.CL,"KGConv, a Conversational Corpus grounded in Wikidata",2023-08-29 13:35:51+00:00,"We present KGConv, a large, conversational corpus of 71k conversations where
each question-answer pair is grounded in a Wikidata fact. Conversations contain
on average 8.6 questions and for each Wikidata fact, we provide multiple
variants (12 on average) of the corresponding question using templates, human
annotations, hand-crafted rules and a question rewriting neural model. We
provide baselines for the task of Knowledge-Based, Conversational Question
Generation. KGConv can further be used for other generation and analysis tasks
such as single-turn question generation from Wikidata triples, question
rewriting, question answering from conversation or from knowledge graphs and
quiz generation.",http://arxiv.org/pdf/2308.15298v1
2308.15293v1,cs.SI,A Hybrid Membership Latent Distance Model for Unsigned and Signed Integer Weighted Networks,2023-08-29 13:30:48+00:00,"Graph representation learning (GRL) has become a prominent tool for
furthering the understanding of complex networks providing tools for network
embedding, link prediction, and node classification. In this paper, we propose
the Hybrid Membership-Latent Distance Model (HM-LDM) by exploring how a Latent
Distance Model (LDM) can be constrained to a latent simplex. By controlling the
edge lengths of the corners of the simplex, the volume of the latent space can
be systematically controlled. Thereby communities are revealed as the space
becomes more constrained, with hard memberships being recovered as the simplex
volume goes to zero. We further explore a recent likelihood formulation for
signed networks utilizing the Skellam distribution to account for signed
weighted networks and extend the HM-LDM to the signed Hybrid Membership-Latent
Distance Model (sHM-LDM). Importantly, the induced likelihood function
explicitly attracts nodes with positive links and deters nodes from having
negative interactions. We demonstrate the utility of HM-LDM and sHM-LDM on
several real networks. We find that the procedures successfully identify
prominent distinct structures, as well as how nodes relate to the extracted
aspects providing favorable performances in terms of link prediction when
compared to prominent baselines. Furthermore, the learned soft memberships
enable easily interpretable network visualizations highlighting distinct
patterns.",http://arxiv.org/pdf/2308.15293v1
2308.15272v1,cs.AI,Empowering LLM to use Smartphone for Intelligent Task Automation,2023-08-29 13:02:30+00:00,"Mobile task automation is an attractive technique that aims to enable
voice-based hands-free user interaction with smartphones. However, existing
approaches suffer from poor scalability due to the limited language
understanding ability and the non-trivial manual efforts required from
developers or end-users. The recent advance of large language models (LLMs) in
language understanding and reasoning inspires us to rethink the problem from a
model-centric perspective, where task preparation, comprehension, and execution
are handled by a unified language model. In this work, we introduce AutoDroid,
a mobile task automation system that can handle arbitrary tasks on any Android
application without manual efforts. The key insight is to combine the
commonsense knowledge of LLMs and domain-specific knowledge of apps through
automated dynamic analysis. The main components include a functionality-aware
UI representation method that bridges the UI with the LLM, exploration-based
memory injection techniques that augment the app-specific domain knowledge of
LLM, and a multi-granularity query optimization module that reduces the cost of
model inference. We integrate AutoDroid with off-the-shelf LLMs including
online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a
new benchmark for memory-augmented Android task automation with 158 common
tasks. The results demonstrated that AutoDroid is able to precisely generate
actions with an accuracy of 90.9%, and complete tasks with a success rate of
71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo,
benchmark suites, and source code of AutoDroid will be released at
https://autodroid-sys.github.io/.",http://arxiv.org/pdf/2308.15272v1
2308.15256v1,eess.AS,Let There Be Sound: Reconstructing High Quality Speech from Silent Videos,2023-08-29 12:30:53+00:00,"The goal of this work is to reconstruct high quality speech from lip motions
alone, a task also known as lip-to-speech. A key challenge of lip-to-speech
systems is the one-to-many mapping caused by (1) the existence of homophenes
and (2) multiple speech variations, resulting in a mispronounced and
over-smoothed speech. In this paper, we propose a novel lip-to-speech system
that significantly improves the generation quality by alleviating the
one-to-many mapping problem from multiple perspectives. Specifically, we
incorporate (1) self-supervised speech representations to disambiguate
homophenes, and (2) acoustic variance information to model diverse speech
styles. Additionally, to better solve the aforementioned problem, we employ a
flow based post-net which captures and refines the details of the generated
speech. We perform extensive experiments and demonstrate that our method
achieves the generation quality close to that of real human utterance,
outperforming existing methods in terms of speech naturalness and
intelligibility by a large margin. Synthesised samples are available at the
anonymous demo page: https://mm.kaist.ac.kr/projects/LTBS.",http://arxiv.org/pdf/2308.15256v1
2308.15244v1,cs.IR,Knowledge-based Multiple Adaptive Spaces Fusion for Recommendation,2023-08-29 12:11:16+00:00,"Since Knowledge Graphs (KGs) contain rich semantic information, recently
there has been an influx of KG-enhanced recommendation methods. Most of
existing methods are entirely designed based on euclidean space without
considering curvature. However, recent studies have revealed that a tremendous
graph-structured data exhibits highly non-euclidean properties. Motivated by
these observations, in this work, we propose a knowledge-based multiple
adaptive spaces fusion method for recommendation, namely MCKG. Unlike existing
methods that solely adopt a specific manifold, we introduce the unified space
that is compatible with hyperbolic, euclidean and spherical spaces.
Furthermore, we fuse the multiple unified spaces in an attention manner to
obtain the high-quality embeddings for better knowledge propagation. In
addition, we propose a geometry-aware optimization strategy which enables the
pull and push processes benefited from both hyperbolic and spherical spaces.
Specifically, in hyperbolic space, we set smaller margins in the area near to
the origin, which is conducive to distinguishing between highly similar
positive items and negative ones. At the same time, we set larger margins in
the area far from the origin to ensure the model has sufficient error
tolerance. The similar manner also applies to spherical spaces. Extensive
experiments on three real-world datasets demonstrate that the MCKG has a
significant improvement over state-of-the-art recommendation methods. Further
ablation experiments verify the importance of multi-space fusion and
geometry-aware optimization strategy, justifying the rationality and
effectiveness of MCKG.",http://arxiv.org/pdf/2308.15244v1
2308.15239v1,cs.AI,Natural language to SQL in low-code platforms,2023-08-29 11:59:02+00:00,"One of the developers' biggest challenges in low-code platforms is retrieving
data from a database using SQL queries. Here, we propose a pipeline allowing
developers to write natural language (NL) to retrieve data. In this study, we
collect, label, and validate data covering the SQL queries most often performed
by OutSystems users. We use that data to train a NL model that generates SQL.
Alongside this, we describe the entire pipeline, which comprises a feedback
loop that allows us to quickly collect production data and use it to retrain
our SQL generation model. Using crowd-sourcing, we collect 26k NL and SQL pairs
and obtain an additional 1k pairs from production data. Finally, we develop a
UI that allows developers to input a NL query in a prompt and receive a
user-friendly representation of the resulting SQL query. We use A/B testing to
compare four different models in production and observe a 240% improvement in
terms of adoption of the feature, 220% in terms of engagement rate, and a 90%
decrease in failure rate when compared against the first model that we put into
production, showcasing the effectiveness of our pipeline in continuously
improving our feature.",http://arxiv.org/pdf/2308.15239v1
2308.16061v1,cs.CR,Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning,2023-08-30 14:36:25+00:00,"Ransomware-as-a-service (RaaS) is increasing the scale and complexity of
ransomware attacks. Understanding the internal operations behind RaaS has been
a challenge due to the illegality of such activities. The recent chat leak of
the Conti RaaS operator, one of the most infamous ransomware operators on the
international scene, offers a key opportunity to better understand the inner
workings of such organizations. This paper analyzes the main topic discussions
in the Conti chat leak using machine learning techniques such as Natural
Language Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as
visualization strategies. Five discussion topics are found: 1) Business, 2)
Technical, 3) Internal tasking/Management, 4) Malware, and 5) Customer
Service/Problem Solving. Moreover, the distribution of topics among Conti
members shows that only 4% of individuals have specialized discussions while
almost all individuals (96%) are all-rounders, meaning that their discussions
revolve around the five topics. The results also indicate that a significant
proportion of Conti discussions are non-tech related. This study thus
highlights that running such large RaaS operations requires a workforce skilled
beyond technical abilities, with individuals involved in various tasks, from
management to customer service or problem solving. The discussion topics also
show that the organization behind the Conti RaaS oper5086933ator shares
similarities with a large firm. We conclude that, although RaaS represents an
example of specialization in the cybercrime industry, only a few members are
specialized in one topic, while the rest runs and coordinates the RaaS
operation.",http://arxiv.org/pdf/2308.16061v1
2308.15982v1,cs.CL,MerA: Merging Pretrained Adapters For Few-Shot Learning,2023-08-30 12:10:17+00:00,"Adapter tuning, which updates only a few parameters, has become a mainstream
method for fine-tuning pretrained language models to downstream tasks. However,
it often yields subpar results in few-shot learning. AdapterFusion, which
assembles pretrained adapters using composition layers tailored to specific
tasks, is a possible solution but significantly increases trainable parameters
and deployment costs. Despite this, our preliminary study reveals that even
single adapters can outperform Adapterfusion in few-shot learning, urging us to
propose \textbf{\texttt{Merging Pretrained Adapters}} (MerA) that efficiently
incorporates pretrained adapters to a single model through model fusion.
Extensive experiments on two PLMs demonstrate that MerA achieves substantial
improvements compared to both single adapters and AdapterFusion. To further
enhance the capacity of MerA, we also introduce a simple yet effective
technique, referred to as the ""\textit{same-track}"" setting, that merges
adapters from the same track of pretraining tasks. With the implementation of
the ""\textit{same-track}"" setting, we observe even more impressive gains,
surpassing the performance of both full fine-tuning and adapter tuning by a
substantial margin, e.g., 3.5\% in MRPC and 5.0\% in MNLI.",http://arxiv.org/pdf/2308.15982v1
2308.15961v1,cs.CV,Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting,2023-08-30 11:35:21+00:00,"The task of radiology reporting comprises describing and interpreting the
medical findings in radiographic images, including description of their
location and appearance. Automated approaches to radiology reporting require
the image to be encoded into a suitable token representation for input to the
language model. Previous methods commonly use convolutional neural networks to
encode an image into a series of image-level feature map representations.
However, the generated reports often exhibit realistic style but imperfect
accuracy. Inspired by recent works for image captioning in the general domain
in which each visual token corresponds to an object detected in an image, we
investigate whether using local tokens corresponding to anatomical structures
can improve the quality of the generated reports. We introduce a novel
adaptation of Faster R-CNN in which finding detection is performed for the
candidate bounding boxes extracted during anatomical structure localisation. We
use the resulting bounding box feature representations as our set of
finding-aware anatomical tokens. This encourages the extracted anatomical
tokens to be informative about the findings they contain (required for the
final task of radiology reporting). Evaluating on the MIMIC-CXR dataset of
chest X-Ray images, we show that task-aware anatomical tokens give
state-of-the-art performance when integrated into an automated reporting
pipeline, yielding generated reports with improved clinical accuracy.",http://arxiv.org/pdf/2308.15961v1
2308.15952v1,cs.CL,Benchmarking Multilabel Topic Classification in the Kyrgyz Language,2023-08-30 11:02:26+00:00,"Kyrgyz is a very underrepresented language in terms of modern natural
language processing resources. In this work, we present a new public benchmark
for topic classification in Kyrgyz, introducing a dataset based on collected
and annotated data from the news site 24.KG and presenting several baseline
models for news classification in the multilabel setting. We train and evaluate
both classical statistical and neural models, reporting the scores, discussing
the results, and proposing directions for future work.",http://arxiv.org/pdf/2308.15952v1
2308.15930v1,cs.CL,LLaSM: Large Language and Speech Model,2023-08-30 10:12:39+00:00,"Multi-modal large language models have garnered significant interest
recently. Though, most of the works focus on vision-language multi-modal models
providing strong capabilities in following vision-and-language instructions.
However, we claim that speech is also an important modality through which
humans interact with the world. Hence, it is crucial for a general-purpose
assistant to be able to follow multi-modal speech-and-language instructions. In
this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an
end-to-end trained large multi-modal speech-language model with cross-modal
conversational abilities, capable of following speech-and-language
instructions. Our early experiments show that LLaSM demonstrates a more
convenient and natural way for humans to interact with artificial intelligence.
Specifically, we also release a large Speech Instruction Following dataset
LLaSM-Audio-Instructions. Code and demo are available at
https://github.com/LinkSoul-AI/LLaSM and
https://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions
dataset is available at
https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.",http://arxiv.org/pdf/2308.15930v1
2308.15885v1,cs.LG,Towards One-Shot Learning for Text Classification using Inductive Logic Programming,2023-08-30 09:04:06+00:00,"With the ever-increasing potential of AI to perform personalised tasks, it is
becoming essential to develop new machine learning techniques which are
data-efficient and do not require hundreds or thousands of training data. In
this paper, we explore an Inductive Logic Programming approach for one-shot
text classification. In particular, we explore the framework of
Meta-Interpretive Learning (MIL), along with using common-sense background
knowledge extracted from ConceptNet. Results indicate that MIL can learn text
classification rules from a small number of training examples. Moreover, the
higher complexity of chosen examples, the higher accuracy of the outcome.",http://arxiv.org/pdf/2308.15885v1
2308.15813v1,cs.CL,Knowledge-grounded Natural Language Recommendation Explanation,2023-08-30 07:36:12+00:00,"Explanations accompanied by a recommendation can assist users in
understanding the decision made by recommendation systems, which in turn
increases a user's confidence and trust in the system. Recently, research has
focused on generating natural language explanations in a human-readable format.
Thus far, the proposed approaches leverage item reviews written by users, which
are often subjective, sparse in language, and unable to account for new items
that have not been purchased or reviewed before. Instead, we aim to generate
fact-grounded recommendation explanations that are objectively described with
item features while implicitly considering a user's preferences, based on the
user's purchase history. To achieve this, we propose a knowledge graph (KG)
approach to natural language explainable recommendation. Our approach draws on
user-item features through a novel collaborative filtering-based KG
representation to produce fact-grounded, personalized explanations, while
jointly learning user-item representations for recommendation scoring.
Experimental results show that our approach consistently outperforms previous
state-of-the-art models on natural language explainable recommendation.",http://arxiv.org/pdf/2308.15813v1
2308.15793v1,cs.CL,HAlf-MAsked Model for Named Entity Sentiment analysis,2023-08-30 06:53:24+00:00,"Named Entity Sentiment analysis (NESA) is one of the most actively developing
application domains in Natural Language Processing (NLP). Social media NESA is
a significant field of opinion analysis since detecting and tracking sentiment
trends in the news flow is crucial for building various analytical systems and
monitoring the media image of specific people or companies. In this paper, we
study different transformers-based solutions NESA in RuSentNE-23 evaluation.
Despite the effectiveness of the BERT-like models, they can still struggle with
certain challenges, such as overfitting, which appeared to be the main obstacle
in achieving high accuracy on the RuSentNE-23 data. We present several
approaches to overcome this problem, among which there is a novel technique of
additional pass over given data with masked entity before making the final
prediction so that we can combine logits from the model when it knows the exact
entity it predicts sentiment for and when it does not. Utilizing this
technique, we ensemble multiple BERT- like models trained on different subsets
of data to improve overall performance. Our proposed model achieves the best
result on RuSentNE-23 evaluation data and demonstrates improved consistency in
entity-level sentiment analysis.",http://arxiv.org/pdf/2308.15793v1
2308.15772v1,cs.CL,Task-Based MoE for Multitask Multilingual Machine Translation,2023-08-30 05:41:29+00:00,"Mixture-of-experts (MoE) architecture has been proven a powerful method for
diverse tasks in training deep models in many applications. However, current
MoE implementations are task agnostic, treating all tokens from different tasks
in the same manner. In this work, we instead design a novel method that
incorporates task information into MoE models at different granular levels with
shared dynamic task-based adapters. Our experiments and analysis show the
advantages of our approaches over the dense and canonical MoE models on
multi-task multilingual machine translations. With task-specific adapters, our
models can additionally generalize to new tasks efficiently.",http://arxiv.org/pdf/2308.15772v1
2308.15745v1,cs.CL,Cyberbullying Detection for Low-resource Languages and Dialects: Review of the State of the Art,2023-08-30 03:52:28+00:00,"The struggle of social media platforms to moderate content in a timely
manner, encourages users to abuse such platforms to spread vulgar or abusive
language, which, when performed repeatedly becomes cyberbullying a social
problem taking place in virtual environments, yet with real-world consequences,
such as depression, withdrawal, or even suicide attempts of its victims.
Systems for the automatic detection and mitigation of cyberbullying have been
developed but, unfortunately, the vast majority of them are for the English
language, with only a handful available for low-resource languages. To estimate
the present state of research and recognize the needs for further development,
in this paper we present a comprehensive systematic survey of studies done so
far for automatic cyberbullying detection in low-resource languages. We
analyzed all studies on this topic that were available. We investigated more
than seventy published studies on automatic detection of cyberbullying or
related language in low-resource languages and dialects that were published
between around 2017 and January 2023. There are 23 low-resource languages and
dialects covered by this paper, including Bangla, Hindi, Dravidian languages
and others. In the survey, we identify some of the research gaps of previous
studies, which include the lack of reliable definitions of cyberbullying and
its relevant subcategories, biases in the acquisition, and annotation of data.
Based on recognizing those research gaps, we provide some suggestions for
improving the general research conduct in cyberbullying detection, with a
primary focus on low-resource languages. Based on those proposed suggestions,
we collect and release a cyberbullying dataset in the Chittagonian dialect of
Bangla and propose a number of initial ML solutions trained on that dataset. In
addition, pre-trained transformer-based the BanglaBERT model was also
attempted.",http://arxiv.org/pdf/2308.15745v1
2308.15727v1,cs.CL,Quantifying and Analyzing Entity-level Memorization in Large Language Models,2023-08-30 03:06:47+00:00,"Large language models (LLMs) have been proven capable of memorizing their
training data, which can be extracted through specifically designed prompts. As
the scale of datasets continues to grow, privacy risks arising from
memorization have attracted increasing attention. Quantifying language model
memorization helps evaluate potential privacy risks. However, prior works on
quantifying memorization require access to the precise original data or incur
substantial computational overhead, making it difficult for applications in
real-world language models. To this end, we propose a fine-grained,
entity-level definition to quantify memorization with conditions and metrics
closer to real-world scenarios. In addition, we also present an approach for
efficiently extracting sensitive entities from autoregressive language models.
We conduct extensive experiments based on the proposed, probing language
models' ability to reconstruct sensitive entities under different settings. We
find that language models have strong memorization at the entity level and are
able to reproduce the training data even with partial leakages. The results
demonstrate that LLMs not only memorize their training data but also understand
associations between entities. These findings necessitate that trainers of LLMs
exercise greater prudence regarding model memorization, adopting memorization
mitigation techniques to preclude privacy violations.",http://arxiv.org/pdf/2308.15727v1
2308.15448v1,cs.CL,Vulgar Remarks Detection in Chittagonian Dialect of Bangla,2023-08-29 17:19:32+00:00,"The negative effects of online bullying and harassment are increasing with
Internet popularity, especially in social media. One solution is using natural
language processing (NLP) and machine learning (ML) methods for the automatic
detection of harmful remarks, but these methods are limited in low-resource
languages like the Chittagonian dialect of Bangla.This study focuses on
detecting vulgar remarks in social media using supervised ML and deep learning
algorithms.Logistic Regression achieved promising accuracy (0.91) while simple
RNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the
issue that NN algorithms require more data.",http://arxiv.org/pdf/2308.15448v1
2308.15517v1,cs.CL,"Document AI: A Comparative Study of Transformer-Based, Graph-Based Models, and Convolutional Neural Networks For Document Layout Analysis",2023-08-29 16:58:03+00:00,"Document AI aims to automatically analyze documents by leveraging natural
language processing and computer vision techniques. One of the major tasks of
Document AI is document layout analysis, which structures document pages by
interpreting the content and spatial relationships of layout, image, and text.
This task can be image-centric, wherein the aim is to identify and label
various regions such as authors and paragraphs, or text-centric, where the
focus is on classifying individual words in a document. Although there are
increasingly sophisticated methods for improving layout analysis, doubts remain
about the extent to which their findings can be generalized to a broader
context. Specifically, prior work developed systems based on very different
architectures, such as transformer-based, graph-based, and CNNs. However, no
work has mentioned the effectiveness of these models in a comparative analysis.
Moreover, while language-independent Document AI models capable of knowledge
transfer have been developed, it remains to be investigated to what degree they
can effectively transfer knowledge. In this study, we aim to fill these gaps by
conducting a comparative evaluation of state-of-the-art models in document
layout analysis and investigating the potential of cross-lingual layout
analysis by utilizing machine translation techniques.",http://arxiv.org/pdf/2308.15517v1
2308.15419v1,cs.CL,"Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",2023-08-29 16:24:09+00:00,"How do language models learn to make predictions during pre-training? To
study this question, we extract learning curves from five autoregressive
English language model pre-training runs, for 1M tokens in context. We observe
that the language models generate short repetitive phrases before learning to
generate longer and more coherent text. We quantify the final surprisal,
within-run variability, age of acquisition, forgettability, and cross-run
variability of learning curves for individual tokens in context. More frequent
tokens reach lower final surprisals, exhibit less variability within and across
pre-training runs, are learned earlier, and are less likely to be ""forgotten""
during pre-training. Higher n-gram probabilities further accentuate these
effects. Independent of the target token, shorter and more frequent contexts
correlate with marginally more stable and quickly acquired predictions. Effects
of part-of-speech are also small, although nouns tend to be acquired later and
less stably than verbs, adverbs, and adjectives. Our work contributes to a
better understanding of language model pre-training dynamics and informs the
deployment of stable language models in practice.",http://arxiv.org/pdf/2308.15419v1
2308.15399v1,cs.CL,Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?,2023-08-29 15:57:32+00:00,"Making moral judgments is an essential step toward developing ethical AI
systems. Prevalent approaches are mostly implemented in a bottom-up manner,
which uses a large set of annotated data to train models based on crowd-sourced
opinions about morality. These approaches have been criticized for potentially
overgeneralizing a limited group of annotators' moral stances and lacking
explainability. In contrast, top-down approaches make moral judgments grounded
in a set of principles. However, it remains conceptual due to the incapability
of previous language models and the unsolved debate among moral principles. In
this study, we propose a flexible framework to steer Large Language Models
(LLMs) to perform moral reasoning with well-established moral theories from
interdisciplinary research. The theory-guided top-down framework can
incorporate various moral theories. Our experiments demonstrate the
effectiveness of the proposed framework on datasets derived from moral
theories. Furthermore, we show the alignment between different moral theories
and existing morality datasets. Our analysis exhibits the potentials and flaws
in existing resources (models and datasets) in developing explainable moral
judgment-making systems.",http://arxiv.org/pdf/2308.15399v1
2308.15363v1,cs.DB,Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation,2023-08-29 14:59:54+00:00,"Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL
task. However, the absence of a systematical benchmark inhibits the development
of designing effective, efficient and economic LLM-based Text-to-SQL solutions.
To address this challenge, in this paper, we first conduct a systematical and
extensive comparison over existing prompt engineering methods, including
question representation, example selection and example organization, and with
these experimental results, we elaborates their pros and cons. Based on these
findings, we propose a new integrated solution, named DAIL-SQL, which refreshes
the Spider leaderboard with 86.6% execution accuracy and sets a new bar.
Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize
the token efficiency in prompt engineering and compare the prior studies under
this metric. Additionally, we investigate open-source LLMs in in-context
learning, and further enhance their performance with task-specific supervised
fine-tuning. Our explorations highlight open-source LLMs' potential in
Text-to-SQL, as well as the advantages and disadvantages of the task-specific
supervised fine-tuning. We hope that our work provides a deeper understanding
of Text-to-SQL with LLMs, and inspire further investigations and broad
applications.",http://arxiv.org/pdf/2308.15363v1
2308.15352v1,cs.CL,Historical patterns of rice farming explain modern-day language use in China and Japan more than modernization and urbanization,2023-08-29 14:47:08+00:00,"We used natural language processing to analyze a billion words to study
cultural differences on Weibo, one of China's largest social media platforms.
We compared predictions from two common explanations about cultural differences
in China (economic development and urban-rural differences) against the
less-obvious legacy of rice versus wheat farming. Rice farmers had to
coordinate shared irrigation networks and exchange labor to cope with higher
labor requirements. In contrast, wheat relied on rainfall and required half as
much labor. We test whether this legacy made southern China more
interdependent. Across all word categories, rice explained twice as much
variance as economic development and urbanization. Rice areas used more words
reflecting tight social ties, holistic thought, and a cautious, prevention
orientation. We then used Twitter data comparing prefectures in Japan, which
largely replicated the results from China. This provides crucial evidence of
the rice theory in a different nation, language, and platform.",http://arxiv.org/pdf/2308.15352v1
2308.15299v1,cs.CL,TaskLAMA: Probing the Complex Task Understanding of Language Models,2023-08-29 13:36:45+00:00,"Structured Complex Task Decomposition (SCTD) is the problem of breaking down
a complex real-world task (such as planning a wedding) into a directed acyclic
graph over individual steps that contribute to achieving the task, with edges
specifying temporal dependencies between them. SCTD is an important component
of assistive planning tools, and a challenge for commonsense reasoning systems.
We probe how accurately SCTD can be done with the knowledge extracted from
Large Language Models (LLMs). We introduce a high-quality human-annotated
dataset for this problem and novel metrics to fairly assess performance of LLMs
against several baselines. Our experiments reveal that LLMs are able to
decompose complex tasks into individual steps effectively, with a relative
improvement of 15% to 280% over the best baseline. We also propose a number of
approaches to further improve their performance, with a relative improvement of
7% to 37% over the base model. However, we find that LLMs still struggle to
predict pairwise temporal dependencies, which reveals a gap in their
understanding of complex tasks.",http://arxiv.org/pdf/2308.15299v1
2308.15262v1,cs.CV,Enhancing OCR Performance through Post-OCR Models: Adopting Glyph Embedding for Improved Correction,2023-08-29 12:41:50+00:00,"The study investigates the potential of post-OCR models to overcome
limitations in OCR models and explores the impact of incorporating glyph
embedding on post-OCR correction performance. In this study, we have developed
our own post-OCR correction model. The novelty of our approach lies in
embedding the OCR output using CharBERT and our unique embedding technique,
capturing the visual characteristics of characters. Our findings show that
post-OCR correction effectively addresses deficiencies in inferior OCR models,
and glyph embedding enables the model to achieve superior results, including
the ability to correct individual words.",http://arxiv.org/pdf/2308.15262v1
2308.15246v1,cs.CL,A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation,2023-08-29 12:12:53+00:00,"Neural Machine Translation (NMT) models have been shown to be vulnerable to
adversarial attacks, wherein carefully crafted perturbations of the input can
mislead the target model. In this paper, we introduce ACT, a novel adversarial
attack framework against NMT systems guided by a classifier. In our attack, the
adversary aims to craft meaning-preserving adversarial examples whose
translations by the NMT model belong to a different class than the original
translations in the target language. Unlike previous attacks, our new approach
has a more substantial effect on the translation by altering the overall
meaning, which leads to a different class determined by a classifier. To
evaluate the robustness of NMT models to this attack, we propose enhancements
to existing black-box word-replacement-based attacks by incorporating output
translations of the target NMT model and the output logits of a classifier
within the attack process. Extensive experiments in various settings, including
a comparison with existing untargeted attacks, demonstrate that the proposed
attack is considerably more successful in altering the class of the output
translation and has more effect on the translation. This new paradigm can show
the vulnerabilities of NMT systems by focusing on the class of translation
rather than the mere translation quality as studied traditionally.",http://arxiv.org/pdf/2308.15246v1
2308.15235v1,cs.CL,PronounFlow: A Hybrid Approach for Calibrating Pronouns in Sentences,2023-08-29 11:46:27+00:00,"Flip through any book or listen to any song lyrics, and you will come across
pronouns that, in certain cases, can hinder meaning comprehension, especially
for machines. As the role of having cognitive machines becomes pervasive in our
lives, numerous systems have been developed to resolve pronouns under various
challenges. Commensurate with this, it is believed that having systems able to
disambiguate pronouns in sentences will help towards the endowment of machines
with commonsense and reasoning abilities like those found in humans. However,
one problem these systems face with modern English is the lack of gender
pronouns, where people try to alternate by using masculine, feminine, or plural
to avoid the whole issue. Since humanity aims to the building of systems in the
full-bodied sense we usually reserve for people, what happens when pronouns in
written text, like plural or epicene ones, refer to unspecified entities whose
gender is not necessarily known? Wouldn't that put extra barriers to existing
coreference resolution systems? Towards answering those questions, through the
implementation of a neural-symbolic system that utilizes the best of both
worlds, we are employing PronounFlow, a system that reads any English sentence
with pronouns and entities, identifies which of them are not tied to each
other, and makes suggestions on which to use to avoid biases. Undertaken
experiments show that PronounFlow not only alternates pronouns in sentences
based on the collective human knowledge around us but also considerably helps
coreference resolution systems with the pronoun disambiguation process.",http://arxiv.org/pdf/2308.15235v1
2308.15232v1,cs.LG,Classification-Aware Neural Topic Model Combined With Interpretable Analysis -- For Conflict Classification,2023-08-29 11:40:24+00:00,"A large number of conflict events are affecting the world all the time. In
order to analyse such conflict events effectively, this paper presents a
Classification-Aware Neural Topic Model (CANTM-IA) for Conflict Information
Classification and Topic Discovery. The model provides a reliable
interpretation of classification results and discovered topics by introducing
interpretability analysis. At the same time, interpretation is introduced into
the model architecture to improve the classification performance of the model
and to allow interpretation to focus further on the details of the data.
Finally, the model architecture is optimised to reduce the complexity of the
model.",http://arxiv.org/pdf/2308.15232v1
2308.15231v1,cs.CL,"Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering",2023-08-29 11:40:03+00:00,"This paper evaluates the extent to which current Large Language Models (LLMs)
can capture task-oriented multi-party conversations (MPCs). We have recorded
and transcribed 29 MPCs between patients, their companions, and a social robot
in a hospital. We then annotated this corpus for multi-party goal-tracking and
intent-slot recognition. People share goals, answer each other's goals, and
provide other people's goals in MPCs - none of which occur in dyadic
interactions. To understand user goals in MPCs, we compared three methods in
zero-shot and few-shot settings: we fine-tuned T5, created pre-training tasks
to train DialogLM using LED, and employed prompt engineering techniques with
GPT-3.5-turbo, to determine which approach can complete this novel task with
limited data. GPT-3.5-turbo significantly outperformed the others in a few-shot
setting. The `reasoning' style prompt, when given 7% of the corpus as example
annotated conversations, was the best performing method. It correctly annotated
62.32% of the goal tracking MPCs, and 69.57% of the intent-slot recognition
MPCs. A `story' style prompt increased model hallucination, which could be
detrimental if deployed in safety-critical settings. We conclude that
multi-party conversations still challenge state-of-the-art LLMs.",http://arxiv.org/pdf/2308.15231v1
2308.15226v1,cs.CV,CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation,2023-08-29 11:29:43+00:00,"There has been a growing interest in developing multimodal machine
translation (MMT) systems that enhance neural machine translation (NMT) with
visual knowledge. This problem setup involves using images as auxiliary
information during training, and more recently, eliminating their use during
inference. Towards this end, previous works face a challenge in training
powerful MMT models from scratch due to the scarcity of annotated multilingual
vision-language data, especially for low-resource languages. Simultaneously,
there has been an influx of multilingual pre-trained models for NMT and
multimodal pre-trained models for vision-language tasks, primarily in English,
which have shown exceptional generalisation ability. However, these are not
directly applicable to MMT since they do not provide aligned multimodal
multilingual features for generative tasks. To alleviate this issue, instead of
designing complex modules for MMT, we propose CLIPTrans, which simply adapts
the independently pre-trained multimodal M-CLIP and the multilingual mBART. In
order to align their embedding spaces, mBART is conditioned on the M-CLIP
features by a prefix sequence generated through a lightweight mapping network.
We train this in a two-stage pipeline which warms up the model with image
captioning before the actual translation task. Through experiments, we
demonstrate the merits of this framework and consequently push forward the
state-of-the-art across standard benchmarks by an average of +2.67 BLEU. The
code can be found at www.github.com/devaansh100/CLIPTrans.",http://arxiv.org/pdf/2308.15226v1
2308.15214v2,cs.CL,"FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions",2023-08-29 11:08:40+00:00,"We demonstrate an embodied conversational agent that can function as a
receptionist and generate a mixture of open and closed-domain dialogue along
with facial expressions, by using a large language model (LLM) to develop an
engaging conversation. We deployed the system onto a Furhat robot, which is
highly expressive and capable of using both verbal and nonverbal cues during
interaction. The system was designed specifically for the National Robotarium
to interact with visitors through natural conversations, providing them with
information about the facilities, research, news, upcoming events, etc. The
system utilises the state-of-the-art GPT-3.5 model to generate such information
along with domain-general conversations and facial expressions based on prompt
engineering.",http://arxiv.org/pdf/2308.15214v2
2308.15209v1,cs.CL,Shared Lexical Items as Triggers of Code Switching,2023-08-29 10:55:44+00:00,"Why do bilingual speakers code-switch (mix their two languages)? Among the
several theories that attempt to explain this natural and ubiquitous
phenomenon, the Triggering Hypothesis relates code-switching to the presence of
lexical triggers, specifically cognates and proper names, adjacent to the
switch point. We provide a fuller, more nuanced and refined exploration of the
triggering hypothesis, based on five large datasets in three language pairs,
reflecting both spoken and written bilingual interactions. Our results show
that words that are assumed to reside in a mental lexicon shared by both
languages indeed trigger code-switching; that the tendency to switch depends on
the distance of the trigger from the switch point; and on whether the trigger
precedes or succeeds the switch; but not on the etymology of the trigger words.
We thus provide strong, robust, evidence-based confirmation to several
hypotheses on the relationships between lexical triggers and code-switching.",http://arxiv.org/pdf/2308.15209v1
2308.15202v1,cs.CL,Benchmarking the Generation of Fact Checking Explanations,2023-08-29 10:40:46+00:00,"Fighting misinformation is a challenging, yet crucial, task. Despite the
growing number of experts being involved in manual fact-checking, this activity
is time-consuming and cannot keep up with the ever-increasing amount of Fake
News produced daily. Hence, automating this process is necessary to help curb
misinformation. Thus far, researchers have mainly focused on claim veracity
classification. In this paper, instead, we address the generation of
justifications (textual explanation of why a claim is classified as either true
or false) and benchmark it with novel datasets and advanced baselines. In
particular, we focus on summarization approaches over unstructured knowledge
(i.e. news articles) and we experiment with several extractive and abstractive
strategies. We employed two datasets with different styles and structures, in
order to assess the generalizability of our findings. Results show that in
justification production summarization benefits from the claim information,
and, in particular, that a claim-driven extractive step improves abstractive
summarization performances. Finally, we show that although cross-dataset
experiments suffer from performance degradation, a unique model trained on a
combination of the two datasets is able to retain style information in an
efficient manner.",http://arxiv.org/pdf/2308.15202v1
2308.15192v1,cs.AI,Enhancing Psychological Counseling with Large Language Model: A Multifaceted Decision-Support System for Non-Professionals,2023-08-29 10:20:53+00:00,"In the contemporary landscape of social media, an alarming number of users
express negative emotions, some of which manifest as strong suicidal
intentions. This situation underscores a profound need for trained
psychological counselors who can enact effective mental interventions. However,
the development of these professionals is often an imperative but
time-consuming task. Consequently, the mobilization of non-professionals or
volunteers in this capacity emerges as a pressing concern. Leveraging the
capabilities of artificial intelligence, and in particular, the recent advances
in large language models, offers a viable solution to this challenge. This
paper introduces a novel model constructed on the foundation of large language
models to fully assist non-professionals in providing psychological
interventions on online user discourses. This framework makes it plausible to
harness the power of non-professional counselors in a meaningful way. A
comprehensive study was conducted involving ten professional psychological
counselors of varying expertise, evaluating the system across five critical
dimensions. The findings affirm that our system is capable of analyzing
patients' issues with relative accuracy and proffering professional-level
strategies recommendations, thereby enhancing support for non-professionals.
This research serves as a compelling validation of the application of large
language models in the field of psychology and lays the groundwork for a new
paradigm of community-based mental health support.",http://arxiv.org/pdf/2308.15192v1
2308.15154v1,cs.SI,The Anatomy of Conspirators: Unveiling Traits using a Comprehensive Twitter Dataset,2023-08-29 09:35:23+00:00,"The discourse around conspiracy theories is currently thriving amidst the
rampant misinformation prevalent in online environments. Research in this field
has been focused on detecting conspiracy theories on social media, often
relying on limited datasets. In this study, we present a novel methodology for
constructing a Twitter dataset that encompasses accounts engaged in
conspiracy-related activities throughout the year 2022. Our approach centers on
data collection that is independent of specific conspiracy theories and
information operations. Additionally, our dataset includes a control group
comprising randomly selected users who can be fairly compared to the
individuals involved in conspiracy activities. This comprehensive collection
effort yielded a total of 15K accounts and 37M tweets extracted from their
timelines. We conduct a comparative analysis of the two groups across three
dimensions: topics, profiles, and behavioral characteristics. The results
indicate that conspiracy and control users exhibit similarity in terms of their
profile metadata characteristics. However, they diverge significantly in terms
of behavior and activity, particularly regarding the discussed topics, the
terminology used, and their stance on trending subjects. Interestingly, there
is no significant disparity in the presence of bot users between the two
groups, suggesting that conspiracy and automation are orthogonal concepts.
Finally, we develop a classifier to identify conspiracy users using 93
features, some of which are commonly employed in literature for troll
identification. The results demonstrate a high accuracy level (with an average
F1 score of 0.98%), enabling us to uncover the most discriminative features
associated with conspiracy-related accounts.",http://arxiv.org/pdf/2308.15154v1
2308.15126v1,cs.LG,Evaluation and Analysis of Hallucination in Large Vision-Language Models,2023-08-29 08:51:24+00:00,"Large Vision-Language Models (LVLMs) have recently achieved remarkable
success. However, LVLMs are still plagued by the hallucination problem, which
limits the practicality in many scenarios. Hallucination refers to the
information of LVLMs' responses that does not exist in the visual input, which
poses potential risks of substantial consequences. There has been limited work
studying hallucination evaluation in LVLMs. In this paper, we propose
Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based
hallucination evaluation framework. HaELM achieves an approximate 95%
performance comparable to ChatGPT and has additional advantages including low
cost, reproducibility, privacy preservation and local deployment. Leveraging
the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we
analyze the factors contributing to hallucination in LVLMs and offer helpful
suggestions to mitigate the hallucination problem. Our training data and human
annotation hallucination data will be made public soon.",http://arxiv.org/pdf/2308.15126v1
2308.15122v2,cs.CL,SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT,2023-08-29 08:41:16+00:00,"Spiking neural networks (SNNs) offer a promising avenue to implement deep
neural networks in a more energy-efficient way. However, the network
architectures of existing SNNs for language tasks are too simplistic, and deep
architectures have not been fully explored, resulting in a significant
performance gap compared to mainstream transformer-based networks such as BERT.
To this end, we improve a recently-proposed spiking transformer (i.e.,
Spikformer) to make it possible to process language tasks and propose a
two-stage knowledge distillation method for training it, which combines
pre-training by distilling knowledge from BERT with a large collection of
unlabelled texts and fine-tuning with task-specific instances via knowledge
distillation again from the BERT fine-tuned on the same training examples.
Through extensive experimentation, we show that the models trained with our
method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve
comparable results to BERTs on text classification tasks for both English and
Chinese with much less energy consumption.",http://arxiv.org/pdf/2308.15122v2
2308.15118v1,cs.CL,Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills,2023-08-29 08:36:30+00:00,"While large language models have made strides in natural language processing,
their proficiency in complex reasoning tasks requiring formal language
comprehension, such as chess, remains less investigated. This paper probes the
performance of ChatGPT, a sophisticated language model by OpenAI in tackling
such complex reasoning tasks, using chess as a case study. Through robust
metrics examining both the legality and quality of moves, we assess ChatGPT's
understanding of the chessboard, adherence to chess rules, and strategic
decision-making abilities. Our evaluation identifies limitations within
ChatGPT's attention mechanism that affect its formal language comprehension and
uncovers the model's underdeveloped self-regulation abilities. Our study also
reveals ChatGPT's propensity for a coherent strategy in its gameplay and a
noticeable uptick in decision-making assertiveness when the model is presented
with a greater volume of natural language or possesses a more lucid
understanding of the state of the chessboard. These findings contribute to the
growing exploration of language models' abilities beyond natural language
processing, providing valuable information for future research towards models
demonstrating human-like cognitive abilities.",http://arxiv.org/pdf/2308.15118v1
2308.15097v1,cs.AI,Sequential annotations for naturally-occurring HRI: first insights,2023-08-29 08:07:26+00:00,"We explain the methodology we developed for improving the interactions
accomplished by an embedded conversational agent, drawing from Conversation
Analytic sequential and multimodal analysis. The use case is a Pepper robot
that is expected to inform and orient users in a library. In order to propose
and learn better interactive schema, we are creating a corpus of
naturally-occurring interactions that will be made available to the community.
To do so, we propose an annotation practice based on some theoretical
underpinnings about the use of language and multimodal resources in human-robot
interaction. CCS CONCEPTS $\bullet$ Computing methodologies $\rightarrow$
Discourse, dialogue and pragmatics; $\bullet$ Human-centered computing
$\rightarrow$ Text input; HCI theory, concepts and models; Field studies.",http://arxiv.org/pdf/2308.15097v1
2308.15090v1,cs.CL,Killing two birds with one stone: Can an audio captioning system also be used for audio-text retrieval?,2023-08-29 07:53:17+00:00,"Automated Audio Captioning (AAC) aims to develop systems capable of
describing an audio recording using a textual sentence. In contrast, Audio-Text
Retrieval (ATR) systems seek to find the best matching audio recording(s) for a
given textual query (Text-to-Audio) or vice versa (Audio-to-Text). These tasks
require different types of systems: AAC employs a sequence-to-sequence model,
while ATR utilizes a ranking model that compares audio and text representations
within a shared projection subspace. However, this work investigates the
relationship between AAC and ATR by exploring the ATR capabilities of an
unmodified AAC system, without fine-tuning for the new task. Our AAC system
consists of an audio encoder (ConvNeXt-Tiny) trained on AudioSet for audio
tagging, and a transformer decoder responsible for generating sentences. For
AAC, it achieves a high SPIDEr-FL score of 0.298 on Clotho and 0.472 on
AudioCaps on average. For ATR, we propose using the standard Cross-Entropy loss
values obtained for any audio/caption pair. Experimental results on the Clotho
and AudioCaps datasets demonstrate decent recall values using this simple
approach. For instance, we obtained a Text-to-Audio R@1 value of 0.382 for
Au-dioCaps, which is above the current state-of-the-art method without external
data. Interestingly, we observe that normalizing the loss values was necessary
for Audio-to-Text retrieval.",http://arxiv.org/pdf/2308.15090v1
2308.15055v1,cs.CL,Taxonomic Loss for Morphological Glossing of Low-Resource Languages,2023-08-29 06:31:21+00:00,"Morpheme glossing is a critical task in automated language documentation and
can benefit other downstream applications greatly. While state-of-the-art
glossing systems perform very well for languages with large amounts of existing
data, it is more difficult to create useful models for low-resource languages.
In this paper, we propose the use of a taxonomic loss function that exploits
morphological information to make morphological glossing more performant when
data is scarce. We find that while the use of this loss function does not
outperform a standard loss function with regards to single-label prediction
accuracy, it produces better predictions when considering the top-n predicted
labels. We suggest this property makes the taxonomic loss function useful in a
human-in-the-loop annotation setting.",http://arxiv.org/pdf/2308.15055v1
2308.15053v2,cs.CL,Adapting Text-based Dialogue State Tracker for Spoken Dialogues,2023-08-29 06:27:58+00:00,"Although there have been remarkable advances in dialogue systems through the
dialogue systems technology competition (DSTC), it remains one of the key
challenges to building a robust task-oriented dialogue system with a speech
interface. Most of the progress has been made for text-based dialogue systems
since there are abundant datasets with written corpora while those with spoken
dialogues are very scarce. However, as can be seen from voice assistant systems
such as Siri and Alexa, it is of practical importance to transfer the success
to spoken dialogues. In this paper, we describe our engineering effort in
building a highly successful model that participated in the speech-aware
dialogue systems technology challenge track in DSTC11. Our model consists of
three major modules: (1) automatic speech recognition error correction to
bridge the gap between the spoken and the text utterances, (2) text-based
dialogue system (D3ST) for estimating the slots and values using slot
descriptions, and (3) post-processing for recovering the error of the estimated
slot value. Our experiments show that it is important to use an explicit
automatic speech recognition error correction module, post-processing, and data
augmentation to adapt a text-based dialogue state tracker for spoken dialogue
corpora.",http://arxiv.org/pdf/2308.15053v2
2308.15047v1,cs.LG,Large language models converge toward human-like concept organization,2023-08-29 06:09:47+00:00,"Large language models show human-like performance in knowledge extraction,
reasoning and dialogue, but it remains controversial whether this performance
is best explained by memorization and pattern matching, or whether it reflects
human-like inferential semantics and world knowledge. Knowledge bases such as
WikiData provide large-scale, high-quality representations of inferential
semantics and world knowledge. We show that large language models learn to
organize concepts in ways that are strikingly similar to how concepts are
organized in such knowledge bases. Knowledge bases model collective,
institutional knowledge, and large language models seem to induce such
knowledge from raw text. We show that bigger and better models exhibit more
human-like concept organization, across four families of language models and
three knowledge graph embeddings.",http://arxiv.org/pdf/2308.15047v1
2308.15027v1,cs.IR,Improving Neural Ranking Models with Traditional IR Methods,2023-08-29 05:18:47+00:00,"Neural ranking methods based on large transformer models have recently gained
significant attention in the information retrieval community, and have been
adopted by major commercial solutions. Nevertheless, they are computationally
expensive to create, and require a great deal of labeled data for specialized
corpora. In this paper, we explore a low resource alternative which is a
bag-of-embedding model for document retrieval and find that it is competitive
with large transformer models fine tuned on information retrieval tasks. Our
results show that a simple combination of TF-IDF, a traditional keyword
matching method, with a shallow embedding model provides a low cost path to
compete well with the performance of complex neural ranking models on 3
datasets. Furthermore, adding TF-IDF measures improves the performance of
large-scale fine tuned models on these tasks.",http://arxiv.org/pdf/2308.15027v1
2308.15022v1,cs.CL,Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models,2023-08-29 04:59:53+00:00,"Most open-domain dialogue systems suffer from forgetting important
information, especially in a long-term conversation. Existing works usually
train the specific retriever or summarizer to obtain key information from the
past, which is time-consuming and highly depends on the quality of labeled
data. To alleviate this problem, we propose to recursively generate summaries/
memory using large language models (LLMs) to enhance long-term memory ability.
Specifically, our method first stimulates LLMs to memorize small dialogue
contexts and then recursively produce new memory using previous memory and
following contexts. Finally, the LLM can easily generate a highly consistent
response with the help of the latest memory. We evaluate our method using
ChatGPT and text-davinci-003, and the experiments on the widely-used public
dataset show that our method can generate more consistent responses in a
long-context conversation. Notably, our method is a potential solution to
enable the LLM to model the extremely long context. Code and scripts will be
released later.",http://arxiv.org/pdf/2308.15022v1
2308.15010v1,cs.CL,TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification,2023-08-29 04:16:57+00:00,"Text classification is one of the most imperative tasks in natural language
processing (NLP). Recent advances with pre-trained language models (PLMs) have
shown remarkable success on this task. However, the satisfying results obtained
by PLMs heavily depend on the large amounts of task-specific labeled data,
which may not be feasible in many application scenarios due to data access and
privacy constraints. The recently-proposed prompt-based fine-tuning paradigm
improves the performance of PLMs for few-shot text classification with
task-specific templates. Yet, it is unclear how the prompting knowledge can be
transferred across tasks, for the purpose of mutual reinforcement. We propose
TransPrompt v2, a novel transferable prompting framework for few-shot learning
across similar or distant text classification tasks. For learning across
similar tasks, we employ a multi-task meta-knowledge acquisition (MMA)
procedure to train a meta-learner that captures the cross-task transferable
knowledge. For learning across distant tasks, we further inject the task type
descriptions into the prompt, and capture the intra-type and inter-type prompt
embeddings among multiple distant tasks. Additionally, two de-biasing
techniques are further designed to make the trained meta-learner more
task-agnostic and unbiased towards any tasks. After that, the meta-learner can
be adapted to each specific task with better parameters initialization.
Extensive experiments show that TransPrompt v2 outperforms single-task and
cross-task strong baselines over multiple NLP tasks and datasets. We further
show that the meta-learner can effectively improve the performance of PLMs on
previously unseen tasks. In addition, TransPrompt v2 also outperforms strong
fine-tuning baselines when learning with full training sets.",http://arxiv.org/pdf/2308.15010v1
2308.14951v1,cs.CL,Robust Open-Set Spoken Language Identification and the CU MultiLang Dataset,2023-08-29 00:44:27+00:00,"Most state-of-the-art spoken language identification models are closed-set;
in other words, they can only output a language label from the set of classes
they were trained on. Open-set spoken language identification systems, however,
gain the ability to detect when an input exhibits none of the original
languages. In this paper, we implement a novel approach to open-set spoken
language identification that uses MFCC and pitch features, a TDNN model to
extract meaningful feature embeddings, confidence thresholding on softmax
outputs, and LDA and pLDA for learning to classify new unknown languages. We
present a spoken language identification system that achieves 91.76% accuracy
on trained languages and has the capability to adapt to unknown languages on
the fly. To that end, we also built the CU MultiLang Dataset, a large and
diverse multilingual speech corpus which was used to train and evaluate our
system.",http://arxiv.org/pdf/2308.14951v1
2308.14921v1,cs.CL,Gender bias and stereotypes in Large Language Models,2023-08-28 22:32:05+00:00,"Large Language Models (LLMs) have made substantial progress in the past
several months, shattering state-of-the-art benchmarks in many domains. This
paper investigates LLMs' behavior with respect to gender stereotypes, a known
issue for prior models. We use a simple paradigm to test the presence of gender
bias, building on but differing from WinoBias, a commonly used gender bias
dataset, which is likely to be included in the training data of current LLMs.
We test four recently published LLMs and demonstrate that they express biased
assumptions about men and women's occupations. Our contributions in this paper
are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that
stereotypically aligns with a person's gender; (b) these choices align with
people's perceptions better than with the ground truth as reflected in official
job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in
perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in
sentence structure 95% of the time in our study items, but when explicitly
prompted, they recognize the ambiguity; (e) LLMs provide explanations for their
choices that are factually inaccurate and likely obscure the true reason behind
their predictions. That is, they provide rationalizations of their biased
behavior. This highlights a key property of these models: LLMs are trained on
imbalanced datasets; as such, even with the recent successes of reinforcement
learning with human feedback, they tend to reflect those imbalances back at us.
As with other types of societal biases, we suggest that LLMs must be carefully
tested to ensure that they treat minoritized individuals and communities
equitably.",http://arxiv.org/pdf/2308.14921v1
2308.14905v1,cs.CL,Neural approaches to spoken content embedding,2023-08-28 21:16:08+00:00,"Comparing spoken segments is a central operation to speech processing.
Traditional approaches in this area have favored frame-level dynamic
programming algorithms, such as dynamic time warping, because they require no
supervision, but they are limited in performance and efficiency. As an
alternative, acoustic word embeddings -- fixed-dimensional vector
representations of variable-length spoken word segments -- have begun to be
considered for such tasks as well. However, the current space of such
discriminative embedding models, training approaches, and their application to
real-world downstream tasks is limited. We start by considering ``single-view""
training losses where the goal is to learn an acoustic word embedding model
that separates same-word and different-word spoken segment pairs. Then, we
consider ``multi-view"" contrastive losses. In this setting, acoustic word
embeddings are learned jointly with embeddings of character sequences to
generate acoustically grounded embeddings of written words, or acoustically
grounded word embeddings.
  In this thesis, we contribute new discriminative acoustic word embedding
(AWE) and acoustically grounded word embedding (AGWE) approaches based on
recurrent neural networks (RNNs). We improve model training in terms of both
efficiency and performance. We take these developments beyond English to
several low-resource languages and show that multilingual training improves
performance when labeled data is limited. We apply our embedding models, both
monolingual and multilingual, to the downstream tasks of query-by-example
speech search and automatic speech recognition. Finally, we show how our
embedding approaches compare with and complement more recent self-supervised
speech models.",http://arxiv.org/pdf/2308.14905v1
2308.14903v1,cs.CL,MEMORY-VQ: Compression for Tractable Internet-Scale Memory,2023-08-28 21:11:18+00:00,"Retrieval augmentation is a powerful but expensive method to make language
models more knowledgeable about the world. Memory-based methods like LUMEN
pre-compute token representations for retrieved passages to drastically speed
up inference. However, memory also leads to much greater storage requirements
from storing pre-computed representations.
  We propose MEMORY-VQ, a new method to reduce storage requirements of
memory-augmented models without sacrificing performance. Our method uses a
vector quantization variational autoencoder (VQ-VAE) to compress token
representations. We apply MEMORY-VQ to the LUMEN model to obtain LUMEN-VQ, a
memory model that achieves a 16x compression rate with comparable performance
on the KILT benchmark. LUMEN-VQ enables practical retrieval augmentation even
for extremely large retrieval corpora.",http://arxiv.org/pdf/2308.14903v1
2308.14894v1,cs.CL,Multiscale Contextual Learning for Speech Emotion Recognition in Emergency Call Center Conversations,2023-08-28 20:31:45+00:00,"Emotion recognition in conversations is essential for ensuring advanced
human-machine interactions. However, creating robust and accurate emotion
recognition systems in real life is challenging, mainly due to the scarcity of
emotion datasets collected in the wild and the inability to take into account
the dialogue context. The CEMO dataset, composed of conversations between
agents and patients during emergency calls to a French call center, fills this
gap. The nature of these interactions highlights the role of the emotional flow
of the conversation in predicting patient emotions, as context can often make a
difference in understanding actual feelings. This paper presents a multi-scale
conversational context learning approach for speech emotion recognition, which
takes advantage of this hypothesis. We investigated this approach on both
speech transcriptions and acoustic segments. Experimentally, our method uses
the previous or next information of the targeted segment. In the text domain,
we tested the context window using a wide range of tokens (from 10 to 100) and
at the speech turns level, considering inputs from both the same and opposing
speakers. According to our tests, the context derived from previous tokens has
a more significant influence on accurate prediction than the following tokens.
Furthermore, taking the last speech turn of the same speaker in the
conversation seems useful. In the acoustic domain, we conducted an in-depth
analysis of the impact of the surrounding emotions on the prediction. While
multi-scale conversational context learning using Transformers can enhance
performance in the textual modality for emergency call recordings,
incorporating acoustic context is more challenging.",http://arxiv.org/pdf/2308.14894v1
2308.14873v1,cs.CL,CommunityFish: A Poisson-based Document Scaling With Hierarchical Clustering,2023-08-28 19:52:18+00:00,"Document scaling has been a key component in text-as-data applications for
social scientists and a major field of interest for political researchers, who
aim at uncovering differences between speakers or parties with the help of
different probabilistic and non-probabilistic approaches. Yet, most of these
techniques are either built upon the agnostically bag-of-word hypothesis or use
prior information borrowed from external sources that might embed the results
with a significant bias. If the corpus has long been considered as a collection
of documents, it can also be seen as a dense network of connected words whose
structure could be clustered to differentiate independent groups of words,
based on their co-occurrences in documents, known as communities. This paper
introduces CommunityFish as an augmented version of Wordfish based on a
hierarchical clustering, namely the Louvain algorithm, on the word space to
yield communities as semantic and independent n-grams emerging from the corpus
and use them as an input to Wordfish method, instead of considering the word
space. This strategy emphasizes the interpretability of the results, since
communities have a non-overlapping structure, hence a crucial informative power
in discriminating parties or speakers, in addition to allowing a faster
execution of the Poisson scaling model. Aside from yielding communities,
assumed to be subtopic proxies, the application of this technique outperforms
the classic Wordfish model by highlighting historical developments in the U.S.
State of the Union addresses and was found to replicate the prevailing
political stance in Germany when using the corpus of parties' legislative
manifestos.",http://arxiv.org/pdf/2308.14873v1
2308.14850v1,cs.CL,Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models,2023-08-28 19:11:52+00:00,"This report introduces the Attention Visualizer package, which is crafted to
visually illustrate the significance of individual words in encoder-only
transformer-based models. In contrast to other methods that center on tokens
and self-attention scores, our approach will examine the words and their impact
on the final embedding representation. Libraries like this play a crucial role
in enhancing the interpretability and explainability of neural networks. They
offer the opportunity to illuminate their internal mechanisms, providing a
better understanding of how they operate and can be enhanced. You can access
the code and review examples on the following GitHub repository:
https://github.com/AlaFalaki/AttentionVisualizer.",http://arxiv.org/pdf/2308.14850v1
2308.14683v1,cs.CL,Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts,2023-08-28 16:18:50+00:00,"Detecting online sexual predatory behaviours and abusive language on social
media platforms has become a critical area of research due to the growing
concerns about online safety, especially for vulnerable populations such as
children and adolescents. Researchers have been exploring various techniques
and approaches to develop effective detection systems that can identify and
mitigate these risks. Recent development of large language models (LLMs) has
opened a new opportunity to address this problem more effectively. This paper
proposes an approach to detection of online sexual predatory chats and abusive
language using the open-source pretrained Llama 2 7B-parameter model, recently
released by Meta GenAI. We fine-tune the LLM using datasets with different
sizes, imbalance degrees, and languages (i.e., English, Roman Urdu and Urdu).
Based on the power of LLMs, our approach is generic and automated without a
manual search for a synergy between feature extraction and classifier design
steps like conventional methods in this domain. Experimental results show a
strong performance of the proposed approach, which performs proficiently and
consistently across three distinct datasets with five sets of experiments. This
study's outcomes indicate that the proposed method can be implemented in
real-world applications (even with non-English languages) for flagging sexual
predators, offensive or toxic content, hate speech, and discriminatory language
in online discussions and comments to maintain respectful internet or digital
communities. Furthermore, it can be employed for solving text classification
problems with other potential applications such as sentiment analysis, spam and
phishing detection, sorting legal documents, fake news detection, language
identification, user intent recognition, text-based product categorization,
medical record analysis, and resume screening.",http://arxiv.org/pdf/2308.14683v1
2308.14669v1,cs.CL,ANER: Arabic and Arabizi Named Entity Recognition using Transformer-Based Approach,2023-08-28 15:54:48+00:00,"One of the main tasks of Natural Language Processing (NLP), is Named Entity
Recognition (NER). It is used in many applications and also can be used as an
intermediate step for other tasks. We present ANER, a web-based named entity
recognizer for the Arabic, and Arabizi languages. The model is built upon BERT,
which is a transformer-based encoder. It can recognize 50 different entity
classes, covering various fields. We trained our model on the WikiFANE\_Gold
dataset which consists of Wikipedia articles. We achieved an F1 score of
88.7\%, which beats CAMeL Tools' F1 score of 83\% on the ANERcorp dataset,
which has only 4 classes. We also got an F1 score of 77.7\% on the
NewsFANE\_Gold dataset which contains out-of-domain data from News articles.
The system is deployed on a user-friendly web interface that accepts users'
inputs in Arabic, or Arabizi. It allows users to explore the entities in the
text by highlighting them. It can also direct users to get information about
entities through Wikipedia directly. We added the ability to do NER using our
model, or CAMeL Tools' model through our website. ANER is publicly accessible
at \url{http://www.aner.online}. We also deployed our model on HuggingFace at
https://huggingface.co/boda/ANER, to allow developers to test and use it.",http://arxiv.org/pdf/2308.14669v1
2308.14654v1,cs.CL,Joint Multiple Intent Detection and Slot Filling with Supervised Contrastive Learning and Self-Distillation,2023-08-28 15:36:33+00:00,"Multiple intent detection and slot filling are two fundamental and crucial
tasks in spoken language understanding. Motivated by the fact that the two
tasks are closely related, joint models that can detect intents and extract
slots simultaneously are preferred to individual models that perform each task
independently. The accuracy of a joint model depends heavily on the ability of
the model to transfer information between the two tasks so that the result of
one task can correct the result of the other. In addition, since a joint model
has multiple outputs, how to train the model effectively is also challenging.
In this paper, we present a method for multiple intent detection and slot
filling by addressing these challenges. First, we propose a bidirectional joint
model that explicitly employs intent information to recognize slots and slot
features to detect intents. Second, we introduce a novel method for training
the proposed joint model using supervised contrastive learning and
self-distillation. Experimental results on two benchmark datasets MixATIS and
MixSNIPS show that our method outperforms state-of-the-art models in both
tasks. The results also demonstrate the contributions of both bidirectional
design and the training method to the accuracy improvement. Our source code is
available at https://github.com/anhtunguyen98/BiSLU",http://arxiv.org/pdf/2308.14654v1
2308.14641v2,cs.CL,Challenges of GPT-3-based Conversational Agents for Healthcare,2023-08-28 15:12:34+00:00,"The potential to provide patients with faster information access while
allowing medical specialists to concentrate on critical tasks makes medical
domain dialog agents appealing. However, the integration of large-language
models (LLMs) into these agents presents certain limitations that may result in
serious consequences. This paper investigates the challenges and risks of using
GPT-3-based models for medical question-answering (MedQA). We perform several
evaluations contextualized in terms of standard medical principles. We provide
a procedure for manually designing patient queries to stress-test high-risk
limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to
respond adequately to these queries, generating erroneous medical information,
unsafe recommendations, and content that may be considered offensive.",http://arxiv.org/pdf/2308.14641v2
2308.14634v1,cs.CL,Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance,2023-08-28 15:04:16+00:00,"We propose the use of conversational GPT models for easy and quick few-shot
text classification in the financial domain using the Banking77 dataset. Our
approach involves in-context learning with GPT-3.5 and GPT-4, which minimizes
the technical expertise required and eliminates the need for expensive GPU
computing while yielding quick and accurate results. Additionally, we fine-tune
other pre-trained, masked language models with SetFit, a recent contrastive
learning technique, to achieve state-of-the-art results both in full-data and
few-shot settings. Our findings show that querying GPT-3.5 and GPT-4 can
outperform fine-tuned, non-generative models even with fewer examples. However,
subscription fees associated with these solutions may be considered costly for
small organizations. Lastly, we find that generative models perform better on
the given task when shown representative samples selected by a human expert
rather than when shown random ones. We conclude that a) our proposed methods
offer a practical solution for few-shot tasks in datasets with limited label
availability, and b) our state-of-the-art results can inspire future work in
the area.",http://arxiv.org/pdf/2308.14634v1
2308.14608v1,cs.LG,AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics,2023-08-28 14:23:04+00:00,"The introduction of ChatGPT and the subsequent improvement of Large Language
Models (LLMs) have prompted more and more individuals to turn to the use of
ChatBots, both for information and assistance with decision-making. However,
the information the user is after is often not formulated by these ChatBots
objectively enough to be provided with a definite, globally accepted answer.
  Controversial topics, such as ""religion"", ""gender identity"", ""freedom of
speech"", and ""equality"", among others, can be a source of conflict as partisan
or biased answers can reinforce preconceived notions or promote disinformation.
By exposing ChatGPT to such debatable questions, we aim to understand its level
of awareness and if existing models are subject to socio-political and/or
economic biases. We also aim to explore how AI-generated answers compare to
human ones. For exploring this, we use a dataset of a social media platform
created for the purpose of debating human-generated claims on polemic subjects
among users, dubbed Kialo.
  Our results show that while previous versions of ChatGPT have had important
issues with controversial topics, more recent versions of ChatGPT
(gpt-3.5-turbo) are no longer manifesting significant explicit biases in
several knowledge areas. In particular, it is well-moderated regarding economic
aspects. However, it still maintains degrees of implicit libertarian leaning
toward right-winged ideals which suggest the need for increased moderation from
the socio-political point of view. In terms of domain knowledge on
controversial topics, with the exception of the ""Philosophical"" category,
ChatGPT is performing well in keeping up with the collective human level of
knowledge. Finally, we see that sources of Bing AI have slightly more tendency
to the center when compared to human answers. All the analyses we make are
generalizable to other types of biases and domains.",http://arxiv.org/pdf/2308.14608v1
2308.14536v1,cs.CL,Spoken Language Intelligence of Large Language Models for Language Learning,2023-08-28 12:47:41+00:00,"People have long hoped for a conversational system that can assist in
real-life situations, and recent progress on large language models (LLMs) is
bringing this idea closer to reality. While LLMs are often impressive in
performance, their efficacy in real-world scenarios that demand expert
knowledge remains unclear. LLMs are believed to hold the most potential and
value in education, especially in the development of Artificial intelligence
(AI) based virtual teachers capable of facilitating language learning. Our
focus is centered on evaluating the efficacy of LLMs in the realm of education,
specifically in the areas of spoken language learning which encompass
phonetics, phonology, and second language acquisition. We introduce a new
multiple-choice question dataset to evaluate the effectiveness of LLMs in the
aforementioned scenarios, including understanding and application of spoken
language knowledge. In addition, we investigate the influence of various
prompting techniques such as zero- and few-shot method (prepending the question
with question-answer exemplars), chain-of-thought (CoT, think step-by-step),
in-domain exampler and external tools (Google, Wikipedia). We conducted
large-scale evaluation on popular LLMs (20 distinct models) using these
methods. We achieved significant performance improvements compared to the
zero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% ->
63.1%; LLaMA2-70B-Chat, 42.2% -> 48.6%). We found that models of different
sizes have good understanding of concepts in phonetics, phonology, and second
language acquisition, but show limitations in reasoning for real-world
problems. Additionally, we also explore preliminary findings on conversational
communication.",http://arxiv.org/pdf/2308.14536v1
2308.14533v1,cs.CL,A Multi-Task Semantic Decomposition Framework with Task-specific Pre-training for Few-Shot NER,2023-08-28 12:46:21+00:00,"The objective of few-shot named entity recognition is to identify named
entities with limited labeled instances. Previous works have primarily focused
on optimizing the traditional token-wise classification framework, while
neglecting the exploration of information based on NER data characteristics. To
address this issue, we propose a Multi-Task Semantic Decomposition Framework
via Joint Task-specific Pre-training (MSDP) for few-shot NER. Drawing
inspiration from demonstration-based and contrastive learning, we introduce two
novel pre-training tasks: Demonstration-based Masked Language Modeling (MLM)
and Class Contrastive Discrimination. These tasks effectively incorporate
entity boundary information and enhance entity representation in Pre-trained
Language Models (PLMs). In the downstream main task, we introduce a multi-task
joint optimization framework with the semantic decomposing method, which
facilitates the model to integrate two different semantic information for
entity classification. Experimental results of two few-shot NER benchmarks
demonstrate that MSDP consistently outperforms strong baselines by a large
margin. Extensive analyses validate the effectiveness and generalization of
MSDP.",http://arxiv.org/pdf/2308.14533v1
2308.14508v1,cs.CL,"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",2023-08-28 11:53:40+00:00,"Although large language models (LLMs) demonstrate impressive performance for
many language tasks, most of them can only handle texts a few thousand tokens
long, limiting their applications on longer sequence inputs, such as books,
reports, and codebases. Recent works have proposed methods to improve LLMs'
long context capabilities by extending context windows and more sophisticated
memory mechanisms. However, comprehensive benchmarks tailored for evaluating
long context understanding are lacking. In this paper, we introduce LongBench,
the first bilingual, multi-task benchmark for long context understanding,
enabling a more rigorous evaluation of long context understanding. LongBench
comprises 21 datasets across 6 task categories in both English and Chinese,
with an average length of 6,711 words (English) and 13,386 characters
(Chinese). These tasks cover key long-text application areas including
single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,
and code completion. All datasets in LongBench are standardized into a unified
format, allowing for effortless automatic evaluation of LLMs. Upon
comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial
model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still
struggles on longer contexts. (2) Scaled position embedding and fine-tuning on
longer sequences lead to substantial improvement on long context understanding.
(3) Context compression technique such as retrieval brings improvement for
model with weak ability on long contexts, but the performance still lags behind
models that have strong long context understanding capability. The code and
datasets are available at https://github.com/THUDM/LongBench.",http://arxiv.org/pdf/2308.14508v1
2308.14484v1,cs.CL,Multimodal Detection of Social Spambots in Twitter using Transformers,2023-08-28 10:51:11+00:00,"Although not all bots are malicious, the vast majority of them are
responsible for spreading misinformation and manipulating the public opinion
about several issues, i.e., elections and many more. Therefore, the early
detection of social spambots is crucial. Although there have been proposed
methods for detecting bots in social media, there are still substantial
limitations. For instance, existing research initiatives still extract a large
number of features and train traditional machine learning algorithms or use
GloVe embeddings and train LSTMs. However, feature extraction is a tedious
procedure demanding domain expertise. Also, language models based on
transformers have been proved to be better than LSTMs. Other approaches create
large graphs and train graph neural networks requiring in this way many hours
for training and access to computational resources. To tackle these
limitations, this is the first study employing only the user description field
and images of three channels denoting the type and content of tweets posted by
the users. Firstly, we create digital DNA sequences, transform them to 3d
images, and apply pretrained models of the vision domain, including
EfficientNet, AlexNet, VGG16, etc. Next, we propose a multimodal approach,
where we use TwHIN-BERT for getting the textual representation of the user
description field and employ VGG16 for acquiring the visual representation for
the image modality. We propose three different fusion methods, namely
concatenation, gated multimodal unit, and crossmodal attention, for fusing the
different modalities and compare their performances. Extensive experiments
conducted on the Cresci '17 dataset demonstrate valuable advantages of our
introduced approaches over state-of-the-art ones reaching Accuracy up to
99.98%.",http://arxiv.org/pdf/2308.14484v1
2308.14482v1,cs.CL,An Empirical Study of Consistency Regularization for End-to-End Speech-to-Text Translation,2023-08-28 10:44:18+00:00,"Consistency regularization methods, such as R-Drop (Liang et al., 2021) and
CrossConST (Gao et al., 2023), have achieved impressive supervised and
zero-shot performance in the neural machine translation (NMT) field. Can we
also boost end-to-end (E2E) speech-to-text translation (ST) by leveraging
consistency regularization? In this paper, we conduct empirical studies on
intra-modal and cross-modal consistency and propose two training strategies,
SimRegCR and SimZeroCR, for E2E ST in regular and zero-shot scenarios.
Experiments on the MuST-C benchmark show that our approaches achieve
state-of-the-art (SOTA) performance in most translation directions. The
analyses prove that regularization brought by the intra-modal consistency,
instead of modality gap, is crucial for the regular E2E ST, and the cross-modal
consistency could close the modality gap and boost the zero-shot E2E ST
performance.",http://arxiv.org/pdf/2308.14482v1
2308.14436v1,cs.CL,Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA,2023-08-28 09:22:02+00:00,"Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with factual information such as entities and relations in KBs.
However, traditional Pre-trained Language Models (PLMs) are directly
pre-trained on large-scale natural language corpus, which poses challenges for
them in understanding and representing complex subgraphs in structured KBs. To
bridge the gap between texts and structured KBs, we propose a Structured
Knowledge-aware Pre-training method (SKP). In the pre-training stage, we
introduce two novel structured knowledge-aware tasks, guiding the model to
effectively learn the implicit relationship and better representations of
complex subgraphs. In downstream KBQA task, we further design an efficient
linearization strategy and an interval attention mechanism, which assist the
model to better encode complex subgraphs and shield the interference of
irrelevant subgraphs during reasoning respectively. Detailed experiments and
analyses on WebQSP verify the effectiveness of SKP, especially the significant
improvement in subgraph retrieval (+4.08% H@10).",http://arxiv.org/pdf/2308.14436v1
2308.14429v1,cs.CL,Biomedical Entity Linking with Triple-aware Pre-Training,2023-08-28 09:06:28+00:00,"Linking biomedical entities is an essential aspect in biomedical natural
language processing tasks, such as text mining and question answering. However,
a difficulty of linking the biomedical entities using current large language
models (LLM) trained on a general corpus is that biomedical entities are
scarcely distributed in texts and therefore have been rarely seen during
training by the LLM. At the same time, those LLMs are not aware of high level
semantic connection between different biomedical entities, which are useful in
identifying similar concepts in different textual contexts. To cope with
aforementioned problems, some recent works focused on injecting knowledge graph
information into LLMs. However, former methods either ignore the relational
knowledge of the entities or lead to catastrophic forgetting. Therefore, we
propose a novel framework to pre-train the powerful generative LLM by a corpus
synthesized from a KG. In the evaluations we are unable to confirm the benefit
of including synonym, description or relational information.",http://arxiv.org/pdf/2308.14429v1
2308.14423v1,cs.CL,GADePo: Graph-Assisted Declarative Pooling Transformers for Document-Level Relation Extraction,2023-08-28 09:04:03+00:00,"Document-level relation extraction aims to identify relationships between
entities within a document. Current methods rely on text-based encoders and
employ various hand-coded pooling heuristics to aggregate information from
entity mentions and associated contexts. In this paper, we replace these rigid
pooling functions with explicit graph relations by leveraging the intrinsic
graph processing capabilities of the Transformer model. We propose a joint
text-graph Transformer model, and a graph-assisted declarative pooling (GADePo)
specification of the input which provides explicit and high-level instructions
for information aggregation. This allows the pooling process to be guided by
domain-specific knowledge or desired outcomes but still learned by the
Transformer, leading to more flexible and customizable pooling strategies. We
extensively evaluate our method across diverse datasets and models, and show
that our approach yields promising results that are comparable to those
achieved by the hand-coded pooling functions.",http://arxiv.org/pdf/2308.14423v1
2308.14391v1,cs.CV,FIRE: Food Image to REcipe generation,2023-08-28 08:14:20+00:00,"Food computing has emerged as a prominent multidisciplinary field of research
in recent years. An ambitious goal of food computing is to develop end-to-end
intelligent systems capable of autonomously producing recipe information for a
food image. Current image-to-recipe methods are retrieval-based and their
success depends heavily on the dataset size and diversity, as well as the
quality of learned embeddings. Meanwhile, the emergence of powerful
attention-based vision and language models presents a promising avenue for
accurate and generalizable recipe generation, which has yet to be extensively
explored. This paper proposes FIRE, a novel multimodal methodology tailored to
recipe generation in the food computing domain, which generates the food title,
ingredients, and cooking instructions based on input food images. FIRE
leverages the BLIP model to generate titles, utilizes a Vision Transformer with
a decoder for ingredient extraction, and employs the T5 model to generate
recipes incorporating titles and ingredients as inputs. We showcase two
practical applications that can benefit from integrating FIRE with large
language model prompting: recipe customization to fit recipes to user
preferences and recipe-to-code transformation to enable automated cooking
processes. Our experimental findings validate the efficacy of our proposed
approach, underscoring its potential for future advancements and widespread
adoption in food computing.",http://arxiv.org/pdf/2308.14391v1
2308.14359v2,cs.AI,Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks,2023-08-28 07:11:27+00:00,"Human emotion understanding is pivotal in making conversational technology
mainstream. We view speech emotion understanding as a perception task which is
a more realistic setting. With varying contexts (languages, demographics, etc.)
different share of people perceive the same speech segment as a non-unanimous
emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics
ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset
of multilingual speakers and multi-label regression target of 'emotion share'
or perception of that emotion. We demonstrate that the training scheme of
different foundation models dictates their effectiveness for tasks beyond
speech recognition, especially for non-semantic speech tasks like emotion
understanding. This is a very complex task due to multilingual speakers,
variability in the target labels, and inherent imbalance in the regression
dataset. Our results show that HuBERT-Large with a self-attention-based
light-weight sequence model provides 4.6% improvement over the reported
baseline.",http://arxiv.org/pdf/2308.14359v2
2308.14353v1,cs.CL,"ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models",2023-08-28 06:56:44+00:00,"The unprecedented performance of large language models (LLMs) requires
comprehensive and accurate evaluation. We argue that for LLMs evaluation,
benchmarks need to be comprehensive and systematic. To this end, we propose the
ZhuJiu benchmark, which has the following strengths: (1) Multi-dimensional
ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions
covering 51 tasks. Especially, we also propose a new benchmark that focuses on
knowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration:
We use 3 different yet complementary evaluation methods to comprehensively
evaluate LLMs, which can ensure the authority and accuracy of the evaluation
results. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering
benchmark that fully assesses LLMs in Chinese, while also providing equally
robust evaluation abilities in English. (4) Avoiding potential data leakage: To
avoid data leakage, we construct evaluation data specifically for 37 tasks. We
evaluate 10 current mainstream LLMs and conduct an in-depth discussion and
analysis of their results. The ZhuJiu benchmark and open-participation
leaderboard are publicly released at http://www.zhujiu-benchmark.com/ and we
also provide a demo video at https://youtu.be/qypkJ89L1Ic.",http://arxiv.org/pdf/2308.14353v1
2308.14352v1,cs.LG,EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models,2023-08-28 06:56:08+00:00,"Large Language Models (LLMs) such as GPTs and LLaMa have ushered in a
revolution in machine intelligence, owing to their exceptional capabilities in
a wide range of machine learning tasks. However, the transition of LLMs from
data centers to edge devices presents a set of challenges and opportunities.
While this shift can enhance privacy and availability, it is hampered by the
enormous parameter sizes of these models, leading to impractical runtime costs.
In light of these considerations, we introduce EdgeMoE, the first on-device
inference engine tailored for mixture-of-expert (MoE) LLMs, a popular variant
of sparse LLMs that exhibit nearly constant computational complexity as their
parameter size scales. EdgeMoE achieves both memory and computational
efficiency by strategically partitioning the model across the storage
hierarchy. Specifically, non-expert weights are stored in the device's memory,
while expert weights are kept in external storage and are fetched into memory
only when they are activated. This design is underpinned by a crucial insight
that expert weights, though voluminous, are infrequently accessed due to sparse
activation patterns. To further mitigate the overhead associated with expert
I/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wise
bitwidth adaptation: This method reduces the size of expert weights with an
acceptable level of accuracy loss. (2) Expert management: It predicts the
experts that will be activated in advance and preloads them into the
compute-I/O pipeline, thus further optimizing the process. In empirical
evaluations conducted on well-established MoE LLMs and various edge devices,
EdgeMoE demonstrates substantial memory savings and performance improvements
when compared to competitive baseline solutions.",http://arxiv.org/pdf/2308.14352v1
2308.14346v1,cs.CL,DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation,2023-08-28 06:41:49+00:00,"We propose DISC-MedLLM, a comprehensive solution that leverages Large
Language Models (LLMs) to provide accurate and truthful medical response in
end-to-end conversational healthcare services. To construct high-quality
Supervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing
medical knowledge-graphs, reconstructing real-world dialogues, and
incorporating human-guided preference rephrasing. These datasets are
instrumental in training DISC-MedLLM, surpassing existing medical LLMs in both
single-turn and multi-turn consultation scenarios. Extensive experimental
results demonstrate the effectiveness of the proposed model in bridging the gap
between general language models and real-world medical consultation.
Additionally, we release the constructed dataset and model weights to further
contribute to research and development. Further details and resources can be
found at https://github.com/FudanDISC/DISC-MedLLM",http://arxiv.org/pdf/2308.14346v1
2308.14337v1,cs.AI,Cognitive Effects in Large Language Models,2023-08-28 06:30:33+00:00,"Large Language Models (LLMs) such as ChatGPT have received enormous attention
over the past year and are now used by hundreds of millions of people every
day. The rapid adoption of this technology naturally raises questions about the
possible biases such models might exhibit. In this work, we tested one of these
models (GPT-3) on a range of cognitive effects, which are systematic patterns
that are usually found in human cognitive tasks. We found that LLMs are indeed
prone to several human cognitive effects. Specifically, we show that the
priming, distance, SNARC, and size congruity effects were presented with GPT-3,
while the anchoring effect is absent. We describe our methodology, and
specifically the way we converted real-world experiments to text-based
experiments. Finally, we speculate on the possible reasons why GPT-3 exhibits
these effects and discuss whether they are imitated or reinvented.",http://arxiv.org/pdf/2308.14337v1
2308.14321v1,cs.CL,Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction,2023-08-28 06:05:18+00:00,"Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.",http://arxiv.org/pdf/2308.14321v1
2308.14306v2,cs.CL,Evaluating the Robustness to Instructions of Large Language Models,2023-08-28 04:57:07+00:00,"Recently, Instruction fine-tuning has risen to prominence as a potential
method for enhancing the zero-shot capabilities of Large Language Models (LLMs)
on novel tasks. This technique has shown an exceptional ability to boost the
performance of moderately sized LLMs, sometimes even reaching performance
levels comparable to those of much larger model variants. The focus is on the
robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an
exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional
Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction
datasets as case studies. We carried out a comprehensive evaluation of these
instruction-following LLMs which have been tuned based on open-domain
instructions and task-oriented instructions. The main discussion is their
performance and robustness towards instructions. We have observed that in most
cases, the model's performance in dealing with unfamiliar instructions tends to
worsen significantly, and the robustness of the model for RE instructions
deteriorates compared to QA. Further, we discovered that up until a certain
parameter size threshold (3B), the performance of the FLAN-T5 model improves as
the parameter count increases. The robustness of different scales of FLAN-T5
models to RE instruction is worse than the robustness to QA instruction.",http://arxiv.org/pdf/2308.14306v2
2308.14280v1,cs.CL,FonMTL: Towards Multitask Learning for the Fon Language,2023-08-28 03:26:21+00:00,"The Fon language, spoken by an average 2 million of people, is a truly
low-resourced African language, with a limited online presence, and existing
datasets (just to name but a few). Multitask learning is a learning paradigm
that aims to improve the generalization capacity of a model by sharing
knowledge across different but related tasks: this could be prevalent in very
data-scarce scenarios. In this paper, we present the first explorative approach
to multitask learning, for model capabilities enhancement in Natural Language
Processing for the Fon language. Specifically, we explore the tasks of Named
Entity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage
two language model heads as encoders to build shared representations for the
inputs, and we use linear layers blocks for classification relative to each
task. Our results on the NER and POS tasks for Fon, show competitive (or
better) performances compared to several multilingual pretrained language
models finetuned on single tasks. Additionally, we perform a few ablation
studies to leverage the efficiency of two different loss combination strategies
and find out that the equal loss weighting approach works best in our case. Our
code is open-sourced at https://github.com/bonaventuredossou/multitask_fon.",http://arxiv.org/pdf/2308.14280v1
2308.14272v1,cs.CL,Goodhart's Law Applies to NLP's Explanation Benchmarks,2023-08-28 03:03:03+00:00,"Despite the rising popularity of saliency-based explanations, the research
community remains at an impasse, facing doubts concerning their purpose,
efficacy, and tendency to contradict each other. Seeking to unite the
community's efforts around common goals, several recent works have proposed
evaluation metrics. In this paper, we critically examine two sets of metrics:
the ERASER metrics (comprehensiveness and sufficiency) and the EVAL-X metrics,
focusing our inquiry on natural language processing. First, we show that we can
inflate a model's comprehensiveness and sufficiency scores dramatically without
altering its predictions or explanations on in-distribution test inputs. Our
strategy exploits the tendency for extracted explanations and their complements
to be ""out-of-support"" relative to each other and in-distribution inputs. Next,
we demonstrate that the EVAL-X metrics can be inflated arbitrarily by a simple
method that encodes the label, even though EVAL-X is precisely motivated to
address such exploits. Our results raise doubts about the ability of current
metrics to guide explainability research, underscoring the need for a broader
reassessment of what precisely these metrics are intended to capture.",http://arxiv.org/pdf/2308.14272v1
2308.14266v1,cs.CL,SalesBot 2.0: A Human-Like Intent-Guided Chit-Chat Dataset,2023-08-28 02:48:49+00:00,"In recent research on dialogue systems and corpora, there has been a
significant focus on two distinct categories: task-oriented (TOD) and
open-domain (chit-chat) dialogues. TOD systems aim to satisfy specific user
goals, such as finding a movie to watch, whereas open-domain systems primarily
focus on generating engaging conversations. A recent study by Chiu et al.
(2022) introduced SalesBot, which provides simulators and a dataset with
one-turn transition from chit-chat to task-oriented dialogues. However, the
previously generated data solely relied on BlenderBot, which raised concerns
about its long-turn naturalness and consistency during a conversation. To
address this issue, this paper aims to build SalesBot 2.0, a revised version of
the published data, by leveraging the commonsense knowledge of large language
models (LLMs) through proper prompting. The objective is to gradually bridge
the gap between chit-chat and TOD towards better naturalness and consistency.
The newly released large-scale dataset with detailed annotations exhibits
smoother transitions between topics and is more human-like in terms of
naturalness and consistency. It can serve as a valuable resource for both
academic research and commercial applications. Furthermore, our proposed
framework can be applied to generate numerous dialogues with various target
intents.",http://arxiv.org/pdf/2308.14266v1
2308.14242v1,cs.AI,The Cultural Psychology of Large Language Models: Is ChatGPT a Holistic or Analytic Thinker?,2023-08-28 01:05:18+00:00,"The prevalent use of Large Language Models (LLMs) has necessitated studying
their mental models, yielding noteworthy theoretical and practical
implications. Current research has demonstrated that state-of-the-art LLMs,
such as ChatGPT, exhibit certain theory of mind capabilities and possess
relatively stable Big Five and/or MBTI personality traits. In addition,
cognitive process features form an essential component of these mental models.
Research in cultural psychology indicated significant differences in the
cognitive processes of Eastern and Western people when processing information
and making judgments. While Westerners predominantly exhibit analytical
thinking that isolates things from their environment to analyze their nature
independently, Easterners often showcase holistic thinking, emphasizing
relationships and adopting a global viewpoint. In our research, we probed the
cultural cognitive traits of ChatGPT. We employed two scales that directly
measure the cognitive process: the Analysis-Holism Scale (AHS) and the Triadic
Categorization Task (TCT). Additionally, we used two scales that investigate
the value differences shaped by cultural thinking: the Dialectical Self Scale
(DSS) and the Self-construal Scale (SCS). In cognitive process tests (AHS/TCT),
ChatGPT consistently tends towards Eastern holistic thinking, but regarding
value judgments (DSS/SCS), ChatGPT does not significantly lean towards the East
or the West. We suggest that the result could be attributed to both the
training paradigm and the training data in LLM development. We discuss the
potential value of this finding for AI research and directions for future
research.",http://arxiv.org/pdf/2308.14242v1
2308.14217v1,cs.DB,Generations of Knowledge Graphs: The Crazy Ideas and the Business Impact,2023-08-27 22:35:27+00:00,"Knowledge Graphs (KGs) have been used to support a wide range of
applications, from web search to personal assistant. In this paper, we describe
three generations of knowledge graphs: entity-based KGs, which have been
supporting general search and question answering (e.g., at Google and Bing);
text-rich KGs, which have been supporting search and recommendations for
products, bio-informatics, etc. (e.g., at Amazon and Alibaba); and the emerging
integration of KGs and LLMs, which we call dual neural KGs. We describe the
characteristics of each generation of KGs, the crazy ideas behind the scenes in
constructing such KGs, and the techniques developed over time to enable
industry impact. In addition, we use KGs as examples to demonstrate a recipe to
evolve research ideas from innovations to production practice, and then to the
next level of innovations, to advance both science and business.",http://arxiv.org/pdf/2308.14217v1
2308.14199v1,cs.AI,Symbolic and Language Agnostic Large Language Models,2023-08-27 20:24:33+00:00,"We argue that the relative success of large language models (LLMs) is not a
reflection on the symbolic vs. subsymbolic debate but a reflection on employing
an appropriate strategy of bottom-up reverse engineering of language at scale.
However, due to the subsymbolic nature of these models whatever knowledge these
systems acquire about language will always be buried in millions of
microfeatures (weights) none of which is meaningful on its own. Moreover, and
due to their stochastic nature, these models will often fail in capturing
various inferential aspects that are prevalent in natural language. What we
suggest here is employing the successful bottom-up strategy in a symbolic
setting, producing symbolic, language agnostic and ontologically grounded large
language models.",http://arxiv.org/pdf/2308.14199v1
2308.14186v1,cs.CL,Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations,2023-08-27 19:22:12+00:00,"The language ability of Large Language Models (LLMs) is often unbalanced
towards English because of the imbalance in the distribution of the
pre-training data. This disparity is demanded in further fine-tuning and
affecting the cross-lingual abilities of LLMs. In this paper, we propose to
empower Instructiontuned LLMs (It-LLMs) in languages other than English by
building semantic alignment between them. Hence, we propose CrossAlpaca, an
It-LLM with cross-lingual instruction-following and Translation-following
demonstrations to improve semantic alignment between languages. We validate our
approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA
and adapted versions of MMLU and BBH. Our models, tested over six different
languages, outperform the It-LLMs tuned on monolingual data. The final results
show that instruction tuning on non-English data is not enough and that
semantic alignment can be further improved by Translation-following
demonstrations.",http://arxiv.org/pdf/2308.14186v1
2308.14182v1,cs.CL,Generative AI for Business Strategy: Using Foundation Models to Create Business Strategy Tools,2023-08-27 19:03:12+00:00,"Generative models (foundation models) such as LLMs (large language models)
are having a large impact on multiple fields. In this work, we propose the use
of such models for business decision making. In particular, we combine
unstructured textual data sources (e.g., news data) with multiple foundation
models (namely, GPT4, transformer-based Named Entity Recognition (NER) models
and Entailment-based Zero-shot Classifiers (ZSC)) to derive IT (information
technology) artifacts in the form of a (sequence of) signed business networks.
We posit that such artifacts can inform business stakeholders about the state
of the market and their own positioning as well as provide quantitative
insights into improving their future outlook.",http://arxiv.org/pdf/2308.14182v1
2308.14179v1,cs.CL,Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP,2023-08-27 18:46:47+00:00,"Mechanistic interpretability seeks to understand the neural mechanisms that
enable specific behaviors in Large Language Models (LLMs) by leveraging
causality-based methods. While these approaches have identified neural circuits
that copy spans of text, capture factual knowledge, and more, they remain
unusable for multimodal models since adapting these tools to the
vision-language domain requires considerable architectural changes. In this
work, we adapt a unimodal causal tracing tool to BLIP to enable the study of
the neural mechanisms underlying image-conditioned text generation. We
demonstrate our approach on a visual question answering dataset, highlighting
the causal relevance of later layer representations for all tokens.
Furthermore, we release our BLIP causal tracing tool as open source to enable
further experimentation in vision-language mechanistic interpretability by the
community. Our code is available at
https://github.com/vedantpalit/Towards-Vision-Language-Mechanistic-Interpretability.",http://arxiv.org/pdf/2308.14179v1
2308.14149v1,cs.CL,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",2023-08-27 16:14:19+00:00,"Generative pre-trained transformer (GPT) models have revolutionized the field
of natural language processing (NLP) with remarkable performance in various
tasks and also extend their power to multimodal domains. Despite their success,
large GPT models like GPT-4 face inherent limitations such as considerable
size, high computational requirements, complex deployment processes, and closed
development loops. These constraints restrict their widespread adoption and
raise concerns regarding their responsible development and usage. The need for
user-friendly, relatively small, and open-sourced alternative GPT models arises
from the desire to overcome these limitations while retaining high performance.
In this survey paper, we provide an examination of alternative open-sourced
models of large GPTs, focusing on user-friendly and relatively small models
that facilitate easier deployment and accessibility. Through this extensive
survey, we aim to equip researchers, practitioners, and enthusiasts with a
thorough understanding of user-friendly and relatively small open-sourced
models of large GPTs, their current state, challenges, and future research
directions, inspiring the development of more efficient, accessible, and
versatile GPT models that cater to the broader scientific community and advance
the field of general artificial intelligence. The source contents are
continuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.",http://arxiv.org/pdf/2308.14149v1
2308.14132v1,cs.CL,Detecting Language Model Attacks with Perplexity,2023-08-27 15:20:06+00:00,"A novel hack involving Large Language Models (LLMs) has emerged, leveraging
adversarial suffixes to trick models into generating perilous responses. This
method has garnered considerable attention from reputable media outlets such as
the New York Times and Wired, thereby influencing public perception regarding
the security and safety of LLMs. In this study, we advocate the utilization of
perplexity as one of the means to recognize such potential attacks. The
underlying concept behind these hacks revolves around appending an unusually
constructed string of text to a harmful query that would otherwise be blocked.
This maneuver confuses the protective mechanisms and tricks the model into
generating a forbidden response. Such scenarios could result in providing
detailed instructions to a malicious user for constructing explosives or
orchestrating a bank heist. Our investigation demonstrates the feasibility of
employing perplexity, a prevalent natural language processing metric, to detect
these adversarial tactics before generating a forbidden response. By evaluating
the perplexity of queries with and without such adversarial suffixes using an
open-source LLM, we discovered that nearly 90 percent were above a perplexity
of 1000. This contrast underscores the efficacy of perplexity for detecting
this type of exploit.",http://arxiv.org/pdf/2308.14132v1
2308.14120v2,cs.LG,Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies,2023-08-27 14:28:38+00:00,"A knowledge gap persists between Machine Learning (ML) developers (e.g., data
scientists) and practitioners (e.g., clinicians), hampering the full
utilization of ML for clinical data analysis. We investigated the potential of
the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this
gap and perform ML analyses efficiently. Real-world clinical datasets and study
details from large trials across various medical specialties were presented to
chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed
state-of-the-art ML models based on the original study's training data to
predict clinical outcomes such as cancer development, cancer progression,
disease complications, or biomarkers such as pathogenic gene sequences.
Strikingly, these ML models matched or outperformed their published
counterparts. We conclude that chatGPT ADA offers a promising avenue to
democratize ML in medicine, making advanced analytics accessible to non-ML
experts and promoting broader applications in medical research and practice.",http://arxiv.org/pdf/2308.14120v2
2308.14115v1,cs.CL,Situated Natural Language Explanations,2023-08-27 14:14:28+00:00,"Natural language is among the most accessible tools for explaining decisions
to humans, and large pretrained language models (PLMs) have demonstrated
impressive abilities to generate coherent natural language explanations (NLE).
The existing NLE research perspectives do not take the audience into account.
An NLE can have high textual quality, but it might not accommodate audiences'
needs and preference. To address this limitation, we propose an alternative
perspective, situated NLE, including a situated generation framework and a
situated evaluation framework. On the generation side, we propose simple prompt
engineering methods that adapt the NLEs to situations. In human studies, the
annotators preferred the situated NLEs. On the evaluation side, we set up
automated evaluation scores in lexical, semantic, and pragmatic categories. The
scores can be used to select the most suitable prompts to generate NLEs.
Situated NLE provides a perspective to conduct further research on automatic
NLE generations.",http://arxiv.org/pdf/2308.14115v1
2308.14089v1,cs.CL,MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records,2023-08-27 12:24:39+00:00,"The ability of large language models (LLMs) to follow natural language
instructions with human-level fluency suggests many opportunities in healthcare
to reduce administrative burden and improve quality of care. However,
evaluating LLMs on realistic text generation tasks for healthcare remains
challenging. Existing question answering datasets for electronic health record
(EHR) data fail to capture the complexity of information needs and
documentation burdens experienced by clinicians. To address these challenges,
we introduce MedAlign, a benchmark dataset of 983 natural language instructions
for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes
clinician-written reference responses for 303 instructions, and provides 276
longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to
evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality
of each LLM response. We found high error rates, ranging from 35% (GPT-4) to
68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k
context lengths for GPT-4. Finally, we report correlations between clinician
rankings and automated natural language generation metrics as a way to rank
LLMs without human review. We make MedAlign available under a research data use
agreement to enable LLM evaluations on tasks aligned with clinician needs and
preferences.",http://arxiv.org/pdf/2308.14089v1
2308.14077v1,cs.FL,An Analysis of On-the-fly Determinization of Finite-state Automata,2023-08-27 11:51:27+00:00,"In this paper we establish an abstraction of on-the-fly determinization of
finite-state automata using transition monoids and demonstrate how it can be
applied to bound the asymptotics. We present algebraic and combinatorial
properties that are sufficient for a polynomial state complexity of the
deterministic automaton constructed on-the-fly. A special case of our findings
is that automata with many non-deterministic transitions almost always admit a
determinization of polynomial complexity. Furthermore, we extend our ideas to
weighted finite-state automata.",http://arxiv.org/pdf/2308.14077v1
2308.16150v1,eess.IV,Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI,2023-08-30 17:16:02+00:00,"Unsupervised anomaly segmentation aims to detect patterns that are distinct
from any patterns processed during training, commonly called abnormal or
out-of-distribution patterns, without providing any associated manual
segmentations. Since anomalies during deployment can lead to model failure,
detecting the anomaly can enhance the reliability of models, which is valuable
in high-risk domains like medical imaging. This paper introduces Masked
Modality Cycles with Conditional Diffusion (MMCCD), a method that enables
segmentation of anomalies across diverse patterns in multimodal MRI. The method
is based on two fundamental ideas. First, we propose the use of cyclic modality
translation as a mechanism for enabling abnormality detection.
Image-translation models learn tissue-specific modality mappings, which are
characteristic of tissue physiology. Thus, these learned mappings fail to
translate tissues or image patterns that have never been encountered during
training, and the error enables their segmentation. Furthermore, we combine
image translation with a masked conditional diffusion model, which attempts to
`imagine' what tissue exists under a masked area, further exposing unknown
patterns as the generative model fails to recreate them. We evaluate our method
on a proxy task by training on healthy-looking slices of BraTS2021
multi-modality MRIs and testing on slices with tumors. We show that our method
compares favorably to previous unsupervised approaches based on image
reconstruction and denoising with autoencoders and diffusion models.",http://arxiv.org/pdf/2308.16150v1
2308.16139v1,cs.CV,MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision,2023-08-30 16:52:20+00:00,"We present MedShapeNet, a large collection of anatomical shapes (e.g., bones,
organs, vessels) and 3D surgical instrument models. Prior to the deep learning
era, the broad application of statistical shape models (SSMs) in medical image
analysis is evidence that shapes have been commonly used to describe medical
data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in
medical imaging are predominantly voxel-based. In computer vision, on the
contrary, shapes (including, voxel occupancy grids, meshes, point clouds and
implicit surface models) are preferred data representations in 3D, as seen from
the numerous shape-related publications in premier vision conferences, such as
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as
well as the increasing popularity of ShapeNet (about 51,300 models) and
Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is
created as an alternative to these commonly used shape benchmarks to facilitate
the translation of data-driven vision algorithms to medical applications, and
it extends the opportunities to adapt SOTA vision algorithms to solve critical
medical problems. Besides, the majority of the medical shapes in MedShapeNet
are modeled directly on the imaging data of real patients, and therefore it
complements well existing shape benchmarks comprising of computer-aided design
(CAD) models. MedShapeNet currently includes more than 100,000 medical shapes,
and provides annotations in the form of paired data. It is therefore also a
freely available repository of 3D models for extended reality (virtual reality
- VR, augmented reality - AR, mixed reality - MR) and medical 3D printing. This
white paper describes in detail the motivations behind MedShapeNet, the shape
acquisition procedures, the use cases, as well as the usage of the online shape
search portal: https://medshapenet.ikim.nrw/",http://arxiv.org/pdf/2308.16139v1
2308.16122v1,cs.LG,Spatial Graph Coarsening: Weather and Weekday Prediction with London's Bike-Sharing Service using GNN,2023-08-30 16:21:02+00:00,"This study introduced the use of Graph Neural Network (GNN) for predicting
the weather and weekday of a day in London, from the dataset of Santander
Cycles bike-sharing system as a graph classification task. The proposed GNN
models newly introduced (i) a concatenation operator of graph features with
trained node embeddings and (ii) a graph coarsening operator based on
geographical contiguity, namely ""Spatial Graph Coarsening"". With the node
features of land-use characteristics and number of households around the bike
stations and graph features of temperatures in the city, our proposed models
outperformed the baseline model in cross-entropy loss and accuracy of the
validation dataset.",http://arxiv.org/pdf/2308.16122v1
2308.16105v1,cs.LG,Advanced Deep Regression Models for Forecasting Time Series Oil Production,2023-08-30 15:54:06+00:00,"Global oil demand is rapidly increasing and is expected to reach 106.3
million barrels per day by 2040. Thus, it is vital for hydrocarbon extraction
industries to forecast their production to optimize their operations and avoid
losses. Big companies have realized that exploiting the power of deep learning
(DL) and the massive amount of data from various oil wells for this purpose can
save a lot of operational costs and reduce unwanted environmental impacts. In
this direction, researchers have proposed models using conventional machine
learning (ML) techniques for oil production forecasting. However, these
techniques are inappropriate for this problem as they can not capture
historical patterns found in time series data, resulting in inaccurate
predictions. This research aims to overcome these issues by developing advanced
data-driven regression models using sequential convolutions and long short-term
memory (LSTM) units. Exhaustive analyses are conducted to select the optimal
sequence length, model hyperparameters, and cross-well dataset formation to
build highly generalized robust models. A comprehensive experimental study on
Volve oilfield data validates the proposed models. It reveals that the
LSTM-based sequence learning model can predict oil production better than the
1-D convolutional neural network (CNN) with mean absolute error (MAE) and R2
score of 111.16 and 0.98, respectively. It is also found that the LSTM-based
model performs better than all the existing state-of-the-art solutions and
achieves a 37% improvement compared to a standard linear regression, which is
considered the baseline model in this work.",http://arxiv.org/pdf/2308.16105v1
2308.16059v1,stat.ML,A Parameter-Free Two-Bit Covariance Estimator with Improved Operator Norm Error Rate,2023-08-30 14:31:24+00:00,"A covariance matrix estimator using two bits per entry was recently developed
by Dirksen, Maly and Rauhut [Annals of Statistics, 50(6), pp. 3538-3562]. The
estimator achieves near minimax rate for general sub-Gaussian distributions,
but also suffers from two downsides: theoretically, there is an essential gap
on operator norm error between their estimator and sample covariance when the
diagonal of the covariance matrix is dominated by only a few entries;
practically, its performance heavily relies on the dithering scale, which needs
to be tuned according to some unknown parameters. In this work, we propose a
new 2-bit covariance matrix estimator that simultaneously addresses both
issues. Unlike the sign quantizer associated with uniform dither in Dirksen et
al., we adopt a triangular dither prior to a 2-bit quantizer inspired by the
multi-bit uniform quantizer. By employing dithering scales varying across
entries, our estimator enjoys an improved operator norm error rate that depends
on the effective rank of the underlying covariance matrix rather than the
ambient dimension, thus closing the theoretical gap. Moreover, our proposed
method eliminates the need of any tuning parameter, as the dithering scales are
entirely determined by the data. Experimental results under Gaussian samples
are provided to showcase the impressive numerical performance of our estimator.
Remarkably, by halving the dithering scales, our estimator oftentimes achieves
operator norm errors less than twice of the errors of sample covariance.",http://arxiv.org/pdf/2308.16059v1
2308.16056v1,cs.LG,Low-Rank Multitask Learning based on Tensorized SVMs and LSSVMs,2023-08-30 14:28:26+00:00,"Multitask learning (MTL) leverages task-relatedness to enhance performance.
With the emergence of multimodal data, tasks can now be referenced by multiple
indices. In this paper, we employ high-order tensors, with each mode
corresponding to a task index, to naturally represent tasks referenced by
multiple indices and preserve their structural relations. Based on this
representation, we propose a general framework of low-rank MTL methods with
tensorized support vector machines (SVMs) and least square support vector
machines (LSSVMs), where the CP factorization is deployed over the coefficient
tensor. Our approach allows to model the task relation through a linear
combination of shared factors weighted by task-specific factors and is
generalized to both classification and regression problems. Through the
alternating optimization scheme and the Lagrangian function, each subproblem is
transformed into a convex problem, formulated as a quadratic programming or
linear system in the dual form. In contrast to previous MTL frameworks, our
decision function in the dual induces a weighted kernel function with a
task-coupling term characterized by the similarities of the task-specific
factors, better revealing the explicit relations across tasks in MTL.
Experimental results validate the effectiveness and superiority of our proposed
methods compared to existing state-of-the-art approaches in MTL. The code of
implementation will be available at https://github.com/liujiani0216/TSVM-MTL.",http://arxiv.org/pdf/2308.16056v1
2308.16022v1,stat.ML,PAVI: Plate-Amortized Variational Inference,2023-08-30 13:22:20+00:00,"Given observed data and a probabilistic generative model, Bayesian inference
searches for the distribution of the model's parameters that could have yielded
the data. Inference is challenging for large population studies where millions
of measurements are performed over a cohort of hundreds of subjects, resulting
in a massive parameter space. This large cardinality renders off-the-shelf
Variational Inference (VI) computationally impractical.
  In this work, we design structured VI families that efficiently tackle large
population studies. Our main idea is to share the parameterization and learning
across the different i.i.d. variables in a generative model, symbolized by the
model's \textit{plates}. We name this concept \textit{plate amortization}.
Contrary to off-the-shelf stochastic VI, which slows down inference, plate
amortization results in orders of magnitude faster to train variational
distributions.
  Applied to large-scale hierarchical problems, PAVI yields expressive,
parsimoniously parameterized VI with an affordable training time. This faster
convergence effectively unlocks inference in those large regimes. We illustrate
the practical utility of PAVI through a challenging Neuroimaging example
featuring 400 million latent parameters, demonstrating a significant step
towards scalable and expressive Variational Inference.",http://arxiv.org/pdf/2308.16022v1
2308.15984v1,cs.CV,Learning Structure-from-Motion with Graph Attention Networks,2023-08-30 12:13:13+00:00,"In this paper we tackle the problem of learning Structure-from-Motion (SfM)
through the use of graph attention networks. SfM is a classic computer vision
problem that is solved though iterative minimization of reprojection errors,
referred to as Bundle Adjustment (BA), starting from a good initialization. In
order to obtain a good enough initialization to BA, conventional methods rely
on a sequence of sub-problems (such as pairwise pose estimation, pose averaging
or triangulation) which provides an initial solution that can then be refined
using BA. In this work we replace these sub-problems by learning a model that
takes as input the 2D keypoints detected across multiple views, and outputs the
corresponding camera poses and 3D keypoint coordinates. Our model takes
advantage of graph neural networks to learn SfM-specific primitives, and we
show that it can be used for fast inference of the reconstruction for new and
unseen sequences. The experimental results show that the proposed model
outperforms competing learning-based methods, and challenges COLMAP while
having lower runtime.",http://arxiv.org/pdf/2308.15984v1
2308.15973v1,cs.NI,Demo: A Digital Twin of the 5G Radio Access Network for Anomaly Detection Functionality,2023-08-30 11:51:38+00:00,"Recently, the concept of digital twins (DTs) has received significant
attention within the realm of 5G/6G. This demonstration shows an innovative DT
design and implementation framework tailored toward integration within the 5G
infrastructure. The proposed DT enables near real-time anomaly detection
capability pertaining to user connectivity. It empowers the 5G system to
proactively execute decisions for resource control and connection restoration.",http://arxiv.org/pdf/2308.15973v1
2308.15936v1,cs.DS,Jaccard-constrained dense subgraph discovery,2023-08-30 10:33:02+00:00,"Finding dense subgraphs is a core problem in graph mining with many
applications in diverse domains. At the same time many real-world networks vary
over time, that is, the dataset can be represented as a sequence of graph
snapshots. Hence, it is natural to consider the question of finding dense
subgraphs in a temporal network that are allowed to vary over time to a certain
degree. In this paper, we search for dense subgraphs that have large pairwise
Jaccard similarity coefficients. More formally, given a set of graph snapshots
and a weight $\lambda$, we find a collection of dense subgraphs such that the
sum of densities of the induced subgraphs plus the sum of Jaccard indices,
weighted by $\lambda$, is maximized. We prove that this problem is NP-hard. To
discover dense subgraphs with good objective value, we present an iterative
algorithm which runs in $\mathcal{O}(n^2k^2 + m \log n + k^3 n)$ time per
single iteration, and a greedy algorithm which runs in $\mathcal{O}(n^2k^2 + m
\log n + k^3 n)$ time, where $k$ is the length of the graph sequence and $n$
and $m$ denote number of nodes and total number of edges respectively. We show
experimentally that our algorithms are efficient, they can find ground truth in
synthetic datasets and provide interpretable results from real-world datasets.
Finally, we present a case study that shows the usefulness of our problem.",http://arxiv.org/pdf/2308.15936v1
2308.15873v1,cs.LG,"Minimum Width for Deep, Narrow MLP: A Diffeomorphism and the Whitney Embedding Theorem Approach",2023-08-30 08:58:23+00:00,"Recently, there has been significant attention on determining the minimum
width for the universal approximation property of deep, narrow MLPs. Among
these challenges, approximating a continuous function under the uniform norm is
important and challenging, with the gap between its lower and upper bound being
hard to narrow. In this regard, we propose a novel upper bound for the minimum
width, given by $\operatorname{max}(2d_x+1, d_y) + \alpha(\sigma)$, to achieve
uniform approximation in deep narrow MLPs, where $0\leq \alpha(\sigma)\leq 2$
represents the constant depending on the activation function. We demonstrate
this bound through two key proofs. First, we establish that deep, narrow MLPs
with little additional width can approximate diffeomorphisms. Secondly, we
utilize the Whitney embedding theorem to show that any continuous function can
be approximated by embeddings, further decomposed into linear transformations
and diffeomorphisms.",http://arxiv.org/pdf/2308.15873v1
2308.15856v1,cs.LG,Domain Generalization without Excess Empirical Risk,2023-08-30 08:46:46+00:00,"Given data from diverse sets of distinct distributions, domain generalization
aims to learn models that generalize to unseen distributions. A common approach
is designing a data-driven surrogate penalty to capture generalization and
minimize the empirical risk jointly with the penalty. We argue that a
significant failure mode of this recipe is an excess risk due to an erroneous
penalty or hardness in joint optimization. We present an approach that
eliminates this problem. Instead of jointly minimizing empirical risk with the
penalty, we minimize the penalty under the constraint of optimality of the
empirical risk. This change guarantees that the domain generalization penalty
cannot impair optimization of the empirical risk, i.e., in-distribution
performance. To solve the proposed optimization problem, we demonstrate an
exciting connection to rate-distortion theory and utilize its tools to design
an efficient method. Our approach can be applied to any penalty-based domain
generalization method, and we demonstrate its effectiveness by applying it to
three examplar methods from the literature, showing significant improvements.",http://arxiv.org/pdf/2308.15856v1
2308.15838v1,stat.ML,"Adaptive Lasso, Transfer Lasso, and Beyond: An Asymptotic Perspective",2023-08-30 08:21:46+00:00,"This paper presents a comprehensive exploration of the theoretical properties
inherent in the Adaptive Lasso and the Transfer Lasso. The Adaptive Lasso, a
well-established method, employs regularization divided by initial estimators
and is characterized by asymptotic normality and variable selection
consistency. In contrast, the recently proposed Transfer Lasso employs
regularization subtracted by initial estimators with the demonstrated capacity
to curtail non-asymptotic estimation errors. A pivotal question thus emerges:
Given the distinct ways the Adaptive Lasso and the Transfer Lasso employ
initial estimators, what benefits or drawbacks does this disparity confer upon
each method? This paper conducts a theoretical examination of the asymptotic
properties of the Transfer Lasso, thereby elucidating its differentiation from
the Adaptive Lasso. Informed by the findings of this analysis, we introduce a
novel method, one that amalgamates the strengths and compensates for the
weaknesses of both methods. The paper concludes with validations of our theory
and comparisons of the methods via simulation experiments.",http://arxiv.org/pdf/2308.15838v1
2308.15783v1,cs.CR,Split Without a Leak: Reducing Privacy Leakage in Split Learning,2023-08-30 06:28:42+00:00,"The popularity of Deep Learning (DL) makes the privacy of sensitive data more
imperative than ever. As a result, various privacy-preserving techniques have
been implemented to preserve user data privacy in DL. Among various
privacy-preserving techniques, collaborative learning techniques, such as Split
Learning (SL) have been utilized to accelerate the learning and prediction
process. Initially, SL was considered a promising approach to data privacy.
However, subsequent research has demonstrated that SL is susceptible to many
types of attacks and, therefore, it cannot serve as a privacy-preserving
technique. Meanwhile, countermeasures using a combination of SL and encryption
have also been introduced to achieve privacy-preserving deep learning. In this
work, we propose a hybrid approach using SL and Homomorphic Encryption (HE).
The idea behind it is that the client encrypts the activation map (the output
of the split layer between the client and the server) before sending it to the
server. Hence, during both forward and backward propagation, the server cannot
reconstruct the client's input data from the intermediate activation map. This
improvement is important as it reduces privacy leakage compared to other
SL-based works, where the server can gain valuable information about the
client's input. In addition, on the MIT-BIH dataset, our proposed hybrid
approach using SL and HE yields faster training time (about 6 times) and
significantly reduced communication overhead (almost 160 times) compared to
other HE-based approaches, thereby offering improved privacy protection for
sensitive data in DL.",http://arxiv.org/pdf/2308.15783v1
2308.15730v1,cs.LG,Fully Embedded Time-Series Generative Adversarial Networks,2023-08-30 03:14:02+00:00,"Generative Adversarial Networks (GANs) should produce synthetic data that
fits the underlying distribution of the data being modeled. For real valued
time-series data, this implies the need to simultaneously capture the static
distribution of the data, but also the full temporal distribution of the data
for any potential time horizon. This temporal element produces a more complex
problem that can potentially leave current solutions under-constrained,
unstable during training, or prone to varying degrees of mode collapse. In
FETSGAN, entire sequences are translated directly to the generator's sampling
space using a seq2seq style adversarial auto encoder (AAE), where adversarial
training is used to match the training distribution in both the feature space
and the lower dimensional sampling space. This additional constraint provides a
loose assurance that the temporal distribution of the synthetic samples will
not collapse. In addition, the First Above Threshold (FAT) operator is
introduced to supplement the reconstruction of encoded sequences, which
improves training stability and the overall quality of the synthetic data being
generated. These novel contributions demonstrate a significant improvement to
the current state of the art for adversarial learners in qualitative measures
of temporal similarity and quantitative predictive ability of data generated
through FETSGAN.",http://arxiv.org/pdf/2308.15730v1
2308.15712v1,astro-ph.SR,Exploring Deep Learning for Full-disk Solar Flare Prediction with Empirical Insights from Guided Grad-CAM Explanations,2023-08-30 02:24:09+00:00,"This study progresses solar flare prediction research by presenting a
full-disk deep-learning model to forecast $\geq$M-class solar flares and
evaluating its efficacy on both central (within $\pm$70$^\circ$) and near-limb
(beyond $\pm$70$^\circ$) events, showcasing qualitative assessment of post hoc
explanations for the model's predictions, and providing empirical findings from
human-centered quantitative assessments of these explanations. Our model is
trained using hourly full-disk line-of-sight magnetogram images to predict
$\geq$M-class solar flares within the subsequent 24-hour prediction window.
Additionally, we apply the Guided Gradient-weighted Class Activation Mapping
(Guided Grad-CAM) attribution method to interpret our model's predictions and
evaluate the explanations. Our analysis unveils that full-disk solar flare
predictions correspond with active region characteristics. The following points
represent the most important findings of our study: (1) Our deep learning
models achieved an average true skill statistic (TSS) of $\sim$0.51 and a
Heidke skill score (HSS) of $\sim$0.38, exhibiting skill to predict solar
flares where for central locations the average recall is $\sim$0.75 (recall
values for X- and M-class are 0.95 and 0.73 respectively) and for the near-limb
flares the average recall is $\sim$0.52 (recall values for X- and M-class are
0.74 and 0.50 respectively); (2) qualitative examination of the model's
explanations reveals that it discerns and leverages features linked to active
regions in both central and near-limb locations within full-disk magnetograms
to produce respective predictions. In essence, our models grasp the shape and
texture-based properties of flaring active regions, even in proximity to limb
areas -- a novel and essential capability with considerable significance for
operational forecasting systems.",http://arxiv.org/pdf/2308.15712v1
2308.15709v1,cs.LG,Threshold KNN-Shapley: A Linear-Time and Privacy-Friendly Approach to Data Valuation,2023-08-30 02:12:00+00:00,"Data valuation, a critical aspect of data-centric ML research, aims to
quantify the usefulness of individual data sources in training machine learning
(ML) models. However, data valuation faces significant yet frequently
overlooked privacy challenges despite its importance. This paper studies these
challenges with a focus on KNN-Shapley, one of the most practical data
valuation methods nowadays. We first emphasize the inherent privacy risks of
KNN-Shapley, and demonstrate the significant technical difficulties in adapting
KNN-Shapley to accommodate differential privacy (DP). To overcome these
challenges, we introduce TKNN-Shapley, a refined variant of KNN-Shapley that is
privacy-friendly, allowing for straightforward modifications to incorporate DP
guarantee (DP-TKNN-Shapley). We show that DP-TKNN-Shapley has several
advantages and offers a superior privacy-utility tradeoff compared to naively
privatized KNN-Shapley in discerning data quality. Moreover, even non-private
TKNN-Shapley achieves comparable performance as KNN-Shapley. Overall, our
findings suggest that TKNN-Shapley is a promising alternative to KNN-Shapley,
particularly for real-world applications involving sensitive data.",http://arxiv.org/pdf/2308.15709v1
2308.15703v1,cs.IR,Fragment and Integrate Network (FIN): A Novel Spatial-Temporal Modeling Based on Long Sequential Behavior for Online Food Ordering Click-Through Rate Prediction,2023-08-30 01:56:57+00:00,"Spatial-temporal information has been proven to be of great significance for
click-through rate prediction tasks in online Location-Based Services (LBS),
especially in mainstream food ordering platforms such as DoorDash, Uber Eats,
Meituan, and Ele.me. Modeling user spatial-temporal preferences with sequential
behavior data has become a hot topic in recommendation systems and online
advertising. However, most of existing methods either lack the representation
of rich spatial-temporal information or only handle user behaviors with limited
length, e.g. 100. In this paper, we tackle these problems by designing a new
spatial-temporal modeling paradigm named Fragment and Integrate Network (FIN).
FIN consists of two networks: (i) Fragment Network (FN) extracts Multiple
Sub-Sequences (MSS) from lifelong sequential behavior data, and captures the
specific spatial-temporal representation by modeling each MSS respectively.
Here both a simplified attention and a complicated attention are adopted to
balance the performance gain and resource consumption. (ii) Integrate Network
(IN) builds a new integrated sequence by utilizing spatial-temporal interaction
on MSS and captures the comprehensive spatial-temporal representation by
modeling the integrated sequence with a complicated attention. Both public
datasets and production datasets have demonstrated the accuracy and scalability
of FIN. Since 2022, FIN has been fully deployed in the recommendation
advertising system of Ele.me, one of the most popular online food ordering
platforms in China, obtaining 5.7% improvement on Click-Through Rate (CTR) and
7.3% increase on Revenue Per Mille (RPM).",http://arxiv.org/pdf/2308.15703v1
2308.15697v1,cs.LG,Segmenting mechanically heterogeneous domains via unsupervised learning,2023-08-30 01:40:38+00:00,"From biological organs to soft robotics, highly deformable materials are
essential components of natural and engineered systems. These highly deformable
materials can have heterogeneous material properties, and can experience
heterogeneous deformations with or without underlying material heterogeneity.
Many recent works have established that computational modeling approaches are
well suited for understanding and predicting the consequences of material
heterogeneity and for interpreting observed heterogeneous strain fields. In
particular, there has been significant work towards developing inverse analysis
approaches that can convert observed kinematic quantities (e.g., displacement,
strain) to material properties and mechanical state. Despite the success of
these approaches, they are not necessarily generalizable and often rely on
tight control and knowledge of boundary conditions. Here, we will build on the
recent advances (and ubiquity) of machine learning approaches to explore
alternative approaches to detect patterns in heterogeneous material properties
and mechanical behavior. Specifically, we will explore unsupervised learning
approaches to clustering and ensemble clutering to identify heterogeneous
regions. Overall, we find that these approaches are effective, yet limited in
their abilities. Through this initial exploration (where all data and code is
published alongside this manuscript), we set the stage for future studies that
more specifically adapt these methods to mechanical data.",http://arxiv.org/pdf/2308.15697v1
2308.15673v1,cs.CR,MDTD: A Multi Domain Trojan Detector for Deep Neural Networks,2023-08-30 00:03:03+00:00,"Machine learning models that use deep neural networks (DNNs) are vulnerable
to backdoor attacks. An adversary carrying out a backdoor attack embeds a
predefined perturbation called a trigger into a small subset of input samples
and trains the DNN such that the presence of the trigger in the input results
in an adversary-desired output class. Such adversarial retraining however needs
to ensure that outputs for inputs without the trigger remain unaffected and
provide high classification accuracy on clean samples. In this paper, we
propose MDTD, a Multi-Domain Trojan Detector for DNNs, which detects inputs
containing a Trojan trigger at testing time. MDTD does not require knowledge of
trigger-embedding strategy of the attacker and can be applied to a pre-trained
DNN model with image, audio, or graph-based inputs. MDTD leverages an insight
that input samples containing a Trojan trigger are located relatively farther
away from a decision boundary than clean samples. MDTD estimates the distance
to a decision boundary using adversarial learning methods and uses this
distance to infer whether a test-time input sample is Trojaned or not. We
evaluate MDTD against state-of-the-art Trojan detection methods across five
widely used image-based datasets: CIFAR100, CIFAR10, GTSRB, SVHN, and
Flowers102; four graph-based datasets: AIDS, WinMal, Toxicant, and COLLAB; and
the SpeechCommand audio dataset. MDTD effectively identifies samples that
contain different types of Trojan triggers. We evaluate MDTD against adaptive
attacks where an adversary trains a robust DNN to increase (decrease) distance
of benign (Trojan) inputs from a decision boundary.",http://arxiv.org/pdf/2308.15673v1
2308.15667v1,cs.LG,Bridging Distribution Learning and Image Clustering in High-dimensional Space,2023-08-29 23:35:36+00:00,"Distribution learning focuses on learning the probability density function
from a set of data samples. In contrast, clustering aims to group similar
objects together in an unsupervised manner. Usually, these two tasks are
considered unrelated. However, the relationship between the two may be
indirectly correlated, with Gaussian Mixture Models (GMM) acting as a bridge.
In this paper, we focus on exploring the correlation between distribution
learning and clustering, with the motivation to fill the gap between these two
fields, utilizing an autoencoder (AE) to encode images into a high-dimensional
latent space. Then, Monte-Carlo Marginalization (MCMarg) and Kullback-Leibler
(KL) divergence loss are used to fit the Gaussian components of the GMM and
learn the data distribution. Finally, image clustering is achieved through each
Gaussian component of GMM. Yet, the ""curse of dimensionality"" poses severe
challenges for most clustering algorithms. Compared with the classic
Expectation-Maximization (EM) Algorithm, experimental results show that MCMarg
and KL divergence can greatly alleviate the difficulty. Based on the
experimental results, we believe distribution learning can exploit the
potential of GMM in image clustering within high-dimensional space.",http://arxiv.org/pdf/2308.15667v1
2308.15651v1,cs.IR,Ensuring User-side Fairness in Dynamic Recommender Systems,2023-08-29 22:03:17+00:00,"User-side group fairness is crucial for modern recommender systems, as it
aims to alleviate performance disparity between groups of users defined by
sensitive attributes such as gender, race, or age. We find that the disparity
tends to persist or even increase over time. This calls for effective ways to
address user-side fairness in a dynamic environment, which has been
infrequently explored in the literature. However, fairness-constrained
re-ranking, a typical method to ensure user-side fairness (i.e., reducing
performance disparity), faces two fundamental challenges in the dynamic
setting: (1) non-differentiability of the ranking-based fairness constraint,
which hinders the end-to-end training paradigm, and (2) time-inefficiency,
which impedes quick adaptation to changes in user preferences. In this paper,
we propose FAir Dynamic rEcommender (FADE), an end-to-end framework with
fine-tuning strategy to dynamically alleviate performance disparity. To tackle
the above challenges, FADE uses a novel fairness loss designed to be
differentiable and lightweight to fine-tune model parameters to ensure both
user-side fairness and high-quality recommendations. Via extensive experiments
on the real-world dataset, we empirically demonstrate that FADE effectively and
efficiently reduces performance disparity, and furthermore, FADE improves
overall recommendation quality over time compared to not using any new data.",http://arxiv.org/pdf/2308.15651v1
2308.15642v1,cs.LG,Clustering Without an Eigengap,2023-08-29 21:27:21+00:00,"We study graph clustering in the Stochastic Block Model (SBM) in the presence
of both large clusters and small, unrecoverable clusters. Previous approaches
achieving exact recovery do not allow any small clusters of size $o(\sqrt{n})$,
or require a size gap between the smallest recovered cluster and the largest
non-recovered cluster. We provide an algorithm based on semidefinite
programming (SDP) which removes these requirements and provably recovers large
clusters regardless of the remaining cluster sizes. Mid-sized clusters pose
unique challenges to the analysis, since their proximity to the recovery
threshold makes them highly sensitive to small noise perturbations and
precludes a closed-form candidate solution. We develop novel techniques,
including a leave-one-out-style argument which controls the correlation between
SDP solutions and noise vectors even when the removal of one row of noise can
drastically change the SDP solution. We also develop improved eigenvalue
perturbation bounds of potential independent interest. Using our gap-free
clustering procedure, we obtain efficient algorithms for the problem of
clustering with a faulty oracle with superior query complexities, notably
achieving $o(n^2)$ sample complexity even in the presence of a large number of
small clusters. Our gap-free clustering procedure also leads to improved
algorithms for recursive clustering. Our results extend to certain
heterogeneous probability settings that are challenging for alternative
algorithms.",http://arxiv.org/pdf/2308.15642v1
2308.15640v1,cond-mat.mtrl-sci,Identifying Constitutive Parameters for Complex Hyperelastic Solids using Physics-Informed Neural Networks,2023-08-29 21:25:24+00:00,"Identifying constitutive parameters in engineering and biological materials,
particularly those with intricate geometries and mechanical behaviors, remains
a longstanding challenge. The recent advent of Physics-Informed Neural Networks
(PINNs) offers promising solutions, but current frameworks are often limited to
basic constitutive laws and encounter practical constraints when combined with
experimental data. In this paper, we introduce a new PINN-based framework
designed to identify material parameters for soft materials, specifically those
exhibiting complex constitutive behaviors, under large deformation in plane
stress conditions. Distinctively, our model emphasizes training PINNs with
multi-modal time-dependent experimental datasets consisting of full-field
deformation and loading history, ensuring algorithm robustness even amidst
noisy data. Our results reveal that our framework can accurately identify
constitutive parameters of the incompressible Arruda-Boyce model for samples
with intricate geometries, maintaining an error below 5%, even with an
experimental noise level of 5%. We believe our framework sets the stage for a
transformative approach in modulus identification for complex solids,
especially for those with geometrical and constitutive intricate.",http://arxiv.org/pdf/2308.15640v1
2308.15618v1,cs.CV,RACR-MIL: Weakly Supervised Skin Cancer Grading using Rank-Aware Contextual Reasoning on Whole Slide Images,2023-08-29 20:25:49+00:00,"Cutaneous squamous cell cancer (cSCC) is the second most common skin cancer
in the US. It is diagnosed by manual multi-class tumor grading using a tissue
whole slide image (WSI), which is subjective and suffers from inter-pathologist
variability. We propose an automated weakly-supervised grading approach for
cSCC WSIs that is trained using WSI-level grade and does not require
fine-grained tumor annotations. The proposed model, RACR-MIL, transforms each
WSI into a bag of tiled patches and leverages attention-based multiple-instance
learning to assign a WSI-level grade. We propose three key innovations to
address general as well as cSCC-specific challenges in tumor grading. First, we
leverage spatial and semantic proximity to define a WSI graph that encodes both
local and non-local dependencies between tumor regions and leverage graph
attention convolution to derive contextual patch features. Second, we introduce
a novel ordinal ranking constraint on the patch attention network to ensure
that higher-grade tumor regions are assigned higher attention. Third, we use
tumor depth as an auxiliary task to improve grade classification in a multitask
learning framework. RACR-MIL achieves 2-9% improvement in grade classification
over existing weakly-supervised approaches on a dataset of 718 cSCC tissue
images and localizes the tumor better. The model achieves 5-20% higher accuracy
in difficult-to-classify high-risk grade classes and is robust to class
imbalance.",http://arxiv.org/pdf/2308.15618v1
2308.15614v1,cs.LG,Everything Perturbed All at Once: Enabling Differentiable Graph Attacks,2023-08-29 20:14:42+00:00,"As powerful tools for representation learning on graphs, graph neural
networks (GNNs) have played an important role in applications including social
networks, recommendation systems, and online web services. However, GNNs have
been shown to be vulnerable to adversarial attacks, which can significantly
degrade their effectiveness. Recent state-of-the-art approaches in adversarial
attacks rely on gradient-based meta-learning to selectively perturb a single
edge with the highest attack score until they reach the budget constraint.
While effective in identifying vulnerable links, these methods are plagued by
high computational costs. By leveraging continuous relaxation and
parameterization of the graph structure, we propose a novel attack method
called Differentiable Graph Attack (DGA) to efficiently generate effective
attacks and meanwhile eliminate the need for costly retraining. Compared to the
state-of-the-art, DGA achieves nearly equivalent attack performance with 6
times less training time and 11 times smaller GPU memory footprint on different
benchmark datasets. Additionally, we provide extensive experimental analyses of
the transferability of the DGA among different graph models, as well as its
robustness against widely-used defense mechanisms.",http://arxiv.org/pdf/2308.15614v1
2308.15613v1,stat.CO,Mixed Variational Flows for Discrete Variables,2023-08-29 20:13:37+00:00,"Variational flows allow practitioners to learn complex continuous
distributions, but approximating discrete distributions remains a challenge.
Current methodologies typically embed the discrete target in a continuous space
- usually via continuous relaxation or dequantization - and then apply a
continuous flow. These approaches involve a surrogate target that may not
capture the original discrete target, might have biased or unstable gradients,
and can create a difficult optimization problem. In this work, we develop a
variational flow family for discrete distributions without any continuous
embedding. First, we develop a measure-preserving and discrete (MAD) invertible
map that leaves the discrete target invariant, and then create a mixed
variational flow (MAD Mix) based on that map. We also develop an extension to
MAD Mix that handles joint discrete and continuous models. Our experiments
suggest that MAD Mix produces more reliable approximations than
continuous-embedding flows while being significantly faster to train.",http://arxiv.org/pdf/2308.15613v1
2308.15605v1,cs.LG,Measurement Tampering Detection Benchmark,2023-08-29 19:54:37+00:00,"When training powerful AI systems to perform complex tasks, it may be
challenging to provide training signals which are robust to optimization. One
concern is measurement tampering, where the AI system manipulates multiple
measurements to create the illusion of good results instead of achieving the
desired outcome. In this work, we build four new text-based datasets to
evaluate measurement tampering detection techniques on large language models.
Concretely, given sets of text inputs and measurements aimed at determining if
some outcome occurred, as well as a base model able to accurately predict
measurements, the goal is to determine if examples where all measurements
indicate the outcome actually had the outcome occur, or if this was caused by
measurement tampering. We demonstrate techniques that outperform simple
baselines on most datasets, but don't achieve maximum performance. We believe
there is significant room for improvement for both techniques and datasets, and
we are excited for future work tackling measurement tampering.",http://arxiv.org/pdf/2308.15605v1
2308.15602v1,cs.DC,An Experimental Comparison of Partitioning Strategies for Distributed Graph Neural Network Training,2023-08-29 19:47:31+00:00,"Recently, graph neural networks (GNNs) have gained much attention as a
growing area of deep learning capable of learning on graph-structured data.
However, the computational and memory requirements for training GNNs on
large-scale graphs can exceed the capabilities of single machines or GPUs,
making distributed GNN training a promising direction for large-scale GNN
training. A prerequisite for distributed GNN training is to partition the input
graph into smaller parts that are distributed among multiple machines of a
compute cluster. Although graph partitioning has been extensively studied with
regard to graph analytics and graph databases, its effect on GNN training
performance is largely unexplored.
  In this paper, we study the effectiveness of graph partitioning for
distributed GNN training. Our study aims to understand how different factors
such as GNN parameters, mini-batch size, graph type, features size, and
scale-out factor influence the effectiveness of graph partitioning. We conduct
experiments with two different GNN systems using vertex and edge partitioning.
We found that graph partitioning is a crucial pre-processing step that can
heavily reduce the training time and memory footprint. Furthermore, our results
show that invested partitioning time can be amortized by reduced GNN training,
making it a relevant optimization.",http://arxiv.org/pdf/2308.15602v1
2308.15575v1,cs.LG,Prototype Fission: Closing Set for Robust Open-set Semi-supervised Learning,2023-08-29 19:04:42+00:00,"Semi-supervised Learning (SSL) has been proven vulnerable to
out-of-distribution (OOD) samples in realistic large-scale unsupervised
datasets due to over-confident pseudo-labeling OODs as in-distribution (ID). A
key underlying problem is class-wise latent space spreading from closed seen
space to open unseen space, and the bias is further magnified in SSL's
self-training loops. To close the ID distribution set so that OODs are better
rejected for safe SSL, we propose Prototype Fission(PF) to divide class-wise
latent spaces into compact sub-spaces by automatic fine-grained latent space
mining, driven by coarse-grained labels only. Specifically, we form multiple
unique learnable sub-class prototypes for each class, optimized towards both
diversity and consistency. The Diversity Modeling term encourages samples to be
clustered by one of the multiple sub-class prototypes, while the Consistency
Modeling term clusters all samples of the same class to a global prototype.
Instead of ""opening set"", i.e., modeling OOD distribution, Prototype Fission
""closes set"" and makes it hard for OOD samples to fit in sub-class latent
space. Therefore, PF is compatible with existing methods for further
performance gains. Extensive experiments validate the effectiveness of our
method in open-set SSL settings in terms of successfully forming sub-classes,
discriminating OODs from IDs and improving overall accuracy. Codes will be
released.",http://arxiv.org/pdf/2308.15575v1
2308.15564v1,eess.IV,Learning Sequential Information in Task-based fMRI for Synthetic Data Augmentation,2023-08-29 18:36:21+00:00,"Insufficiency of training data is a persistent issue in medical image
analysis, especially for task-based functional magnetic resonance images (fMRI)
with spatio-temporal imaging data acquired using specific cognitive tasks. In
this paper, we propose an approach for generating synthetic fMRI sequences that
can then be used to create augmented training datasets in downstream learning
tasks. To synthesize high-resolution task-specific fMRI, we adapt the
$\alpha$-GAN structure, leveraging advantages of both GAN and variational
autoencoder models, and propose different alternatives in aggregating temporal
information. The synthetic images are evaluated from multiple perspectives
including visualizations and an autism spectrum disorder (ASD) classification
task. The results show that the synthetic task-based fMRI can provide effective
data augmentation in learning the ASD classification task.",http://arxiv.org/pdf/2308.15564v1
2308.15559v1,cs.LG,Glocal Explanations of Expected Goal Models in Soccer,2023-08-29 18:29:56+00:00,"The expected goal models have gained popularity, but their interpretability
is often limited, especially when trained using black-box methods. Explainable
artificial intelligence tools have emerged to enhance model transparency and
extract descriptive knowledge for a single observation or for all observations.
However, explaining black-box models for a specific group of observations may
be more useful in some domains. This paper introduces the glocal explanations
(between local and global levels) of the expected goal models to enable
performance analysis at the team and player levels by proposing the use of
aggregated versions of the SHAP values and partial dependence profiles. This
allows knowledge to be extracted from the expected goal model for a player or
team rather than just a single shot. In addition, we conducted real-data
applications to illustrate the usefulness of aggregated SHAP and aggregated
profiles. The paper concludes with remarks on the potential of these
explanations for performance analysis in soccer analytics.",http://arxiv.org/pdf/2308.15559v1
2308.15553v1,cs.IR,Dimensionality Reduction Using pseudo-Boolean polynomials For Cluster Analysis,2023-08-29 18:19:36+00:00,"We introduce usage of a reduction property of penalty-based formulation of
pseudo-Boolean polynomials as a mechanism for invariant dimensionality
reduction in cluster analysis processes. In our experiments, we show that
multidimensional data, like 4-dimensional Iris Flower dataset can be reduced to
2-dimensional space while the 30-dimensional Wisconsin Diagnostic Breast Cancer
(WDBC) dataset can be reduced to 3-dimensional space, and by searching lines or
planes that lie between reduced samples we can extract clusters in a linear and
unbiased manner with competitive accuracies, reproducibility and clear
interpretation.",http://arxiv.org/pdf/2308.15553v1
2308.15552v1,cs.LG,Pure Exploration under Mediators' Feedback,2023-08-29 18:18:21+00:00,"Stochastic multi-armed bandits are a sequential-decision-making framework,
where, at each interaction step, the learner selects an arm and observes a
stochastic reward. Within the context of best-arm identification (BAI)
problems, the goal of the agent lies in finding the optimal arm, i.e., the one
with highest expected reward, as accurately and efficiently as possible.
Nevertheless, the sequential interaction protocol of classical BAI problems,
where the agent has complete control over the arm being pulled at each round,
does not effectively model several decision-making problems of interest (e.g.,
off-policy learning, partially controllable environments, and human feedback).
For this reason, in this work, we propose a novel strict generalization of the
classical BAI problem that we refer to as best-arm identification under
mediators' feedback (BAI-MF). More specifically, we consider the scenario in
which the learner has access to a set of mediators, each of which selects the
arms on the agent's behalf according to a stochastic and possibly unknown
policy. The mediator, then, communicates back to the agent the pulled arm
together with the observed reward. In this setting, the agent's goal lies in
sequentially choosing which mediator to query to identify with high probability
the optimal arm while minimizing the identification time, i.e., the sample
complexity. To this end, we first derive and analyze a statistical lower bound
on the sample complexity specific to our general mediator feedback scenario.
Then, we propose a sequential decision-making strategy for discovering the best
arm under the assumption that the mediators' policies are known to the learner.
As our theory verifies, this algorithm matches the lower bound both almost
surely and in expectation. Finally, we extend these results to cases where the
mediators' policies are unknown to the learner obtaining comparable results.",http://arxiv.org/pdf/2308.15552v1
2308.15479v1,cs.CV,3D Adversarial Augmentations for Robust Out-of-Domain Predictions,2023-08-29 17:58:55+00:00,"Since real-world training datasets cannot properly sample the long tail of
the underlying data distribution, corner cases and rare out-of-domain samples
can severely hinder the performance of state-of-the-art models. This problem
becomes even more severe for dense tasks, such as 3D semantic segmentation,
where points of non-standard objects can be confidently associated to the wrong
class. In this work, we focus on improving the generalization to out-of-domain
data. We achieve this by augmenting the training set with adversarial examples.
First, we learn a set of vectors that deform the objects in an adversarial
fashion. To prevent the adversarial examples from being too far from the
existing data distribution, we preserve their plausibility through a series of
constraints, ensuring sensor-awareness and shapes smoothness. Then, we perform
adversarial augmentation by applying the learned sample-independent vectors to
the available objects when training a model. We conduct extensive experiments
across a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D
object detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D
semantic segmentation. Despite training on a standard single dataset, our
approach substantially improves the robustness and generalization of both 3D
object detection and 3D semantic segmentation methods to out-of-domain data.",http://arxiv.org/pdf/2308.15479v1
2308.15478v1,cs.LG,An Adaptive Tangent Feature Perspective of Neural Networks,2023-08-29 17:57:20+00:00,"In order to better understand feature learning in neural networks, we propose
a framework for understanding linear models in tangent feature space where the
features are allowed to be transformed during training. We consider linear
transformations of features, resulting in a joint optimization over parameters
and transformations with a bilinear interpolation constraint. We show that this
optimization problem has an equivalent linearly constrained optimization with
structured regularization that encourages approximately low rank solutions.
Specializing to neural network structure, we gain insights into how the
features and thus the kernel function change, providing additional nuance to
the phenomenon of kernel alignment when the target function is poorly
represented using tangent features. In addition to verifying our theoretical
observations in real neural networks on a simple regression problem, we
empirically show that an adaptive feature implementation of tangent feature
classification has an order of magnitude lower sample complexity than the fixed
tangent feature model on MNIST and CIFAR-10.",http://arxiv.org/pdf/2308.15478v1
2308.15470v2,cs.LG,Policy composition in reinforcement learning via multi-objective policy optimization,2023-08-29 17:50:27+00:00,"We enable reinforcement learning agents to learn successful behavior policies
by utilizing relevant pre-existing teacher policies. The teacher policies are
introduced as objectives, in addition to the task objective, in a
multi-objective policy optimization setting. Using the Multi-Objective Maximum
a Posteriori Policy Optimization algorithm (Abdolmaleki et al. 2020), we show
that teacher policies can help speed up learning, particularly in the absence
of shaping rewards. In two domains with continuous observation and action
spaces, our agents successfully compose teacher policies in sequence and in
parallel, and are also able to further extend the policies of the teachers in
order to solve the task.
  Depending on the specified combination of task and teacher(s), teacher(s) may
naturally act to limit the final performance of an agent. The extent to which
agents are required to adhere to teacher policies are determined by
hyperparameters which determine both the effect of teachers on learning speed
and the eventual performance of the agent on the task. In the humanoid domain
(Tassa et al. 2018), we also equip agents with the ability to control the
selection of teachers. With this ability, agents are able to meaningfully
compose from the teacher policies to achieve a superior task reward on the walk
task than in cases without access to the teacher policies. We show the
resemblance of composed task policies with the corresponding teacher policies
through videos.",http://arxiv.org/pdf/2308.15470v2
2308.15466v1,cs.LG,Input margins can predict generalization too,2023-08-29 17:47:42+00:00,"Understanding generalization in deep neural networks is an active area of
research. A promising avenue of exploration has been that of margin
measurements: the shortest distance to the decision boundary for a given sample
or its representation internal to the network. While margins have been shown to
be correlated with the generalization ability of a model when measured at its
hidden representations (hidden margins), no such link between large margins and
generalization has been established for input margins. We show that while input
margins are not generally predictive of generalization, they can be if the
search space is appropriately constrained. We develop such a measure based on
input margins, which we refer to as `constrained margins'. The predictive power
of this new measure is demonstrated on the 'Predicting Generalization in Deep
Learning' (PGDL) dataset and contrasted with hidden representation margins. We
find that constrained margins achieve highly competitive scores and outperform
other margin measurements in general. This provides a novel insight on the
relationship between generalization and classification margins, and highlights
the importance of considering the data manifold for investigations of
generalization in DNNs.",http://arxiv.org/pdf/2308.15466v1
2308.15461v1,cs.CV,Canonical Factors for Hybrid Neural Fields,2023-08-29 17:38:33+00:00,"Factored feature volumes offer a simple way to build more compact, efficient,
and intepretable neural fields, but also introduce biases that are not
necessarily beneficial for real-world data. In this work, we (1) characterize
the undesirable biases that these architectures have for axis-aligned signals
-- they can lead to radiance field reconstruction differences of as high as 2
PSNR -- and (2) explore how learning a set of canonicalizing transformations
can improve representations by removing these biases. We prove in a
two-dimensional model problem that simultaneously learning these
transformations together with scene appearance succeeds with drastically
improved efficiency. We validate the resulting architectures, which we call
TILTED, using image, signed distance, and radiance field reconstruction tasks,
where we observe improvements across quality, robustness, compactness, and
runtime. Results demonstrate that TILTED can enable capabilities comparable to
baselines that are 2x larger, while highlighting weaknesses of neural field
evaluation procedures.",http://arxiv.org/pdf/2308.15461v1
2308.15434v1,cs.LG,Random feature approximation for general spectral methods,2023-08-29 16:56:03+00:00,"Random feature approximation is arguably one of the most popular techniques
to speed up kernel methods in large scale algorithms and provides a theoretical
approach to the analysis of deep neural networks. We analyze generalization
properties for a large class of spectral regularization methods combined with
random features, containing kernel methods with implicit regularization such as
gradient descent or explicit methods like Tikhonov regularization. For our
estimators we obtain optimal learning rates over regularity classes (even for
classes that are not included in the reproducing kernel Hilbert space), which
are defined through appropriate source conditions. This improves or completes
previous results obtained in related settings for specific kernel algorithms.",http://arxiv.org/pdf/2308.15434v1
2308.15410v1,astro-ph.SR,Probabilistic solar flare forecasting using historical magnetogram data,2023-08-29 16:10:20+00:00,"Solar flare forecasting research using machine learning (ML) has focused on
high resolution magnetogram data from the SDO/HMI era covering Solar Cycle 24
and the start of Solar Cycle 25, with some efforts looking back to SOHO/MDI for
data from Solar Cycle 23. In this paper, we consider over 4 solar cycles of
daily historical magnetogram data from multiple instruments. This is the first
attempt to take advantage of this historical data for ML-based flare
forecasting. We apply a convolutional neural network (CNN) to extract features
from full-disk magnetograms together with a logistic regression model to
incorporate scalar features based on magnetograms and flaring history. We use
an ensemble approach to generate calibrated probabilistic forecasts of M-class
or larger flares in the next 24 hours. Overall, we find that including
historical data improves forecasting skill and reliability. We show that single
frame magnetograms do not contain significantly more relevant information than
can be summarized in a small number of scalar features, and that flaring
history has greater predictive power than our CNN-extracted features. This
indicates the importance of including temporal information in flare forecasting
models.",http://arxiv.org/pdf/2308.15410v1
2308.15405v1,cs.LG,Robust Long-Tailed Learning via Label-Aware Bounded CVaR,2023-08-29 16:07:18+00:00,"Data in the real-world classification problems are always imbalanced or
long-tailed, wherein the majority classes have the most of the samples that
dominate the model training. In such setting, the naive model tends to have
poor performance on the minority classes. Previously, a variety of loss
modifications have been proposed to address the long-tailed leaning problem,
while these methods either treat the samples in the same class
indiscriminatingly or lack a theoretical guarantee. In this paper, we propose
two novel approaches based on CVaR (Conditional Value at Risk) to improve the
performance of long-tailed learning with a solid theoretical ground.
Specifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss
to overcome the pessimistic result of the original CVaR, and further design the
optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we
additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to
stabilize the optimization process, where we also offer the theoretical
support. Extensive experiments on real-world datasets with long-tailed label
distributions verify the superiority of our proposed methods.",http://arxiv.org/pdf/2308.15405v1
2308.15395v1,cs.LG,The CausalBench challenge: A machine learning contest for gene network inference from single-cell perturbation data,2023-08-29 15:54:15+00:00,"In drug discovery, mapping interactions between genes within cellular systems
is a crucial early step. This helps formulate hypotheses regarding molecular
mechanisms that could potentially be targeted by future medicines. The
CausalBench Challenge was an initiative to invite the machine learning
community to advance the state of the art in constructing gene-gene interaction
networks. These networks, derived from large-scale, real-world datasets of
single cells under various perturbations, are crucial for understanding the
causal mechanisms underlying disease biology. Using the framework provided by
the CausalBench benchmark, participants were tasked with enhancing the capacity
of the state of the art methods to leverage large-scale genetic perturbation
data. This report provides an analysis and summary of the methods submitted
during the challenge to give a partial image of the state of the art at the
time of the challenge. The winning solutions significantly improved performance
compared to previous baselines, establishing a new state of the art for this
critical task in biology and medicine.",http://arxiv.org/pdf/2308.15395v1
2308.15386v1,eess.IV,Shape-Margin Knowledge Augmented Network for Thyroid Nodule Segmentation and Diagnosis,2023-08-29 15:29:06+00:00,"Thyroid nodule segmentation is a crucial step in the diagnostic procedure of
physicians and computer-aided diagnosis systems. Mostly, current studies treat
segmentation and diagnosis as independent tasks without considering the
correlation between these tasks. The sequence steps of these independent tasks
in computer-aided diagnosis systems may lead to the accumulation of errors.
Therefore, it is worth combining them as a whole through exploring the
relationship between thyroid nodule segmentation and diagnosis. According to
the thyroid imaging reporting and data system (TI-RADS), the assessment of
shape and margin characteristics is the prerequisite for the discrimination of
benign and malignant thyroid nodules. These characteristics can be observed in
the thyroid nodule segmentation masks. Inspired by the diagnostic procedure of
TI-RADS, this paper proposes a shape-margin knowledge augmented network
(SkaNet) for simultaneously thyroid nodule segmentation and diagnosis. Due to
the similarity in visual features between segmentation and diagnosis, SkaNet
shares visual features in the feature extraction stage and then utilizes a
dual-branch architecture to perform thyroid nodule segmentation and diagnosis
tasks simultaneously. To enhance effective discriminative features, an
exponential mixture module is devised, which incorporates convolutional feature
maps and self-attention maps by exponential weighting. Then, SkaNet is jointly
optimized by a knowledge augmented multi-task loss function with a constraint
penalty term. It embeds shape and margin characteristics through numerical
computation and models the relationship between the thyroid nodule diagnosis
results and segmentation masks.",http://arxiv.org/pdf/2308.15386v1
2308.15370v1,stat.ML,Multi-Response Heteroscedastic Gaussian Process Models and Their Inference,2023-08-29 15:06:47+00:00,"Despite the widespread utilization of Gaussian process models for versatile
nonparametric modeling, they exhibit limitations in effectively capturing
abrupt changes in function smoothness and accommodating relationships with
heteroscedastic errors. Addressing these shortcomings, the heteroscedastic
Gaussian process (HeGP) regression seeks to introduce flexibility by
acknowledging the variability of residual variances across covariates in the
regression model. In this work, we extend the HeGP concept, expanding its scope
beyond regression tasks to encompass classification and state-space models. To
achieve this, we propose a novel framework where the Gaussian process is
coupled with a covariate-induced precision matrix process, adopting a mixture
formulation. This approach enables the modeling of heteroscedastic covariance
functions across covariates. To mitigate the computational challenges posed by
sampling, we employ variational inference to approximate the posterior and
facilitate posterior predictive modeling. Additionally, our training process
leverages an EM algorithm featuring closed-form M-step updates to efficiently
evaluate the heteroscedastic covariance function. A notable feature of our
model is its consistent performance on multivariate responses, accommodating
various types (continuous or categorical) seamlessly. Through a combination of
simulations and real-world applications in climatology, we illustrate the
model's prowess and advantages. By overcoming the limitations of traditional
Gaussian process models, our proposed framework offers a robust and versatile
tool for a wide array of applications.",http://arxiv.org/pdf/2308.15370v1
2308.15364v1,cs.LG,Heterogeneous Multi-Task Gaussian Cox Processes,2023-08-29 15:01:01+00:00,"This paper presents a novel extension of multi-task Gaussian Cox processes
for modeling multiple heterogeneous correlated tasks jointly, e.g.,
classification and regression, via multi-output Gaussian processes (MOGP). A
MOGP prior over the parameters of the dedicated likelihoods for classification,
regression and point process tasks can facilitate sharing of information
between heterogeneous tasks, while allowing for nonparametric parameter
estimation. To circumvent the non-conjugate Bayesian inference in the MOGP
modulated heterogeneous multi-task framework, we employ the data augmentation
technique and derive a mean-field approximation to realize closed-form
iterative updates for estimating model parameters. We demonstrate the
performance and inference on both 1D synthetic data as well as 2D urban data of
Vancouver.",http://arxiv.org/pdf/2308.15364v1
2308.15349v1,cs.LG,Lie-Poisson Neural Networks (LPNets): Data-Based Computing of Hamiltonian Systems with Symmetries,2023-08-29 14:45:23+00:00,"An accurate data-based prediction of the long-term evolution of Hamiltonian
systems requires a network that preserves the appropriate structure under each
time step. Every Hamiltonian system contains two essential ingredients: the
Poisson bracket and the Hamiltonian. Hamiltonian systems with symmetries, whose
paradigm examples are the Lie-Poisson systems, have been shown to describe a
broad category of physical phenomena, from satellite motion to underwater
vehicles, fluids, geophysical applications, complex fluids, and plasma physics.
The Poisson bracket in these systems comes from the symmetries, while the
Hamiltonian comes from the underlying physics. We view the symmetry of the
system as primary, hence the Lie-Poisson bracket is known exactly, whereas the
Hamiltonian is regarded as coming from physics and is considered not known, or
known approximately. Using this approach, we develop a network based on
transformations that exactly preserve the Poisson bracket and the special
functions of the Lie-Poisson systems (Casimirs) to machine precision. We
present two flavors of such systems: one, where the parameters of
transformations are computed from data using a dense neural network (LPNets),
and another, where the composition of transformations is used as building
blocks (G-LPNets). We also show how to adapt these methods to a larger class of
Poisson brackets. We apply the resulting methods to several examples, such as
rigid body (satellite) motion, underwater vehicles, a particle in a magnetic
field, and others. The methods developed in this paper are important for the
construction of accurate data-based methods for simulating the long-term
dynamics of physical systems.",http://arxiv.org/pdf/2308.15349v1
2308.15344v1,cs.LG,Imperceptible Adversarial Attack on Deep Neural Networks from Image Boundary,2023-08-29 14:41:05+00:00,"Although Deep Neural Networks (DNNs), such as the convolutional neural
networks (CNN) and Vision Transformers (ViTs), have been successfully applied
in the field of computer vision, they are demonstrated to be vulnerable to
well-sought Adversarial Examples (AEs) that can easily fool the DNNs. The
research in AEs has been active, and many adversarial attacks and explanations
have been proposed since they were discovered in 2014. The mystery of the AE's
existence is still an open question, and many studies suggest that DNN training
algorithms have blind spots. The salient objects usually do not overlap with
boundaries; hence, the boundaries are not the DNN model's attention.
Nevertheless, recent studies show that the boundaries can dominate the behavior
of the DNN models. Hence, this study aims to look at the AEs from a different
perspective and proposes an imperceptible adversarial attack that systemically
attacks the input image boundary for finding the AEs. The experimental results
have shown that the proposed boundary attacking method effectively attacks six
CNN models and the ViT using only 32% of the input image content (from the
boundaries) with an average success rate (SR) of 95.2% and an average peak
signal-to-noise ratio of 41.37 dB. Correlation analyses are conducted,
including the relation between the adversarial boundary's width and the SR and
how the adversarial boundary changes the DNN model's attention. This paper's
discoveries can potentially advance the understanding of AEs and provide a
different perspective on how AEs can be constructed.",http://arxiv.org/pdf/2308.15344v1
2308.15327v1,cs.RO,Enhancing Robot Learning through Learned Human-Attention Feature Maps,2023-08-29 14:23:44+00:00,"Robust and efficient learning remains a challenging problem in robotics, in
particular with complex visual inputs. Inspired by human attention mechanism,
with which we quickly process complex visual scenes and react to changes in the
environment, we think that embedding auxiliary information about focus point
into robot learning would enhance efficiency and robustness of the learning
process. In this paper, we propose a novel approach to model and emulate the
human attention with an approximate prediction model. We then leverage this
output and feed it as a structured auxiliary feature map into downstream
learning tasks. We validate this idea by learning a prediction model from
human-gaze recordings of manual driving in the real world. We test our approach
on two learning tasks - object detection and imitation learning. Our
experiments demonstrate that the inclusion of predicted human attention leads
to improved robustness of the trained models to out-of-distribution samples and
faster learning in low-data regime settings. Our work highlights the potential
of incorporating structured auxiliary information in representation learning
for robotics and opens up new avenues for research in this direction. All code
and data are available online.",http://arxiv.org/pdf/2308.15327v1
2308.15323v1,cs.CV,Occlusion-Aware Deep Convolutional Neural Network via Homogeneous Tanh-transforms for Face Parsing,2023-08-29 14:20:13+00:00,"Face parsing infers a pixel-wise label map for each semantic facial
component. Previous methods generally work well for uncovered faces, however
overlook the facial occlusion and ignore some contextual area outside a single
face, especially when facial occlusion has become a common situation during the
COVID-19 epidemic. Inspired by the illumination theory of image, we propose a
novel homogeneous tanh-transforms for image preprocessing, which made up of
four tanh-transforms, that fuse the central vision and the peripheral vision
together. Our proposed method addresses the dilemma of face parsing under
occlusion and compresses more information of surrounding context. Based on
homogeneous tanh-transforms, we propose an occlusion-aware convolutional neural
network for occluded face parsing. It combines the information both in
Tanh-polar space and Tanh-Cartesian space, capable of enhancing receptive
fields. Furthermore, we introduce an occlusion-aware loss to focus on the
boundaries of occluded regions. The network is simple and flexible, and can be
trained end-to-end. To facilitate future research of occluded face parsing, we
also contribute a new cleaned face parsing dataset, which is manually purified
from several academic or industrial datasets, including CelebAMask-HQ,
Short-video Face Parsing as well as Helen dataset and will make it public.
Experiments demonstrate that our method surpasses state-of-art methods of face
parsing under occlusion.",http://arxiv.org/pdf/2308.15323v1
2308.15507v1,eess.IV,unORANIC: Unsupervised Orthogonalization of Anatomy and Image-Characteristic Features,2023-08-29 13:37:13+00:00,"We introduce unORANIC, an unsupervised approach that uses an adapted loss
function to drive the orthogonalization of anatomy and image-characteristic
features. The method is versatile for diverse modalities and tasks, as it does
not require domain knowledge, paired data samples, or labels. During test time
unORANIC is applied to potentially corrupted images, orthogonalizing their
anatomy and characteristic components, to subsequently reconstruct
corruption-free images, showing their domain-invariant anatomy only. This
feature orthogonalization further improves generalization and robustness
against corruptions. We confirm this qualitatively and quantitatively on 5
distinct datasets by assessing unORANIC's classification accuracy, corruption
detection and revision capabilities. Our approach shows promise for enhancing
the generalizability and robustness of practical applications in medical image
analysis. The source code is available at
https://github.com/sdoerrich97/unORANIC.",http://arxiv.org/pdf/2308.15507v1
2308.15291v1,eess.SP,"Towards quantitative precision for ECG analysis: Leveraging state space models, self-supervision and patient metadata",2023-08-29 13:25:26+00:00,"Deep learning has emerged as the preferred modeling approach for automatic
ECG analysis. In this study, we investigate three elements aimed at improving
the quantitative accuracy of such systems. These components consistently
enhance performance beyond the existing state-of-the-art, which is
predominantly based on convolutional models. Firstly, we explore more
expressive architectures by exploiting structured state space models (SSMs).
These models have shown promise in capturing long-term dependencies in time
series data. By incorporating SSMs into our approach, we not only achieve
better performance, but also gain insights into long-standing questions in the
field. Specifically, for standard diagnostic tasks, we find no advantage in
using higher sampling rates such as 500Hz compared to 100Hz. Similarly,
extending the input size of the model beyond 3 seconds does not lead to
significant improvements. Secondly, we demonstrate that self-supervised
learning using contrastive predictive coding can further improve the
performance of SSMs. By leveraging self-supervision, we enable the model to
learn more robust and representative features, leading to improved analysis
accuracy. Lastly, we depart from synthetic benchmarking scenarios and
incorporate basic demographic metadata alongside the ECG signal as input. This
inclusion of patient metadata departs from the conventional practice of relying
solely on the signal itself. Remarkably, this addition consistently yields
positive effects on predictive performance. We firmly believe that all three
components should be considered when developing next-generation ECG analysis
algorithms.",http://arxiv.org/pdf/2308.15291v1
2308.15283v1,cs.LG,Structural Node Embeddings with Homomorphism Counts,2023-08-29 13:14:53+00:00,"Graph homomorphism counts, first explored by Lov\'asz in 1967, have recently
garnered interest as a powerful tool in graph-based machine learning. Grohe
(PODS 2020) proposed the theoretical foundations for using homomorphism counts
in machine learning on graph level as well as node level tasks. By their very
nature, these capture local structural information, which enables the creation
of robust structural embeddings. While a first approach for graph level tasks
has been made by Nguyen and Maehara (ICML 2020), we experimentally show the
effectiveness of homomorphism count based node embeddings. Enriched with node
labels, node weights, and edge weights, these offer an interpretable
representation of graph data, allowing for enhanced explainability of machine
learning models.
  We propose a theoretical framework for isomorphism-invariant homomorphism
count based embeddings which lend themselves to a wide variety of downstream
tasks. Our approach capitalises on the efficient computability of graph
homomorphism counts for bounded treewidth graph classes, rendering it a
practical solution for real-world applications. We demonstrate their
expressivity through experiments on benchmark datasets. Although our results do
not match the accuracy of state-of-the-art neural architectures, they are
comparable to other advanced graph learning models. Remarkably, our approach
demarcates itself by ensuring explainability for each individual feature. By
integrating interpretable machine learning algorithms like SVMs or Random
Forests, we establish a seamless, end-to-end explainable pipeline. Our study
contributes to the advancement of graph-based techniques that offer both
performance and interpretability.",http://arxiv.org/pdf/2308.15283v1
2308.15250v1,cs.LG,The Relative Gaussian Mechanism and its Application to Private Gradient Descent,2023-08-29 12:16:57+00:00,"The Gaussian Mechanism (GM), which consists in adding Gaussian noise to a
vector-valued query before releasing it, is a standard privacy protection
mechanism. In particular, given that the query respects some L2 sensitivity
property (the L2 distance between outputs on any two neighboring inputs is
bounded), GM guarantees R\'enyi Differential Privacy (RDP). Unfortunately,
precisely bounding the L2 sensitivity can be hard, thus leading to loose
privacy bounds. In this work, we consider a Relative L2 sensitivity assumption,
in which the bound on the distance between two query outputs may also depend on
their norm. Leveraging this assumption, we introduce the Relative Gaussian
Mechanism (RGM), in which the variance of the noise depends on the norm of the
output. We prove tight bounds on the RDP parameters under relative L2
sensitivity, and characterize the privacy loss incurred by using
output-dependent noise. In particular, we show that RGM naturally adapts to a
latent variable that would control the norm of the output. Finally, we
instantiate our framework to show tight guarantees for Private Gradient
Descent, a problem that naturally fits our relative L2 sensitivity assumption.",http://arxiv.org/pdf/2308.15250v1
2308.15243v1,cs.CY,Reliability Gaps Between Groups in COMPAS Dataset,2023-08-29 12:09:22+00:00,"This paper investigates the inter-rater reliability of risk assessment
instruments (RAIs). The main question is whether different, socially salient
groups are affected differently by a lack of inter-rater reliability of RAIs,
that is, whether mistakes with respect to different groups affects them
differently. The question is investigated with a simulation study of the COMPAS
dataset. A controlled degree of noise is injected into the input data of a
predictive model; the noise can be interpreted as a synthetic rater that makes
mistakes. The main finding is that there are systematic differences in output
reliability between groups in the COMPAS dataset. The sign of the difference
depends on the kind of inter-rater statistic that is used (Cohen's Kappa,
Byrt's PABAK, ICC), and in particular whether or not a correction of
predictions prevalences of the groups is used.",http://arxiv.org/pdf/2308.15243v1
2308.15237v1,cs.CR,Assessing Cyclostationary Malware Detection via Feature Selection and Classification,2023-08-29 11:52:31+00:00,"Cyclostationarity involves periodic statistical variations in signals and
processes, commonly used in signal analysis and network security. In the
context of attacks, cyclostationarity helps detect malicious behaviors within
network traffic, such as traffic patterns in Distributed Denial of Service
(DDoS) attacks or hidden communication channels in malware. This approach
enhances security by identifying abnormal patterns and informing Network
Intrusion Detection Systems (NIDSs) to recognize potential attacks, enhancing
protection against both known and novel threats. This research focuses on
identifying cyclostationary malware behavior and its detection. The main goal
is to pinpoint essential cyclostationary features used in NIDSs. These features
are extracted using algorithms such as Boruta and Principal Component Analysis
(PCA), and then categorized to find the most significant cyclostationary
patterns. The aim of this article is to reveal periodically changing malware
behaviors through cyclostationarity. The study highlights the importance of
spotting cyclostationary malware in NIDSs by using established datasets like
KDD99, NSL-KDD, and the UGRansome dataset. The UGRansome dataset is designed
for anomaly detection research and includes both normal and abnormal network
threat categories of zero-day attacks. A comparison is made using the Random
Forest (RF) and Support Vector Machine (SVM) algorithms, while also evaluating
the effectiveness of Boruta and PCA. The findings show that PCA is more
promising than using Boruta alone for extracting cyclostationary network
feature patterns. Additionally, the analysis identifies the internet protocol
as the most noticeable cyclostationary feature pattern used by malware.
Notably, the UGRansome dataset outperforms the KDD99 and NSL-KDD, achieving 99%
accuracy in signature malware detection using the RF algorithm and 98% with the
SVM.",http://arxiv.org/pdf/2308.15237v1
2308.16146v1,math.NA,Growth factors of orthogonal matrices and local behavior of Gaussian elimination with partial and complete pivoting,2023-08-30 17:01:17+00:00,"Gaussian elimination (GE) is the most used dense linear solver. Error
analysis of GE with selected pivoting strategies on well-conditioned systems
can focus on studying the behavior of growth factors. Although exponential
growth is possible with GE with partial pivoting (GEPP), growth tends to stay
much smaller in practice. Support for this behavior was provided last year by
Huang and Tikhomirov's average-case analysis of GEPP, which showed GEPP growth
factors stay at most polynomial with very high probability when using small
Gaussian perturbations. GE with complete pivoting (GECP) has also seen a lot of
recent interest, with recent improvements to lower bounds on worst-case GECP
growth provided by Edelman and Urschel earlier this year. We are interested in
studying how GEPP and GECP behave on the same linear systems as well as
studying large growth on particular subclasses of matrices, including
orthogonal matrices. We will also study systems when GECP leads to larger
growth than GEPP, which will lead to new empirical lower bounds on how much
worse GECP can behave compared to GEPP in terms of growth. We also present an
empirical study on a family of exponential GEPP growth matrices whose
polynomial behavior in small neighborhoods limits to the initial GECP growth
factor.",http://arxiv.org/pdf/2308.16146v1
2308.16124v1,physics.plasm-ph,Towards Robust Solvers for Nuclear Fusion Simulations Using JOREK: A Numerical Analysis Perspective,2023-08-30 16:21:43+00:00,"One of the most well-established codes for modeling non-linear
Magnetohydrodynamics (MHD) for tokamak reactors is JOREK, which solves these
equations with a B\'ezier surface based finite element method. This code
produces a highly sparse but also very large linear system. The main solver
behind the code uses the Generalized Minimum Residual Method (GMRES) with a
physics-based preconditioner, but even with the preconditioner there are issues
with memory and computation costs and the solver does not always converge well.
This work contains the first thorough study of the mathematical properties of
the underlying linear system. It enables us to diagnose and pinpoint the cause
of hampered convergence. In particular, analyzing the spectral properties of
the matrix and the preconditioned system with numerical linear algebra
techniques, will open the door to research and investigate more performant
solver strategies, such as projection methods.",http://arxiv.org/pdf/2308.16124v1
2308.16097v1,math.OC,Quasioptimal alternating projections and their use in low-rank approximation of matrices and tensors,2023-08-30 15:45:42+00:00,"We study the convergence of specific inexact alternating projections for two
non-convex sets in a Euclidean space. The $\sigma$-quasioptimal metric
projection ($\sigma \geq 1$) of a point $x$ onto a set $A$ consists of points
in $A$ the distance to which is at most $\sigma$ times larger than the minimal
distance $\mathrm{dist}(x,A)$. We prove that quasioptimal alternating
projections, when one or both projections are quasioptimal, converge locally
and linearly under the usual regularity assumptions on the two sets and their
intersection. The theory is motivated by the successful application of
alternating projections to low-rank matrix and tensor approximation. We focus
on two problems -- nonnegative low-rank approximation and low-rank
approximation in the maximum norm -- and develop fast alternating-projection
algorithms for matrices and tensor trains based on cross approximation and
acceleration techniques. The numerical experiments confirm that the proposed
methods are efficient and suggest that they can be used to regularise various
low-rank computational routines.",http://arxiv.org/pdf/2308.16097v1
2308.16081v1,math.NA,Abstract Fractional Cauchy Problem: Existence of Propagators and Inhomogeneous Solution Representation,2023-08-30 15:14:19+00:00,"We consider a Cauchy problem for the inhomogeneous differential equation
given in terms of an unbounded linear operator $A$ and the Caputo fractional
derivative of order $\alpha \in (0, 2)$ in time. The previously known
representation of the mild solution to such a problem does not have a
conventional variation-of-constants like form, with the propagator derived from
the associated homogeneous problem. Instead, it relies on the existence of two
propagators with different analytical properties. This fact limits theoretical
and especially numerical applicability of the existing solution representation.
Here, we propose an alternative representation of the mild solution to the
given problem that consolidates the solution formulas for sub-parabolic,
parabolic and sub-hyperbolic equations with a positive sectorial operator $A$
and non-zero initial data. The new representation is solely based on the
propagator of the homogeneous problem and, therefore, can be considered as a
more natural fractional extension of the solution to the classical parabolic
Cauchy problem. By exploiting a trade-off between the regularity assumptions on
the initial data in terms of the fractional powers of $A$ and the regularity
assumptions on the right-hand side in time, we show that the proposed solution
formula is strongly convergent for $t \geq 0$ under considerably weaker
assumptions compared to the standard results from the literature. Crucially,
the achieved relaxation of space regularity assumptions ensures that the new
solution representation is practically feasible for any $\alpha \in (0, 2)$ and
is amenable to the numerical evaluation using uniformly accurate
quadrature-based algorithms.",http://arxiv.org/pdf/2308.16081v1
2308.16012v1,math.NA,Geometric integration on symmetric spaces,2023-08-30 13:13:14+00:00,"We consider geometric numerical integration algorithms for differential
equations evolving on symmetric spaces. The integrators are constructed from
canonical operations on the symmetric space, its Lie triple system (LTS), and
the exponential from the LTS to the symmetric space. Examples of symmetric
spaces are n-spheres and Grassmann manifolds, the space of positive definite
symmetric matrices, Lie groups with a symmetric product, and elliptic and
hyperbolic spaces with constant sectional curvatures. We illustrate the
abstract algorithm with concrete examples. In particular for the n-sphere and
the n-dimensional hyperbolic space the resulting algorithms are very simple and
cost only O(n) operations per step.",http://arxiv.org/pdf/2308.16012v1
2308.15741v1,math.NA,Stability and regularization for ill-posed Cauchy problem of a stochastic parabolic differential equation,2023-08-30 03:39:14+00:00,"In this paper, we investigate an ill-posed Cauchy problem involving a
stochastic parabolic equation. We first establish a Carleman estimate for this
equation. Leveraging this estimate, we are able to derive the conditional
stability and convergence rate of the Tikhonov regularization method for the
aforementioned ill-posed Cauchy problem. To complement our theoretical
analysis, we employ kernel-based learning theory to implement the completed
Tikhonov regularization method for several numerical examples.",http://arxiv.org/pdf/2308.15741v1
2308.15729v1,cs.CG,Computing Geodesic Paths Encoding a Curvature Prior,2023-08-30 03:11:43+00:00,"In this paper, we introduce an efficient method for computing curves
minimizing a variant of the Euler-Mumford elastica energy, with fixed endpoints
and tangents at these endpoints, where the bending energy is enhanced with a
user defined and data-driven scalar-valued term referred to as the curvature
prior. In order to guarantee that the globally optimal curve is extracted, the
proposed method involves the numerical computation of the viscosity solution to
a specific static Hamilton-Jacobi-Bellman (HJB) partial differential equation
(PDE). For that purpose, we derive the explicit Hamiltonian associated to this
variant model equipped with a curvature prior, discretize the resulting HJB PDE
using an adaptive finite difference scheme, and solve it in a single pass using
a generalized Fast-Marching method. In addition, we also present a practical
method for estimating the curvature prior values from image data, designed for
the task of accurately tracking curvilinear structure centerlines. Numerical
experiments on synthetic and real image data illustrate the advantages of the
considered variant of the elastica model with a prior curvature enhancement in
complex scenarios where challenging geometric structures appear.",http://arxiv.org/pdf/2308.15729v1
2308.15715v1,math.NA,Dynamic properties of double porosity/permeability model,2023-08-30 02:32:29+00:00,"Understanding fluid movement in multi-pored materials is vital for energy
security and physiology. For instance, shale (a geological material) and bone
(a biological material) exhibit multiple pore networks. Double
porosity/permeability models provide a mechanics-based approach to describe
hydrodynamics in aforesaid porous materials. However, current theoretical
results primarily address state-state response, and their counterparts in the
transient regime are still wanting. The primary aim of this paper is to fill
this knowledge gap. We present three principal properties -- with rigorous
mathematical arguments -- that the solutions under the double
porosity/permeability model satisfy in the transient regime: backward-in-time
uniqueness, reciprocity, and a variational principle. We employ the ``energy
method"" -- by exploiting the physical total kinetic energy of the flowing fluid
-- to establish the first property and Cauchy-Riemann convolutions to prove the
next two. The results reported in this paper -- that qualitatively describe the
dynamics of fluid flow in double-pored media -- have (a) theoretical
significance, (b) practical applications, and (c) considerable pedagogical
value. In particular, these results will benefit practitioners and
computational scientists in checking the accuracy of numerical simulators. The
backward-in-time uniqueness lays a firm theoretical foundation for pursuing
inverse problems in which one predicts the prescribed initial conditions based
on data available about the solution at a later instance.",http://arxiv.org/pdf/2308.15715v1
2308.15683v1,physics.comp-ph,A spectrum adaptive kernel polynomial method,2023-08-30 00:52:33+00:00,"The kernel polynomial method (KPM) is a powerful numerical method for
approximating spectral densities. Typical implementations of the KPM require an
a prior estimate for an interval containing the support of the target spectral
density, and while such estimates can be obtained by classical techniques, this
incurs addition computational costs. We propose an spectrum adaptive KPM based
on the Lanczos algorithm without reorthogonalization which allows the selection
of KPM parameters to be deferred to after the expensive computation is
finished. Theoretical results from numerical analysis are given to justify the
suitability of the Lanczos algorithm for our approach, even in finite precision
arithmetic. While conceptually simple, the paradigm of decoupling computation
from approximation has a number of practical and pedagogical benefits which we
highlight with numerical examples.",http://arxiv.org/pdf/2308.15683v1
2308.15666v1,math.NA,Convergence of non-linear diagonal frame filtering for regularizing inverse problems,2023-08-29 23:32:26+00:00,"Inverse problems are core issues in several scientific areas, including
signal processing and medical imaging. As inverse problems typically suffer
from instability with respect to data perturbations, a variety of
regularization techniques have been proposed. In particular, the use of
filtered diagonal frame decompositions has proven to be effective and
computationally efficient. However, the existing convergence analysis applies
only to linear filters and a few non-linear filters such as soft thresholding.
In this paper, we analyze the filtered diagonal frame decomposition with
general non-linear filters. In particular, our results generalize SVD-based
spectral filtering from linear to non-linear filters as a special case. We
present three strategies to demonstrate convergence. The first two strategies
relate non-linear diagonal frame filtering to variational regularization and
plug-and-play regularization, respectively. The third strategy allows us to
relax the assumptions involved and still obtain a full convergence analysis.",http://arxiv.org/pdf/2308.15666v1
2308.15621v1,math.NA,A hybridizable discontinuous Galerkin method for the coupled Navier-Stokes/Biot problem,2023-08-29 20:34:40+00:00,"In this paper we present a hybridizable discontinuous Galerkin method for the
time-dependent Navier-Stokes equations coupled to the quasi-static
poroelasticity equations via interface conditions. We determine a bound on the
data that guarantees stability and well-posedness of the fully discrete problem
and prove a priori error estimates. A numerical example confirms our analysis.",http://arxiv.org/pdf/2308.15621v1
2308.15450v1,math.OC,Gauss-Newton oriented greedy algorithms for the reconstruction of operators in nonlinear dynamics,2023-08-29 17:20:59+00:00,"This paper is devoted to the development and convergence analysis of greedy
reconstruction algorithms based on the strategy presented in [Y. Maday and J.
Salomon, Joint Proceedings of the 48th IEEE Conference on Decision and Control
and the 28th Chinese Control Conference, 2009, pp. 375--379]. These procedures
allow the design of a sequence of control functions that ease the
identification of unknown operators in nonlinear dynamical systems. The
original strategy of greedy reconstruction algorithms is based on an
offline/online decomposition of the reconstruction process and an ansatz for
the unknown operator obtained by an a priori chosen set of linearly independent
matrices. In the previous work [S. Buchwald, G. Ciaramella and J. Salomon, SIAM
J. Control Optim., 59(6), pp. 4511-4537], convergence results were obtained in
the case of linear identification problems. We tackle here the more general
case of nonlinear systems. More precisely, we introduce a new greedy algorithm
based on the linearized system. Then, we show that the controls obtained with
this new algorithm lead to the local convergence of the classical Gauss-Newton
method applied to the online nonlinear identification problem. We then extend
this result to the controls obtained on nonlinear systems where a local
convergence result is also proved. The main convergence results are obtained
for the reconstruction of drift operators in dynamical systems with linear and
bilinear control structures.",http://arxiv.org/pdf/2308.15450v1
2308.15409v2,math.NA,An Incremental SVD Method for Non-Fickian Flows in Porous Media: Addressing Storage and Computational Challenges,2023-08-29 16:09:48+00:00,"It is well known that the numerical solution of the Non-Fickian flows at the
current stage depends on all previous time instances. Consequently, the storage
requirement increases linearly, while the computational complexity grows
quadratically with the number of time steps. This presents a significant
challenge for numerical simulations, and to the best of our knowledge, it
remains an unresolved issue. In this paper, we make the assumption that the
solution data exhibits approximate low rank. Here, we present a memory-free
algorithm, based on the incremental SVD technique, that exhibits only linear
growth in computational complexity as the number of time steps increases. We
prove that the error between the solutions generated by the conventional
algorithm and our innovative approach lies within the scope of machine error.
Numerical experiments are showcased to affirm the accuracy and efficiency gains
in terms of both memory usage and computational expenses.",http://arxiv.org/pdf/2308.15409v2
2308.15400v1,math.NA,A coupled high-accuracy phase-field fluid-structure interaction framework for Stokes fluid-filled fracture surrounded by an elastic medium,2023-08-29 15:57:38+00:00,"In this work, we couple a high-accuracy phase-field fracture reconstruction
approach iteratively to fluid-structure interaction. The key motivation is to
utilize phase-field modelling to compute the fracture path. A mesh
reconstruction allows a switch from interface-capturing to interface-tracking
in which the coupling conditions can be realized in a highly accurate fashion.
Consequently, inside the fracture, a Stokes flow can be modelled that is
coupled to the surrounding elastic medium. A fully coupled approach is obtained
by iterating between the phase-field and the fluid-structure interaction model.
The resulting algorithm is demonstrated for several numerical examples of
quasi-static brittle fractures. We consider both stationary and
quasi-stationary problems. In the latter, the dynamics arise through an
incrementally-increasing given pressure.",http://arxiv.org/pdf/2308.15400v1
2308.15375v1,math.NA,A Reduced-Order Model for Nonlinear Radiative Transfer Problems Based on Moment Equations and POD-Petrov-Galerkin Projection of the Normalized Boltzmann Transport Equation,2023-08-29 15:13:34+00:00,"A data-driven projection-based reduced-order model (ROM) for nonlinear
thermal radiative transfer (TRT) problems is presented. The TRT ROM is
formulated by (i) a hierarchy of low-order quasidiffusion (aka variable
Eddington factor) equations for moments of the radiation intensity and (ii) the
normalized Boltzmann transport equation (BTE). The multilevel system of moment
equations is derived by projection of the BTE onto a sequence of subspaces
which represent elements of the phase space of the problem. Exact closure for
the moment equations is provided by the Eddington tensor. A Petrov-Galerkin
(PG) projection of the normalized BTE is formulated using a proper orthogonal
decomposition (POD) basis representing the normalized radiation intensity over
the whole phase space and time. The Eddington tensor linearly depends on the
solution of the normalized BTE. By linear superposition of the POD basis
functions, a low-rank expansion of the Eddington tensor is constructed with
coefficients defined by the PG projected normalized BTE. The material energy
balance (MEB) equation is coupled with the effective grey low-order equations
which exist on the same dimensional scale as the MEB equation. The resulting
TRT ROM is structure and asymptotic preserving. A detailed analysis of the ROM
is performed on the classical Fleck-Cummings (F-C) TRT multigroup test problem
in 2D geometry. Numerical results are presented to demonstrate the ROM's
effectiveness in the simulation of radiation wave phenomena. The ROM is shown
to produce solutions with sufficiently high accuracy while using low-rank
approximation of the normalized BTE solution. Essential physical
characteristics of supersonic radiation wave are preserved in the ROM
solutions.",http://arxiv.org/pdf/2308.15375v1
2308.15336v1,math.OC,"Second-order methods for quartically-regularised cubic polynomials, with applications to high-order tensor methods",2023-08-29 14:30:36+00:00,"There has been growing interest in high-order tensor methods for nonconvex
optimization, with adaptive regularization, as they possess better/optimal
worst-case evaluation complexity globally and faster convergence
asymptotically. These algorithms crucially rely on repeatedly minimizing
nonconvex multivariate Taylor-based polynomial sub-problems, at least locally.
Finding efficient techniques for the solution of these sub-problems, beyond the
second-order case, has been an open question. This paper proposes a
second-order method, Quadratic Quartic Regularisation (QQR), for efficiently
minimizing nonconvex quartically-regularized cubic polynomials, such as the
AR$p$ sub-problem [3] with $p=3$. Inspired by [35], QQR approximates the
third-order tensor term by a linear combination of quadratic and quartic terms,
yielding (possibly nonconvex) local models that are solvable to global
optimality. In order to achieve accuracy $\epsilon$ in the first-order
criticality of the sub-problem, we show that the error in the QQR method
decreases either linearly or by at least $\mathcal{O}(\epsilon^{4/3})$ for
locally convex iterations, while in the sufficiently nonconvex case, by at
least $\mathcal{O}(\epsilon)$; thus improving, on these types of iterations,
the general cubic-regularization bound. Preliminary numerical experiments
indicate that two QQR variants perform competitively with state-of-the-art
approaches such as ARC (also known as AR$p$ with $p=2$), achieving either a
lower objective value or iteration counts.",http://arxiv.org/pdf/2308.15336v1
2308.15331v1,math.NA,"High-order quasi-Helmholtz Projectors: Definition, Analyses, Algorithms",2023-08-29 14:27:45+00:00,"The accuracy of the electric field integral equation (EFIE) can be
substantially improved using high-order discretizations. However, this equation
suffers from ill-conditioning and deleterious numerical effects in the
low-frequency regime, often jeopardizing its solution. This can be fixed using
quasi-Helmholtz decompositions, in which the source and testing elements are
separated into their solenoidal and non-solenoidal contributions, then rescaled
in order to avoid both the low-frequency conditioning breakdown and the loss of
numerical accuracy. However, standard quasi-Helmholtz decompositions require
handling discretized differential operators that often worsen the
mesh-refinement ill-conditioning and require the finding of the topological
cycles of the geometry, which can be expensive when modeling complex
scatterers, especially in high-order. This paper solves these drawbacks by
presenting the first extension of the quasi-Helmholtz projectors to high-order
discretizations and their application to the stabilization of the EFIE when
discretized with high-order basis functions. Our strategy will not require the
identification of the cycles and will provide constant condition numbers for
decreasing frequencies. Theoretical considerations will be accompanied by
numerical results showing the effectiveness of our method in complex scenarios.",http://arxiv.org/pdf/2308.15331v1
2308.15325v1,math.NA,Adaptivity in Local Kernel Based Methods for Approximating the Action of Linear Operators,2023-08-29 14:21:42+00:00,"Building on the successes of local kernel methods for approximating the
solutions to partial differential equations (PDE) and the evaluation of
definite integrals (quadrature/cubature), a local estimate of the error in such
approximations is developed. This estimate is useful for determining locations
in the solution domain where increased node density (equivalently, reduction in
the spacing between nodes) can decrease the error in the solution. An adaptive
procedure for adding nodes to the domain for both the approximation of
derivatives and the approximate evaluation of definite integrals is described.
This method efficiently computes the error estimate at a set of prescribed
points and adds new nodes for approximation where the error is too large.
Computational experiments demonstrate close agreement between the error
estimate and actual absolute error in the approximation. Such methods are
necessary or desirable when approximating solutions to PDE (or in the case of
quadrature/cubature), where the initial data and subsequent solution (or
integrand) exhibit localized features that require significant refinement to
resolve and where uniform increases in the density of nodes across the entire
computational domain is not possible or too burdensome.",http://arxiv.org/pdf/2308.15325v1
2308.15314v1,math.NA,Linearly convergent nonoverlapping domain decomposition methods for quasilinear parabolic equations,2023-08-29 14:01:08+00:00,"We prove linear convergence for a new family of modified Dirichlet--Neumann
methods applied to quasilinear parabolic equations, as well as the convergence
of the Robin--Robin method. Such nonoverlapping domain decomposition methods
are commonly employed for the parallelization of partial differential equation
solvers. Convergence has been extensively studied for elliptic equations, but
in the case of parabolic equations there are hardly any convergence results
that are not relying on strong regularity assumptions. Hence, we construct a
new framework for analyzing domain decomposition methods applied to quasilinear
parabolic problems, based on fractional time derivatives and time-dependent
Steklov--Poincar\'e operators. The convergence analysis is conducted without
assuming restrictive regularity assumptions on the solutions or the numerical
iterates. We also prove that these continuous convergence results extend to the
discrete case obtained when combining domain decompositions with space-time
finite elements.",http://arxiv.org/pdf/2308.15314v1
2308.15307v1,math.NA,Compositional maps for registration in complex geometries,2023-08-29 13:47:06+00:00,"We develop and analyze a parametric registration procedure for manifolds
associated with the solutions to parametric partial differential equations in
two-dimensional domains. Given the domain $\Omega \subset \mathbb{R}^2$ and the
manifold $M=\{ u_{\mu} : \mu\in P\}$ associated with the parameter domain $P
\subset \mathbb{R}^P$ and the parametric field $\mu\mapsto u_{\mu} \in
L^2(\Omega)$, our approach takes as input a set of snapshots from $M$ and
returns a parameter-dependent mapping $\Phi: \Omega \times P \to \Omega$, which
tracks coherent features (e.g., shocks, shear layers) of the solution field and
ultimately simplifies the task of model reduction. We consider mappings of the
form $\Phi=\texttt{N}(\mathbf{a})$ where $\texttt{N}:\mathbb{R}^M \to {\rm
Lip}(\Omega; \mathbb{R}^2)$ is a suitable linear or nonlinear operator; then,
we state the registration problem as an unconstrained optimization statement
for the coefficients $\mathbf{a}$. We identify minimal requirements for the
operator $\texttt{N}$ to ensure the satisfaction of the bijectivity constraint;
we propose a class of compositional maps that satisfy the desired requirements
and enable non-trivial deformations over curved boundaries of $\Omega$; we
develop a thorough analysis of the proposed ansatz for polytopal domains and we
discuss the approximation properties for general curved domains. We perform
numerical experiments for a parametric inviscid transonic compressible flow
past a cascade of turbine blades to illustrate the many features of the method.",http://arxiv.org/pdf/2308.15307v1
2308.15217v1,math.NA,Computational analysis for competition flows in arteriovenous fistulas based on non-contrast magnetic resonance imaging,2023-08-29 11:15:12+00:00,"Introduction: Characteristics of hemodynamics strongly affect the patency of
arteriovenous fistula (AVF) in hemodialysis patients. Because of pressure
balance changes among arteries after AVF construction, regurgitating flow
occurs in some patients.
  Methods: Based on phase-contrast MRI measurements, flow types around the
anastomotic site are classified to the three different types of splitting,
merging, and one-way, where merging type incorporates regurgitating flow. We
have performed computational simulations to analyze characteristic differences
among these types.
  Results: In the merging type, a characteristic spiral flow is observed in AVF
causing strong wall shear stress and large pressure drop, whereas the splitting
type shows a smooth flow and gives a smaller pressure drop. The one-way case is
intermediate between splitting and merging types.
  Conclusion: Regurgitation brings about high wall shear stress near the
anastomotic site because of instabilities induced by merging phenomena, for
which type careful follow-up examinations are regarded as necessary.",http://arxiv.org/pdf/2308.15217v1
2308.15201v1,math.NA,On Hermitian interpolation of first order data with locally generated C1-splines over triangular meshes,2023-08-29 10:39:16+00:00,"Given a system of triangles in the plane $\mathbb{R}^2$ along with given data
of function and gradient values at the vertices, we describe the general
pattern of local linear methods invoving only four smooth standard shape
functions which results in a spline function fitting the given value and
gradient data value with ${\cal C}^1$-coupling along the edges of the
triangles. We characterize their invariance properties with relavance for the
construction of interpolation surfaces over triangularizations of scanned 3D
data. %The described procedures are local linear and affine invariant. The
numerically simplest procedures among them leaving invarant all polynomials of
2-variables with degree 0 resp 1 involve only polynomials of 5-th resp. 6-th
degree, but the characteizations give rise to a huge variety of procedures with
non-polynomial shape functions.",http://arxiv.org/pdf/2308.15201v1
2308.15195v1,math.NA,MCMS-RBM: Multi-Component Multi-State Reduced Basis Method toward Efficient Transition Pathway Identification for Crystals and Quasicrystals,2023-08-29 10:22:46+00:00,"Due to quasicrystals having long-range orientational order but without
translational symmetry, traditional numerical methods usually suffer when
applied as is. In the past decade, the projection method has emerged as a
prominent solver for quasiperiodic problems. Transforming them into a higher
dimensional but periodic ones, the projection method facilitates the
application of the fast Fourier transform. However, the computational
complexity inevitably becomes high which significantly impedes e.g. the
generation of the phase diagram since a high-fidelity simulation of a problem
whose dimension is doubled must be performed for numerous times.
  To address the computational challenge of quasiperiodic problems based on the
projection method, this paper proposes a multi-component multi-state reduced
basis method (MCMS-RBM). Featuring multiple components with each providing
reduction functionality for one branch of the problem induced by one part of
the parameter domain, the MCMS-RBM does not resort to the parameter domain
configurations (e.g. phase diagrams) a priori. It enriches each component in a
greedy fashion via a phase-transition guided exploration of the multiple states
inherent to the problem. Adopting the empirical interpolation method, the
resulting online-efficient method vastly accelerates the generation of a
delicate phase diagram to a matter of minutes for a parametrized two-turn-four
dimensional Lifshitz-Petrich model with two length scales. Moreover, it
furnishes surrogate and equally accurate field variables anywhere in the
parameter domain.",http://arxiv.org/pdf/2308.15195v1
2308.15182v1,math.NA,Stabilised finite element method for Stokes problem with nonlinear slip condition,2023-08-29 10:03:25+00:00,"This work introduces a stabilised finite element formulation for the Stokes
flow problem with a nonlinear slip boundary condition of friction type. The
boundary condition is enforced with the help of an additional Lagrange
multiplier and the stabilised formulation is based on simultaneously
stabilising both the pressure and the Lagrange multiplier. We establish the
stability and the a priori error analyses, and perform a numerical convergence
study in order to verify the theory.",http://arxiv.org/pdf/2308.15182v1
2308.15145v1,math.OC,Limited memory gradient methods for unconstrained optimization,2023-08-29 09:23:25+00:00,"The limited memory steepest descent method (Fletcher, 2012) for unconstrained
optimization problems stores a few past gradients to compute multiple stepsizes
at once. We review this method and propose new variants. For strictly convex
quadratic objective functions, we study the numerical behavior of different
techniques to compute new stepsizes. In particular, we introduce a method to
improve the use of harmonic Ritz values. We also show the existence of a secant
condition associated with LMSD, where the approximating Hessian is projected
onto a low-dimensional space. In the general nonlinear case, we propose two new
alternatives to Fletcher's method: first, the addition of symmetry constraints
to the secant condition valid for the quadratic case; second, a perturbation of
the last differences between consecutive gradients, to satisfy multiple secant
equations simultaneously. We show that Fletcher's method can also be
interpreted from this viewpoint.",http://arxiv.org/pdf/2308.15145v1
2308.15106v1,math.NA,On factorization of rank-one auto-correlation matrix polynomials,2023-08-29 08:14:11+00:00,"This article characterizes the rank-one factorization of auto-correlation
matrix polynomials. We establish a sufficient and necessary uniqueness
condition for uniqueness of the factorization based on the greatest common
divisor (GCD) of multiple polynomials. In the unique case, we show that the
factorization can be carried out explicitly using GCDs. In the non-unique case,
the number of non-trivially different factorizations is given and all solutions
are enumerated.",http://arxiv.org/pdf/2308.15106v1
2308.15092v1,math.NA,Can We Rely on AI?,2023-08-29 07:58:19+00:00,"Over the last decade, adversarial attack algorithms have revealed
instabilities in deep learning tools. These algorithms raise issues regarding
safety, reliability and interpretability in artificial intelligence; especially
in high risk settings. From a practical perspective, there has been a war of
escalation between those developing attack and defence strategies. At a more
theoretical level, researchers have also studied bigger picture questions
concerning the existence and computability of attacks. Here we give a brief
overview of the topic, focusing on aspects that are likely to be of interest to
researchers in applied and computational mathematics.",http://arxiv.org/pdf/2308.15092v1
2308.15089v1,math.NA,Optimal error bounds on time-splitting methods for the nonlinear Schrdinger equation with low regularity potential and nonlinearity,2023-08-29 07:53:05+00:00,"We establish optimal error bounds on time-splitting methods for the nonlinear
Schr\""odinger equation with low regularity potential and typical power-type
nonlinearity $ f(\rho) = \rho^\sigma $, where $ \rho:=|\psi|^2 $ is the density
with $ \psi $ the wave function and $ \sigma > 0 $ the exponent of the
nonlinearity. For the first-order Lie-Trotter time-splitting method, optimal $
L^2 $-norm error bound is proved for $L^\infty$-potential and $ \sigma > 0 $,
and optimal $H^1$-norm error bound is obtained for $ W^{1, 4} $-potential and $
\sigma \geq 1/2 $. For the second-order Strang time-splitting method, optimal $
L^2 $-norm error bound is established for $H^2$-potential and $ \sigma \geq 1
$, and optimal $H^1$-norm error bound is proved for $H^3$-potential and $
\sigma \geq 3/2 $. Compared to those error estimates of time-splitting methods
in the literature, our optimal error bounds either improve the convergence
rates under the same regularity assumptions or significantly relax the
regularity requirements on potential and nonlinearity for optimal convergence
orders. A key ingredient in our proof is to adopt a new technique called
\textit{regularity compensation oscillation} (RCO), where low frequency modes
are analyzed by phase cancellation, and high frequency modes are estimated by
regularity of the solution. Extensive numerical results are reported to confirm
our error estimates and to demonstrate that they are sharp.",http://arxiv.org/pdf/2308.15089v1
2308.15083v1,math.AP,Hyperbolicity of a semi-Lagrangian formulation of the hydrostatic free-surface Euler system,2023-08-29 07:38:41+00:00,"By a semi-Lagrangian change of coordinates, the hydrostatic Euler equations
describing free-surface sheared flows is rewritten as a system of quasilinear
equations, where stability conditions can be determined by the analysis of its
hyperbolic structure. This new system can be written as a quasi linear system
in time and horizontal variables and involves no more vertical derivatives.
However, the coefficients in front of the horizontal derivatives include an
integral operator acting on the new vertical variable. The spectrum of these
operators is studied in detail, in particular it includes a continuous part.
Riemann invariants are then determined as conserved quantities along the
characteristic curves. Examples of solutions are provided, in particular
stationary solutions and solutions blowing-up in finite time. Eventually, we
propose an exact multi-layer $\mathbb{P}_0$-discretization, which could be used
to solve numerically this semi-Lagrangian system, and analyze the eigenvalues
of the corresponding discretized operator to investigate the hyperbolic nature
of the approximated system.",http://arxiv.org/pdf/2308.15083v1
2308.15054v1,math.OC,A Geometric Algorithm for Maximizing the Distance over an Intersection of Balls to a Given Point,2023-08-29 06:30:31+00:00,"In this paper the authors propose a polynomial algorithm which allows the
computation of the farthest in an intersection of balls to a given point under
three additional hypothesis: the farthest is unique, the distance to it is
known and its magnitude is known. As a use case the authors analyze the subset
sum problem SSP(S,T) for a given $S\in \mathbb{R}^n$ and $T \in \mathbb{R}$.
The proposed approach is to write the SSP as a distance maximization over an
intersection of balls. It was shown that the SSP has a solution if and only if
the maximum value of the distance has a predefined value. This together with
the fact that a solution is a corner of the unit hypercube, allows the authors
to apply the proposed geometry results to find a solution to the SSP under the
hypothesis that is unique.",http://arxiv.org/pdf/2308.15054v1
2308.15041v1,math.NA,Optimization via conformal Hamiltonian systems on manifolds,2023-08-29 05:50:26+00:00,"In this work we propose a method to perform optimization on manifolds. We
assume to have an objective function $f$ defined on a manifold and think of it
as the potential energy of a mechanical system. By adding a momentum-dependent
kinetic energy we define its Hamiltonian function, which allows us to write the
corresponding Hamiltonian system. We make it conformal by introducing a
dissipation term: the result is the continuous model of our scheme. We solve it
via splitting methods (Lie-Trotter and leapfrog): we combine the RATTLE scheme,
approximating the conserved flow, with the exact dissipated flow. The result is
a conformal symplectic method for constant stepsizes. We also propose an
adaptive stepsize version of it. We test it on an example, the minimization of
a function defined on a sphere, and compare it with the usual gradient descent
method.",http://arxiv.org/pdf/2308.15041v1
2308.14958v1,math.NA,Robust topology optimisation of lattice structures with spatially correlated uncertainties,2023-08-29 01:16:42+00:00,"The uncertainties in material and other properties of structures are usually
spatially correlated. We introduce an efficient technique for representing and
processing spatially correlated random fields in robust topology optimisation
of lattice structures. Robust optimisation considers the statistics of the
structural response to obtain a design whose performance is less sensitive to
the specific realisation of the random field. We represent Gaussian random
fields on lattices by leveraging the established link between random fields and
stochastic partial differential equations (SPDEs). It is known that the
precision matrix, i.e. the inverse of the covariance matrix, of a random field
with Mat\'ern covariance is equal to the finite element stiffness matrix of a
possibly fractional PDE with a second-order elliptic operator. We consider the
discretisation of the PDE on the lattice to obtain a random field which, by
design, considers its geometry and connectivity. The so-obtained random field
can be interpreted as a physics-informed prior by the hypothesis that the
elliptic SPDE models the physical processes occurring during manufacturing,
like heat and mass diffusion. Although the proposed approach is general, we
demonstrate its application to lattices modelled as pin-jointed trusses with
uncertainties in member Young's moduli. We consider as a cost function the
weighted sum of the expectation and standard deviation of the structural
compliance. To compute the expectation and standard deviation and their
gradients with respect to member cross-sections we use a first-order Taylor
series approximation. The cost function and its gradient are computed using
only sparse matrix operations. We demonstrate the efficiency of the proposed
approach using several lattice examples with isotropic, anisotropic and
non-stationary random fields and up to eighty thousand random and optimisation
variables.",http://arxiv.org/pdf/2308.14958v1
2308.14933v1,math.NA,A hybridizable discontinuous Galerkin method for the dual-porosity-Stokes problem,2023-08-28 23:14:57+00:00,"We introduce and analyze a hybridizable discontinuous Galerkin (HDG) method
for the dual-porosity-Stokes problem. This coupled problem describes the
interaction between free flow in macrofractures/conduits, governed by the
Stokes equations, and flow in microfractures/matrix, governed by a
dual-porosity model. We prove that the HDG method is strongly conservative,
well-posed, and give an a priori error analysis showing dependence on the
problem parameters. Our theoretical findings are corroborated by numerical
examples",http://arxiv.org/pdf/2308.14933v1
2308.14872v1,math.NA,Consistency and convergence of flux-corrected finite element methods for nonlinear hyperbolic problems,2023-08-28 19:49:28+00:00,"We investigate the consistency and convergence of flux-corrected finite
element approximations in the context of nonlinear hyperbolic conservation
laws. In particular, we focus on a monolithic convex limiting approach and
prove a Lax--Wendroff-type theorem for the corresponding semi-discrete problem.
A key component of our analysis is the use of a weak estimate on bounded
variation, which follows from the semi-discrete entropy stability property of
the method under investigation. For the Euler equations of gas dynamics, we
prove the weak convergence of the flux-corrected finite element scheme to a
dissipative weak solution. If a strong solution exists, the sequence of
numerical approximations converges strongly to the strong solution.",http://arxiv.org/pdf/2308.14872v1
2308.14639v1,math.NA,A Rational Krylov Subspace Method for the Computation of the Matrix Exponential Operator,2023-08-28 15:09:13+00:00,"The computation of approximating e^tA B, where A is a large sparse matrix and
B is a rectangular matrix, serves as a crucial element in numerous scientific
and engineering calculations. A powerful way to consider this problem is to use
Krylov subspace methods. The purpose of this work is to approximate the matrix
exponential and some Cauchy-Stieltjes functions on a block vectors B of R^n*p
using a rational block Lanczos algorithm. We also derive some error estimates
and error bound for the convergence of the rational approximation and finally
numerical results attest to the computational efficiency of the proposed
method.",http://arxiv.org/pdf/2308.14639v1
2308.14573v1,math.NA,Linearizing Anhysteretic Magnetization Curves: A Novel Algorithm for Finding Simulation Parameters and Magnetic Moments,2023-08-28 13:37:43+00:00,"This paper proposes a new method for determining the simulation parameters of
the Jiles-Atherton Model used to simulate the first magnetization curve and
hysteresis loop in ferromagnetic materials. The Jiles-Atherton Model is an
important tool in engineering applications due to its relatively simple
differential formulation. However, determining the simulation parameters for
the anhysteretic curve is challenging. Several methods have been proposed,
primarily based on mathematical aspects of the anhysteretic and first
magnetization curves and hysteresis loops. This paper focuses on finding the
magnetic moments of the material, which are used to define the simulation
parameters for its anhysteretic curve. The proposed method involves using the
susceptibility of the material and a linear approximation of a paramagnet to
find the magnetic moments. The simulation parameters can then be found based on
the magnetic moments. The method is validated theoretically and experimentally
and offers a more physical approach to finding simulation parameters for the
anhysteretic curve and a simplified way of determining the magnetic moments of
the material.",http://arxiv.org/pdf/2308.14573v1
2308.14537v1,math.NA,Solving parametric elliptic interface problems via interfaced operator network,2023-08-28 12:49:08+00:00,"Learning operator mapping between infinite-dimensional Banach spaces via
neural networks has attracted a considerable amount of attention in recent
years. In this work, we propose an interfaced operator network (IONet) to solve
parametric elliptic interface PDEs, where different coefficients, source terms
and boundary conditions are considered as input features. To capture the
discontinuities of both input functions and output solutions across the
interface, IONet divides the entire domain into several separate sub-domains
according to the interface, and leverages multiple branch networks and truck
networks. Each branch network extracts latent representations of input
functions at a fixed number of sensors on a specific sub-domain, and each truck
network is responsible for output solutions on one sub-domain. In addition,
tailored physics-informed loss of IONet is proposed to ensure physical
consistency, which greatly reduces the requirement for training datasets and
makes IONet effective without any paired input-output observations in the
interior of the computational domain. Extensive numerical studies show that
IONet outperforms existing state-of-the-art deep operator networks in terms of
accuracy, efficiency, and versatility.",http://arxiv.org/pdf/2308.14537v1
2308.14490v1,math.NA,Efficient least squares approximation and collocation methods using radial basis functions,2023-08-28 11:07:38+00:00,"We describe an efficient method for the approximation of functions using
radial basis functions (RBFs), and extend this to a solver for boundary value
problems on irregular domains. The method is based on RBFs with centers on a
regular grid defined on a bounding box, with some of the centers outside the
computational domain. The equation is discretized using collocation with
oversampling, with collocation points inside the domain only, resulting in a
rectangular linear system to be solved in a least squares sense. The goal of
this paper is the efficient solution of that rectangular system. We show that
the least squares problem splits into a regular part, which can be expedited
with the FFT, and a low rank perturbation, which is treated separately with a
direct solver. The rank of the perturbation is influenced by the irregular
shape of the domain and by the weak enforcement of boundary conditions at
points along the boundary. The solver extends the AZ algorithm which was
previously proposed for function approximation involving frames and other
overcomplete sets. The solver has near optimal log-linear complexity for
univariate problems, and loses optimality for higher-dimensional problems but
remains faster than a direct solver.",http://arxiv.org/pdf/2308.14490v1
2308.14487v1,math.NA,Deep multi-step mixed algorithm for high dimensional non-linear PDEs and associated BSDEs,2023-08-28 11:00:32+00:00,"We propose a new multistep deep learning-based algorithm for the resolution
of moderate to high dimensional nonlinear backward stochastic differential
equations (BSDEs) and their corresponding parabolic partial differential
equations (PDE). Our algorithm relies on the iterated time discretisation of
the BSDE and approximates its solution and gradient using deep neural networks
and automatic differentiation at each time step. The approximations are
obtained by sequential minimisation of local quadratic loss functions at each
time step through stochastic gradient descent. We provide an analysis of
approximation error in the case of a network architecture with weight
constraints requiring only low regularity conditions on the generator of the
BSDE. The algorithm increases accuracy from its single step parent model and
has reduced complexity when compared to similar models in the literature.",http://arxiv.org/pdf/2308.14487v1
2308.14479v1,math.NA,A convergent interacting particle method for computing KPP front speeds in random flows,2023-08-28 10:38:31+00:00,"We aim to efficiently compute spreading speeds of
reaction-diffusion-advection (RDA) fronts in divergence free random flows under
the Kolmogorov-Petrovsky-Piskunov (KPP) nonlinearity. We study a stochastic
interacting particle method (IPM) for the reduced principal eigenvalue
(Lyapunov exponent) problem of an associated linear advection-diffusion
operator with spatially random coefficients. The Fourier representation of the
random advection field and the Feynman-Kac (FK) formula of the principal
eigenvalue (Lyapunov exponent) form the foundation of our method implemented as
a genetic evolution algorithm. The particles undergo advection-diffusion, and
mutation/selection through a fitness function originated in the FK semigroup.
We analyze convergence of the algorithm based on operator splitting, present
numerical results on representative flows such as 2D cellular flow and 3D
Arnold-Beltrami-Childress (ABC) flow under random perturbations. The 2D
examples serve as a consistency check with semi-Lagrangian computation. The 3D
results demonstrate that IPM, being mesh free and self-adaptive, is simple to
implement and efficient for computing front spreading speeds in the
advection-dominated regime for high-dimensional random flows on unbounded
domains where no truncation is needed.",http://arxiv.org/pdf/2308.14479v1
2308.14431v1,math.NA,Two-Scale Finite Element Approximation of a Homogenized Plate Model,2023-08-28 09:09:34+00:00,"This paper studies the discretization of a homogenization and dimension
reduction model for the elastic deformation of microstructured thin plates
proposed by Hornung, Neukamm, and Vel\v{c}i\'c in 2014. Thereby, a nonlinear
bending energy is based on a homogenized quadratic form which acts on the
second fundamental form associated with the elastic deformation. Convergence is
proven for a multi-affine finite element discretization of the involved
three-dimensional microscopic cell problems and a discrete Kirchhoff triangle
discretization of the two-dimensional isometry-constrained macroscopic problem.
Finally, the convergence properties are numerically verified in selected test
cases and qualitatively compared with deformation experiments for
microstructured sheets of paper.",http://arxiv.org/pdf/2308.14431v1
2308.14405v1,math.NA,Simulation of Permittivity and Conductivity Graded Materials for HVDC GIL for Different Voltage Forms,2023-08-28 08:40:21+00:00,"Functionally graded materials (FGM) are applied in HVDC gas insulated lines
(GIL) to control the electric field within the DC insulation system. In HVDC
GIL, FGM with a spatial distribution of the electric conductivity
(conductivity-FGM) is applied to control the electric field under DC steady
state condition. However, besides DC steady state, different DC conditions
occur, e.g. DC-on process, polarity reversal and lightning impulse. Under these
conditions conductivity-FGM is not sufficient to control the electric field,
since these conditions result in transient capacitive fields, where the
permittivity is decisive for the electric field. In this paper, we suggest
combining conductivity-FGM and a spatial distribution of permittivity
(permittivity-FGM) in the spacer material to control the electric field around
DC-GIL spacer for various DC-conditions, considering nonlinear material models
for the insulating gas and the epoxy spacer. A variation of the spatial
distribution of permittivity and conductivity in the spacer is investigated in
this paper for an effective field reduction. The results show a reduction of
the electric field intensity up to 65.8 %, when conductivity/permittivity-FGM
is applied.",http://arxiv.org/pdf/2308.14405v1
2308.14222v1,math.NA,Accurate complex Jacobi rotations,2023-08-27 22:46:18+00:00,"This note shows how to compute, to high relative accuracy under mild
assumptions, complex Jacobi rotations for diagonalization of Hermitian matrices
of order two, using the correctly rounded functions $\mathtt{cr\_hypot}$ and
$\mathtt{cr\_rsqrt}$, proposed for standardization in the C programming
language as recommended by the IEEE-754 floating-point standard. The rounding
to nearest (ties to even) and the non-stop arithmetic are assumed. The
numerical examples compare the observed with theoretical bounds on the relative
errors in the rotations' elements, and show that the maximal observed departure
of the rotations' determinants from unity is smaller than that of the
transformations computed by LAPACK.",http://arxiv.org/pdf/2308.14222v1
2308.14188v1,math.NA,Bayesian deep operator learning for homogenized to fine-scale maps for multiscale PDE,2023-08-27 19:36:53+00:00,"We present a new framework for computing fine-scale solutions of multiscale
Partial Differential Equations (PDEs) using operator learning tools. Obtaining
fine-scale solutions of multiscale PDEs can be challenging, but there are many
inexpensive computational methods for obtaining coarse-scale solutions.
Additionally, in many real-world applications, fine-scale solutions can only be
observed at a limited number of locations. In order to obtain approximations or
predictions of fine-scale solutions over general regions of interest, we
propose to learn the operator mapping from coarse-scale solutions to fine-scale
solutions using a limited number (and possibly noisy) observations of the
fine-scale solutions. The approach is to train multi-fidelity homogenization
maps using mathematically motivated neural operators. The operator learning
framework can efficiently obtain the solution of multiscale PDEs at any
arbitrary point, making our proposed framework a mesh-free solver. We verify
our results on multiple numerical examples showing that our approach is an
efficient mesh-free solver for multiscale PDEs.",http://arxiv.org/pdf/2308.14188v1
2308.14143v1,math.OC,Ensemble-localized Kernel Density Estimation with Applications to the Ensemble Gaussian Mixture Filter,2023-08-27 15:54:17+00:00,"The ensemble Gaussian mixture filter (EnGMF) is a powerful filter for highly
non-Gaussian and non-linear models that has practical utility in the case of a
small number of samples, and theoretical convergence to full Bayesian inference
in the ensemble limit. We aim to increase the utility of the EnGMF by
introducing an ensemble-local notion of covariance into the kernel density
estimation (KDE) step for the prior distribution. We prove that in the Gaussian
case, our new ensemble-localized KDE technique is exactly the same as more
traditional KDE techniques. We also show an example of a non-Gaussian
distribution that can fail to be approximated by canonical KDE methods, but can
be approximated exactly by our new KDE technique. We showcase our new KDE
technique on a simple bivariate problem, showing that it has nice qualitative
and quantitative properties, and significantly improves the estimate of the
prior and posterior distributions for all ensemble sizes tested. We
additionally show the utility of the proposed methodology for sequential
filtering for the Lorenz '63 equations, achieving a significant reduction in
error, and less conservative behavior in the uncertainty estimate with respect
to traditional techniques.",http://arxiv.org/pdf/2308.14143v1
2308.14127v1,math.NA,Information geometric regularization of the barotropic Euler equation,2023-08-27 15:03:32+00:00,"A key numerical difficulty in compressible fluid dynamics is the formation of
shock waves. Shock waves feature jump discontinuities in the velocity and
density of the fluid and thus preclude the existence of classical solutions to
the compressible Euler equations. Weak ""entropy"" solutions are commonly defined
by viscous regularization, but even small amounts of viscosity can
substantially change the long-term behavior of the solution. In this work, we
propose an inviscid regularization based on ideas from semidefinite programming
and information geometry. From a Lagrangian perspective, shock formation in
entropy solutions amounts to inelastic collisions of fluid particles. Their
trajectories are akin to that of projected gradient descent on a feasible set
of nonintersecting paths. We regularize these trajectories by replacing them
with solution paths of interior point methods based on log determinantal
barrier functions. These paths are geodesic curves with respect to the
information geometry induced by the barrier function. Thus, our regularization
amounts to replacing the Euclidean geometry of phase space with a suitable
information geometry. We extend this idea to infinite families of paths by
viewing Euler's equations as a dynamical system on a diffeomorphism manifold.
Our regularization embeds this manifold into an information geometric ambient
space, equipping it with a geodesically complete geometry. Expressing the
resulting Lagrangian equations in Eulerian form, we derive a regularized Euler
equation in conservation form. Numerical experiments on one and two-dimensional
problems show its promise as a numerical tool.",http://arxiv.org/pdf/2308.14127v1
2308.14080v1,math.OC,The Global R-linear Convergence of Nesterov's Accelerated Gradient Method with Unknown Strongly Convex Parameter,2023-08-27 11:56:20+00:00,"The Nesterov accelerated gradient (NAG) method is an important
extrapolation-based numerical algorithm that accelerates the convergence of the
gradient descent method in convex optimization. When dealing with an objective
function that is $\mu$-strongly convex, selecting extrapolation coefficients
dependent on $\mu$ enables global R-linear convergence. In cases $\mu$ is
unknown, a commonly adopted approach is to set the extrapolation coefficient
using the original NAG method, referred to as NAG-c. This choice allows for
achieving the optimal iteration complexity among first-order methods for
general convex problems. However, it remains an open question whether the NAG-c
method exhibits global R-linear convergence for strongly convex problems. In
this work, we answer this question positively by establishing the Q-linear
convergence of certain constructed Lyapunov sequences. Furthermore, we extend
our result to the global R-linear convergence of the accelerated proximal
gradient method, which is employed for solving strongly convex composite
optimization problems with nonsmooth terms in the objective function.
Interestingly, these results contradict the findings of the continuous
counterpart of the NAG-c method in [Su, Boyd, and Cand\'es, J. Mach. Learn.
Res., 2016, 17(153), 1-43], where the convergence rate by the suggested
ordinary differential equation cannot exceed $O(1/{\tt poly}(k))$ for strongly
convex functions.",http://arxiv.org/pdf/2308.14080v1
2308.13999v1,math.NA,A Milstein-type method for highly non-linear non-autonomous time-changed stochastic differential equations,2023-08-27 04:02:38+00:00,"A Milstein-type method is proposed for some highly non-linear non-autonomous
time-changed stochastic differential equations (SDEs). The spatial variables in
the coefficients of the time-changed SDEs satisfy the super-linear growth
condition and the temporal variables obey some H\""older's continuity condition.
The strong convergence in the finite time is studied and the convergence order
is obtained.",http://arxiv.org/pdf/2308.13999v1
2308.13986v1,math.NA,A Deep Learning Method for Computing Eigenvalues of the Fractional Schrdinger Operator,2023-08-27 02:17:07+00:00,"We present a novel deep learning method for computing eigenvalues of the
fractional Schr\""odinger operator. Our approach combines a newly developed loss
function with an innovative neural network architecture that incorporates prior
knowledge of the problem. These improvements enable our method to handle both
high-dimensional problems and problems posed on irregular bounded domains. We
successfully compute up to the first 30 eigenvalues for various fractional
Schr\""odinger operators. As an application, we share a conjecture to the
fractional order isospectral problem that has not yet been studied.",http://arxiv.org/pdf/2308.13986v1
2308.13840v1,math.NA,Optimal Transport-inspired Deep Learning Framework for Slow-Decaying Problems: Exploiting Sinkhorn Loss and Wasserstein Kernel,2023-08-26 10:24:43+00:00,"Reduced order models (ROMs) are widely used in scientific computing to tackle
high-dimensional systems. However, traditional ROM methods may only partially
capture the intrinsic geometric characteristics of the data. These
characteristics encompass the underlying structure, relationships, and
essential features crucial for accurate modeling.
  To overcome this limitation, we propose a novel ROM framework that integrates
optimal transport (OT) theory and neural network-based methods. Specifically,
we investigate the Kernel Proper Orthogonal Decomposition (kPOD) method
exploiting the Wasserstein distance as the custom kernel, and we efficiently
train the resulting neural network (NN) employing the Sinkhorn algorithm. By
leveraging an OT-based nonlinear reduction, the presented framework can capture
the geometric structure of the data, which is crucial for accurate learning of
the reduced solution manifold. When compared with traditional metrics such as
mean squared error or cross-entropy, exploiting the Sinkhorn divergence as the
loss function enhances stability during training, robustness against
overfitting and noise, and accelerates convergence.
  To showcase the approach's effectiveness, we conduct experiments on a set of
challenging test cases exhibiting a slow decay of the Kolmogorov n-width. The
results show that our framework outperforms traditional ROM methods in terms of
accuracy and computational efficiency.",http://arxiv.org/pdf/2308.13840v1
2308.13819v1,cs.LG,Guaranteed Stable Quadratic Models and their applications in SINDy and Operator Inference,2023-08-26 09:00:31+00:00,"Scientific machine learning for learning dynamical systems is a powerful tool
that combines data-driven modeling models, physics-based modeling, and
empirical knowledge. It plays an essential role in an engineering design cycle
and digital twinning. In this work, we primarily focus on an operator inference
methodology that builds dynamical models, preferably in low-dimension, with a
prior hypothesis on the model structure, often determined by known physics or
given by experts. Then, for inference, we aim to learn the operators of a model
by setting up an appropriate optimization problem. One of the critical
properties of dynamical systems is{stability. However, such a property is not
guaranteed by the inferred models. In this work, we propose inference
formulations to learn quadratic models, which are stable by design. Precisely,
we discuss the parameterization of quadratic systems that are locally and
globally stable. Moreover, for quadratic systems with no stable point yet
bounded (e.g., Chaotic Lorenz model), we discuss an attractive trapping region
philosophy and a parameterization of such systems. Using those
parameterizations, we set up inference problems, which are then solved using a
gradient-based optimization method. Furthermore, to avoid numerical derivatives
and still learn continuous systems, we make use of an integration form of
differential equations. We present several numerical examples, illustrating the
preservation of stability and discussing its comparison with the existing
state-of-the-art approach to infer operators. By means of numerical examples,
we also demonstrate how proposed methods are employed to discover governing
equations and energy-preserving models.",http://arxiv.org/pdf/2308.13819v1
2308.13709v1,cs.IT,Fast and Low-Memory Compressive Sensing Algorithms for Low Tucker-Rank Tensor Approximation from Streamed Measurements,2023-08-25 23:44:47+00:00,"In this paper we consider the problem of recovering a low-rank Tucker
approximation to a massive tensor based solely on structured random compressive
measurements. Crucially, the proposed random measurement ensembles are both
designed to be compactly represented (i.e., low-memory), and can also be
efficiently computed in one-pass over the tensor. Thus, the proposed
compressive sensing approach may be used to produce a low-rank factorization of
a huge tensor that is too large to store in memory with a total memory
footprint on the order of the much smaller desired low-rank factorization. In
addition, the compressive sensing recovery algorithm itself (which takes the
compressive measurements as input, and then outputs a low-rank factorization)
also runs in a time which principally depends only on the size of the sought
factorization, making its runtime sub-linear in the size of the large tensor
one is approximating. Finally, unlike prior works related to (streaming)
algorithms for low-rank tensor approximation from such compressive
measurements, we present a unified analysis of both Kronecker and Khatri-Rao
structured measurement ensembles culminating in error guarantees comparing the
error of our recovery algorithm's approximation of the input tensor to the best
possible low-rank Tucker approximation error achievable for the tensor by any
possible algorithm. We further include an empirical study of the proposed
approach that verifies our theoretical findings and explores various trade-offs
of parameters of interest.",http://arxiv.org/pdf/2308.13709v1
2308.13476v1,math.NA,Stand-alone Multigrid for Helmholtz Revisited: Towards Convergence Using Standard Components,2023-08-25 16:28:50+00:00,"Getting standard multigrid to work efficiently for the high-frequency
Helmholtz equation has been an open problem in applied mathematics for years.
Much effort has been dedicated to finding solution methods which can use
multigrid components to obtain solvers with a linear time complexity. In this
work we present one among the first stand-alone multigrid solvers for the 2D
Helmholtz equation using both a constant and non-constant wavenumber model
problem. We use standard smoothing techniques and do not impose any
restrictions on the number of grid points per wavelength on the coarse-grid. As
a result we are able to obtain a full V- and W-cycle algorithm. The key
features of the algorithm are the use of higher-order inter-grid transfer
operators combined with a complex constant in the coarsening process. Using
weighted-Jacobi smoothing, we obtain a solver which is $h-$independent and
scales linearly with the wavenumber $k$. Numerical results using 1 to 5
GMRES(3) smoothing steps approach $k-$ and $h-$ independent convergence, when
combined with the higher-order inter-grid transfer operators and a small or
even zero complex shift. The proposed algorithm provides an important step
towards the perpetuating branch of research in finding scalable solvers for
challenging wave propagation problems.",http://arxiv.org/pdf/2308.13476v1
2308.13333v1,cs.RO,Small Celestial Body Exploration with CubeSat Swarms,2023-08-25 12:09:31+00:00,"This work presents a large-scale simulation study investigating the
deployment and operation of distributed swarms of CubeSats for interplanetary
missions to small celestial bodies. Utilizing Taylor numerical integration and
advanced collision detection techniques, we explore the potential of large
CubeSat swarms in capturing gravity signals and reconstructing the internal
mass distribution of a small celestial body while minimizing risks and Delta V
budget. Our results offer insight into the applicability of this approach for
future deep space exploration missions.",http://arxiv.org/pdf/2308.13333v1
2308.13295v1,math.NA,Resolution-independent generative models based on operator learning for physics-constrained Bayesian inverse problems,2023-08-25 10:41:00+00:00,"The Bayesian inference approach is widely used to tackle inverse problems due
to its versatile and natural ability to handle ill-posedness. However, it often
faces challenges when dealing with situations involving continuous fields or
large-resolution discrete representations (high-dimensional). Moreover, the
prior distribution of unknown parameters is commonly difficult to be
determined. In this study, an Operator Learning-based Generative Adversarial
Network (OL-GAN) is proposed and integrated into the Bayesian inference
framework to handle these issues. Unlike most Bayesian approaches, the
distinctive characteristic of the proposed method is to learn the joint
distribution of parameters and responses. By leveraging the trained generative
model, the posteriors of the unknown parameters can theoretically be
approximated by any sampling algorithm (e.g., Markov Chain Monte Carlo, MCMC)
in a low-dimensional latent space shared by the components of the joint
distribution. The latent space is typically a simple and easy-to-sample
distribution (e.g., Gaussian, uniform), which significantly reduces the
computational cost associated with the Bayesian inference while avoiding prior
selection concerns. Furthermore, incorporating operator learning enables
resolution-independent in the generator. Predictions can be obtained at desired
coordinates, and inversions can be performed even if the observation data are
misaligned with the training data. Finally, the effectiveness of the proposed
method is validated through several numerical experiments.",http://arxiv.org/pdf/2308.13295v1
2308.13224v1,math.PR,Exponential Euler method for stiff stochastic differential equations with additive fractional Brownian noise,2023-08-25 07:41:59+00:00,"We discuss a system of stochastic differential equations with a stiff linear
term and additive noise driven by fractional Brownian motions (fBms) with Hurst
parameter H>1/2, which arise e. g., from spatial approximations of stochastic
partial differential equations. For their numerical approximation, we present
an exponential Euler scheme and show that it converges in the strong sense with
an exact rate close to the Hurst parameter H. Further, based on [2], we
conclude the existence of a unique stationary solution of the exponential Euler
scheme that is pathwise asymptotically stable.",http://arxiv.org/pdf/2308.13224v1
2308.13214v1,math.NA,Gl-QFOM and Gl-QGMRES: two efficient algorithms for quaternion linear systems with multiple right-hand sides,2023-08-25 07:24:53+00:00,"In this paper, we propose the global quaternion full orthogonalization
(Gl-QFOM) and global quaternion generalized minimum residual (Gl-QGMRES)
methods, which are built upon global orthogonal and oblique projections onto a
quaternion matrix Krylov subspace, for solving quaternion linear systems with
multiple right-hand sides. We first develop the global quaternion Arnoldi
procedure to preserve the quaternion Hessenberg form during the iterations. We
then establish the convergence analysis of the proposed methods, and show how
to apply them to solve the Sylvester quaternion matrix equation. Numerical
examples are provided to illustrate the effectiveness of our methods compared
with the traditional Gl-FOM and Gl-GMRES iterations for the real
representations of the original linear systems.",http://arxiv.org/pdf/2308.13214v1
2308.13195v1,math.NA,Preconditioning for Generalized Jacobians with the $$-Condition Number,2023-08-25 06:20:20+00:00,"Preconditioning is essential in iterative methods for solving linear systems
of equations. We study a nonclassic matrix condition number, the
$\omega$-condition number, in the context of optimal conditioning for low rank
updating of positive definite matrices. For a positive definite matrix, this
condition measure is the ratio of the arithmetic and geometric means of the
eigenvalues. In particular, we concentrate on linear systems with low rank
updates of positive definite matrices which are close to singular. These
systems arise in the contexts of nonsmooth Newton methods using generalized
Jacobians. We derive an explicit formula for the optimal
$\omega$-preconditioned update in this framework.
  Evaluating or estimating the classical condition number $\kappa$ can be
expensive. We show that the $\omega$-condition number can be evaluated exactly
following a Cholesky or LU factorization and it estimates the actual condition
of a linear system significantly better. Moreover, our empirical results show a
significant decrease in the number of iterations required for a requested
accuracy in the residual during an iterative method, i.e., these results
confirm the efficacy of using the $\omega$-condition number compared to the
classical condition number.",http://arxiv.org/pdf/2308.13195v1
2308.13152v1,math.NA,The time dimensional reduction method to determine the initial conditions without the knowledge of damping coefficients,2023-08-25 03:11:30+00:00,"This paper aims to reconstruct the initial condition of a hyperbolic equation
with an unknown damping coefficient. Our approach involves approximating the
hyperbolic equation's solution by its truncated Fourier expansion in the time
domain and using a polynomial-exponential basis. This truncation process
facilitates the elimination of the time variable, consequently, yielding a
system of quasi-linear elliptic equations. To globally solve the system without
needing an accurate initial guess, we employ the Carleman contraction
principle. We provide several numerical examples to illustrate the efficacy of
our method. The method not only delivers precise solutions but also showcases
remarkable computational efficiency.",http://arxiv.org/pdf/2308.13152v1
2308.13123v1,cs.CE,Multiscale modeling of thermal properties in Polyurethane incorporated with phase change materials composites: A case study,2023-08-25 00:29:56+00:00,"Polyurethane (PU) is an ideal thermal insulation material due to its
excellent thermal properties. The incorporation of Phase Change Materials
(PCMs) capsules into Polyurethane (PU) has been shown to be effective in
building envelopes. This design can significantly increase the stability of the
indoor thermal environment and reduce the fluctuation of indoor air
temperature. We develop a multiscale model of a PU-PCM foam composite and study
the thermal conductivity of this material. Later, the design of materials can
be optimized by obtaining thermal conductivity. We conduct a case study based
on the performance of this optimized material to fully consider the thermal
comfort of the occupants of a building envelope with the application of PU-PCMs
composites in a single room. At the same time, we also predict the energy
consumption of this case. All the outcomes show that this design is promising,
enabling the passive design of building energy and significantly improving
occupants' comfort.",http://arxiv.org/pdf/2308.13123v1
2308.13055v1,physics.flu-dyn,On the dynamics of the boundary vorticity for incompressible viscous flows,2023-08-24 19:42:54+00:00,"The dynamical equation of the boundary vorticity has been obtained, which
shows that the viscosity at a solid wall is doubled as if the fluid became more
viscous at the boundary. For certain viscous flows the boundary vorticity can
be determined via the dynamical equation up to bounded errors for all time,
without the need of knowing the details of the main stream flows. We then
validate the dynamical equation by carrying out stochastic direct numerical
simulations (i.e. the random vortex method for wall-bounded incompressible
viscous flows) by two different means of updating the boundary vorticity, one
using mollifiers of the Biot-Savart singular integral kernel, another using the
dynamical equations.",http://arxiv.org/pdf/2308.13055v1
2308.12939v1,cs.LG,Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries,2023-08-24 17:29:57+00:00,"Recently deep learning surrogates and neural operators have shown promise in
solving partial differential equations (PDEs). However, they often require a
large amount of training data and are limited to bounded domains. In this work,
we present a novel physics-informed neural operator method to solve
parametrized boundary value problems without labeled data. By reformulating the
PDEs into boundary integral equations (BIEs), we can train the operator network
solely on the boundary of the domain. This approach reduces the number of
required sample points from $O(N^d)$ to $O(N^{d-1})$, where $d$ is the domain's
dimension, leading to a significant acceleration of the training process.
Additionally, our method can handle unbounded problems, which are unattainable
for existing physics-informed neural networks (PINNs) and neural operators. Our
numerical experiments show the effectiveness of parametrized complex geometries
and unbounded problems.",http://arxiv.org/pdf/2308.12939v1
2308.12907v1,math.NA,New time domain decomposition methods for parabolic control problems I: Dirichlet-Neumann and Neumann-Dirichlet algorithms,2023-08-24 16:31:01+00:00,"We present new Dirichlet-Neumann and Neumann-Dirichlet algorithms with a time
domain decomposition applied to unconstrained parabolic optimal control
problems. After a spatial semi-discretization, we use the Lagrange multiplier
approach to derive a coupled forward-backward optimality system, which can then
be solved using a time domain decomposition. Due to the forward-backward
structure of the optimality system, three variants can be found for the
Dirichlet-Neumann and Neumann-Dirichlet algorithms. We analyze their
convergence behavior and determine the optimal relaxation parameter for each
algorithm. Our analysis reveals that the most natural algorithms are actually
only good smoothers, and there are better choices which lead to efficient
solvers. We illustrate our analysis with numerical experiments.",http://arxiv.org/pdf/2308.12907v1
2308.12891v1,math.NA,A class of Discontinuous Galerkin methods for nonlinear variational problems,2023-08-24 16:11:23+00:00,"In the context of Discontinuous Galerkin methods, we study approximations of
nonlinear variational problems associated with convex energies. We propose
element-wise nonconforming finite element methods to discretize the continuous
minimisation problem. Using $\Gamma$-convergence arguments we show that the
discrete minimisers converge to the unique minimiser of the continuous problem
as the mesh parameter tends to zero, under the additional contribution of
appropriately defined penalty terms at the level of the discrete energies. We
finally substantiate the feasibility of our methods by numerical examples.",http://arxiv.org/pdf/2308.12891v1
2308.12886v1,math.NA,Linear implicit approximations of invariant measures of semi-linear SDEs with non-globally Lipschitz coefficients,2023-08-24 16:03:40+00:00,"This article investigates the weak approximation towards the invariant
measure of semi-linear stochastic differential equations (SDEs) under
non-globally Lipschitz coefficients. For this purpose, we propose a
linear-theta-projected Euler (LTPE) scheme, which also admits an invariant
measure, to handle the potential influence of the linear stiffness. Under
certain assumptions, both the SDE and the corresponding LTPE method are shown
to converge exponentially to the underlying invariant measures, respectively.
Moreover, with time-independent regularity estimates for the corresponding
Kolmogorov equation, the weak error between the numerical invariant measure and
the original one can be guaranteed with an order one. Numerical experiments are
provided to verify our theoretical findings.",http://arxiv.org/pdf/2308.12886v1
2308.12884v1,math.NA,A second-order length-preserving and unconditionally energy stable rotational discrete gradient method for Oseen-Frank gradient flows,2023-08-24 16:01:25+00:00,"We present a second-order strictly length-preserving and unconditionally
energy-stable rotational discrete gradient (Rdg) scheme for the numerical
approximation of the Oseen-Frank gradient flows with anisotropic elastic energy
functional. Two essential ingredients of the Rdg method are reformulation of
the length constrained gradient flow into an unconstrained rotational form and
discrete gradient discretization for the energy variation. Besides the
well-known mean-value and Gonzalez discrete gradients, we propose a novel
Oseen-Frank discrete gradient, specifically designed for the solution of
Oseen-Frank gradient flow. We prove that the proposed Oseen-Frank discrete
gradient satisfies the energy difference relation, thus the resultant Rdg
scheme is energy stable. Numerical experiments demonstrate the efficiency and
accuracy of the proposed Rdg method and its capability for providing reliable
simulation results with highly disparate elastic coefficients.",http://arxiv.org/pdf/2308.12884v1
2308.12865v1,math.NA,A highly efficient and accurate divergence-free spectral method for curl-curl equation in two and three dimensions,2023-08-24 15:43:12+00:00,"In this paper, we present a fast divergence-free spectral algorithm (FDSA)
for the curl-curl problem. Divergence-free bases in two and three dimensions
are constructed by using the generalized Jacobi polynomials. An accurate
spectral method with exact preservation of the divergence-free constraint
point-wisely is then proposed, and its corresponding error estimate is
established. We then present a highly efficient solution algorithm based on a
combination of matrix-free preconditioned Krylov subspace iterative method and
a fully diagonalizable auxiliary problem, which is derived from the spectral
discretisations of generalized eigenvalue problems of Laplace and biharmonic
operators. We rigorously prove that the dimensions of the invariant subspace of
the preconditioned linear system resulting from the divergence-free spectral
method with respect to the dominate eigenvalue $1$, are $(N-3)^2$ and
$2(N-3)^3$ for two- and three-dimensional problems with $(N-1)^2$ and
$2(N-1)^3$ unknowns, respectively. Thus, the proposed method usually takes only
several iterations to converge, and astonishingly, as the problem size
(polynomial order) increases, the number of iterations will decrease, even for
highly indefinite system and oscillatory solutions. As a result, the
computational cost of the solution algorithm is only a small multiple of $N^3$
and $N^4$ floating number operations for 2D and 3D problems, respectively.
Plenty of numerical examples for solving the curl-curl problem with both
constant and variable coefficients in two and three dimensions are presented to
demonstrate the accuracy and efficiency of the proposed method.",http://arxiv.org/pdf/2308.12865v1
2308.12864v1,cs.LG,Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution,2023-08-24 15:39:01+00:00,"In this article, we present a novel data assimilation strategy in pore-scale
imaging and demonstrate that this makes it possible to robustly address
reactive inverse problems incorporating Uncertainty Quantification (UQ).
Pore-scale modeling of reactive flow offers a valuable opportunity to
investigate the evolution of macro-scale properties subject to dynamic
processes. Yet, they suffer from imaging limitations arising from the
associated X-ray microtomography (X-ray microCT) process, which induces
discrepancies in the properties estimates. Assessment of the kinetic parameters
also raises challenges, as reactive coefficients are critical parameters that
can cover a wide range of values. We account for these two issues and ensure
reliable calibration of pore-scale modeling, based on dynamical microCT images,
by integrating uncertainty quantification in the workflow.
  The present method is based on a multitasking formulation of reactive inverse
problems combining data-driven and physics-informed techniques in calcite
dissolution. This allows quantifying morphological uncertainties on the
porosity field and estimating reactive parameter ranges through prescribed PDE
models with a latent concentration field and dynamical microCT. The data
assimilation strategy relies on sequential reinforcement incorporating
successively additional PDE constraints. We guarantee robust and unbiased
uncertainty quantification by straightforward adaptive weighting of Bayesian
Physics-Informed Neural Networks (BPINNs), ensuring reliable micro-porosity
changes during geochemical transformations. We demonstrate successful Bayesian
Inference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT
images with meaningful posterior distribution on the reactive parameters and
dimensionless numbers.",http://arxiv.org/pdf/2308.12864v1
2308.12807v1,math.NA,Denoising Particle-In-Cell Data via Smoothness-Increasing Accuracy-Conserving Filters with Application to Bohm Speed Computation,2023-08-24 14:10:05+00:00,"The simulation of plasma physics is computationally expensive because the
underlying physical system is of high dimensions, requiring three spatial
dimensions and three velocity dimensions. One popular numerical approach is
Particle-In-Cell (PIC) methods owing to its ease of implementation and
favorable scalability in high-dimensional problems. An unfortunate drawback of
the method is the introduction of statistical noise resulting from the use of
finitely many particles. In this paper we examine the application of the
Smoothness-Increasing Accuracy-Conserving (SIAC) family of convolution kernel
filters as denoisers for moment data arising from PIC simulations. We show that
SIAC filtering is a promising tool to denoise PIC data in the physical space as
well as capture the appropriate scales in the Fourier space. Furthermore, we
demonstrate how the application of the SIAC technique reduces the amount of
information necessary in the computation of quantities of interest in plasma
physics such as the Bohm speed.",http://arxiv.org/pdf/2308.12807v1
2308.12781v1,math.NA,A Riemannian optimization method to compute the nearest singular pencil,2023-08-24 13:37:35+00:00,"Given a square pencil $A+ \lambda B$, where $A$ and $B$ are complex matrices,
we consider the problem of finding the singular pencil nearest to it in the
Frobenius distance. This problem is known to be very difficult, and the few
algorithms available in the literature can only deal efficiently with pencils
of very small size. We show that the problem is equivalent to minimizing a
certain objective function over the Riemannian manifold $SU(n) \times SU(n)$,
where $SU(n)$ denotes the special unitary group. With minor modifications, the
same approach extends to the case of finding a nearest singular pencil with a
specified minimal index. This novel perspective is based on the generalized
Schur form of pencils, and yields a competitive numerical method, by pairing it
with an algorithm capable of doing optimization on a Riemannian manifold. We
provide numerical experiments that show that the resulting method allows us to
deal with pencils of much larger size than alternative techniques, yielding
candidate minimizers of comparable or better quality. In the course of our
analysis, we also obtain a number of new theoretical results related to the
generalized Schur form of a (regular or singular) square pencil and to the
minimal index of a singular square pencil whose nullity is $1$.",http://arxiv.org/pdf/2308.12781v1
2308.12764v1,math.NA,Dirichlet-Neumann and Neumann-Neumann Methods for Elliptic Control Problems,2023-08-24 13:12:56+00:00,"We present the Dirichlet-Neumann (DN) and Neumann-Neumann (NN) methods
applied to the optimal control problems arising from elliptic partial
differential equations (PDEs) under the $H^{-1}$ regularization. We use the
Lagrange multiplier approach to derive a forward-backward optimality system
with the $L^2$ regularization, and a singular perturbed Poisson equation with
the $H^{-1}$ regularization. The $H^{-1}$ regularization thus avoids solving a
coupled bi-Laplacian problem, yet the solutions are less regular. The singular
perturbed Poisson equation is then solved by using the DN and NN methods, and a
detailed analysis is given both in the one-dimensional and two-dimensional
case. Finally, we provide some numerical experiments with conclusions.",http://arxiv.org/pdf/2308.12764v1
2308.12716v1,math.NA,Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks,2023-08-24 11:31:24+00:00,"This paper explores the ability of physics-informed neural networks (PINNs)
to solve forward and inverse problems of contact mechanics for small
deformation elasticity. We deploy PINNs in a mixed-variable formulation
enhanced by output transformation to enforce Dirichlet and Neumann boundary
conditions as hard constraints. Inequality constraints of contact problems,
namely Karush-Kuhn-Tucker (KKT) type conditions, are enforced as soft
constraints by incorporating them into the loss function during network
training. To formulate the loss function contribution of KKT constraints,
existing approaches applied to elastoplasticity problems are investigated and
we explore a nonlinear complementarity problem (NCP) function, namely
Fischer-Burmeister, which possesses advantageous characteristics in terms of
optimization. Based on the Hertzian contact problem, we show that PINNs can
serve as pure partial differential equation (PDE) solver, as data-enhanced
forward model, as inverse solver for parameter identification, and as
fast-to-evaluate surrogate model. Furthermore, we demonstrate the importance of
choosing proper hyperparameters, e.g. loss weights, and a combination of Adam
and L-BFGS-B optimizers aiming for better results in terms of accuracy and
training time.",http://arxiv.org/pdf/2308.12716v1
2308.12683v1,math.NA,The key to the enhanced performance of slab-like topologically interlocked structures with non-planar blocks,2023-08-24 09:48:38+00:00,"Topologically interlocked structures are assemblies of interlocking blocks
that hold together solely through contact. Such structures have been shown to
exhibit high strength, energy dissipation, and crack arrest properties. Recent
studies on beam-like topologically interlocked structures have shown that, with
non-planar blocks, it is possible to reach levels of strength and
work-to-failure which are otherwise possible only with unrealistically high
friction coefficients. While non-planar blocks have been extensively used for
slab-like assemblies, many questions in that context are still not fully
understood. Specifically, it is unclear what are the exact characteristics of
non-planar surface morphologies which can potentially improve the enhanced
mechanical response of slab-like assemblies. In addition, it is unclear if
slab-like structures with non-planar surface blocks can reach a saturated
response with realistic friction coefficient values, as is the case with
beam-like ones. Here, we investigate such fundamental questions using numerical
simulations. We show that, by using non-planar blocks, it is possible to reach
saturation to the response capacity of the structure with a realistic friction
coefficient. Furthermore, we show that the key morphology parameter responsible
for the enhanced performance is the local angle of inclination at the top of
the loaded block. Lastly, we show that non-planar morphologies lead to improved
work-to-failure and ultimate deflection, which cannot be attained with
planar-faced blocks. These findings shed new light on topologically interlocked
structures with non-planar blocks, allowing for a better understanding of their
strengths and potential applications.",http://arxiv.org/pdf/2308.12683v1
2308.12393v1,cs.LG,Machine learning in parameter estimation of nonlinear systems,2023-08-23 19:20:24+00:00,"Accurately estimating parameters in complex nonlinear systems is crucial
across scientific and engineering fields. We present a novel approach for
parameter estimation using a neural network with the Huber loss function. This
method taps into deep learning's abilities to uncover parameters governing
intricate behaviors in nonlinear equations. We validate our approach using
synthetic data and predefined functions that model system dynamics. By training
the neural network with noisy time series data, it fine-tunes the Huber loss
function to converge to accurate parameters. We apply our method to damped
oscillators, Van der Pol oscillators, Lotka-Volterra systems, and Lorenz
systems under multiplicative noise. The trained neural network accurately
estimates parameters, evident from closely matching latent dynamics. Comparing
true and estimated trajectories visually reinforces our method's precision and
robustness. Our study underscores the Huber loss-guided neural network as a
versatile tool for parameter estimation, effectively uncovering complex
relationships in nonlinear systems. The method navigates noise and uncertainty
adeptly, showcasing its adaptability to real-world challenges.",http://arxiv.org/pdf/2308.12393v1
2308.12279v1,cs.LG,On-Manifold Projected Gradient Descent,2023-08-23 17:50:50+00:00,"This work provides a computable, direct, and mathematically rigorous
approximation to the differential geometry of class manifolds for
high-dimensional data, along with nonlinear projections from input space onto
these class manifolds. The tools are applied to the setting of neural network
image classifiers, where we generate novel, on-manifold data samples, and
implement a projected gradient descent algorithm for on-manifold adversarial
training. The susceptibility of neural networks (NNs) to adversarial attack
highlights the brittle nature of NN decision boundaries in input space.
Introducing adversarial examples during training has been shown to reduce the
susceptibility of NNs to adversarial attack; however, it has also been shown to
reduce the accuracy of the classifier if the examples are not valid examples
for that class. Realistic ""on-manifold"" examples have been previously generated
from class manifolds in the latent of an autoencoder. Our work explores these
phenomena in a geometric and computational setting that is much closer to the
raw, high-dimensional input space than can be provided by VAE or other black
box dimensionality reductions. We employ conformally invariant diffusion maps
(CIDM) to approximate class manifolds in diffusion coordinates, and develop the
Nystr\""{o}m projection to project novel points onto class manifolds in this
setting. On top of the manifold approximation, we leverage the spectral
exterior calculus (SEC) to determine geometric quantities such as tangent
vectors of the manifold. We use these tools to obtain adversarial examples that
reside on a class manifold, yet fool a classifier. These misclassifications
then become explainable in terms of human-understandable manipulations within
the data, by expressing the on-manifold adversary in the semantic basis on the
manifold.",http://arxiv.org/pdf/2308.12279v1
2308.12255v1,math.NA,Absorbing boundary conditions for the Helmholtz equation using Gauss-Legendre quadrature reduced integrations,2023-08-23 17:15:20+00:00,"We introduce a new class of absorbing boundary conditions (ABCs) for the
Helmholtz equation. The proposed ABCs can be derived from a certain simple
class of perfectly matched layers using $L$ discrete layers and using the $Q_N$
Lagrange finite element in conjunction with the $N$-point Gauss-Legendre
quadrature reduced integration rule. The proposed ABCs are classified by a
tuple $(L,N)$, and achieve reflection error of order $O(R^{2LN})$ for some
$R<1$. The new ABCs generalise the perfectly matched discrete layers proposed
by Guddati and Lim [Int. J. Numer. Meth. Engng 66 (6) (2006) 949-977],
including them as type $(L,1)$. An analysis of the proposed ABCs is performed
motivated by the work of Ainsworth [J. Comput. Phys. 198 (1) (2004) 106-130].
The new ABCs facilitate numerical implementations of the Helmholtz problem with
ABCs if $Q_N$ finite elements are used in the physical domain. Moreover, giving
more insight, the analysis presented in this work potentially aids with
developing ABCs in related areas.",http://arxiv.org/pdf/2308.12255v1
2308.12164v1,math.NA,A robust family of exponential attractors for a linear time discretization of the Cahn-Hilliard equation with a source term,2023-08-23 14:28:21+00:00,"We consider a linear implicit-explicit (IMEX) time discretization of the
Cahn-Hilliard equation with a source term, endowed with Dirichlet boundary
conditions. For every time step small enough, we build an exponential attractor
of the discrete-in-time dynamical system associated to the discretization. We
prove that, as the time step tends to 0, this attractor converges for the
symmmetric Hausdorff distance to an exponential attractor of the
continuous-in-time dynamical system associated with the PDE. We also prove that
the fractal dimension of the exponential attractor (and consequently, of the
global attractor) is bounded by a constant independent of the time step. The
results also apply to the classical Cahn-Hilliard equation with Neumann
boundary conditions.",http://arxiv.org/pdf/2308.12164v1
2308.12145v1,math.NA,Modeling excitable cells with the EMI equations: spectral analysis and iterative solution strategy,2023-08-23 14:01:07+00:00,"In this work, we are interested in solving large linear systems stemming from
the Extra-Membrane-Intra (EMI) model, which is employed for simulating
excitable tissues at a cellular scale. After setting the related systems of
partial differential equations (PDEs) equipped with proper boundary conditions,
we provide numerical approximation schemes for the EMI PDEs and focus on the
resulting large linear systems. We first give a relatively complete spectral
analysis using tools from the theory of Generalized Locally Toeplitz matrix
sequences. The obtained spectral information is used for designing appropriate
(preconditioned) Krylov solvers. We show, through numerical experiments, that
the presented solution strategy is robust w.r.t. problem and discretization
parameters, efficient and scalable.",http://arxiv.org/pdf/2308.12145v1
2308.12130v1,math.NA,Space-time hybridizable discontinuous Galerkin method for advection-diffusion on deforming domains: The advection-dominated regime,2023-08-23 13:36:12+00:00,"We analyze a space-time hybridizable discontinuous Galerkin method to solve
the time-dependent advection-diffusion equation on deforming domains. We prove
stability of the discretization in the advection-dominated regime by using
weighted test functions and derive a priori space-time error estimates. A
numerical example illustrates the theoretical results.",http://arxiv.org/pdf/2308.12130v1
2308.12104v1,math.NA,Computational Modeling of Coupled Interactions of Fluid Membranes with Embedded Filaments,2023-08-23 12:47:26+00:00,"In this work, we present a computational formulation based on continuum
mechanics to study the interaction of fluid membranes embedded with
semiflexible filaments. This is motivated by systems in membrane biology, such
as cytoskeletal networks and protein filaments aiding the cell fission process.
We model the membrane as a fluid shell via the Helfrich-Canham energy and the
filament as a one-dimensional Cosserat continuum. We assume the filament to be
tethered to the surface of the membrane in a way that it is allowed to float on
the surface freely. The novel filament-membrane coupling, which is anticipated
to yield interesting physics, also gives rise to unique computational
challenges, which we address in this work. We present validation results and
apply the formulation to certain problems inspired by cellular biology.",http://arxiv.org/pdf/2308.12104v1
2308.16159v1,physics.class-ph,Three-body periodic collisionless equal-mass free-fall orbits revisited,2023-08-30 17:28:16+00:00,"Li and Liao announced (2019) discovery of 313 periodic collisionless orbits'
initial conditions (i.c.s), 30 of which have equal masses, and 18 of these 30
orbits have physical periods (scale-invariant periods) $T^{*}=T|E|^{3/2}<80$.
That work left a lot to be desired, however, both in terms of logical
consistency and of numerical efficiency. We have conducted a new search for
periodic free-fall orbits, limited to the equal-mass case. Our search produced
24,582 i.c.s of equal-mass periodic orbits with scale-invariant period
$T^{*}<80$, corresponding to 12,409 distinct solutions, 236 of which are
self-dual.",http://arxiv.org/pdf/2308.16159v1
2308.16043v1,astro-ph.SR,COCONUT-MF: Two-fluid ion-neutral global coronal modelling,2023-08-30 14:03:47+00:00,"The global coronal model COCONUT was originally developed to replace models
such as the WSA model in space weather forecasting to improve the physical
accuracy of the predictions. This model has, however, several simplifications
implemented in its formulation to allow for rapid convergence, one of which
includes a single-fluid treatment. In this paper, we have two goals. Firstly,
we aim to introduce a novel multi-fluid global coronal model and validate it
with simple cases as well as with real data-driven applications. Secondly, we
aim to investigate to what extent considering a single-fluid plasma in the
global coronal model might affect the resulting plasma dynamics, and thus
whether the assumptions on which the single-fluid coronal model is based are
justified. We developed a multi-fluid global coronal model, COCONUT-MF, which
resolves the ion and neutral fluid equations separately. While this model is
still steady-state and thus does not resolve unsteady processes, it can account
for charge exchange, chemical and collisional contributions. We present the
results of the ion-neutral modelling for a dipole, a minimum of solar activity,
and a solar maximum. We demonstrate the higher accuracy of the applied AUSM+
scheme compared to HLL. Subsequently, we also evaluate the effects of the
considered ion-neutral coupling terms on the resulting plasma dynamics. Despite
the very low concentration of neutrals, these terms still affect the flow field
to a limited but non-negligible extent (up to 5 to 10% locally). Even though
the coronal plasma is generally assumed to be collisionless, our results show
that there is sufficient collisionality in it to couple the two fluids.
Follow-up work will include extension of the model to lower atmospheric layers
of the Sun and inclusion of more advanced physical terms such as heating and
radiation.",http://arxiv.org/pdf/2308.16043v1
2308.15967v1,physics.plasm-ph,On the Efficiency of Disorder-induced Heating of Ultracold Plasmas,2023-08-30 11:44:31+00:00,"Starting from the beginning of their research in the early 2000's, the
ultracold plasmas were considered as a promising tool to achieve considerable
values of the Coulomb coupling parameter for electrons. Unfortunately, this was
found to be precluded by a sharp spontaneous increase of temperature, which was
commonly attributed to the so-called disorder-induced heating (DIH). It is the
aim of the present paper to quantify this effect as function of the initial
ionic disorder and, thereby, to estimate the efficiency of its mitigation,
e.g., by the Rydberg blockade. As a result of the performed simulations, we
found that the dynamics of electrons exhibited a well-expressed transition from
the case of the quasi-regular arrangement of ions to the disordered one; the
magnitude of the effect being about 30%. Thereby, we can conclude that the
two-step formation of ultracold plasmas - involving the intermediate stage of
the blockaded Rydberg gas - can really serve as a tool to increase the degree
of Coulomb coupling, but the efficiency of this method is moderate.",http://arxiv.org/pdf/2308.15967v1
2308.15835v1,astro-ph.HE,Prediction and Anomaly Detection of accelerated particles in PIC simulations using neural networks,2023-08-30 08:19:53+00:00,"Acceleration processes that occur in astrophysical plasmas produce cosmic
rays that are observed on Earth. To study particle acceleration, fully-kinetic
particle-in-cell (PIC) simulations are often used as they can unveil the
microphysics of energization processes. Tracing of individual particles in PIC
simulations is particularly useful in this regard. However, by-eye inspection
of particle trajectories includes a high level of bias and uncertainty in
pinpointing specific acceleration mechanisms that affect particles. Here we
present a new approach that uses neural networks to aid individual particle
data analysis. We demonstrate this approach on the test data that consists of
252,000 electrons which have been traced in a PIC simulation of a
non-relativistic high Mach number perpendicular shock, in which we observe the
two-stream electrostatic Buneman instability to pre-accelerate a portion of
electrons to nonthermal energies. We perform classification, regression and
anomaly detection by using a Convolutional Neural Network. We show that
regardless of how noisy and imbalanced the datasets are, the regression and
classification are able to predict the final energies of particles with high
accuracy, whereas anomaly detection is able to discern between energetic and
non-energetic particles. The methodology proposed may considerably simplify
particle classification in large-scale PIC and also hybrid kinetic simulations.",http://arxiv.org/pdf/2308.15835v1
2308.15407v1,physics.flu-dyn,Bounded flows of dense gases,2023-08-29 16:08:12+00:00,"Numerical solutions of the Enskog equation obtained employing a
Finite-Difference Lattice Boltzmann (FDLB) and a Direct Simulation Monte Carlo
(DSMC)-like particle method (PM) are systematically compared to determine the
range of applicability of the simplified Enskog collision operator implemented
in the Lattice Boltzmann framework. Three types of bounded flows of dense gases
- namely the Fourier, the Couette, and the Poiseuille flows - are investigated
for a wide range of input parameters. For low to moderate reduced density, the
proposed FDLB model exhibits commendable accuracy for all bounded flows tested
in this study, with substantially lower computational cost than the PM method.",http://arxiv.org/pdf/2308.15407v1
2308.15301v2,physics.data-an,Convexity constraints on linear background models for electron energy-loss spectra,2023-08-29 13:40:15+00:00,"In this paper convexity constraints are derived for a background model of
electron energy loss spectra (EELS) that is linear in the fitting parameters.
The model outperforms a power-law both on experimental and simulated
backgrounds, especially for wide energy ranges, and thus improves elemental
quantification results. Owing to the model's linearity, the constraints can be
imposed through fitting by quadratic programming. This has important advantages
over conventional nonlinear power-law fitting such as high speed and a
guaranteed unique solution without need for initial parameters. As such, the
need for user input is significantly reduced, which is essential for
unsupervised treatment of large data sets. This is demonstrated on a demanding
spectrum image of a semiconductor device sample with a high number of elements
over a wide energy range.",http://arxiv.org/pdf/2308.15301v2
2308.14948v1,astro-ph.IM,Machine Learning for Mini-EUSO Telescope Data Analysis,2023-08-29 00:04:19+00:00,"Neural networks as well as other methods of machine learning (ML) are known
to be highly efficient in different classification tasks, including
classification of images and videos. Mini- EUSO is a wide-field-of-view imaging
telescope that operates onboard the International Space Station since 2019
collecting data on miscellaneous processes that take place in the atmosphere of
Earth in the UV range. Here we briefly present our results on the development
of ML-based approaches for recognition and classification of track-like signals
in the Mini-EUSO data, among them meteors, space debris and signals the light
curves and kinematics of which are similar to those expected from extensive air
showers generated by ultra-high-energy cosmic rays. We show that even simple
neural networks demonstrate impressive performance in solving these tasks.",http://arxiv.org/pdf/2308.14948v1
2308.14890v1,physics.chem-ph,QCMATH: Mathematica modules for electronic structure calculations,2023-08-28 20:25:32+00:00,"We introduce \textsc{qcmath}, a user-friendly quantum chemistry software
tailored for electronic structure calculations, implemented using the Wolfram
Mathematica language and available at \url{https://github.com/LCPQ/qcmath}.
This software, designed with accessibility in mind, takes advantage of the
symbolic capabilities intrinsic to Mathematica. Its primary goal is to provide
a supportive environment for newcomers to the field of quantum chemistry,
enabling them to easily conceptualize, develop, and test their own ideas. The
functionalities of \textsc{qcmath} encompass a broad spectrum of methods,
catering to both ground- and excited-state calculations. We provide a
comprehensive overview of these capabilities, complemented by essential
theoretical insights. To facilitate ease of use, we offer an exhaustive
blueprint of the software's architecture. Furthermore, we provide users with
comprehensive guides, addressing both the operational aspects and the more
intricate programming facets of \textsc{qcmath}.",http://arxiv.org/pdf/2308.14890v1
2308.14885v1,cond-mat.stat-mech,Inferring phase transitions and critical exponents from limited observations with Thermodynamic Maps,2023-08-28 20:13:39+00:00,"Phase transitions are ubiquitous across life, yet hard to quantify and
describe accurately. In this work, we develop an approach for characterizing
generic attributes of phase transitions from very limited observations made
deep within different phases' domains of stability. Our approach is called
Thermodynamic Maps, which combines statistical mechanics and molecular
simulations with score-based generative models. Thermodynamic Maps enable
learning the temperature dependence of arbitrary thermodynamic observables
across a wide range of temperatures. We show its usefulness by calculating
phase transition attributes such as melting temperature, temperature-dependent
heat capacities, and critical exponents. For instance, we demonstrate the
ability of thermodynamic maps to infer the ferromagnetic phase transition of
the Ising model, including temperature-dependent heat capacity and critical
exponents, despite never having seen samples from the transition region. In
addition, we efficiently characterize the temperature-dependent conformational
ensemble and compute melting curves of the two RNA systems GCAA tetraloop and
HIV-TAR, which are notoriously hard to sample due to glassy-like landscapes.",http://arxiv.org/pdf/2308.14885v1
2308.14789v1,hep-th,Scattering with Neural Operators,2023-08-28 18:00:00+00:00,"Recent advances in machine learning establish the ability of certain
neural-network architectures called neural operators to approximate maps
between function spaces. Motivated by a prospect of employing them in
fundamental physics, we examine applications to scattering processes in quantum
mechanics. We use an iterated variant of Fourier neural operators to learn the
physics of Schr\""odinger operators, which map from the space of initial wave
functions and potentials to the final wave functions. These deep operator
learning ideas are put to test in two concrete problems: a neural operator
predicting the time evolution of a wave packet scattering off a central
potential in $1+1$ dimensions, and the double-slit experiment in $2+1$
dimensions. At inference, neural operators can become orders of magnitude more
efficient compared to traditional finite-difference solvers.",http://arxiv.org/pdf/2308.14789v1
2308.14788v1,quant-ph,Floquet Topology Stabilized with Non-Hermitian Driving,2023-08-28 17:55:27+00:00,"This study presents a mechanism that enables the stabilization of Floquet
systems indefintely; albeit in a manner that allows for noise during each
Floquet cycle. This is due to the fact that external qubits are added after
each Floquet cycle and these external qubits obtain information about the
Floquet system (in this case the Floquet system is the Anomalous
Floquet-Anderson Insulator). This information is used to correct the system for
noise after which these qubits are removed. The fact that these external qubits
are added and then removed after performing operations on the system is what
allows for this process to be referred to as non-Hermitian driving. The
external qubits effectively act to carry away entropy of the system and
therefore allow for the Floquet system to be cooled. In addition, another
mechanism is found where the periodic implementation of entanglement for every
site of the system at a high frequency during the normal time evolution allows
for the system to be highly localized in a manner similar to Anderson
localization.",http://arxiv.org/pdf/2308.14788v1
2308.14618v1,physics.chem-ph,Seniority and Hierarchy Configuration Interaction for Radicals and Excited States,2023-08-28 14:42:13+00:00,"Hierarchy configuration interaction (hCI) has been recently introduced as an
alternative configuration interaction (CI) route combining excitation degree
and seniority number, which showed to efficiently recover both dynamic and
static correlations for closed-shell molecular systems
[\href{https://doi.org/10.1021/acs.jpclett.2c00730}{\textit{J.~Phys.~Chem.~Lett.}~\textbf{2022},
\textit{13}, 4342}]. Here, we generalize hCI for an arbitrary reference
determinant, allowing calculations for radicals and for excited states in a
state-specific way. We gauge this route against excitation-based CI (eCI) and
seniority-based CI (sCI) by evaluating how different ground-state properties of
radicals converge to the full CI limit. We find that hCI outperforms or matches
eCI, whereas sCI is far less accurate, in line with previous observations for
closed-shell molecules. Employing the second-order Epstein-Nesbet perturbation
theory as a correction significantly accelerates the convergence of hCI and
eCI. We further explore various hCI and sCI models to calculate excitation
energies of closed- and open-shell systems. Our results underline that both the
choice of the reference determinant and the set of orbitals drive the fine
balance between correlation of ground and excited states. State-specific hCI2
and higher order models perform similarly to their eCI counterparts, whereas
lower orders of hCI deliver poor results. In turn, sCI1 produces decent
excitation energies for radicals, encouraging the development of related
seniority-based coupled cluster methods.",http://arxiv.org/pdf/2308.14618v1
2308.14239v1,quant-ph,Quantum Next Generation Reservoir Computing: An Efficient Quantum Algorithm for Forecasting Quantum Dynamics,2023-08-28 00:34:40+00:00,"Next Generation Reservoir Computing (NG-RC) is a modern class of model-free
machine learning that enables an accurate forecasting of time series data
generated by dynamical systems. We demonstrate that NG-RC can accurately
predict full many-body quantum dynamics, instead of merely concentrating on the
dynamics of observables, which is the conventional application of reservoir
computing. In addition, we apply a technique which we refer to as skipping
ahead to predict far future states accurately without the need to extract
information about the intermediate states. However, adopting a classical NG-RC
for many-body quantum dynamics prediction is computationally prohibitive due to
the large Hilbert space of sample input data. In this work, we propose an
end-to-end quantum algorithm for many-body quantum dynamics forecasting with a
quantum computational speedup via the block-encoding technique. This proposal
presents an efficient model-free quantum scheme to forecast quantum dynamics
coherently, bypassing inductive biases incurred in a model-based approach.",http://arxiv.org/pdf/2308.14239v1
2308.13863v1,physics.comp-ph,Full-scale ab initio simulations of laser-driven atomistic dynamics,2023-08-26 12:46:45+00:00,"The coupling of excited states and ionic dynamics is the basic and
challenging point for the materials response at extreme conditions. In
laboratory, the intense laser produces transient nature and complexity with
highly nonequilibrium states, making it extremely difficult and interesting for
both experimental measurements and theoretical methods. With the inclusion of
laser-excited states, we extended ab initio method into the direct simulations
of whole laser-driven microscopic dynamics from solid to liquid. We constructed
the framework of combining the electron-temperaturedependent deep neural
network potential energy surface with hybrid atomistic-continuum approach,
controlling non-adiabatic energy exchange and atomistic dynamics, which enables
consistent interpretation of experimental data. By large scale ab inito
simulations, we demonstrate that the nonthermal effects introduced by hot
electrons play a dominant role in modulating the lattice dynamics,
thermodynamic pathway, and structural transformation. We highlight that the
present work provides a path to realistic computational studies of laser-driven
processes, thus bridging the gap between experiments and simulations.",http://arxiv.org/pdf/2308.13863v1
2308.13727v1,physics.plasm-ph,Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: II. dynamics forecasting,2023-08-26 01:48:29+00:00,"In part I of the article, we demonstrated that a variant of the Dynamic Mode
Decomposition (DMD) algorithm based on variable projection optimization, called
Optimized DMD (OPT-DMD), enables a robust identification of the dominant
spatiotemporally coherent modes underlying the data across various test cases
representing different physical parameters in an ExB simulation configuration.
As the OPT-DMD can be constrained to produce stable reduced-order models (ROMs)
by construction, in this paper, we extend the application of the OPT-DMD and
investigate the capabilities of the linear ROM from this algorithm toward
forecasting in time of the plasma dynamics in configurations representative of
the radial-azimuthal and axial-azimuthal cross-sections of a Hall thruster and
over a range of simulation parameters in each test case. The predictive
capacity of the OPT-DMD ROM is assessed primarily in terms of short-term
dynamics forecast or, in other words, for large ratios of training-to-test
data. However, the utility of the ROM for long-term dynamics forecasting is
also presented for an example case in the radial-azimuthal configuration. The
model's predictive performance is heterogeneous across various test cases.
Nonetheless, a remarkable predictiveness is observed in the test cases that do
not exhibit highly transient behaviors. Moreover, in all investigated cases,
the error between the ground-truth and the reconstructed data from the OPT-DMD
ROM remains bounded over time within both the training and the test window. As
a result, despite its limitation in terms of generalized applicability to all
plasma conditions, the OPT-DMD is proven as a reliable method to develop low
computational cost and highly predictive data-driven reduced-order models in
systems with a quasi-periodic global evolution of the plasma state.",http://arxiv.org/pdf/2308.13727v1
2308.13726v1,physics.plasm-ph,Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: I. Extraction of spatiotemporally coherent patterns,2023-08-26 01:37:52+00:00,"In this two-part article, we evaluate the utility and the generalizability of
the Dynamic Mode Decomposition (DMD) algorithm for data-driven analysis and
reduced-order modelling of plasma dynamics in cross-field ExB configurations.
The DMD algorithm is an interpretable data-driven method that finds a best-fit
linear model describing the time evolution of spatiotemporally coherent
structures (patterns) in data. We have applied the DMD to extensive
high-fidelity datasets generated using a particle-in-cell (PIC) code based on a
cost-efficient reduced-order PIC scheme. In this part, we first provide an
overview of the concept of DMD and its underpinning Proper Orthogonal and
Singular Value Decomposition methods. Two of the main DMD variants are next
introduced. We then present and discuss the results of the DMD application in
terms of the identification and extraction of the dominant spatiotemporal modes
from high-fidelity data over a range of simulation conditions. We demonstrate
that the DMD variant based on variable projection optimization (OPT-DMD)
outperforms the basic DMD method in identification of the modes underlying the
data, leading to notably more reliable reconstruction of the ground-truth.
Furthermore, we show in multiple test cases that the discrete frequency
spectrum of OPT-DMD-extracted modes is consistent with the temporal spectrum
from the Fast Fourier Transform of the data. This observation implies that the
OPT-DMD augments the conventional spectral analyses by being able to uniquely
reveal the spatial structure of the dominant modes in the frequency spectra,
thus, yielding more accessible, comprehensive information on the spatiotemporal
characteristics of the plasma phenomena.",http://arxiv.org/pdf/2308.13726v1
2308.13692v1,cond-mat.mtrl-sci,Enhanced Spin Hall Ratio in Two-Dimensional III-V Semiconductors,2023-08-25 22:21:27+00:00,"Spin Hall effect plays a critical role in spintronics since it can convert
charge current to spin current. Using state-of-the-art ab initio calculations
including quadrupole and spin-orbit coupling, the charge and spin transports
have been investigated in pristine and doped two-dimensional III-V
semiconductors. Valence bands induce a strong scattering which limits charge
conductivity in the hole-doped system, where spin Hall conductivity is enhanced
by the spin-orbit splitting, yielding an ultrahigh spin Hall ratio
$\xi\approx0.9$ in GaAs monolayer at room temperature.",http://arxiv.org/pdf/2308.13692v1
2308.13432v1,physics.acc-ph,Dephasingless laser wakefield acceleration in the bubble regime,2023-08-25 15:25:17+00:00,"Laser wakefield accelerators (LWFAs) have electric fields that are orders of
magnitude larger than those of conventional accelerators, promising an
attractive, small-scale alternative for next-generation light sources and
lepton colliders. The maximum energy gain in a single-stage LWFA is limited by
dephasing, which occurs when the trapped particles outrun the accelerating
phase of the wakefield. Here, we demonstrate that a single space-time
structured laser pulse can be used for ionization injection and electron
acceleration over many dephasing lengths in the bubble regime. Simulations of a
dephasingless laser wakefield accelerator driven by a 6.2-J laser pulse show 25
pC of injected charge accelerated over 20 dephasing lengths (1.3 cm) to a
maximum energy of 2.1 GeV. The space-time structured laser pulse features an
ultrashort, programmable-trajectory focus. Accelerating the focus, reducing the
focused spot-size variation, and mitigating unwanted self-focusing stabilize
the electron acceleration, which improves beam quality and leads to projected
energy gains of 125 GeV in a single, sub-meter stage driven by a 500-J pulse.",http://arxiv.org/pdf/2308.13432v1
2308.13299v1,physics.flu-dyn,Microstructure-based prediction of hydrodynamic forces in stationary particle assemblies,2023-08-25 10:50:17+00:00,"In the work, we derive novel hydrodynamic force models to describe the
interaction of a flow with particles in an assembly when only an averaged
resolution of the flow is available. These force models are able to predict the
average drag on the particle assembly, as well as the deviations from the
average drag force and the lift force for each individual particle in the
assembly. To achieve this, PR-DNS of various particle assemblies and flow
regimes are carried out, varying the particle volume fraction up to 0.6, and
the mean particle flow Reynolds number up to 300. To characterize the structure
of the particles in the assembly, a Voronoi tessellation is carried out, and a
number of scalars, vectors and tensors are defined based upon this
tessellation. The microstructure informed hydrodynamic force models are based
on symbolic regressions of these quantities derived from the Voronoi
tessellation, the global particle volume fraction of the particle assembly and
the flow regime represented by the Reynolds number, and the forces on the
individual particles in the assembly.
  The resulting hydrodynamic force models are single expressions can be
directly employed in a Lagrangian particle tracking (LPT) or computational
fluid dynamics/discrete element model (CFD/DEM) framework. By comparing the
results of the newly proposed hydrodynamic force models with an averaged force
model, as is usually adopted in Lagrangian particle tracking simulations, we
show that a significant increase in accuracy can be achieved, without
significantly increasing the cost of the simulation.",http://arxiv.org/pdf/2308.13299v1
2308.13280v1,physics.ao-ph,AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning,2023-08-25 10:02:26+00:00,"The atmosphere affects humans in a multitude of ways, from loss of life due
to adverse weather effects to long-term social and economic impacts on
societies. Computer simulations of atmospheric dynamics are, therefore, of
great importance for the well-being of our and future generations. Here, we
propose AtmoRep, a novel, task-independent stochastic computer model of
atmospheric dynamics that can provide skillful results for a wide range of
applications. AtmoRep uses large-scale representation learning from artificial
intelligence to determine a general description of the highly complex,
stochastic dynamics of the atmosphere from the best available estimate of the
system's historical trajectory as constrained by observations. This is enabled
by a novel self-supervised learning objective and a unique ensemble that
samples from the stochastic model with a variability informed by the one in the
historical record. The task-independent nature of AtmoRep enables skillful
results for a diverse set of applications without specifically training for
them and we demonstrate this for nowcasting, temporal interpolation, model
correction, and counterfactuals. We also show that AtmoRep can be improved with
additional data, for example radar observations, and that it can be extended to
tasks such as downscaling. Our work establishes that large-scale neural
networks can provide skillful, task-independent models of atmospheric dynamics.
With this, they provide a novel means to make the large record of atmospheric
observations accessible for applications and for scientific inquiry,
complementing existing simulations based on first principles.",http://arxiv.org/pdf/2308.13280v1
2308.13222v1,physics.comp-ph,Bayesian Reasoning for Physics Informed Neural Networks,2023-08-25 07:38:50+00:00,"Physics informed neural network (PINN) approach in Bayesian formulation is
presented. We adopt the Bayesian neural network framework formulated by MacKay
(Neural Computation 4 (3) (1992) 448). The posterior densities are obtained
from Laplace approximation. For each model (fit), the so-called evidence is
computed. It is a measure that classifies the hypothesis. The most optimal
solution has the maximal value of the evidence. The Bayesian framework allows
us to control the impact of the boundary contribution to the total loss.
Indeed, the relative weights of loss components are fine-tuned by the Bayesian
algorithm. We solve heat, wave, and Burger's equations. The obtained results
are in good agreement with the exact solutions. All solutions are provided with
the uncertainties computed within the Bayesian framework.",http://arxiv.org/pdf/2308.13222v1
2308.13096v1,cond-mat.mtrl-sci,Electronic Structure Prediction of Multi-million Atom Systems Through Uncertainty Quantification Enabled Transfer Learning,2023-08-24 21:41:29+00:00,"The ground state electron density - obtainable using Kohn-Sham Density
Functional Theory (KS-DFT) simulations - contains a wealth of material
information, making its prediction via machine learning (ML) models attractive.
However, the computational expense of KS-DFT scales cubically with system size
which tends to stymie training data generation, making it difficult to develop
quantifiably accurate ML models that are applicable across many scales and
system configurations. Here, we address these fundamental challenges using
Bayesian neural networks and employ transfer learning to leverage the
multi-scale nature of the training data. Our ML models employ descriptors
involving simple scalar products, comprehensively sample system configurations
through thermalization, and quantify uncertainty in electron density
predictions. We show that our models incur significantly lower data generation
costs while allowing confident - and when verifiable, accurate - predictions
for a wide variety of bulk systems well beyond training, including systems with
defects, different alloy compositions, and at unprecedented, multi-million-atom
scales.",http://arxiv.org/pdf/2308.13096v1
2308.12754v1,cond-mat.soft,PyMembrane: A flexible framework for efficient simulations of elastic and liquid membranes,2023-08-24 13:01:12+00:00,"PyMembrane is a software package for simulating liquid and elastic membranes
using a discretisation of the continuum description based on unstructured
triangulated two-dimensional meshes embedded in three-dimensional space. The
package is written in C++, with a flexible and intuitive Python interface,
allowing for a quick setup, execution and analysis of complex simulations.
PyMembrane follows modern software engineering principles and features a
modular design that allows for straightforward implementation of custom
extensions while ensuring consistency and enabling inexpensive maintenance. A
hallmark feature of this design is the use of a standardized C++ interface
which streamlines adding new functionalities. Furthermore, PyMembrane uses data
structures optimised for unstructured meshes, ensuring efficient mesh
operations and force calculations. By providing several templates for typical
simulations supplemented by extensive documentation, the users can seamlessly
set up and run research-level simulations and extend the package to integrate
additional features, underscoring PyMembrane's commitment to user-centric
design.",http://arxiv.org/pdf/2308.12754v1
2308.13555v1,cond-mat.stat-mech,Probabilistic description of dissipative chaotic scattering,2023-08-24 12:19:32+00:00,"We investigate the extent to which the probabilistic properties of a chaotic
scattering system with dissipation can be understood from the properties of the
dissipation-free system. For large energies $E$, a fully chaotic scattering
leads to an exponential decay of the survival probability $P(t) \sim e^{-\kappa
t}$ with an escape rate $\kappa$ that decreases with $E$. Dissipation
$\gamma>0$ leads to the appearance of different finite-time regimes in $P(t)$.
We show how these different regimes can be understood for small $\gamma\ll 1$
and $t\gg 1/\kappa_0$ from the effective escape rate
$\kappa_\gamma(t)=\kappa_0(E(t))$ (including the non-hyperbolic regime) until
the energy reaches a critical value $E_c$ at which no escape is possible. More
generally, we argue that for small dissipation $\gamma$ and long times $t$ the
surviving trajectories in the dissipative system are distributed according to
the conditionally invariant measure of the conservative system at the
corresponding energy $E(t)<E(0)$. Quantitative predictions of our general
theory are compared with numerical simulations in the Henon-Heiles model.",http://arxiv.org/pdf/2308.13555v1
2308.12717v1,physics.chem-ph,Erfonium: A Hooke Atom with Soft Interaction Potential,2023-08-24 11:31:55+00:00,"Properties of erfonium, a Hooke atom with the Coulomb interaction potential
$1/r$ replaced by a non-singular $\text{erf}(\mu r)/r$ potential are
investigated. The structure of the Hooke atom potential and properties of its
energy spectrum, relative to the ones of the spherical harmonic oscillator and
of harmonium, are analyzed. It is shown, that at a certain value of $\mu$ the
system changes its behavior from a harmonium-like regime to a
harmonic-oscillator-like regime.",http://arxiv.org/pdf/2308.12717v1
2308.12358v1,cond-mat.str-el,variPEPS -- a versatile tensor network library for variational ground state simulations in two spatial dimensions,2023-08-23 18:03:14+00:00,"Tensor networks capture large classes of ground states of phases of quantum
matter faithfully and efficiently. Their manipulation and contraction has
remained a challenge over the years, however. For most of the history, ground
state simulations of two-dimensional quantum lattice systems using (infinite)
projected entangled pair states have relied on what is called a time-evolving
block decimation. In recent years, multiple proposals for the variational
optimization of the quantum state have been put forward, overcoming accuracy
and convergence problems of previously known methods. The incorporation of
automatic differentiation in tensor networks algorithms has ultimately enabled
a new, flexible way for variational simulation of ground states and excited
states. In this work, we review the state of the art of the variational iPEPS
framework. We present and explain the functioning of an efficient,
comprehensive and general tensor network library for the simulation of infinite
two-dimensional systems using iPEPS, with support for flexible unit cells and
different lattice geometries.",http://arxiv.org/pdf/2308.12358v1
2308.12206v1,cond-mat.mtrl-sci,"Plastic deformation mechanisms during nanoindentation of W, Mo, V body-centered cubic single crystals and their corresponding W-Mo, W-V equiatomic random solid solutions",2023-08-23 15:43:21+00:00,"Deformation plasticity mechanisms in alloys and compounds may unveil the
material capacity towards optimal mechanical properties. We conduct a series of
molecular dynamics (MD) simulations to investigate plasticity mechanisms due to
nanoindentation in pure tungsten, molybdenum and vanadium body-centered cubic
single crystals, as well as the also body-centered cubic, equiatomic, random
solid solutions (RSS) of tungsten--molybdenum and tungsten--vanadium alloys.
Our analysis focuses on a thorough, side-by-side comparison of dynamic
deformation processes, defect nucleation, and evolution, along with
corresponding stress--strain curves. We also check the surface morphology of
indented samples through atomic shear strain mapping. As expected, the presence
of Mo and V atoms in W matrices introduces lattice strain and distortion,
increasing material resistance to deformation and slowing down dislocation
mobility of dislocation loops with a Burgers vector of 1/2 $\langle 111
\rangle$. Our side-by-side comparison displays a remarkable suppression of the
plastic zone size in equiatomic W--V RSS, but not in equiatomic W--Mo RSS
alloys, displaying a clear prediction for optimal hardening response equiatomic
W--V RSS alloys. If the small-depth nanoindentation plastic response is
indicative of overall mechanical performance, it is possible to conceive a
novel MD-based pathway towards material design for mechanical applications in
complex, multi-component alloys.",http://arxiv.org/pdf/2308.12206v1
2308.16181v1,physics.optics,Fully Non-Linear Neuromorphic Computing with Linear Wave Scattering,2023-08-30 17:58:06+00:00,"The increasing complexity of neural networks and the energy consumption
associated with training and inference create a need for alternative
neuromorphic approaches, e.g. using optics. Current proposals and
implementations rely on physical non-linearities or opto-electronic conversion
to realise the required non-linear activation function. However, there are
significant challenges with these approaches related to power levels, control,
energy-efficiency, and delays. Here, we present a scheme for a neuromorphic
system that relies on linear wave scattering and yet achieves non-linear
processing with a high expressivity. The key idea is to inject the input via
physical parameters that affect the scattering processes. Moreover, we show
that gradients needed for training can be directly measured in scattering
experiments. We predict classification accuracies on par with results obtained
by standard artificial neural networks. Our proposal can be readily implemented
with existing state-of-the-art, scalable platforms, e.g. in optics, microwave
and electrical circuits, and we propose an integrated-photonics implementation
based on racetrack resonators that achieves high connectivity with a minimal
number of waveguide crossings.",http://arxiv.org/pdf/2308.16181v1
2308.15326v1,physics.soc-ph,Dynamical heterogeneity and universality of power-grids,2023-08-29 14:21:58+00:00,"While weak, tuned asymmetry can improve, strong heterogeneity destroys
synchronization in the electric power system. We study the level of
heterogeneity, by comparing large high voltage (HV) power-grids of Europe and
North America. We provide an analysis of power capacities and loads of various
energy sources from the databases and found heavy tailed distributions with
similar characteristics. Graph topological measures, community structures also
exhibit strong similarities, while the cable admittance distributions can be
well fitted with the same power-laws (PL), related to the length distributions.
The community detection analysis shows the level of synchronization in
different domains of the European HV power grids, by solving a set of swing
equations. We provide numerical evidence for frustrated synchronization and
Chimera states and point out the relation of topology and level of
synchronization in the subsystems. We also provide empirical data analysis of
the frequency heterogeneities within the Hungarian HV network and find
q-Gaussian distributions related to super-statistics of time-lagged
fluctuations, which agree well with former results on the Nordic Grid.",http://arxiv.org/pdf/2308.15326v1
2308.15290v1,hep-ex,A flexible and efficient approach for missing transverse momentum reconstruction,2023-08-29 13:25:24+00:00,"Missing transverse momentum is a crucial observable for physics at hadron
colliders, being the only constraint on the kinematics of ""invisible"" objects
such as neutrinos and hypothetical dark matter particles. Computing missing
transverse momentum at the highest possible precision, particularly in
experiments at the energy frontier, can be a challenging procedure due to
ambiguities in the distribution of energy and momentum between many
reconstructed particle candidates. This paper describes a novel solution for
efficiently encoding information required for the computation of missing
transverse momentum given arbitrary selection criteria for the constituent
reconstructed objects. Pileup suppression using information from both the
calorimeter and the inner detector is an integral component of the
reconstruction procedure. Energy calibration and systematic variations are
naturally supported. Following this strategy, the ATLAS Collaboration has been
able to optimise the use of missing transverse momentum in diverse analyses
throughout Runs 2 and 3 of the Large Hadron Collider and for future analyses.",http://arxiv.org/pdf/2308.15290v1
2308.14407v1,physics.optics,Identifying topology of leaky photonic lattices with machine learning,2023-08-28 08:42:06+00:00,"We show how machine learning techniques can be applied for the classification
of topological phases in leaky photonic lattices using limited measurement
data. We propose an approach based solely on bulk intensity measurements, thus
exempt from the need for complicated phase retrieval procedures. In particular,
we design a fully connected neural network that accurately determines
topological properties from the output intensity distribution in dimerized
waveguide arrays with leaky channels, after propagation of a spatially
localized initial excitation at a finite distance, in a setting that closely
emulates realistic experimental conditions.",http://arxiv.org/pdf/2308.14407v1
2308.13636v2,cond-mat.stat-mech,Non-parametric learning critical behavior in Ising partition functions: PCA entropy and intrinsic dimension,2023-08-25 19:06:22+00:00,"We provide and critically analyze a framework to learn critical behavior in
classical partition functions through the application of non-parametric methods
to data sets of thermal configurations. We illustrate our approach in phase
transitions in 2D and 3D Ising models. First, we extend previous studies on the
intrinsic dimension of 2D partition function data sets, by exploring the effect
of volume in 3D Ising data. We find that as opposed to 2D systems for which
this quantity has been successfully used in unsupervised characterizations of
critical phenomena, in the 3D case its estimation is far more challenging. To
circumvent this limitation, we then use the principal component analysis (PCA)
entropy, a ""Shannon entropy"" of the normalized spectrum of the covariance
matrix. We find a striking qualitative similarity to the thermodynamic entropy,
which the PCA entropy approaches asymptotically. The latter allows us to
extract -- through a conventional finite-size scaling analysis with modest
lattice sizes -- the critical temperature with less than $1\%$ error for both
2D and 3D models while being computationally efficient. The PCA entropy can
readily be applied to characterize correlations and critical phenomena in a
huge variety of many-body problems and suggests a (direct) link between
easy-to-compute quantities and entropies.",http://arxiv.org/pdf/2308.13636v2
2308.13604v2,cond-mat.dis-nn,Network science Ising states of matter,2023-08-25 18:01:03+00:00,"Network science provides very powerful tools for extracting information from
interacting data. Although recently the unsupervised detection of phases of
matter using machine learning has raised significant interest, the full
prediction power of network science has not yet been systematically explored in
this context. Here we fill this gap by providing an in-depth statistical,
combinatorial, geometrical and topological characterization of 2D Ising
snapshot networks (IsingNets) extracted from Monte Carlo simulations of the 2D
Ising model at different temperatures, going across the phase transition. Our
analysis reveals the complex organization properties of IsingNets in both the
ferromagnetic and paramagnetic phases and demonstrates the significant
deviations of the IsingNets with respect to randomized null models. In
particular percolation properties of the IsingNets reflect the existence of the
symmetry between configurations with opposite magnetization below the critical
temperature and the very compact nature of the two emerging giant clusters
revealed by our persistent homology analysis of the IsingNets. Moreover, the
IsingNets display a very broad degree distribution and significant
degree-degree correlations and weight-degree correlations demonstrating that
they encode relevant information present in the configuration space of the 2D
Ising model. The geometrical organization of the critical IsingNets is
reflected in their spectral properties deviating from the one of the null
model. This work reveals the important insights that network science can bring
to the characterization of phases of matter. The set of tools described hereby
can be applied as well to numerical and experimental data.",http://arxiv.org/pdf/2308.13604v2
2308.15490v1,physics.data-an,A method of maximum likelihood fit to data with non-uniform efficiencies,2023-08-25 07:10:27+00:00,"Estimations of physical parameters using data usually involve non-uniform
experimental efficiencies. In this article, a method of maximum likelihood fit
is introduced using the efficiency as a weight, while the probability
distribution function is kept unaffected by the efficiency. A brief proof and
pseudo-experiment studies suggest that this method gives unbiased estimation of
parameters. For cases where the probability distribution function can be
normalized analytically, this method significant reduces the usage of computing
resources.",http://arxiv.org/pdf/2308.15490v1
2308.13117v1,physics.data-an,Probabilistic Mixture Model-Based Spectral Unmixing,2023-08-24 23:46:16+00:00,"Identifying pure components in mixtures is a common yet challenging problem.
This unmixing process requires that mixing preserves the identity of the
components (endmembers), i.e., mixing is linear, and the endmembers must be
spectrally distinct. Even with these requirements met, extracting the
endmembers from a single mixture may be impossible; an ensemble of mixtures
with sufficient diversity is needed. Several spectral unmixing approaches have
been proposed, many of which are connected to hyperspectral imaging. However,
most of them assume highly diverse collections of mixtures and extremely
low-loss spectroscopic measurements. Additionally, these frameworks do not
incorporate the uncertainty inherent in unmixing. We propose a probabilistic
inference approach that explicitly incorporates noise and uncertainty, enabling
us to unmix endmembers in collections of mixtures with limited diversity. We
use a Bayesian mixture model to jointly extract endmember spectra and mixing
parameters while explicitly modeling observation noise and the resulting
inference uncertainties. We obtain approximate distributions over endmember
coordinates for each set of observed spectra while remaining robust to
inference biases from the lack of pure observations and presence of
non-isotropic Gaussian noise. Access to reliable uncertainties on the unmixing
solutions would enable robust solutions as well as informed decision making.",http://arxiv.org/pdf/2308.13117v1
2308.13028v1,quant-ph,Training Neural Networks with Universal Adiabatic Quantum Computing,2023-08-24 18:51:50+00:00,"The training of neural networks (NNs) is a computationally intensive task
requiring significant time and resources. This paper presents a novel approach
to NN training using Adiabatic Quantum Computing (AQC), a paradigm that
leverages the principles of adiabatic evolution to solve optimisation problems.
We propose a universal AQC method that can be implemented on gate quantum
computers, allowing for a broad range of Hamiltonians and thus enabling the
training of expressive neural networks. We apply this approach to various
neural networks with continuous, discrete, and binary weights. Our results
indicate that AQC can very efficiently find the global minimum of the loss
function, offering a promising alternative to classical training methods.",http://arxiv.org/pdf/2308.13028v1
2308.13027v1,quant-ph,Efficient characterization of blinking quantum emitters from scarce data sets via machine learning,2023-08-24 18:51:30+00:00,"Single photon emitters are core building blocks of quantum technologies, with
established and emerging applications ranging from quantum computing and
communication to metrology and sensing. Regardless of their nature, quantum
emitters universally display fluorescence intermittency or photoblinking:
interaction with the environment can cause the emitters to undergo quantum
jumps between on and off states that correlate with higher and lower
photoemission events, respectively. Understanding and quantifying the mechanism
and dynamics of photoblinking is important for both fundamental and practical
reasons. However, the analysis of blinking time traces is often afflicted by
data scarcity. Blinking emitters can photo-bleach and cease to fluoresce over
time scales that are too short for their photodynamics to be captured by
traditional statistical methods. Here, we demonstrate two approaches based on
machine learning that directly address this problem. We present a multi-feature
regression algorithm and a genetic algorithm that allow for the extraction of
blinking on/off switching rates with >85% accuracy, and with >10x less data and
>20x higher precision than traditional methods based on statistical inference.
Our algorithms effectively extend the range of surveyable blinking systems and
trapping dynamics to those that would otherwise be considered too short-lived
to be investigated. They are therefore a powerful tool to help gain a better
understanding of the physical mechanism of photoblinking, with practical
benefits for applications based on quantum emitters that rely on either
mitigating or harnessing the phenomenon.",http://arxiv.org/pdf/2308.13027v1
2308.12724v1,hep-ex,Jet energy calibration with deep learning as a Kubeflow pipeline,2023-08-24 12:02:09+00:00,"Precise measurements of the energy of jets emerging from particle collisions
at the LHC are essential for a vast majority of physics searches at the CMS
experiment. In this study, we leverage well-established deep learning models
for point clouds and CMS open data to improve the energy calibration of
particle jets. To enable production-ready machine learning based jet energy
calibration an end-to-end pipeline is built on the Kubeflow cloud platform. The
pipeline allowed us to scale up our hyperparameter tuning experiments on cloud
resources, and serve optimal models as REST endpoints. We present the results
of the parameter tuning process and analyze the performance of the served
models in terms of inference time and overhead, providing insights for future
work in this direction. The study also demonstrates improvements in both flavor
dependence and resolution of the energy response when compared to the standard
jet energy corrections baseline.",http://arxiv.org/pdf/2308.12724v1
2308.15678v1,q-bio.QM,Understanding step selection analysis through numerical integration,2023-08-30 00:26:54+00:00,"Step selection functions (SSFs) are flexible models to jointly describe
animals' movement and habitat preferences. Their popularity has grown rapidly
and extensions have been developed to increase their utility, including various
distributions to describe movement constraints, interactions to allow movements
to depend on local environmental features, and random effects and latent states
to account for within- and among-individual variability. Although the SSF is a
relatively simple statistical model, its presentation has not been consistent
in the literature, leading to confusion about model flexibility and
interpretation. We believe that part of the confusion has arisen from the
conflation of the SSF model with the methods used for parameter estimation.
Notably, conditional logistic regression can be used to fit SSFs in exponential
form, and this approach is often presented interchangeably with the actual
model (the SSF itself). However, reliance on conditional logistic regression
reduces model flexibility, and suggests a misleading interpretation of step
selection analysis as being equivalent to a case-control study. In this review,
we explicitly distinguish between model formulation and inference technique,
presenting a coherent framework to fit SSFs based on numerical integration and
maximum likelihood estimation. We provide an overview of common numerical
integration techniques, and explain how they relate to step selection analyses.
This framework unifies different model fitting techniques for SSFs, and opens
the way for improved inference. In particular, it makes it straightforward to
model movement with distributions outside the exponential family, and to apply
different SSF formulations to a data set and compare them with AIC. By
separating the model formulation from the inference technique, we hope to
clarify many important concepts in step selection analysis.",http://arxiv.org/pdf/2308.15678v1
2308.15597v1,q-bio.QM,The Quantitative Genetics of Human Disease: 1 Foundations,2023-08-29 19:42:18+00:00,"In this the first of an anticipated four paper series, fundamental results of
quantitative genetics are presented from a first principles approach. While
none of these results are in any sense new, they are presented in extended
detail to precisely distinguish between definition and assumption, with a
further emphasis on distinguishing quantities from their usual approximations.
Terminology frequently encountered in the field of human genetic disease
studies will be defined in terms of their quantitive genetics form. Methods for
estimation of both quantitative genetics and the related human genetics
quantities will be demonstrated. While practitioners in the field of human
quantitative disease studies may find this work pedantic in detail, the
principle target audience for this work is trainees reasonably familiar with
population genetics theory, but with less experience in its application to
human disease studies. We introduce much of this formalism because in later
papers in this series, we demonstrate that common areas of confusion in human
disease studies can be resolved be appealing directly to these formal
definitions. The second paper in this series will discuss polygenic risk
scores. The third paper will concern the question of ""missing"" heritability and
the role interactions may play. The fourth paper will discuss sexually
dimorphic disease and the potential role of the X chromosome.",http://arxiv.org/pdf/2308.15597v1
2308.15510v1,q-bio.QM,Improving homology-directed repair by small molecule agents for genetic engineering in unconventional yeast? -- Learning from the engineering of mammalian systems,2023-08-29 14:26:13+00:00,"The ability to precisely edit genomes by deleting or adding genetic
information enables the study of biological functions and the building of
efficient cell factories. In many unconventional yeasts, such as promising new
hosts for cell factory design but also human pathogenic yeasts and food
spoilers, this progress has been limited by the fact that most yeasts favor
non-homologous end joining (NHEJ) over homologous recombination (HR) as DNA
repair mechanism, impairing genetic access to these hosts. In mammalian cells,
small molecules that either inhibit proteins involved in NHEJ, enhance protein
function in HR, or molecules that arrest the cell cycle in HR-dominant phases
are regarded as promising agents for the simple and transient increase of
HR-mediated genome editing without the need for a priori host engineering. Only
a few of these chemicals have been applied to the engineering of yeast although
the targeted proteins are mostly conserved; making chemical agents a yet
underexplored area in enhancing yeast engineering. Here, we consolidate
knowledge of available small molecules that have been used to improve HR
efficiency in mammalian cells and the few ones that have been used in yeast. We
include available high throughput (HTP)-compatible NHEJ/HR quantification
assays that could be used to screen for and isolate yeast-specific inhibitors.",http://arxiv.org/pdf/2308.15510v1
2308.15127v1,physics.med-ph,Nanozyme-based biosensing for clinical diagnosis of COVID-19: A mini review,2023-08-29 08:51:38+00:00,"Several clinical methods had been utilized for diagnosis of COVID-19 for
instance, real-time reverse transcription-polymerase chain reaction (rRT-PCR),
hematology examination, polymerase chain reaction (PCR), diagnostic guidelines
based on clinical features, and Chest CT scans. However, the accurate current
methods are time-consuming and expensive and other methods are inaccurate. To
solve these drawbacks, nanozyme-based sensors have been developed for the
reliable, accurate, and rapid detection of SARS-CoV-2. The main basis of these
sensors is the detection of color variation of a nanozyme-mediated oxidation
reaction in the presence and the absence of antigens of COVID-19. Besides, some
of methods are based on probing the fluorescence of these systems as the
clinical signal toward detection of SARS-CoV-2. This mini review focused on
overviewing the nanozymes-based methods toward COVID-19 diagnosis. The
historical background of COVID-19 was reviewed. Thereafter, the biomedical
applications of nanozymes was discussed and finally, the recent progress of
early diagnosis of COVID-19 based on nanozymatic systems was briefly reviewed.",http://arxiv.org/pdf/2308.15127v1
2308.15088v1,eess.IV,Using deep learning for an automatic detection and classification of the vascular bifurcations along the Circle of Willis,2023-08-29 07:51:36+00:00,"Most of the intracranial aneurysms (ICA) occur on a specific portion of the
cerebral vascular tree named the Circle of Willis (CoW). More particularly,
they mainly arise onto fifteen of the major arterial bifurcations constituting
this circular structure. Hence, for an efficient and timely diagnosis it is
critical to develop some methods being able to accurately recognize each
Bifurcation of Interest (BoI). Indeed, an automatic extraction of the
bifurcations presenting the higher risk of developing an ICA would offer the
neuroradiologists a quick glance at the most alarming areas. Due to the recent
efforts on Artificial Intelligence, Deep Learning turned out to be the best
performing technology for many pattern recognition tasks. Moreover, various
methods have been particularly designed for medical image analysis purposes.
This study intends to assist the neuroradiologists to promptly locate any
bifurcation presenting a high risk of ICA occurrence. It can be seen as a
Computer Aided Diagnosis scheme, where the Artificial Intelligence facilitates
the access to the regions of interest within the MRI. In this work, we propose
a method for a fully automatic detection and recognition of the bifurcations of
interest forming the Circle of Willis. Several neural networks architectures
have been tested, and we thoroughly evaluate the bifurcation recognition rate.",http://arxiv.org/pdf/2308.15088v1
2308.14942v1,q-bio.QM,Woolf et als GWAS by subtraction is not useful for cross-generational Mendelian randomization studies,2023-08-28 23:49:51+00:00,"Mendelian randomization (MR) is an epidemiological method that can be used to
strengthen causal inference regarding the relationship between a modifiable
environmental exposure and a medically relevant trait and to estimate the
magnitude of this relationship1. Recently, there has been considerable interest
in using MR to examine potential causal relationships between parental
phenotypes and outcomes amongst their offspring. In a recent issue of BMC
Research Notes, Woolf et al (2023) present a new method, GWAS by subtraction,
to derive genome-wide summary statistics for paternal smoking and other
paternal phenotypes with the goal that these estimates can then be used in
downstream (including two sample) MR studies. Whilst a potentially useful goal,
Woolf et al. (2023) focus on the wrong parameter of interest for useful
genome-wide association studies (GWAS) and downstream cross-generational MR
studies, and the estimator that they derive is neither efficient nor
appropriate for such use.",http://arxiv.org/pdf/2308.14942v1
2308.14869v1,q-bio.MN,PROSO Toolbox: a unified protein-constrained genome-scale modelling framework for strain designing and optimization,2023-08-28 19:44:55+00:00,"The genome-scale metabolic model with protein constraint (PC-model) has been
increasingly popular for microbial metabolic simulations. We present PROSO
Toolbox, a unified and simple-to-use PC-model toolbox that takes any
high-quality genome-scale metabolic reconstruction as the input. The toolbox
can construct a PC-model automatically, apply various algorithms for
computational strain design and simulation, and help unveil metabolism from
gene expression data through a state-of-the-art OVERLAY workflow. It also has
detailed tutorials and documentation for maximum accessibility to researchers
from diverse backgrounds. PROSO Toolbox, tutorials, and documentation are
freely available online: https://github.com/QCSB/PROSO-Toolbox.",http://arxiv.org/pdf/2308.14869v1
2308.14549v1,q-bio.QM,Computational modelling of peritoneal dialysis: an overview,2023-08-28 13:08:35+00:00,"Peritoneal dialysis (PD) is becoming more popular as a result of a rising
interest in home dialysis, lower intrusion in social life and longer
preservation of residual kidney function. However, PD has several important
drawbacks: small solute clearance is relatively low compared to hemodialysis
and technique survival is limited. Application of continuous flow,
sorbent-based dialysate regeneration and novel glucose-sparing PD solutions are
some solutions proposed to address the limitations of PD. To optimize and
personalize current and novel PD therapies, patient peritoneal characteristics
interacting with PD techniques need to be studied together and separately as
they interplay. However, considering the multitude of parameters, it would be
difficult, expensive, and time consuming to optimize all parameter settings
only with the help of clinical trials. Mathematical modelling is an exciting
tool to dissect these interacting processes and comprehend PD techniques better
at a patient specific level. In this review, we look at the history of
computational PD models, explore the many ways a computational PD model can be
constructed and review the various existing PD models that can be used to
optimize and personalize PD treatment.",http://arxiv.org/pdf/2308.14549v1
2308.14774v1,eess.AS,EEG-Derived Voice Signature for Attended Speaker Detection,2023-08-28 10:39:03+00:00,"\textit{Objective:} Conventional EEG-based auditory attention detection (AAD)
is achieved by comparing the time-varying speech stimuli and the elicited EEG
signals. However, in order to obtain reliable correlation values, these methods
necessitate a long decision window, resulting in a long detection latency.
Humans have a remarkable ability to recognize and follow a known speaker,
regardless of the spoken content. In this paper, we seek to detect the attended
speaker among the pre-enrolled speakers from the elicited EEG signals. In this
manner, we avoid relying on the speech stimuli for AAD at run-time. In doing
so, we propose a novel EEG-based attended speaker detection (E-ASD) task.
\textit{Methods:} We encode a speaker's voice with a fixed dimensional vector,
known as speaker embedding, and project it to an audio-derived voice signature,
which characterizes the speaker's unique voice regardless of the spoken
content. We hypothesize that such a voice signature also exists in the
listener's brain that can be decoded from the elicited EEG signals, referred to
as EEG-derived voice signature. By comparing the audio-derived voice signature
and the EEG-derived voice signature, we are able to effectively detect the
attended speaker in the listening brain. \textit{Results:} Experiments show
that E-ASD can effectively detect the attended speaker from the 0.5s EEG
decision windows, achieving 99.78\% AAD accuracy, 99.94\% AUC, and 0.27\% EER.
\textit{Conclusion:} We conclude that it is possible to derive the attended
speaker's voice signature from the EEG signals so as to detect the attended
speaker in a listening brain. \textit{Significance:} We present the first proof
of concept for detecting the attended speaker from the elicited EEG signals in
a cocktail party environment. The successful implementation of E-ASD marks a
non-trivial, but crucial step towards smart hearing aids.",http://arxiv.org/pdf/2308.14774v1
2308.13891v1,cs.LG,Drug Interaction Vectors Neural Network: DrIVeNN,2023-08-26 14:24:41+00:00,"Polypharmacy, the concurrent use of multiple drugs to treat a single
condition, is common in patients managing multiple or complex conditions.
However, as more drugs are added to the treatment plan, the risk of adverse
drug events (ADEs) rises rapidly. Many serious ADEs associated with
polypharmacy only become known after the drugs are in use. It is impractical to
test every possible drug combination during clinical trials. This issue is
particularly prevalent among older adults with cardiovascular disease (CVD)
where polypharmacy and ADEs are commonly observed. In this research, our
primary objective was to identify key drug features to build and evaluate a
model for modeling polypharmacy ADEs. Our secondary objective was to assess our
model on a domain-specific case study. We developed a two-layer neural network
that incorporated drug features such as molecular structure, drug-protein
interactions, and mono drug side effects (DrIVeNN). We assessed DrIVeNN using
publicly available side effect databases and determined Principal Component
Analysis (PCA) with a variance threshold of 0.95 as the most effective feature
selection method. DrIVeNN performed moderately better than state-of-the-art
models like RESCAL, DEDICOM, DeepWalk, Decagon, DeepDDI, KGDDI, and KGNN in
terms of AUROC for the drug-drug interaction prediction task. We also conducted
a domain-specific case study centered on the treatment of cardiovascular
disease (CVD). When the best performing model architecture was applied to the
CVD treatment cohort, there was a significant increase in performance from the
general model. We observed an average AUROC for CVD drug pair prediction
increasing from 0.826 (general model) to 0.975 (CVD specific model). Our
findings indicate the strong potential of domain-specific models for improving
the accuracy of drug-drug interaction predictions.",http://arxiv.org/pdf/2308.13891v1
2308.13304v1,eess.IV,Bang and the Artefacts are Gone! Rapid Artefact Removal and Tissue Segmentation in Haematoxylin and Eosin Stained Biopsies,2023-08-25 11:04:35+00:00,"We present H&E Otsu thresholding, a scheme for rapidly detecting tissue in
whole-slide images (WSIs) that eliminates a wide range of undesirable artefacts
such as pen marks and scanning artefacts. Our method involves obtaining a
bid-modal representation of a low-magnification RGB overview image which
enables simple Otsu thresholding to separate tissue from background and
artefacts. We demonstrate our method on WSIs prepared from a wide range of
institutions and WSI digital scanners, each containing substantial artefacts
that cause other methods to fail. The beauty of our approach lies in its
simplicity: manipulating RGB colour space and using Otsu thresholding allows
for the rapid removal of artefacts and segmentation of tissue.",http://arxiv.org/pdf/2308.13304v1
2308.13182v1,cs.CV,Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon,2023-08-25 05:24:23+00:00,"With the advent of digital scanners and deep learning, diagnostic operations
may move from a microscope to a desktop. Hematoxylin and Eosin (H&E) staining
is one of the most frequently used stains for disease analysis, diagnosis, and
grading, but pathologists do need different immunohistochemical (IHC) stains to
analyze specific structures or cells. Obtaining all of these stains (H&E and
different IHCs) on a single specimen is a tedious and time-consuming task.
Consequently, virtual staining has emerged as an essential research direction.
Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for
synthesizing IHC stains from H&E images, and vice versa. Our method expressly
incorporates structural information in the form of edges (in addition to color
data) and employs attention modules exclusively in the decoder of the proposed
generator model. This integration enhances feature localization and preserves
contextual information during the generation process. In addition, a structural
loss is incorporated to ensure accurate structure alignment between the
generated and input markers. To demonstrate the efficacy of the proposed model,
experiments are conducted with two IHC markers emphasizing distinct structures
of glands in the colon: the nucleus of epithelial cells (CDX2) and the
cytoplasm (CK818). Quantitative metrics such as FID and SSIM are frequently
used for the analysis of generative models, but they do not correlate
explicitly with higher-quality virtual staining results. Therefore, we propose
two new quantitative metrics that correlate directly with the virtual staining
specificity of IHC markers.",http://arxiv.org/pdf/2308.13182v1
2308.13171v1,quant-ph,Q-Drug: a Framework to bring Drug Design into Quantum Space using Deep Learning,2023-08-25 04:26:02+00:00,"Optimizing the properties of molecules (materials or drugs) for stronger
toughness, lower toxicity, or better bioavailability has been a long-standing
challenge. In this context, we propose a molecular optimization framework
called Q-Drug (Quantum-inspired optimization algorithm for Drugs) that
leverages quantum-inspired algorithms to optimize molecules on discrete binary
domain variables. The framework begins by encoding the molecules into binary
embeddings using a discrete VAE. The binary embeddings are then used to
construct an Ising energy-like objective function, over which the
state-of-the-art quantum-inspired optimization algorithm is adopted to find the
optima. The binary embeddings corresponding to the optima are decoded to obtain
the optimized molecules. We have tested the framework for optimizing drug
molecule properties and have found that it outperforms other molecular
optimization methods, finding molecules with better properties in 1/20th to
1/10th of the time previously required. The framework can also be deployed
directly on various quantum computing equipment, such as laser pulses CIMs,
FPGA Ising Machines, and quantum computers based on quantum annealing, among
others. Our work demonstrates a new paradigm that leverages the advantages of
quantum computing and AI to solve practically useful problems.",http://arxiv.org/pdf/2308.13171v1
2308.13066v1,cs.LG,Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE,2023-08-24 20:22:22+00:00,"Variational autoencoder (VAE) is a popular method for drug discovery and
various architectures and pipelines have been proposed to improve its
performance. However, VAE approaches are known to suffer from poor manifold
recovery when the data lie on a low-dimensional manifold embedded in a higher
dimensional ambient space [Dai and Wipf, 2019]. The consequences of it in drug
discovery are somewhat under-explored. In this paper, we explore applying a
multi-stage VAE approach, that can improve manifold recovery on a synthetic
dataset, to the field of drug discovery. We experimentally evaluate our
multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability
to improve the property statistics of generated molecules substantially from
pre-existing methods without incorporating property predictors into the
training pipeline. We further fine-tune our models on two curated and much
smaller molecule datasets that target different proteins. Our experiments show
an increase in the number of active molecules generated by the multi-stage VAE
in comparison to their one-stage equivalent. For each of the two tasks, our
baselines include methods that use learned property predictors to incorporate
target metrics directly into the training objective and we discuss
complications that arise with this methodology.",http://arxiv.org/pdf/2308.13066v1
2308.13035v1,q-bio.QM,The intersection of video capsule endoscopy and artificial intelligence: addressing unique challenges using machine learning,2023-08-24 19:00:26+00:00,"Introduction: Technical burdens and time-intensive review processes limit the
practical utility of video capsule endoscopy (VCE). Artificial intelligence
(AI) is poised to address these limitations, but the intersection of AI and VCE
reveals challenges that must first be overcome. We identified five challenges
to address. Challenge #1: VCE data are stochastic and contains significant
artifact. Challenge #2: VCE interpretation is cost-intensive. Challenge #3: VCE
data are inherently imbalanced. Challenge #4: Existing VCE AIMLT are
computationally cumbersome. Challenge #5: Clinicians are hesitant to accept
AIMLT that cannot explain their process.
  Methods: An anatomic landmark detection model was used to test the
application of convolutional neural networks (CNNs) to the task of classifying
VCE data. We also created a tool that assists in expert annotation of VCE data.
We then created more elaborate models using different approaches including a
multi-frame approach, a CNN based on graph representation, and a few-shot
approach based on meta-learning.
  Results: When used on full-length VCE footage, CNNs accurately identified
anatomic landmarks (99.1%), with gradient weighted-class activation mapping
showing the parts of each frame that the CNN used to make its decision. The
graph CNN with weakly supervised learning (accuracy 89.9%, sensitivity of
91.1%), the few-shot model (accuracy 90.8%, precision 91.4%, sensitivity
90.9%), and the multi-frame model (accuracy 97.5%, precision 91.5%, sensitivity
94.8%) performed well. Discussion: Each of these five challenges is addressed,
in part, by one of our AI-based models. Our goal of producing high performance
using lightweight models that aim to improve clinician confidence was achieved.",http://arxiv.org/pdf/2308.13035v1
2308.12780v1,physics.bio-ph,The motility-matrix production switch in Bacillus subtilis -- a modeling perspective,2023-08-24 13:34:01+00:00,"Phenotype switching can be triggered by external stimuli and by intrinsic
stochasticity. Here, we focus on the motility-matrix production switch in
Bacillus subtilis. We use modeling to describe the SinR-SlrR bistable switch
its regulation by SinI, and to distinguish different sources of stochasticity.
Our simulations indicate that intrinsic fluctuations in the synthesis of SinI
are insufficient to drive spontaneous switching and suggest that switching is
triggered by upstream noise from the Spo0A phosphorelay.",http://arxiv.org/pdf/2308.12780v1
2308.12740v1,cs.AI,Human Comprehensible Active Learning of Genome-Scale Metabolic Networks,2023-08-24 12:42:00+00:00,"An important application of Synthetic Biology is the engineering of the host
cell system to yield useful products. However, an increase in the scale of the
host system leads to huge design space and requires a large number of
validation trials with high experimental costs. A comprehensible machine
learning approach that efficiently explores the hypothesis space and guides
experimental design is urgently needed for the Design-Build-Test-Learn (DBTL)
cycle of the host cell system. We introduce a novel machine learning framework
ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive
logical reasoning and actively learns from training examples. In contrast to
numerical models, ILP-iML1515 is built on comprehensible logical
representations of a genome-scale metabolic model and can update the model by
learning new logical structures from auxotrophic mutant trials. The ILP-iML1515
framework 1) allows high-throughput simulations and 2) actively selects
experiments that reduce the experimental cost of learning gene functions in
comparison to randomly selected experiments.",http://arxiv.org/pdf/2308.12740v1
2308.12416v1,eess.IV,Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach,2023-08-23 20:33:22+00:00,"Deep learning models have achieved state-of-the-art results in estimating
brain age, which is an important brain health biomarker, from magnetic
resonance (MR) images. However, most of these models only provide a global age
prediction, and rely on techniques, such as saliency maps to interpret their
results. These saliency maps highlight regions in the input image that were
significant for the model's predictions, but they are hard to be interpreted,
and saliency map values are not directly comparable across different samples.
In this work, we reframe the age prediction problem from MR images to an
image-to-image regression problem where we estimate the brain age for each
brain voxel in MR images. We compare voxel-wise age prediction models against
global age prediction models and their corresponding saliency maps. The results
indicate that voxel-wise age prediction models are more interpretable, since
they provide spatial information about the brain aging process, and they
benefit from being quantitative.",http://arxiv.org/pdf/2308.12416v1
2308.12224v1,q-bio.QM,Enhancing cardiovascular risk prediction through AI-enabled calcium-omics,2023-08-23 16:05:14+00:00,"Background. Coronary artery calcium (CAC) is a powerful predictor of major
adverse cardiovascular events (MACE). Traditional Agatston score simply sums
the calcium, albeit in a non-linear way, leaving room for improved
calcification assessments that will more fully capture the extent of disease.
  Objective. To determine if AI methods using detailed calcification features
(i.e., calcium-omics) can improve MACE prediction.
  Methods. We investigated additional features of calcification including
assessment of mass, volume, density, spatial distribution, territory, etc. We
used a Cox model with elastic-net regularization on 2457 CT calcium score
(CTCS) enriched for MACE events obtained from a large no-cost CLARIFY program
(ClinicalTri-als.gov Identifier: NCT04075162). We employed sampling techniques
to enhance model training. We also investigated Cox models with selected
features to identify explainable high-risk characteristics.
  Results. Our proposed calcium-omics model with modified synthetic down
sampling and up sampling gave C-index (80.5%/71.6%) and two-year AUC
(82.4%/74.8%) for (80:20, training/testing), respectively (sampling was applied
to the training set only). Results compared favorably to Agatston which gave
C-index (71.3%/70.3%) and AUC (71.8%/68.8%), respectively. Among calcium-omics
features, numbers of calcifications, LAD mass, and diffusivity (a measure of
spatial distribution) were important determinants of increased risk, with dense
calcification (>1000HU) associated with lower risk. The calcium-omics model
reclassified 63% of MACE patients to the high risk group in a held-out test.
The categorical net-reclassification index was NRI=0.153.
  Conclusions. AI analysis of coronary calcification can lead to improved
results as compared to Agatston scoring. Our findings suggest the utility of
calcium-omics in improved prediction of risk.",http://arxiv.org/pdf/2308.12224v1
2308.12325v1,q-bio.QM,Predicting Drug Solubility Using Different Machine Learning Methods -- Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network,2023-08-23 15:35:20+00:00,"Predicting the solubility of given molecules is an important task in the
pharmaceutical industry, and consequently this is a well-studied topic. In this
research, we revisited this problem with the advantage of modern computing
resources. We applied two machine learning models, a linear regression model
and a graph convolutional neural network model, on multiple experimental
datasets. Both methods can make reasonable predictions while the GCNN model had
the best performance. However, the current GCNN model is a black box, while
feature importance analysis from the linear regression model offers more
insights into the underlying chemical influences. Using the linear regression
model, we show how each functional group affects the overall solubility.
Ultimately, knowing how chemical structure influences chemical properties is
crucial when designing new drugs. Future work should aim to combine the high
performance of GCNNs with the interpretability of linear regression, unlocking
new advances in next generation high throughput screening.",http://arxiv.org/pdf/2308.12325v1
2308.12188v1,cs.LG,Development and external validation of a lung cancer risk estimation tool using gradient-boosting,2023-08-23 15:25:17+00:00,"Lung cancer is a significant cause of mortality worldwide, emphasizing the
importance of early detection for improved survival rates. In this study, we
propose a machine learning (ML) tool trained on data from the PLCO Cancer
Screening Trial and validated on the NLST to estimate the likelihood of lung
cancer occurrence within five years. The study utilized two datasets, the PLCO
(n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk
factors, clinical measurements, and outcomes related to lung cancer. Data
preprocessing involved removing patients who were not current or former smokers
and those who had died of causes unrelated to lung cancer. Additionally, a
focus was placed on mitigating bias caused by censored data. Feature selection,
hyper-parameter optimization, and model calibration were performed using
XGBoost, an ensemble learning algorithm that combines gradient boosting and
decision trees. The ML model was trained on the pre-processed PLCO dataset and
tested on the NLST dataset. The model incorporated features such as age,
gender, smoking history, medical diagnoses, and family history of lung cancer.
The model was well-calibrated (Brier score=0.044). ROC-AUC was 82% on the PLCO
dataset and 70% on the NLST dataset. PR-AUC was 29% and 11% respectively. When
compared to the USPSTF guidelines for lung cancer screening, our model provided
the same recall with a precision of 13.1% vs. 9.3% on the PLCO dataset and 3.2%
vs. 3.1% on the NLST dataset. The developed ML tool provides a freely available
web application for estimating the likelihood of developing lung cancer within
five years. By utilizing risk factors and clinical data, individuals can assess
their risk and make informed decisions regarding lung cancer screening. This
research contributes to the efforts in early detection and prevention
strategies, aiming to reduce lung cancer-related mortality rates.",http://arxiv.org/pdf/2308.12188v1
2308.16092v1,stat.ME,Likelihood-based inference and forecasting for trawl processes: a stochastic optimization approach,2023-08-30 15:37:48+00:00,"We consider trawl processes, which are stationary and infinitely divisible
stochastic processes and can describe a wide range of statistical properties,
such as heavy tails and long memory. In this paper, we develop the first
likelihood-based methodology for the inference of real-valued trawl processes
and introduce novel deterministic and probabilistic forecasting methods. Being
non-Markovian, with a highly intractable likelihood function, trawl processes
require the use of composite likelihood functions to parsimoniously capture
their statistical properties. We formulate the composite likelihood estimation
as a stochastic optimization problem for which it is feasible to implement
iterative gradient descent methods. We derive novel gradient estimators with
variances that are reduced by several orders of magnitude. We analyze both the
theoretical properties and practical implementation details of these estimators
and release a Python library which can be used to fit a large class of trawl
processes. In a simulation study, we demonstrate that our estimators outperform
the generalized method of moments estimators in terms of both parameter
estimation error and out-of-sample forecasting error. Finally, we formalize a
stochastic chain rule for our gradient estimators. We apply the new theory to
trawl processes and provide a unified likelihood-based methodology for the
inference of both real-valued and integer-valued trawl processes.",http://arxiv.org/pdf/2308.16092v1
2308.15910v1,stat.ME,Sequential Bayesian Predictive Synthesis,2023-08-30 09:38:04+00:00,"Dynamic Bayesian predictive synthesis is a formal approach to coherently
synthesizing multiple predictive distributions into a single distribution. In
sequential analysis, the computation of the synthesized predictive distribution
has heavily relied on the repeated use of the Markov chain Monte Carlo method.
The sequential Monte Carlo method in this problem has also been studied but is
limited to a subclass of linear synthesis with weight constraint but no
intercept. In this study, we provide a custom, Rao-Blackwellized particle
filter for the linear and Gaussian synthesis, supplemented by timely
interventions by the MCMC method to avoid the problem of particle degeneracy.
In an example of predicting US inflation rate, where a sudden burst is observed
in 2020-2022, we confirm the slow adaptation of the predictive distribution. To
overcome this problem, we propose the estimation/averaging of parameters called
discount factors based on the power-discounted likelihoods, which becomes
feasible due to the fast computation by the proposed method.",http://arxiv.org/pdf/2308.15910v1
2308.15681v1,stat.ME,Scalable Estimation of Probit Models with Crossed Random Effects,2023-08-30 00:40:32+00:00,"Crossed random effects structures arise in many scientific contexts. They
raise severe computational problems with likelihood and Bayesian computations
scaling like $N^{3/2}$ or worse for $N$ data points. In this paper we develop a
composite likelihood approach for crossed random effects probit models. For
data arranged in rows and columns, one likelihood uses marginal distributions
of the responses as if they were independent, another uses a hierarchical model
capturing all within row dependence as if the rows were independent and the
third model reverses the roles of rows and columns. We find that this method
has a cost that grows as $\mathrm{O}(N)$ in crossed random effects settings
where using the Laplace approximation has cost that grows superlinearly. We
show how to get consistent estimates of the probit slope and variance
components by maximizing those three likelihoods. The algorithm scales readily
to a data set of five million observations from Stitch Fix.",http://arxiv.org/pdf/2308.15681v1
2308.15443v1,q-fin.ST,Combining predictive distributions of electricity prices: Does minimizing the CRPS lead to optimal decisions in day-ahead bidding?,2023-08-29 17:10:38+00:00,"Probabilistic price forecasting has recently gained attention in power
trading because decisions based on such predictions can yield significantly
higher profits than those made with point forecasts alone. At the same time,
methods are being developed to combine predictive distributions, since no model
is perfect and averaging generally improves forecasting performance. In this
article we address the question of whether using CRPS learning, a novel
weighting technique minimizing the continuous ranked probability score (CRPS),
leads to optimal decisions in day-ahead bidding. To this end, we conduct an
empirical study using hourly day-ahead electricity prices from the German EPEX
market. We find that increasing the diversity of an ensemble can have a
positive impact on accuracy. At the same time, the higher computational cost of
using CRPS learning compared to an equal-weighted aggregation of distributions
is not offset by higher profits, despite significantly more accurate
predictions.",http://arxiv.org/pdf/2308.15443v1
2308.14952v1,stat.CO,Stochastic Variational Inference for GARCH Models,2023-08-29 00:49:47+00:00,"Stochastic variational inference algorithms are derived for fitting various
heteroskedastic time series models. We examine Gaussian, t, and skew-t response
GARCH models and fit these using Gaussian variational approximating densities.
We implement efficient stochastic gradient ascent procedures based on the use
of control variates or the reparameterization trick and demonstrate that the
proposed implementations provide a fast and accurate alternative to Markov
chain Monte Carlo sampling. Additionally, we present sequential updating
versions of our variational algorithms, which are suitable for efficient
portfolio construction and dynamic asset allocation.",http://arxiv.org/pdf/2308.14952v1
2308.14945v2,stat.ML,Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals,2023-08-28 23:51:33+00:00,"We consider the problem of sampling from a distribution governed by a
potential function. This work proposes an explicit score-based MCMC method that
is deterministic, resulting in a deterministic evolution for particles rather
than a stochastic differential equation evolution. The score term is given in
closed form by a regularized Wasserstein proximal, using a kernel convolution
that is approximated by sampling. We demonstrate fast convergence on various
problems and show improved dimensional dependence of mixing time bounds for the
case of Gaussian distributions compared to the unadjusted Langevin algorithm
(ULA) and the Metropolis-adjusted Langevin algorithm (MALA). We additionally
derive closed form expressions for the distributions at each iterate for
quadratic potential functions, characterizing the variance reduction. Empirical
results demonstrate that the particles behave in an organized manner, lying on
level set contours of the potential. Moreover, the posterior mean estimator of
the proposed method is shown to be closer to the maximum a-posteriori estimator
compared to ULA and MALA, in the context of Bayesian logistic regression.",http://arxiv.org/pdf/2308.14945v2
2308.14830v1,stat.AP,COVID anomaly in the correlation analysis of S&P 500 market states,2023-08-28 18:29:48+00:00,"Analyzing market states of the S&P 500 components on a time horizon January
3, 2006 to August 10, 2023, we found the appearance of a new market state not
previously seen and we shall discuss its possible implications as an isolated
state or as a beginning of a new general market condition. We study this in
terms of the Pearson correlation matrix and relative correlation with respect
to the S&P 500 index. In both cases the anomaly shows strongly.",http://arxiv.org/pdf/2308.14830v1
2308.14671v1,stat.ME,A generalized Bayesian stochastic block model for microbiome community detection,2023-08-28 15:57:59+00:00,"Advances in next-generation sequencing technology have enabled the
high-throughput profiling of metagenomes and accelerated the microbiome study.
Recently, there has been a rise in quantitative studies that aim to decipher
the microbiome co-occurrence network and its underlying community structure
based on metagenomic sequence data. Uncovering the complex microbiome community
structure is essential to understanding the role of the microbiome in disease
progression and susceptibility. Taxonomic abundance data generated from
metagenomic sequencing technologies are high-dimensional and compositional,
suffering from uneven sampling depth, over-dispersion, and zero-inflation.
These characteristics often challenge the reliability of the current methods
for microbiome community detection. To this end, we propose a Bayesian
stochastic block model to study the microbiome co-occurrence network based on
the recently developed modified centered-log ratio transformation tailored for
microbiome data analysis. Our model allows us to incorporate taxonomic tree
information using a Markov random field prior. The model parameters are jointly
inferred by using Markov chain Monte Carlo sampling techniques. Our simulation
study showed that the proposed approach performs better than competing methods
even when taxonomic tree information is non-informative. We applied our
approach to a real urinary microbiome dataset from postmenopausal women, the
first time the urinary microbiome co-occurrence network structure has been
studied. In summary, this statistical methodology provides a new tool for
facilitating advanced microbiome studies.",http://arxiv.org/pdf/2308.14671v1
2308.14106v1,stat.CO,Diffusion Schrdinger Bridges for Bayesian Computation,2023-08-27 13:22:55+00:00,"Denoising diffusion models are a novel class of generative models that have
recently become extremely popular in machine learning. In this paper, we
describe how such ideas can also be used to sample from posterior distributions
and, more generally, any target distribution whose density is known up to a
normalizing constant. The key idea is to consider a forward ``noising''
diffusion initialized at the target distribution which ``transports'' this
latter to a normal distribution for long diffusion times. The time-reversal of
this process, the ``denoising'' diffusion, thus ``transports'' the normal
distribution to the target distribution and can be approximated so as to sample
from the target. To accelerate simulation, we show how one can introduce and
approximate a Schr\""{o}dinger bridge between these two distributions, i.e. a
diffusion which transports the normal to the target in finite time.",http://arxiv.org/pdf/2308.14106v1
2308.14048v1,stat.ML,A Bayesian Non-parametric Approach to Generative Models: Integrating Variational Autoencoder and Generative Adversarial Networks using Wasserstein and Maximum Mean Discrepancy,2023-08-27 08:58:31+00:00,"Generative models have emerged as a promising technique for producing
high-quality images that are indistinguishable from real images. Generative
adversarial networks (GANs) and variational autoencoders (VAEs) are two of the
most prominent and widely studied generative models. GANs have demonstrated
excellent performance in generating sharp realistic images and VAEs have shown
strong abilities to generate diverse images. However, GANs suffer from ignoring
a large portion of the possible output space which does not represent the full
diversity of the target distribution, and VAEs tend to produce blurry images.
To fully capitalize on the strengths of both models while mitigating their
weaknesses, we employ a Bayesian non-parametric (BNP) approach to merge GANs
and VAEs. Our procedure incorporates both Wasserstein and maximum mean
discrepancy (MMD) measures in the loss function to enable effective learning of
the latent space and generate diverse and high-quality samples. By fusing the
discriminative power of GANs with the reconstruction capabilities of VAEs, our
novel model achieves superior performance in various generative tasks, such as
anomaly detection and data augmentation. Furthermore, we enhance the model's
capability by employing an extra generator in the code space, which enables us
to explore areas of the code space that the VAE might have overlooked. With a
BNP perspective, we can model the data distribution using an
infinite-dimensional space, which provides greater flexibility in the model and
reduces the risk of overfitting. By utilizing this framework, we can enhance
the performance of both GANs and VAEs to create a more robust generative model
suitable for various applications.",http://arxiv.org/pdf/2308.14048v1
2308.13928v2,stat.ME,A flexible Bayesian tool for CoDa mixed models: logistic-normal distribution with Dirichlet covariance,2023-08-26 18:02:15+00:00,"Compositional Data Analysis (CoDa) has gained popularity in recent years.
This type of data consists of values from disjoint categories that sum up to a
constant. Both Dirichlet regression and logistic-normal regression have become
popular as CoDa analysis methods. However, fitting this kind of multivariate
models presents challenges, especially when structured random effects are
included in the model, such as temporal or spatial effects.
  To overcome these challenges, we propose the logistic-normal Dirichlet Model
(LNDM). We seamlessly incorporate this approach into the R-INLA package,
facilitating model fitting and model prediction within the framework of Latent
Gaussian Models (LGMs). Moreover, we explore metrics like Deviance Information
Criteria (DIC), Watanabe Akaike information criterion (WAIC), and
cross-validation measure conditional predictive ordinate (CPO) for model
selection in R-INLA for CoDa.
  Illustrating LNDM through a simple simulated example and with an ecological
case study on Arabidopsis thaliana in the Iberian Peninsula, we underscore its
potential as an effective tool for managing CoDa and large CoDa databases.",http://arxiv.org/pdf/2308.13928v2
2308.13630v1,stat.ME,Degrees of Freedom: Search Cost and Self-consistency,2023-08-25 18:55:10+00:00,"Model degrees of freedom ($\df$) is a fundamental concept in statistics
because it quantifies the flexibility of a fitting procedure and is
indispensable in model selection. The $\df$ is often intuitively equated with
the number of independent variables in the fitting procedure. But for adaptive
regressions that perform variable selection (e.g., the best subset
regressions), the model $\df$ is larger than the number of selected variables.
The excess part has been defined as the \emph{search degrees of freedom}
($\sdf$) to account for model selection. However, this definition is limited
since it does not consider fitting procedures in augmented space, such as
splines and regression trees; and it does not use the same fitting procedure
for $\sdf$ and $\df$. For example, the lasso's $\sdf$ is defined through the
\emph{relaxed} lasso's $\df$ instead of the lasso's $\df$.
  Here we propose a \emph{modified search degrees of freedom} ($\msdf$) to
directly account for the cost of searching in the original or augmented space.
Since many fitting procedures can be characterized by a linear operator, we
define the search cost as the effort to determine such a linear operator. When
we construct a linear operator for the lasso via the iterative ridge
regression, $\msdf$ offers a new perspective for its search cost. For some
complex procedures such as the multivariate adaptive regression splines (MARS),
the search cost needs to be pre-determined to serve as a tuning parameter for
the procedure itself, but it might be inaccurate. To investigate the inaccurate
pre-determined search cost, we develop two concepts, \emph{nominal} $\df$ and
\emph{actual} $\df$, and formulate a property named \emph{self-consistency}
when there is no gap between the \emph{nominal} $\df$ and the \emph{actual}
$\df$.",http://arxiv.org/pdf/2308.13630v1
2308.13564v1,econ.EM,SGMM: Stochastic Approximation to Generalized Method of Moments,2023-08-25 00:22:45+00:00,"We introduce a new class of algorithms, Stochastic Generalized Method of
Moments (SGMM), for estimation and inference on (overidentified) moment
restriction models. Our SGMM is a novel stochastic approximation alternative to
the popular Hansen (1982) (offline) GMM, and offers fast and scalable
implementation with the ability to handle streaming datasets in real time. We
establish the almost sure convergence, and the (functional) central limit
theorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we
propose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that
can be seamlessly integrated within the SGMM framework. Extensive Monte Carlo
simulations show that as the sample size increases, the SGMM matches the
standard (offline) GMM in terms of estimation accuracy and gains over
computational efficiency, indicating its practical value for both large-scale
and online datasets. We demonstrate the efficacy of our approach by a proof of
concept using two well known empirical examples with large sample sizes.",http://arxiv.org/pdf/2308.13564v1
2308.13068v1,cs.LG,Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology,2023-08-24 20:24:12+00:00,"Multivariate Time Series (MVTS) anomaly detection is a long-standing and
challenging research topic that has attracted tremendous research effort from
both industry and academia recently. However, a careful study of the literature
makes us realize that 1) the community is active but not as organized as other
sibling machine learning communities such as Computer Vision (CV) and Natural
Language Processing (NLP), and 2) most proposed solutions are evaluated using
either inappropriate or highly flawed protocols, with an apparent lack of
scientific foundation. So flawed is one very popular protocol, the so-called
\pa protocol, that a random guess can be shown to systematically outperform
\emph{all} algorithms developed so far. In this paper, we review and evaluate
many recent algorithms using more robust protocols and discuss how a normally
good protocol may have weaknesses in the context of MVTS anomaly detection and
how to mitigate them. We also share our concerns about benchmark datasets,
experiment design and evaluation methodology we observe in many works.
Furthermore, we propose a simple, yet challenging, baseline algorithm based on
Principal Components Analysis (PCA) that surprisingly outperforms many recent
Deep Learning (DL) based approaches on popular benchmark datasets. The main
objective of this work is to stimulate more effort towards important aspects of
the research such as data, experiment design, evaluation methodology and result
interpretability, instead of putting the highest weight on the design of
increasingly more complex and ""fancier"" algorithms.",http://arxiv.org/pdf/2308.13068v1
2308.13033v1,stat.CO,"A Strength and Sparsity Preserving Algorithm for Generating Weighted, Directed Networks with Predetermined Assortativity",2023-08-24 18:59:20+00:00,"Degree-preserving rewiring is a widely used technique for generating
unweighted networks with given assortativity, but for weighted networks, it is
unclear how an analog would preserve the strengths and other critical network
features such as sparsity level. This study introduces a novel approach for
rewiring weighted networks to achieve desired directed assortativity. The
method utilizes a mixed integer programming framework to establish a target
network with predetermined assortativity coefficients, followed by an efficient
rewiring algorithm termed ""strength and sparsity preserving rewiring"" (SSPR).
SSPR retains the node strength distributions and network sparsity after
rewiring. It is also possible to accommodate additional properties like edge
weight distribution with extra computational cost. The optimization scheme can
be used to determine feasible assortativity ranges for an initial network. The
effectiveness of the proposed SSPR algorithm is demonstrated through its
application to two classes of popular network models.",http://arxiv.org/pdf/2308.13033v1
2308.12944v1,quant-ph,Parallel-in-time quantum simulation via Page and Wootters quantum time,2023-08-24 17:32:41+00:00,"In the past few decades, researchers have created a veritable zoo of quantum
algorithm by drawing inspiration from classical computing, information theory,
and even from physical phenomena. Here we present quantum algorithms for
parallel-in-time simulations that are inspired by the Page and Wooters
formalism. In this framework, and thus in our algorithms, the classical
time-variable of quantum mechanics is promoted to the quantum realm by
introducing a Hilbert space of ""clock"" qubits which are then entangled with the
""system"" qubits. We show that our algorithms can compute temporal properties
over $N$ different times of many-body systems by only using $\log(N)$ clock
qubits. As such, we achieve an exponential trade-off between time and spatial
complexities. In addition, we rigorously prove that the entanglement created
between the system qubits and the clock qubits has operational meaning, as it
encodes valuable information about the system's dynamics. We also provide a
circuit depth estimation of all the protocols, showing an exponential advantage
in computation times over traditional sequential in time algorithms. In
particular, for the case when the dynamics are determined by the Aubry-Andre
model, we present a hybrid method for which our algorithms have a depth that
only scales as $\mathcal{O}(\log(N)n)$. As a by product we can relate the
previous schemes to the problem of equilibration of an isolated quantum system,
thus indicating that our framework enable a new dimension for studying
dynamical properties of many-body systems.",http://arxiv.org/pdf/2308.12944v1
2308.12470v1,stat.ME,Scalable Estimation of Multinomial Response Models with Uncertain Consideration Sets,2023-08-23 23:48:47+00:00,"A standard assumption in the fitting of unordered multinomial response models
for J mutually exclusive nominal categories, on cross-sectional or longitudinal
data, is that the responses arise from the same set of J categories between
subjects. However, when responses measure a choice made by the subject, it is
more appropriate to assume that the distribution of multinomial responses is
conditioned on a subject-specific consideration set, where this consideration
set is drawn from the power set of {1,2,...,J}. Because the cardinality of this
power set is exponential in J, estimation is infeasible in general. In this
paper, we provide an approach to overcoming this problem. A key step in the
approach is a probability model over consideration sets, based on a general
representation of probability distributions on contingency tables. Although the
support of this distribution is exponentially large, the posterior distribution
over consideration sets given parameters is typically sparse, and is easily
sampled as part of an MCMC scheme that iterates sampling of subject-specific
consideration sets given parameters, followed by parameters given consideration
sets. The effectiveness of the procedure is documented in simulated
longitudinal data sets with J=100 categories and real data from the cereal
market with J=73 brands.",http://arxiv.org/pdf/2308.12470v1
