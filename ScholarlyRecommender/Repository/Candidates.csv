Id,Category,Title,Published,Abstract,URL
2308.15474v1,cs.CV,A General-Purpose Self-Supervised Model for Computational Pathology,2023-08-29 17:52:10+00:00,"Tissue phenotyping is a fundamental computational pathology (CPath) task in
learning objective characterizations of histopathologic biomarkers in anatomic
pathology. However, whole-slide imaging (WSI) poses a complex computer vision
problem in which the large-scale image resolutions of WSIs and the enormous
diversity of morphological phenotypes preclude large-scale data annotation.
Current efforts have proposed using pretrained image encoders with either
transfer learning from natural image datasets or self-supervised pretraining on
publicly-available histopathology datasets, but have not been extensively
developed and evaluated across diverse tissue types at scale. We introduce UNI,
a general-purpose self-supervised model for pathology, pretrained using over
100 million tissue patches from over 100,000 diagnostic haematoxylin and
eosin-stained WSIs across 20 major tissue types, and evaluated on 33
representative CPath clinical tasks in CPath of varying diagnostic
difficulties. In addition to outperforming previous state-of-the-art models, we
demonstrate new modeling capabilities in CPath such as resolution-agnostic
tissue classification, slide classification using few-shot class prototypes,
and disease subtyping generalization in classifying up to 108 cancer types in
the OncoTree code classification system. UNI advances unsupervised
representation learning at scale in CPath in terms of both pretraining data and
downstream evaluation, enabling data-efficient AI models that can generalize
and transfer to a gamut of diagnostically-challenging tasks and clinical
workflows in anatomic pathology.",http://arxiv.org/pdf/2308.15474v1
2308.15469v1,cs.CV,Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer's Disease Prediction,2023-08-29 17:48:33+00:00,"Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD)
datasets contain valuable tabular data including AD biomarkers and clinical
assessments. Existing computer vision approaches struggle to utilize this
additional information. To address these needs, we propose a generalizable
framework for multimodal contrastive learning of image data and tabular data, a
novel tabular attention module for amplifying and ranking salient features in
tables, and the application of these techniques onto Alzheimer's disease
prediction. Experimental evaulations demonstrate the strength of our framework
by detecting Alzheimer's disease (AD) from over 882 MR image slices from the
ADNI database. We take advantage of the high interpretability of tabular data
and our novel tabular attention approach and through attribution of the
attention scores for each row of the table, we note and rank the most
predominant features. Results show that the model is capable of an accuracy of
over 83.8%, almost a 10% increase from previous state of the art.",http://arxiv.org/pdf/2308.15469v1
2308.15464v1,cs.LG,A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios,2023-08-29 17:44:02+00:00,"Spatiotemporal graph neural networks have achieved state-of-the-art
performance in traffic forecasting. However, they often struggle to forecast
congestion accurately due to the limitations of traditional loss functions.
While accurate forecasting of regular traffic conditions is crucial, a reliable
AI system must also accurately forecast congestion scenarios to maintain safe
and efficient transportation. In this paper, we explore various loss functions
inspired by heavy tail analysis and imbalanced classification problems to
address this issue. We evaluate the efficacy of these loss functions in
forecasting traffic speed, with an emphasis on congestion scenarios. Through
extensive experiments on real-world traffic datasets, we discovered that when
optimizing for Mean Absolute Error (MAE), the MAE-Focal Loss function stands
out as the most effective. When optimizing Mean Squared Error (MSE), Gumbel
Loss proves to be the superior choice. These choices effectively forecast
traffic congestion events without compromising the accuracy of regular traffic
speed forecasts. This research enhances deep learning models' capabilities in
forecasting sudden speed changes due to congestion and underscores the need for
more research in this direction. By elevating the accuracy of congestion
forecasting, we advocate for AI systems that are reliable, secure, and
resilient in practical traffic management scenarios.",http://arxiv.org/pdf/2308.15464v1
2308.15459v1,cs.CL,ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer,2023-08-29 17:36:02+00:00,"Textual style transfer is the task of transforming stylistic properties of
text while preserving meaning. Target ""styles"" can be defined in numerous ways,
ranging from single attributes (e.g, formality) to authorship (e.g,
Shakespeare). Previous unsupervised style-transfer approaches generally rely on
significant amounts of labeled data for only a fixed set of styles or require
large language models. In contrast, we introduce a novel diffusion-based
framework for general-purpose style transfer that can be flexibly adapted to
arbitrary target styles at inference time. Our parameter-efficient approach,
ParaGuide, leverages paraphrase-conditioned diffusion models alongside
gradient-based guidance from both off-the-shelf classifiers and strong existing
style embedders to transform the style of text while preserving semantic
information. We validate the method on the Enron Email Corpus, with both human
and automatic evaluations, and find that it outperforms strong baselines on
formality, sentiment, and even authorship style transfer.",http://arxiv.org/pdf/2308.15459v1
2308.15457v1,cs.LG,From SMOTE to Mixup for Deep Imbalanced Classification,2023-08-29 17:31:26+00:00,"Given imbalanced data, it is hard to train a good classifier using deep
learning because of the poor generalization of minority classes. Traditionally,
the well-known synthetic minority oversampling technique (SMOTE) for data
augmentation, a data mining approach for imbalanced learning, has been used to
improve this generalization. However, it is unclear whether SMOTE also benefits
deep learning. In this work, we study why the original SMOTE is insufficient
for deep learning, and enhance SMOTE using soft labels. Connecting the
resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to
a unified framework that puts traditional and modern data augmentation
techniques under the same umbrella. A careful study within this framework shows
that Mixup improves generalization by implicitly achieving uneven margins
between majority and minority classes. We then propose a novel margin-aware
Mixup technique that more explicitly achieves uneven margins. Extensive
experimental results demonstrate that our proposed technique yields
state-of-the-art performance on deep imbalanced classification while achieving
superior performance on extremely imbalanced data. The code is open-sourced in
our developed package https://github.com/ntucllab/imbalanced-DL to foster
future research in this direction.",http://arxiv.org/pdf/2308.15457v1
2308.15452v1,cs.CL,When Do Program-of-Thoughts Work for Reasoning?,2023-08-29 17:22:39+00:00,"The reasoning capabilities of Large Language Models (LLMs) play a pivotal
role in the realm of embodied artificial intelligence. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.",http://arxiv.org/pdf/2308.15452v1
2308.15427v1,cs.CV,Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction,2023-08-29 16:33:16+00:00,"High-Definition (HD) maps play a crucial role in autonomous driving systems.
Recent methods have attempted to construct HD maps in real-time based on
information obtained from vehicle onboard sensors. However, the performance of
these methods is significantly susceptible to the environment surrounding the
vehicle due to the inherent limitation of onboard sensors, such as weak
capacity for long-range detection. In this study, we demonstrate that
supplementing onboard sensors with satellite maps can enhance the performance
of HD map construction methods, leveraging the broad coverage capability of
satellite maps. For the purpose of further research, we release the satellite
map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose
a hierarchical fusion module that enables better fusion of satellite maps
information with existing methods. Specifically, we design an attention mask
based on segmentation and distance, applying the cross-attention mechanism to
fuse onboard Bird's Eye View (BEV) features and satellite features in
feature-level fusion. An alignment module is introduced before concatenation in
BEV-level fusion to mitigate the impact of misalignment between the two
features. The experimental results on the augmented nuScenes dataset showcase
the seamless integration of our module into three existing HD map construction
methods. It notably enhances their performance in both HD map semantic
segmentation and instance detection tasks.",http://arxiv.org/pdf/2308.15427v1
2308.15397v1,cs.CV,Color Aesthetics: Fuzzy based User-driven Method for Harmony and Preference Prediction,2023-08-29 15:56:38+00:00,"Color is the most important intrinsic sensory feature that has a powerful
impact on product sales. Color is even responsible for raising the aesthetic
senses in our brains. Account for individual differences is crucial in color
aesthetics. It requires user-driven mechanisms for various e-commerce
applications. We propose a method for quantitative evaluation of all types of
perceptual responses to color(s): distinct color preference, color harmony, and
color combination preference. Preference for color schemes can be predicted by
combining preferences for the basic colors and ratings of color harmony.
Harmonious pallets are extracted from big data set using comparison algorithms
based on fuzzy similarity and grouping. The proposed model results in useful
predictions of harmony and preference of multicolored images. For example, in
the context of apparel coordination, it allows predicting a preference for a
look based on clothing colors. Our approach differs from standard aesthetic
models, since in accounts for a personal variation. In addition, it can process
not only lower-order color pairs, but also groups of several colors.",http://arxiv.org/pdf/2308.15397v1
2308.15394v1,cs.AI,Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System,2023-08-29 15:48:49+00:00,"This paper develops a Decentralized Multi-Agent Reinforcement Learning
(Dec-MARL) method to solve the SoC balancing problem in the distributed energy
storage system (DESS). First, the SoC balancing problem is formulated into a
finite Markov decision process with action constraints derived from demand
balance, which can be solved by Dec-MARL. Specifically, the first-order average
consensus algorithm is utilized to expand the observations of the DESS state in
a fully-decentralized way, and the initial actions (i.e., output power) are
decided by the agents (i.e., energy storage units) according to these
observations. In order to get the final actions in the allowable range, a
counterfactual demand balance algorithm is proposed to balance the total demand
and the initial actions. Next, the agents execute the final actions and get
local rewards from the environment, and the DESS steps into the next state.
Finally, through the first-order average consensus algorithm, the agents get
the average reward and the expended observation of the next state for later
training. By the above procedure, Dec-MARL reveals outstanding performance in a
fully-decentralized system without any expert experience or constructing any
complicated model. Besides, it is flexible and can be extended to other
decentralized multi-agent systems straightforwardly. Extensive simulations have
validated the effectiveness and efficiency of Dec-MARL.",http://arxiv.org/pdf/2308.15394v1
2308.15390v1,cs.AI,Bayesian Integration of Information Using Top-Down Modulated WTA Networks,2023-08-29 15:33:51+00:00,"Winner Take All (WTA) circuits a type of Spiking Neural Networks (SNN) have
been suggested as facilitating the brain's ability to process information in a
Bayesian manner. Research has shown that WTA circuits are capable of
approximating hierarchical Bayesian models via Expectation Maximization (EM).
So far, research in this direction has focused on bottom up processes. This is
contrary to neuroscientific evidence that shows that, besides bottom up
processes, top down processes too play a key role in information processing by
the human brain. Several functions ascribed to top down processes include
direction of attention, adjusting for expectations, facilitation of encoding
and recall of learned information, and imagery. This paper explores whether WTA
circuits are suitable for further integrating information represented in
separate WTA networks. Furthermore, it explores whether, and under what
circumstances, top down processes can improve WTA network performance with
respect to inference and learning. The results show that WTA circuits are
capable of integrating the probabilistic information represented by other WTA
networks, and that top down processes can improve a WTA network's inference and
learning performance. Notably, it is able to do this according to key
neuromorphic principles, making it ideal for low-latency and energy efficient
implementation on neuromorphic hardware.",http://arxiv.org/pdf/2308.15390v1
2308.15368v1,cs.RO,RED: A Systematic Real-Time Scheduling Approach for Robotic Environmental Dynamics,2023-08-29 15:04:08+00:00,"Intelligent robots are designed to effectively navigate dynamic and
unpredictable environments laden with moving mechanical elements and objects.
Such environment-induced dynamics, including moving obstacles, can readily
alter the computational demand (e.g., the creation of new tasks) and the
structure of workloads (e.g., precedence constraints among tasks) during
runtime, thereby adversely affecting overall system performance. This challenge
is amplified when multi-task inference is expected on robots operating under
stringent resource and real-time constraints. To address such a challenge, we
introduce RED, a systematic real-time scheduling approach designed to support
multi-task deep neural network workloads in resource-limited robotic systems.
It is designed to adaptively manage the Robotic Environmental Dynamics (RED)
while adhering to real-time constraints. At the core of RED lies a
deadline-based scheduler that employs an intermediate deadline assignment
policy, effectively managing to change workloads and asynchronous inference
prompted by complex, unpredictable environments. This scheduling framework also
facilitates the flexible deployment of MIMONet (multi-input multi-output neural
networks), which are commonly utilized in multi-tasking robotic systems to
circumvent memory bottlenecks. Building on this scheduling framework, RED
recognizes and leverages a unique characteristic of MIMONet: its weight-shared
architecture. To further accommodate and exploit this feature, RED devises a
novel and effective workload refinement and reconstruction process. This
process ensures the scheduling framework's compatibility with MIMONet and
maximizes efficiency.",http://arxiv.org/pdf/2308.15368v1
2308.15367v1,cs.CV,Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation,2023-08-29 15:03:05+00:00,"Federated learning (FL) emerges as a decentralized learning framework which
trains models from multiple distributed clients without sharing their data to
preserve privacy. Recently, large-scale pre-trained models (e.g., Vision
Transformer) have shown a strong capability of deriving robust representations.
However, the data heterogeneity among clients, the limited computation
resources, and the communication bandwidth restrict the deployment of
large-scale models in FL frameworks. To leverage robust representations from
large-scale models while enabling efficient model personalization for
heterogeneous clients, we propose a novel personalized FL framework of
client-specific Prompt Generation (pFedPG), which learns to deploy a
personalized prompt generator at the server for producing client-specific
visual prompts that efficiently adapts frozen backbones to local data
distributions. Our proposed framework jointly optimizes the stages of
personalized prompt adaptation locally and personalized prompt generation
globally. The former aims to train visual prompts that adapt foundation models
to each client, while the latter observes local optimization directions to
generate personalized prompts for all clients. Through extensive experiments on
benchmark datasets, we show that our pFedPG is favorable against
state-of-the-art personalized FL methods under various types of data
heterogeneity, allowing computation and communication efficient model
personalization.",http://arxiv.org/pdf/2308.15367v1
2308.15357v1,cs.RO,Ego-Motion Estimation and Dynamic Motion Separation from 3D Point Clouds for Accumulating Data and Improving 3D Object Detection,2023-08-29 14:53:16+00:00,"New 3+1D high-resolution radar sensors are gaining importance for 3D object
detection in the automotive domain due to their relative affordability and
improved detection compared to classic low-resolution radar sensors. One
limitation of high-resolution radar sensors, compared to lidar sensors, is the
sparsity of the generated point cloud. This sparsity could be partially
overcome by accumulating radar point clouds of subsequent time steps. This
contribution analyzes limitations of accumulating radar point clouds on the
View-of-Delft dataset. By employing different ego-motion estimation approaches,
the dataset's inherent constraints, and possible solutions are analyzed.
Additionally, a learning-based instance motion estimation approach is deployed
to investigate the influence of dynamic motion on the accumulated point cloud
for object detection. Experiments document an improved object detection
performance by applying an ego-motion estimation and dynamic motion correction
approach.",http://arxiv.org/pdf/2308.15357v1
2308.15346v1,cs.CV,Enhancing Mobile Face Anti-Spoofing: A Robust Framework for Diverse Attack Types under Screen Flash,2023-08-29 14:41:40+00:00,"Face anti-spoofing (FAS) is crucial for securing face recognition systems.
However, existing FAS methods with handcrafted binary or pixel-wise labels have
limitations due to diverse presentation attacks (PAs). In this paper, we
propose an attack type robust face anti-spoofing framework under light flash,
called ATR-FAS. Due to imaging differences caused by various attack types,
traditional FAS methods based on single binary classification network may
result in excessive intra-class distance of spoof faces, leading to a challenge
of decision boundary learning. Therefore, we employed multiple networks to
reconstruct multi-frame depth maps as auxiliary supervision, and each network
experts in one type of attack. A dual gate module (DGM) consisting of a type
gate and a frame-attention gate is introduced, which perform attack type
recognition and multi-frame attention generation, respectively. The outputs of
DGM are utilized as weight to mix the result of multiple expert networks. The
multi-experts mixture enables ATR-FAS to generate spoof-differentiated depth
maps, and stably detects spoof faces without being affected by different types
of PAs. Moreover, we design a differential normalization procedure to convert
original flash frames into differential frames. This simple but effective
processing enhances the details in flash frames, aiding in the generation of
depth maps. To verify the effectiveness of our framework, we collected a
large-scale dataset containing 12,660 live and spoof videos with diverse PAs
under dynamic flash from the smartphone screen. Extensive experiments
illustrate that the proposed ATR-FAS significantly outperforms existing
state-of-the-art methods. The code and dataset will be available at
https://github.com/Chaochao-Lin/ATR-FAS.",http://arxiv.org/pdf/2308.15346v1
2308.15339v1,cs.AI,"AI Framework for Early Diagnosis of Coronary Artery Disease: An Integration of Borderline SMOTE, Autoencoders and Convolutional Neural Networks Approach",2023-08-29 14:33:38+00:00,"The accuracy of coronary artery disease (CAD) diagnosis is dependent on a
variety of factors, including demographic, symptom, and medical examination,
ECG, and echocardiography data, among others. In this context, artificial
intelligence (AI) can help clinicians identify high-risk patients early in the
diagnostic process, by synthesizing information from multiple factors. To this
aim, Machine Learning algorithms are used to classify patients based on their
CAD disease risk. In this study, we contribute to this research filed by
developing a methodology for balancing and augmenting data for more accurate
prediction when the data is imbalanced and the sample size is small. The
methodology can be used in a variety of other situations, particularly when
data collection is expensive and the sample size is small. The experimental
results revealed that the average accuracy of our proposed method for CAD
prediction was 95.36, and was higher than random forest (RF), decision tree
(DT), support vector machine (SVM), logistic regression (LR), and artificial
neural network (ANN).",http://arxiv.org/pdf/2308.15339v1
2308.15334v1,cs.CY,A Framework for Responsible Development of Automated Student Feedback with Generative AI,2023-08-29 14:29:57+00:00,"Providing rich feedback to students is essential for supporting student
learning. Recent advances in generative AI, particularly within large language
modelling (LLM), provide the opportunity to deliver repeatable, scalable and
instant automatically generated feedback to students, making abundant a
previously scarce and expensive learning resource. Such an approach is feasible
from a technical perspective due to these recent advances in Artificial
Intelligence (AI) and Natural Language Processing (NLP); while the potential
upside is a strong motivator, doing so introduces a range of potential ethical
issues that must be considered as we apply these technologies. The
attractiveness of AI systems is that they can effectively automate the most
mundane tasks; but this risks introducing a ""tyranny of the majority"", where
the needs of minorities in the long tail are overlooked because they are
difficult to automate.
  Developing machine learning models that can generate valuable and authentic
feedback requires the input of human domain experts. The choices we make in
capturing this expertise -- whose, which, when, and how -- will have
significant consequences for the nature of the resulting feedback. How we
maintain our models will affect how that feedback remains relevant given
temporal changes in context, theory, and prior learning profiles of student
cohorts. These questions are important from an ethical perspective; but they
are also important from an operational perspective. Unless they can be
answered, our AI generated systems will lack the trust necessary for them to be
useful features in the contemporary learning environment.
  This article will outline the frontiers of automated feedback, identify the
ethical issues involved in the provision of automated feedback and present a
framework to assist academics to develop such systems responsibly.",http://arxiv.org/pdf/2308.15334v1
2308.15324v1,cs.AI,FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models,2023-08-29 14:20:17+00:00,"Leveraging ``chain-of-thought (CoT)'' reasoning to elicit rapid and precise
responses from large language models (LLMs) is rapidly attracting research
interest. A notable challenge here is how to design or select optimal prompts.
The process of prompt selection relies on trial and error, involving continuous
adjustments and combinations of input prompts by users based on the
corresponding new responses generated from LLMs. Furthermore, minimal research
has been conducted to explore how LLMs employ the mathematical problem-solving
capabilities learned from user interactions to address issues in narrative
writing. To improve interpretability and explore the balance principle between
generality and personalization under a multi-domain CoT prompt selection
scenario, we propose the Federated Logic rule learning approach (FedLogic). We
introduce a theoretical formalization and interactive emulation of the
multi-domain CoT prompt selection dilemma in the context of federated LLMs. We
cast the problem of joint probability modeling as a bilevel program, where the
CoT prompt selection intricacy can be likened to a fuzzy score-based rule
selection with the LLMs function as rule generators. FedLogic solves this
problem through variational expectation maximization (V-EM). In addition, we
incorporate two KL-divergence constraints within this probabilistic modeling
framework to surmount the intricacies of managing extensive search spaces and
accomplishing cross-domain personalization of CoTs. To the best of our
knowledge, FedLogic is the first interpretable and principled federated
multi-domain CoT prompt selection approach for LLMs.",http://arxiv.org/pdf/2308.15324v1
2308.15321v1,cs.LG,Elucidating the Exposure Bias in Diffusion Models,2023-08-29 14:16:09+00:00,"Diffusion models have demonstrated impressive generative capabilities, but
their 'exposure bias' problem, described as the input mismatch between training
and sampling, lacks in-depth exploration. In this paper, we systematically
investigate the exposure bias problem in diffusion models by first analytically
modelling the sampling distribution, based on which we then attribute the
prediction error at each sampling step as the root cause of the exposure bias
issue. Furthermore, we discuss potential solutions to this issue and propose an
intuitive metric for it. Along with the elucidation of exposure bias, we
propose a simple, yet effective, training-free method called Epsilon Scaling to
alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the
sampling trajectory closer to the vector field learned in the training phase by
scaling down the network output (Epsilon), mitigating the input mismatch
between training and sampling. Experiments on various diffusion frameworks
(ADM, DDPM/DDIM, LDM), unconditional and conditional settings, and
deterministic vs. stochastic sampling verify the effectiveness of our method.",http://arxiv.org/pdf/2308.15321v1
2308.15308v1,cs.LG,On-Device Learning with Binary Neural Networks,2023-08-29 13:48:35+00:00,"Existing Continual Learning (CL) solutions only partially address the
constraints on power, memory and computation of the deep learning models when
deployed on low-power embedded CPUs. In this paper, we propose a CL solution
that embraces the recent advancements in CL field and the efficiency of the
Binary Neural Networks (BNN), that use 1-bit for weights and activations to
efficiently execute deep learning models. We propose a hybrid quantization of
CWR* (an effective CL approach) that considers differently forward and backward
pass in order to retain more precision during gradient update step and at the
same time minimizing the latency overhead. The choice of a binary network as
backbone is essential to meet the constraints of low power devices and, to the
best of authors' knowledge, this is the first attempt to prove on-device
learning with BNN. The experimental validation carried out confirms the
validity and the suitability of the proposed method.",http://arxiv.org/pdf/2308.15308v1
2308.15298v1,cs.CL,"KGConv, a Conversational Corpus grounded in Wikidata",2023-08-29 13:35:51+00:00,"We present KGConv, a large, conversational corpus of 71k conversations where
each question-answer pair is grounded in a Wikidata fact. Conversations contain
on average 8.6 questions and for each Wikidata fact, we provide multiple
variants (12 on average) of the corresponding question using templates, human
annotations, hand-crafted rules and a question rewriting neural model. We
provide baselines for the task of Knowledge-Based, Conversational Question
Generation. KGConv can further be used for other generation and analysis tasks
such as single-turn question generation from Wikidata triples, question
rewriting, question answering from conversation or from knowledge graphs and
quiz generation.",http://arxiv.org/pdf/2308.15298v1
2308.15293v1,cs.SI,A Hybrid Membership Latent Distance Model for Unsigned and Signed Integer Weighted Networks,2023-08-29 13:30:48+00:00,"Graph representation learning (GRL) has become a prominent tool for
furthering the understanding of complex networks providing tools for network
embedding, link prediction, and node classification. In this paper, we propose
the Hybrid Membership-Latent Distance Model (HM-LDM) by exploring how a Latent
Distance Model (LDM) can be constrained to a latent simplex. By controlling the
edge lengths of the corners of the simplex, the volume of the latent space can
be systematically controlled. Thereby communities are revealed as the space
becomes more constrained, with hard memberships being recovered as the simplex
volume goes to zero. We further explore a recent likelihood formulation for
signed networks utilizing the Skellam distribution to account for signed
weighted networks and extend the HM-LDM to the signed Hybrid Membership-Latent
Distance Model (sHM-LDM). Importantly, the induced likelihood function
explicitly attracts nodes with positive links and deters nodes from having
negative interactions. We demonstrate the utility of HM-LDM and sHM-LDM on
several real networks. We find that the procedures successfully identify
prominent distinct structures, as well as how nodes relate to the extracted
aspects providing favorable performances in terms of link prediction when
compared to prominent baselines. Furthermore, the learned soft memberships
enable easily interpretable network visualizations highlighting distinct
patterns.",http://arxiv.org/pdf/2308.15293v1
2308.15272v1,cs.AI,Empowering LLM to use Smartphone for Intelligent Task Automation,2023-08-29 13:02:30+00:00,"Mobile task automation is an attractive technique that aims to enable
voice-based hands-free user interaction with smartphones. However, existing
approaches suffer from poor scalability due to the limited language
understanding ability and the non-trivial manual efforts required from
developers or end-users. The recent advance of large language models (LLMs) in
language understanding and reasoning inspires us to rethink the problem from a
model-centric perspective, where task preparation, comprehension, and execution
are handled by a unified language model. In this work, we introduce AutoDroid,
a mobile task automation system that can handle arbitrary tasks on any Android
application without manual efforts. The key insight is to combine the
commonsense knowledge of LLMs and domain-specific knowledge of apps through
automated dynamic analysis. The main components include a functionality-aware
UI representation method that bridges the UI with the LLM, exploration-based
memory injection techniques that augment the app-specific domain knowledge of
LLM, and a multi-granularity query optimization module that reduces the cost of
model inference. We integrate AutoDroid with off-the-shelf LLMs including
online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a
new benchmark for memory-augmented Android task automation with 158 common
tasks. The results demonstrated that AutoDroid is able to precisely generate
actions with an accuracy of 90.9%, and complete tasks with a success rate of
71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo,
benchmark suites, and source code of AutoDroid will be released at
https://autodroid-sys.github.io/.",http://arxiv.org/pdf/2308.15272v1
2308.15256v1,eess.AS,Let There Be Sound: Reconstructing High Quality Speech from Silent Videos,2023-08-29 12:30:53+00:00,"The goal of this work is to reconstruct high quality speech from lip motions
alone, a task also known as lip-to-speech. A key challenge of lip-to-speech
systems is the one-to-many mapping caused by (1) the existence of homophenes
and (2) multiple speech variations, resulting in a mispronounced and
over-smoothed speech. In this paper, we propose a novel lip-to-speech system
that significantly improves the generation quality by alleviating the
one-to-many mapping problem from multiple perspectives. Specifically, we
incorporate (1) self-supervised speech representations to disambiguate
homophenes, and (2) acoustic variance information to model diverse speech
styles. Additionally, to better solve the aforementioned problem, we employ a
flow based post-net which captures and refines the details of the generated
speech. We perform extensive experiments and demonstrate that our method
achieves the generation quality close to that of real human utterance,
outperforming existing methods in terms of speech naturalness and
intelligibility by a large margin. Synthesised samples are available at the
anonymous demo page: https://mm.kaist.ac.kr/projects/LTBS.",http://arxiv.org/pdf/2308.15256v1
2308.15244v1,cs.IR,Knowledge-based Multiple Adaptive Spaces Fusion for Recommendation,2023-08-29 12:11:16+00:00,"Since Knowledge Graphs (KGs) contain rich semantic information, recently
there has been an influx of KG-enhanced recommendation methods. Most of
existing methods are entirely designed based on euclidean space without
considering curvature. However, recent studies have revealed that a tremendous
graph-structured data exhibits highly non-euclidean properties. Motivated by
these observations, in this work, we propose a knowledge-based multiple
adaptive spaces fusion method for recommendation, namely MCKG. Unlike existing
methods that solely adopt a specific manifold, we introduce the unified space
that is compatible with hyperbolic, euclidean and spherical spaces.
Furthermore, we fuse the multiple unified spaces in an attention manner to
obtain the high-quality embeddings for better knowledge propagation. In
addition, we propose a geometry-aware optimization strategy which enables the
pull and push processes benefited from both hyperbolic and spherical spaces.
Specifically, in hyperbolic space, we set smaller margins in the area near to
the origin, which is conducive to distinguishing between highly similar
positive items and negative ones. At the same time, we set larger margins in
the area far from the origin to ensure the model has sufficient error
tolerance. The similar manner also applies to spherical spaces. Extensive
experiments on three real-world datasets demonstrate that the MCKG has a
significant improvement over state-of-the-art recommendation methods. Further
ablation experiments verify the importance of multi-space fusion and
geometry-aware optimization strategy, justifying the rationality and
effectiveness of MCKG.",http://arxiv.org/pdf/2308.15244v1
2308.15239v1,cs.AI,Natural language to SQL in low-code platforms,2023-08-29 11:59:02+00:00,"One of the developers' biggest challenges in low-code platforms is retrieving
data from a database using SQL queries. Here, we propose a pipeline allowing
developers to write natural language (NL) to retrieve data. In this study, we
collect, label, and validate data covering the SQL queries most often performed
by OutSystems users. We use that data to train a NL model that generates SQL.
Alongside this, we describe the entire pipeline, which comprises a feedback
loop that allows us to quickly collect production data and use it to retrain
our SQL generation model. Using crowd-sourcing, we collect 26k NL and SQL pairs
and obtain an additional 1k pairs from production data. Finally, we develop a
UI that allows developers to input a NL query in a prompt and receive a
user-friendly representation of the resulting SQL query. We use A/B testing to
compare four different models in production and observe a 240% improvement in
terms of adoption of the feature, 220% in terms of engagement rate, and a 90%
decrease in failure rate when compared against the first model that we put into
production, showcasing the effectiveness of our pipeline in continuously
improving our feature.",http://arxiv.org/pdf/2308.15239v1
2308.15235v1,cs.CL,PronounFlow: A Hybrid Approach for Calibrating Pronouns in Sentences,2023-08-29 11:46:27+00:00,"Flip through any book or listen to any song lyrics, and you will come across
pronouns that, in certain cases, can hinder meaning comprehension, especially
for machines. As the role of having cognitive machines becomes pervasive in our
lives, numerous systems have been developed to resolve pronouns under various
challenges. Commensurate with this, it is believed that having systems able to
disambiguate pronouns in sentences will help towards the endowment of machines
with commonsense and reasoning abilities like those found in humans. However,
one problem these systems face with modern English is the lack of gender
pronouns, where people try to alternate by using masculine, feminine, or plural
to avoid the whole issue. Since humanity aims to the building of systems in the
full-bodied sense we usually reserve for people, what happens when pronouns in
written text, like plural or epicene ones, refer to unspecified entities whose
gender is not necessarily known? Wouldn't that put extra barriers to existing
coreference resolution systems? Towards answering those questions, through the
implementation of a neural-symbolic system that utilizes the best of both
worlds, we are employing PronounFlow, a system that reads any English sentence
with pronouns and entities, identifies which of them are not tied to each
other, and makes suggestions on which to use to avoid biases. Undertaken
experiments show that PronounFlow not only alternates pronouns in sentences
based on the collective human knowledge around us but also considerably helps
coreference resolution systems with the pronoun disambiguation process.",http://arxiv.org/pdf/2308.15235v1
2308.15230v1,cs.IR,Providing Previously Unseen Users Fair Recommendations Using Variational Autoencoders,2023-08-29 11:37:33+00:00,"An emerging definition of fairness in machine learning requires that models
are oblivious to demographic user information, e.g., a user's gender or age
should not influence the model. Personalized recommender systems are
particularly prone to violating this definition through their explicit user
focus and user modelling. Explicit user modelling is also an aspect that makes
many recommender systems incapable of providing hitherto unseen users with
recommendations. We propose novel approaches for mitigating discrimination in
Variational Autoencoder-based recommender systems by limiting the encoding of
demographic information. The approaches are capable of, and evaluated on,
providing users that are not represented in the training data with fair
recommendations.",http://arxiv.org/pdf/2308.15230v1
2308.15226v1,cs.CV,CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation,2023-08-29 11:29:43+00:00,"There has been a growing interest in developing multimodal machine
translation (MMT) systems that enhance neural machine translation (NMT) with
visual knowledge. This problem setup involves using images as auxiliary
information during training, and more recently, eliminating their use during
inference. Towards this end, previous works face a challenge in training
powerful MMT models from scratch due to the scarcity of annotated multilingual
vision-language data, especially for low-resource languages. Simultaneously,
there has been an influx of multilingual pre-trained models for NMT and
multimodal pre-trained models for vision-language tasks, primarily in English,
which have shown exceptional generalisation ability. However, these are not
directly applicable to MMT since they do not provide aligned multimodal
multilingual features for generative tasks. To alleviate this issue, instead of
designing complex modules for MMT, we propose CLIPTrans, which simply adapts
the independently pre-trained multimodal M-CLIP and the multilingual mBART. In
order to align their embedding spaces, mBART is conditioned on the M-CLIP
features by a prefix sequence generated through a lightweight mapping network.
We train this in a two-stage pipeline which warms up the model with image
captioning before the actual translation task. Through experiments, we
demonstrate the merits of this framework and consequently push forward the
state-of-the-art across standard benchmarks by an average of +2.67 BLEU. The
code can be found at www.github.com/devaansh100/CLIPTrans.",http://arxiv.org/pdf/2308.15226v1
2308.15225v1,q-bio.NC,From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions,2023-08-29 11:27:22+00:00,"Over the past decades, cognitive neuroscientists and behavioral economists
have recognized the value of describing the process of decision making in
detail and modeling the emergence of decisions over time. For example, the time
it takes to decide can reveal more about an agents true hidden preferences than
only the decision itself. Similarly, data that track the ongoing decision
process such as eye movements or neural recordings contain critical information
that can be exploited, even if no decision is made. Here, we argue that
artificial intelligence (AI) research would benefit from a stronger focus on
insights about how decisions emerge over time and incorporate related process
data to improve AI predictions in general and human-AI interactions in
particular. First, we introduce a highly established computational framework
that assumes decisions to emerge from the noisy accumulation of evidence, and
we present related empirical work in psychology, neuroscience, and economics.
Next, we discuss to what extent current approaches in multi-agent AI do or do
not incorporate process data and models of decision making. Finally, we outline
how a more principled inclusion of the evidence-accumulation framework into the
training and use of AI can help to improve human-AI interactions in the future.",http://arxiv.org/pdf/2308.15225v1
2308.15214v1,cs.CL,"FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions",2023-08-29 11:08:40+00:00,"We demonstrate an embodied conversational agent that can function as a
receptionist and generate a mixture of open and closed-domain dialogue along
with facial expressions, by using a large language model (LLM) to develop an
engaging conversation. We deployed the system onto a Furhat robot, which is
highly expressive and capable of using both verbal and nonverbal cues during
interaction. The system was designed specifically for the National Robotarium
to interact with visitors through natural conversations, providing them with
information about the facilities, research, news, upcoming events, etc. The
system utilises the state-of-the-art GPT-3.5 model to generate such information
along with domain-general conversations and facial expressions based on prompt
engineering.",http://arxiv.org/pdf/2308.15214v1
2308.15209v1,cs.CL,Shared Lexical Items as Triggers of Code Switching,2023-08-29 10:55:44+00:00,"Why do bilingual speakers code-switch (mix their two languages)? Among the
several theories that attempt to explain this natural and ubiquitous
phenomenon, the Triggering Hypothesis relates code-switching to the presence of
lexical triggers, specifically cognates and proper names, adjacent to the
switch point. We provide a fuller, more nuanced and refined exploration of the
triggering hypothesis, based on five large datasets in three language pairs,
reflecting both spoken and written bilingual interactions. Our results show
that words that are assumed to reside in a mental lexicon shared by both
languages indeed trigger code-switching; that the tendency to switch depends on
the distance of the trigger from the switch point; and on whether the trigger
precedes or succeeds the switch; but not on the etymology of the trigger words.
We thus provide strong, robust, evidence-based confirmation to several
hypotheses on the relationships between lexical triggers and code-switching.",http://arxiv.org/pdf/2308.15209v1
2308.15197v1,cs.AI,Where Would I Go Next? Large Language Models as Human Mobility Predictors,2023-08-29 10:24:23+00:00,"Accurate human mobility prediction underpins many important applications
across a variety of domains, including epidemic modelling, transport planning,
and emergency responses. Due to the sparsity of mobility data and the
stochastic nature of people's daily activities, achieving precise predictions
of people's locations remains a challenge. While recently developed large
language models (LLMs) have demonstrated superior performance across numerous
language-related tasks, their applicability to human mobility studies remains
unexplored. Addressing this gap, this article delves into the potential of LLMs
for human mobility prediction tasks. We introduce a novel method, LLM-Mob,
which leverages the language understanding and reasoning capabilities of LLMs
for analysing human mobility data. We present concepts of historical stays and
context stays to capture both long-term and short-term dependencies in human
movement and enable time-aware prediction by using time information of the
prediction target. Additionally, we design context-inclusive prompts that
enable LLMs to generate more accurate predictions. Comprehensive evaluations of
our method reveal that LLM-Mob excels in providing accurate and interpretable
predictions, highlighting the untapped potential of LLMs in advancing human
mobility prediction techniques. We posit that our research marks a significant
paradigm shift in human mobility modelling, transitioning from building complex
domain-specific models to harnessing general-purpose LLMs that yield accurate
predictions through language instructions. The code for this work is available
at https://github.com/xlwang233/LLM-Mob.",http://arxiv.org/pdf/2308.15197v1
2308.15194v1,cs.AI,Ensemble of Counterfactual Explainers,2023-08-29 10:21:50+00:00,"In eXplainable Artificial Intelligence (XAI), several counterfactual
explainers have been proposed, each focusing on some desirable properties of
counterfactual instances: minimality, actionability, stability, diversity,
plausibility, discriminative power. We propose an ensemble of counterfactual
explainers that boosts weak explainers, which provide only a subset of such
properties, to a powerful method covering all of them. The ensemble runs weak
explainers on a sample of instances and of features, and it combines their
results by exploiting a diversity-driven selection function. The method is
model-agnostic and, through a wrapping approach based on autoencoders, it is
also data-agnostic.",http://arxiv.org/pdf/2308.15194v1
2308.15192v1,cs.AI,Enhancing Psychological Counseling with Large Language Model: A Multifaceted Decision-Support System for Non-Professionals,2023-08-29 10:20:53+00:00,"In the contemporary landscape of social media, an alarming number of users
express negative emotions, some of which manifest as strong suicidal
intentions. This situation underscores a profound need for trained
psychological counselors who can enact effective mental interventions. However,
the development of these professionals is often an imperative but
time-consuming task. Consequently, the mobilization of non-professionals or
volunteers in this capacity emerges as a pressing concern. Leveraging the
capabilities of artificial intelligence, and in particular, the recent advances
in large language models, offers a viable solution to this challenge. This
paper introduces a novel model constructed on the foundation of large language
models to fully assist non-professionals in providing psychological
interventions on online user discourses. This framework makes it plausible to
harness the power of non-professional counselors in a meaningful way. A
comprehensive study was conducted involving ten professional psychological
counselors of varying expertise, evaluating the system across five critical
dimensions. The findings affirm that our system is capable of analyzing
patients' issues with relative accuracy and proffering professional-level
strategies recommendations, thereby enhancing support for non-professionals.
This research serves as a compelling validation of the application of large
language models in the field of psychology and lays the groundwork for a new
paradigm of community-based mental health support.",http://arxiv.org/pdf/2308.15192v1
2308.15188v1,cs.AI,LTLf Best-Effort Synthesis in Nondeterministic Planning Domains,2023-08-29 10:10:41+00:00,"We study best-effort strategies (aka plans) in fully observable
nondeterministic domains (FOND) for goals expressed in Linear Temporal Logic on
Finite Traces (LTLf). The notion of best-effort strategy has been introduced to
also deal with the scenario when no agent strategy exists that fulfills the
goal against every possible nondeterministic environment reaction. Such
strategies fulfill the goal if possible, and do their best to do so otherwise.
We present a game-theoretic technique for synthesizing best-effort strategies
that exploit the specificity of nondeterministic planning domains. We formally
show its correctness and demonstrate its effectiveness experimentally,
exhibiting a much greater scalability with respect to a direct best-effort
synthesis approach based on re-expressing the planning domain as generic
environment specifications.",http://arxiv.org/pdf/2308.15188v1
2308.15184v1,cs.LO,LTLf Synthesis Under Environment Specifications for Reachability and Safety Properties,2023-08-29 10:05:19+00:00,"In this paper, we study LTLf synthesis under environment specifications for
arbitrary reachability and safety properties. We consider both kinds of
properties for both agent tasks and environment specifications, providing a
complete landscape of synthesis algorithms. For each case, we devise a specific
algorithm (optimal wrt complexity of the problem) and prove its correctness.
The algorithms combine common building blocks in different ways. While some
cases are already studied in literature others are studied here for the first
time.",http://arxiv.org/pdf/2308.15184v1
2308.15178v1,cs.AI,Symbolic LTLf Best-Effort Synthesis,2023-08-29 10:00:33+00:00,"We consider an agent acting to fulfil tasks in a nondeterministic
environment. When a strategy that fulfills the task regardless of how the
environment acts does not exist, the agent should at least avoid adopting
strategies that prevent from fulfilling its task. Best-effort synthesis
captures this intuition. In this paper, we devise and compare various symbolic
approaches for best-effort synthesis in Linear Temporal Logic on finite traces
(LTLf). These approaches are based on the same basic components, however they
change in how these components are combined, and this has a significant impact
on the performance of the approaches as confirmed by our empirical evaluations.",http://arxiv.org/pdf/2308.15178v1
2308.15170v1,cs.CV,A lightweight 3D dense facial landmark estimation model from position map data,2023-08-29 09:53:10+00:00,"The incorporation of 3D data in facial analysis tasks has gained popularity
in recent years. Though it provides a more accurate and detailed representation
of the human face, accruing 3D face data is more complex and expensive than 2D
face images. Either one has to rely on expensive 3D scanners or depth sensors
which are prone to noise. An alternative option is the reconstruction of 3D
faces from uncalibrated 2D images in an unsupervised way without any ground
truth 3D data. However, such approaches are computationally expensive and the
learned model size is not suitable for mobile or other edge device
applications. Predicting dense 3D landmarks over the whole face can overcome
this issue. As there is no public dataset available containing dense landmarks,
we propose a pipeline to create a dense keypoint training dataset containing
520 key points across the whole face from an existing facial position map data.
We train a lightweight MobileNet-based regressor model with the generated data.
As we do not have access to any evaluation dataset with dense landmarks in it
we evaluate our model against the 68 keypoint detection task. Experimental
results show that our trained model outperforms many of the existing methods in
spite of its lower model size and minimal computational cost. Also, the
qualitative evaluation shows the efficiency of our trained models in extreme
head pose angles as well as other facial variations and occlusions.",http://arxiv.org/pdf/2308.15170v1
2308.15168v1,cs.AI,Ontologies in Digital Twins: A Systematic Literature Review,2023-08-29 09:52:21+00:00,"Digital Twins (DT) facilitate monitoring and reasoning processes in
cyber-physical systems. They have progressively gained popularity over the past
years because of intense research activity and industrial advancements.
Cognitive Twins is a novel concept, recently coined to refer to the involvement
of Semantic Web technology in DTs. Recent studies address the relevance of
ontologies and knowledge graphs in the context of DTs, in terms of knowledge
representation, interoperability and automatic reasoning. However, there is no
comprehensive analysis of how semantic technologies, and specifically
ontologies, are utilized within DTs. This Systematic Literature Review (SLR) is
based on the analysis of 82 research articles, that either propose or benefit
from ontologies with respect to DT. The paper uses different analysis
perspectives, including a structural analysis based on a reference DT
architecture, and an application-specific analysis to specifically address the
different domains, such as Manufacturing and Infrastructure. The review also
identifies open issues and possible research directions on the usage of
ontologies and knowledge graphs in DTs.",http://arxiv.org/pdf/2308.15168v1
2308.15143v1,cs.RO,Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models,2023-08-29 09:22:12+00:00,"Summarizing knowledge from animals and human beings inspires robotic
innovations. In this work, we propose a framework for driving legged robots act
like real animals with lifelike agility and strategy in complex environments.
Inspired by large pre-trained models witnessed with impressive performance in
language and image understanding, we introduce the power of advanced deep
generative models to produce motor control signals stimulating legged robots to
act like real animals. Unlike conventional controllers and end-to-end RL
methods that are task-specific, we propose to pre-train generative models over
animal motion datasets to preserve expressive knowledge of animal behavior. The
pre-trained model holds sufficient primitive-level knowledge yet is
environment-agnostic. It is then reused for a successive stage of learning to
align with the environments by traversing a number of challenging obstacles
that are rarely considered in previous approaches, including creeping through
narrow spaces, jumping over hurdles, freerunning over scattered blocks, etc.
Finally, a task-specific controller is trained to solve complex downstream
tasks by reusing the knowledge from previous stages. Enriching the knowledge
regarding each stage does not affect the usage of other levels of knowledge.
This flexible framework offers the possibility of continual knowledge
accumulation at different levels. We successfully apply the trained multi-level
controllers to the MAX robot, a quadrupedal robot developed in-house, to mimic
animals, traverse complex obstacles, and play in a designed challenging
multi-agent Chase Tag Game, where lifelike agility and strategy emerge on the
robots. The present research pushes the frontier of robot control with new
insights on reusing multi-level pre-trained knowledge and solving highly
complex downstream tasks in the real world.",http://arxiv.org/pdf/2308.15143v1
2308.15142v1,cs.CV,A Multimodal Visual Encoding Model Aided by Introducing Verbal Semantic Information,2023-08-29 09:21:48+00:00,"Biological research has revealed that the verbal semantic information in the
brain cortex, as an additional source, participates in nonverbal semantic
tasks, such as visual encoding. However, previous visual encoding models did
not incorporate verbal semantic information, contradicting this biological
finding. This paper proposes a multimodal visual information encoding network
model based on stimulus images and associated textual information in response
to this issue. Our visual information encoding network model takes stimulus
images as input and leverages textual information generated by a text-image
generation model as verbal semantic information. This approach injects new
information into the visual encoding model. Subsequently, a Transformer network
aligns image and text feature information, creating a multimodal feature space.
A convolutional network then maps from this multimodal feature space to voxel
space, constructing the multimodal visual information encoding network model.
Experimental results demonstrate that the proposed multimodal visual
information encoding network model outperforms previous models under the exact
training cost. In voxel prediction of the left hemisphere of subject 1's brain,
the performance improves by approximately 15.87%, while in the right
hemisphere, the performance improves by about 4.6%. The multimodal visual
encoding network model exhibits superior encoding performance. Additionally,
ablation experiments indicate that our proposed model better simulates the
brain's visual information processing.",http://arxiv.org/pdf/2308.15142v1
2308.15137v1,cs.CV,Abdominal Multi-Organ Segmentation Based on Feature Pyramid Network and Spatial Recurrent Neural Network,2023-08-29 09:13:24+00:00,"As recent advances in AI are causing the decline of conventional diagnostic
methods, the realization of end-to-end diagnosis is fast approaching.
Ultrasound image segmentation is an important step in the diagnostic process.
An accurate and robust segmentation model accelerates the process and reduces
the burden of sonographers. In contrast to previous research, we take two
inherent features of ultrasound images into consideration: (1) different organs
and tissues vary in spatial sizes, (2) the anatomical structures inside human
body form a relatively constant spatial relationship. Based on those two ideas,
we propose a new image segmentation model combining Feature Pyramid Network
(FPN) and Spatial Recurrent Neural Network (SRNN). We discuss why we use FPN to
extract anatomical structures of different scales and how SRNN is implemented
to extract the spatial context features in abdominal ultrasound images.",http://arxiv.org/pdf/2308.15137v1
2308.15126v1,cs.LG,Evaluation and Analysis of Hallucination in Large Vision-Language Models,2023-08-29 08:51:24+00:00,"Large Vision-Language Models (LVLMs) have recently achieved remarkable
success. However, LVLMs are still plagued by the hallucination problem, which
limits the practicality in many scenarios. Hallucination refers to the
information of LVLMs' responses that does not exist in the visual input, which
poses potential risks of substantial consequences. There has been limited work
studying hallucination evaluation in LVLMs. In this paper, we propose
Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based
hallucination evaluation framework. HaELM achieves an approximate 95%
performance comparable to ChatGPT and has additional advantages including low
cost, reproducibility, privacy preservation and local deployment. Leveraging
the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we
analyze the factors contributing to hallucination in LVLMs and offer helpful
suggestions to mitigate the hallucination problem. Our training data and human
annotation hallucination data will be made public soon.",http://arxiv.org/pdf/2308.15126v1
2308.15119v1,cs.AI,AI-Based Facial Emotion Recognition Solutions for Education: A Study of Teacher-User and Other Categories,2023-08-29 08:37:16+00:00,"Existing information on AI-based facial emotion recognition (FER) is not
easily comprehensible by those outside the field of computer science, requiring
cross-disciplinary effort to determine a categorisation framework that promotes
the understanding of this technology, and its impact on users. Most proponents
classify FER in terms of methodology, implementation and analysis; relatively
few by its application in education; and none by its users. This paper is
concerned primarily with (potential) teacher-users of FER tools for education.
It proposes a three-part classification of these teachers, by orientation,
condition and preference, based on a classical taxonomy of affective
educational objectives, and related theories. It also compiles and organises
the types of FER solutions found in or inferred from the literature into
""technology"" and ""applications"" categories, as a prerequisite for structuring
the proposed ""teacher-user"" category. This work has implications for
proponents', critics', and users' understanding of the relationship between
teachers and FER.",http://arxiv.org/pdf/2308.15119v1
2308.15116v1,cs.LG,Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators,2023-08-29 08:29:08+00:00,"Molecular dynamics simulations have emerged as a fundamental instrument for
studying biomolecules. At the same time, it is desirable to perform simulations
of a collection of particles under various conditions in which the molecules
can fluctuate. In this paper, we explore and adapt the soft prompt-based
learning method to molecular dynamics tasks. Our model can remarkably
generalize to unseen and out-of-distribution scenarios with limited training
data. While our work focuses on temperature as a test case, the versatility of
our approach allows for efficient simulation through any continuous dynamic
conditions, such as pressure and volumes. Our framework has two stages: 1)
Pre-trains with data mixing technique, augments molecular structure data and
temperature prompts, then applies a curriculum learning method by increasing
the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework
improves sample-efficiency of fine-tuning process and gives the soft
prompt-tuning better initialization points. Comprehensive experiments reveal
that our framework excels in accuracy for in-domain data and demonstrates
strong generalization capabilities for unseen and out-of-distribution samples.",http://arxiv.org/pdf/2308.15116v1
2308.15099v1,cs.AI,Probabilistic Dataset Reconstruction from Interpretable Models,2023-08-29 08:10:09+00:00,"Interpretability is often pointed out as a key requirement for trustworthy
machine learning. However, learning and releasing models that are inherently
interpretable leaks information regarding the underlying training data. As such
disclosure may directly conflict with privacy, a precise quantification of the
privacy impact of such breach is a fundamental problem. For instance, previous
work have shown that the structure of a decision tree can be leveraged to build
a probabilistic reconstruction of its training dataset, with the uncertainty of
the reconstruction being a relevant metric for the information leak. In this
paper, we propose of a novel framework generalizing these probabilistic
reconstructions in the sense that it can handle other forms of interpretable
models and more generic types of knowledge. In addition, we demonstrate that
under realistic assumptions regarding the interpretable models' structure, the
uncertainty of the reconstruction can be computed efficiently. Finally, we
illustrate the applicability of our approach on both decision trees and rule
lists, by comparing the theoretical information leak associated to either exact
or heuristic learning algorithms. Our results suggest that optimal
interpretable models are often more compact and leak less information regarding
their training data than greedily-built ones, for a given accuracy level.",http://arxiv.org/pdf/2308.15099v1
2308.15097v1,cs.AI,Sequential annotations for naturally-occurring HRI: first insights,2023-08-29 08:07:26+00:00,"We explain the methodology we developed for improving the interactions
accomplished by an embedded conversational agent, drawing from Conversation
Analytic sequential and multimodal analysis. The use case is a Pepper robot
that is expected to inform and orient users in a library. In order to propose
and learn better interactive schema, we are creating a corpus of
naturally-occurring interactions that will be made available to the community.
To do so, we propose an annotation practice based on some theoretical
underpinnings about the use of language and multimodal resources in human-robot
interaction. CCS CONCEPTS $\bullet$ Computing methodologies $\rightarrow$
Discourse, dialogue and pragmatics; $\bullet$ Human-centered computing
$\rightarrow$ Text input; HCI theory, concepts and models; Field studies.",http://arxiv.org/pdf/2308.15097v1
2308.15092v1,math.NA,Can We Rely on AI?,2023-08-29 07:58:19+00:00,"Over the last decade, adversarial attack algorithms have revealed
instabilities in deep learning tools. These algorithms raise issues regarding
safety, reliability and interpretability in artificial intelligence; especially
in high risk settings. From a practical perspective, there has been a war of
escalation between those developing attack and defence strategies. At a more
theoretical level, researchers have also studied bigger picture questions
concerning the existence and computability of attacks. Here we give a brief
overview of the topic, focusing on aspects that are likely to be of interest to
researchers in applied and computational mathematics.",http://arxiv.org/pdf/2308.15092v1
2308.15078v1,cs.AI,LAMBO: Large Language Model Empowered Edge Intelligence,2023-08-29 07:25:42+00:00,"Next-generation edge intelligence is anticipated to bring huge benefits to
various applications, e.g., offloading systems. However, traditional deep
offloading architectures face several issues, including heterogeneous
constraints, partial perception, uncertain generalization, and lack of
tractability. In this context, the integration of offloading with large
language models (LLMs) presents numerous advantages. Therefore, we propose an
LLM-Based Offloading (LAMBO) framework for mobile edge computing (MEC), which
comprises four components: (i) Input embedding (IE), which is used to represent
the information of the offloading system with constraints and prompts through
learnable vectors with high quality; (ii) Asymmetric encoderdecoder (AED)
model, which is a decision-making module with a deep encoder and a shallow
decoder. It can achieve high performance based on multi-head self-attention
schemes; (iii) Actor-critic reinforcement learning (ACRL) module, which is
employed to pre-train the whole AED for different optimization tasks under
corresponding prompts; and (iv) Active learning from expert feedback (ALEF),
which can be used to finetune the decoder part of the AED while adapting to
dynamic environmental changes. Our simulation results corroborate the
advantages of the proposed LAMBO framework.",http://arxiv.org/pdf/2308.15078v1
2308.15069v1,cs.LG,MadSGM: Multivariate Anomaly Detection with Score-based Generative Models,2023-08-29 07:04:50+00:00,"The time-series anomaly detection is one of the most fundamental tasks for
time-series. Unlike the time-series forecasting and classification, the
time-series anomaly detection typically requires unsupervised (or
self-supervised) training since collecting and labeling anomalous observations
are difficult. In addition, most existing methods resort to limited forms of
anomaly measurements and therefore, it is not clear whether they are optimal in
all circumstances. To this end, we present a multivariate time-series anomaly
detector based on score-based generative models, called MadSGM, which considers
the broadest ever set of anomaly measurement factors: i) reconstruction-based,
ii) density-based, and iii) gradient-based anomaly measurements. We also design
a conditional score network and its denoising score matching loss for the
time-series anomaly detection. Experiments on five real-world benchmark
datasets illustrate that MadSGM achieves the most robust and accurate
predictions.",http://arxiv.org/pdf/2308.15069v1
2308.15068v1,cs.AI,A Comprehensive Augmentation Framework for Anomaly Detection,2023-08-29 07:00:35+00:00,"Data augmentation methods are commonly integrated into the training of
anomaly detection models. Previous approaches have primarily focused on
replicating real-world anomalies or enhancing diversity, without considering
that the standard of anomaly varies across different classes, potentially
leading to a biased training distribution.This paper analyzes crucial traits of
simulated anomalies that contribute to the training of reconstructive networks
and condenses them into several methods, thus creating a comprehensive
framework by selectively utilizing appropriate combinations.Furthermore, we
integrate this framework with a reconstruction-based approach and concurrently
propose a split training strategy that alleviates the issue of overfitting
while avoiding introducing interference to the reconstruction process. The
evaluations conducted on the MVTec anomaly detection dataset demonstrate that
our method outperforms the previous state-of-the-art approach, particularly in
terms of object classes.To evaluate generalizability, we generate a simulated
dataset comprising anomalies with diverse characteristics since the original
test samples only include specific types of anomalies and may lead to biased
evaluations. Experimental results demonstrate that our approach exhibits
promising potential for generalizing effectively to various unforeseen
anomalies encountered in real-world scenarios.",http://arxiv.org/pdf/2308.15068v1
2308.15063v1,cs.CV,Learning Cross-modality Information Bottleneck Representation for Heterogeneous Person Re-Identification,2023-08-29 06:55:42+00:00,"Visible-Infrared person re-identification (VI-ReID) is an important and
challenging task in intelligent video surveillance. Existing methods mainly
focus on learning a shared feature space to reduce the modality discrepancy
between visible and infrared modalities, which still leave two problems
underexplored: information redundancy and modality complementarity. To this
end, properly eliminating the identity-irrelevant information as well as making
up for the modality-specific information are critical and remains a challenging
endeavor. To tackle the above problems, we present a novel mutual information
and modality consensus network, namely CMInfoNet, to extract modality-invariant
identity features with the most representative information and reduce the
redundancies. The key insight of our method is to find an optimal
representation to capture more identity-relevant information and compress the
irrelevant parts by optimizing a mutual information bottleneck trade-off.
Besides, we propose an automatically search strategy to find the most prominent
parts that identify the pedestrians. To eliminate the cross- and intra-modality
variations, we also devise a modality consensus module to align the visible and
infrared modalities for task-specific guidance. Moreover, the global-local
feature representations can also be acquired for key parts discrimination.
Experimental results on four benchmarks, i.e., SYSU-MM01, RegDB,
Occluded-DukeMTMC, Occluded-REID, Partial-REID and Partial\_iLIDS dataset, have
demonstrated the effectiveness of CMInfoNet.",http://arxiv.org/pdf/2308.15063v1
2308.15053v1,cs.CL,Adapting text-based dialogue state tracker for spoken dialogues,2023-08-29 06:27:58+00:00,"Although there have been remarkable advances in dialogue systems through the
dialogue systems technology competition (DSTC), it remains one of the key
challenges to building a robust task-oriented dialogue system with a speech
interface. Most of the progress has been made for text-based dialogue systems
since there are abundant datasets with written corpora while those with spoken
dialogues are very scarce. However, as can be seen from voice assistant systems
such as Siri and Alexa, it is of practical importance to transfer the success
to spoken dialogues. In this paper, we describe our engineering effort in
building a highly successful model that participated in the speech-aware
dialogue systems technology challenge track in DSTC11. Our model consists of
three major modules: (1) automatic speech recognition error correction to
bridge the gap between the spoken and the text utterances, (2) text-based
dialogue system (D3ST) for estimating the slots and values using slot
descriptions, and (3) post-processing for recovering the error of the estimated
slot value. Our experiments show that it is important to use an explicit
automatic speech recognition error correction module, post-processing, and data
augmentation to adapt a text-based dialogue state tracker for spoken dialogue
corpora.",http://arxiv.org/pdf/2308.15053v1
2308.15050v1,cs.CV,iBARLE: imBalance-Aware Room Layout Estimation,2023-08-29 06:20:36+00:00,"Room layout estimation predicts layouts from a single panorama. It requires
datasets with large-scale and diverse room shapes to train the models. However,
there are significant imbalances in real-world datasets including the
dimensions of layout complexity, camera locations, and variation in scene
appearance. These issues considerably influence the model training performance.
In this work, we propose the imBalance-Aware Room Layout Estimation (iBARLE)
framework to address these issues. iBARLE consists of (1) Appearance Variation
Generation (AVG) module, which promotes visual appearance domain
generalization, (2) Complex Structure Mix-up (CSMix) module, which enhances
generalizability w.r.t. room structure, and (3) a gradient-based layout
objective function, which allows more effective accounting for occlusions in
complex layouts. All modules are jointly trained and help each other to achieve
the best performance. Experiments and ablation studies based on
ZInD~\cite{cruz2021zillow} dataset illustrate that iBARLE has state-of-the-art
performance compared with other layout estimation baselines.",http://arxiv.org/pdf/2308.15050v1
2308.15039v1,cs.RO,R^3: On-device Real-Time Deep Reinforcement Learning for Autonomous Robotics,2023-08-29 05:48:28+00:00,"Autonomous robotic systems, like autonomous vehicles and robotic search and
rescue, require efficient on-device training for continuous adaptation of Deep
Reinforcement Learning (DRL) models in dynamic environments. This research is
fundamentally motivated by the need to understand and address the challenges of
on-device real-time DRL, which involves balancing timing and algorithm
performance under memory constraints, as exposed through our extensive
empirical studies. This intricate balance requires co-optimizing two pivotal
parameters of DRL training -- batch size and replay buffer size. Configuring
these parameters significantly affects timing and algorithm performance, while
both (unfortunately) require substantial memory allocation to achieve
near-optimal performance.
  This paper presents R^3, a holistic solution for managing timing, memory, and
algorithm performance in on-device real-time DRL training. R^3 employs (i) a
deadline-driven feedback loop with dynamic batch sizing for optimizing timing,
(ii) efficient memory management to reduce memory footprint and allow larger
replay buffer sizes, and (iii) a runtime coordinator guided by heuristic
analysis and a runtime profiler for dynamically adjusting memory resource
reservations. These components collaboratively tackle the trade-offs in
on-device DRL training, improving timing and algorithm performance while
minimizing the risk of out-of-memory (OOM) errors.
  We implemented and evaluated R^3 extensively across various DRL frameworks
and benchmarks on three hardware platforms commonly adopted by autonomous
robotic systems. Additionally, we integrate R^3 with a popular realistic
autonomous car simulator to demonstrate its real-world applicability.
Evaluation results show that R^3 achieves efficacy across diverse platforms,
ensuring consistent latency performance and timing predictability with minimal
overhead.",http://arxiv.org/pdf/2308.15039v1
2308.15030v1,cs.AI,Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping,2023-08-29 05:25:21+00:00,"Mixture of experts (MoE) is a popular technique in deep learning that
improves model capacity with conditionally-activated parallel neural network
modules (experts). However, serving MoE models in resource-constrained
latency-critical edge scenarios is challenging due to the significantly
increased model size and complexity. In this paper, we first analyze the
behavior pattern of MoE models in continuous inference scenarios, which leads
to three key observations about the expert activations, including temporal
locality, exchangeability, and skippable computation. Based on these
observations, we introduce PC-MoE, an inference framework for
resource-constrained continuous MoE model serving. The core of PC-MoE is a new
data structure, Parameter Committee, that intelligently maintains a subset of
important experts in use to reduce resource consumption. The optimal
configuration of Parameter Committee is found offline by a profiling-guided
committee planner, and expert swapping and request handling at runtime are
managed by an adaptive committee scheduler. To evaluate the effectiveness of
PC-MoE, we conduct experiments using state-of-the-art MoE models on common
computer vision and natural language processing tasks. The results demonstrate
optimal trade-offs between resource consumption and model accuracy achieved by
PC-MoE. For instance, on object detection tasks with the Swin-MoE model, our
approach can reduce memory usage and latency by 42.34% and 18.63% with only
0.10% accuracy degradation.",http://arxiv.org/pdf/2308.15030v1
2308.15022v1,cs.CL,Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models,2023-08-29 04:59:53+00:00,"Most open-domain dialogue systems suffer from forgetting important
information, especially in a long-term conversation. Existing works usually
train the specific retriever or summarizer to obtain key information from the
past, which is time-consuming and highly depends on the quality of labeled
data. To alleviate this problem, we propose to recursively generate summaries/
memory using large language models (LLMs) to enhance long-term memory ability.
Specifically, our method first stimulates LLMs to memorize small dialogue
contexts and then recursively produce new memory using previous memory and
following contexts. Finally, the LLM can easily generate a highly consistent
response with the help of the latest memory. We evaluate our method using
ChatGPT and text-davinci-003, and the experiments on the widely-used public
dataset show that our method can generate more consistent responses in a
long-context conversation. Notably, our method is a potential solution to
enable the LLM to model the extremely long context. Code and scripts will be
released later.",http://arxiv.org/pdf/2308.15022v1
2308.15020v1,cs.AI,Massively Parallel Continuous Local Search for Hybrid SAT Solving on GPUs,2023-08-29 04:50:07+00:00,"Although state-of-the-art (SOTA) SAT solvers based on conflict-driven clause
learning (CDCL) have achieved remarkable engineering success, their sequential
nature limits the parallelism that may be extracted for acceleration on
platforms such as the graphics processing unit (GPU). In this work, we propose
FastFourierSAT, a highly parallel hybrid SAT solver based on gradient-driven
continuous local search (CLS). This is realized by a novel parallel algorithm
inspired by the Fast Fourier Transform (FFT)-based convolution for computing
the elementary symmetric polynomials (ESPs), which is the major computational
task in previous CLS methods. The complexity of our algorithm matches the best
previous result. Furthermore, the substantial parallelism inherent in our
algorithm can leverage the GPU for acceleration, demonstrating significant
improvement over the previous CLS approaches. We also propose to incorporate
the restart heuristics in CLS to improve search efficiency. We compare our
approach with the SOTA parallel SAT solvers on several benchmarks. Our results
show that FastFourierSAT computes the gradient 100+ times faster than previous
prototypes implemented on CPU. Moreover, FastFourierSAT solves most instances
and demonstrates promising performance on larger-size instances.",http://arxiv.org/pdf/2308.15020v1
2308.15003v1,cs.AI,Generative Model for Models: Rapid DNN Customization for Diverse Tasks and Resource Constraints,2023-08-29 03:28:14+00:00,"Unlike cloud-based deep learning models that are often large and uniform,
edge-deployed models usually demand customization for domain-specific tasks and
resource-limited environments. Such customization processes can be costly and
time-consuming due to the diversity of edge scenarios and the training load for
each scenario. Although various approaches have been proposed for rapid
resource-oriented customization and task-oriented customization respectively,
achieving both of them at the same time is challenging. Drawing inspiration
from the generative AI and the modular composability of neural networks, we
introduce NN-Factory, an one-for-all framework to generate customized
lightweight models for diverse edge scenarios. The key idea is to use a
generative model to directly produce the customized models, instead of training
them. The main components of NN-Factory include a modular supernet with
pretrained modules that can be conditionally activated to accomplish different
tasks and a generative module assembler that manipulate the modules according
to task and sparsity requirements. Given an edge scenario, NN-Factory can
efficiently customize a compact model specialized in the edge task while
satisfying the edge resource constraints by searching for the optimal strategy
to assemble the modules. Based on experiments on image classification and
object detection tasks with different edge devices, NN-Factory is able to
generate high-quality task- and resource-specific models within few seconds,
faster than conventional model customization approaches by orders of magnitude.",http://arxiv.org/pdf/2308.15003v1
2308.15002v1,cs.AI,Exploring the Limits of Historical Information for Temporal Knowledge Graph Extrapolation,2023-08-29 03:26:38+00:00,"Temporal knowledge graphs, representing the dynamic relationships and
interactions between entities over time, have been identified as a promising
approach for event forecasting. However, a limitation of most temporal
knowledge graph reasoning methods is their heavy reliance on the recurrence or
periodicity of events, which brings challenges to inferring future events
related to entities that lack historical interaction. In fact, the current
state of affairs is often the result of a combination of historical information
and underlying factors that are not directly observable. To this end, we
investigate the limits of historical information for temporal knowledge graph
extrapolation and propose a new event forecasting model called Contrastive
Event Network (CENET) based on a novel training framework of historical
contrastive learning. CENET learns both the historical and non-historical
dependency to distinguish the most potential entities that best match the given
query. Simultaneously, by launching contrastive learning, it trains
representations of queries to probe whether the current moment is more
dependent on historical or non-historical events. These representations further
help train a binary classifier, whose output is a boolean mask, indicating the
related entities in the search space. During the inference process, CENET
employs a mask-based strategy to generate the final results. We evaluate our
proposed model on five benchmark graphs. The results demonstrate that CENET
significantly outperforms all existing methods in most metrics, achieving at
least 8.3% relative improvement of Hits@1 over previous state-of-the-art
baselines on event-based datasets.",http://arxiv.org/pdf/2308.15002v1
2308.14991v1,cs.LG,Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence,2023-08-29 02:43:58+00:00,"Continual learning aims to empower artificial intelligence (AI) with strong
adaptability to the real world. For this purpose, a desirable solution should
properly balance memory stability with learning plasticity, and acquire
sufficient compatibility to capture the observed distributions. Existing
advances mainly focus on preserving memory stability to overcome catastrophic
forgetting, but remain difficult to flexibly accommodate incremental changes as
biological intelligence (BI) does. By modeling a robust Drosophila learning
system that actively regulates forgetting with multiple learning modules, here
we propose a generic approach that appropriately attenuates old memories in
parameter distributions to improve learning plasticity, and accordingly
coordinates a multi-learner architecture to ensure solution compatibility.
Through extensive theoretical and empirical validation, our approach not only
clearly enhances the performance of continual learning, especially over
synaptic regularization methods in task-incremental settings, but also
potentially advances the understanding of neurological adaptive mechanisms,
serving as a novel paradigm to progress AI and BI together.",http://arxiv.org/pdf/2308.14991v1
2308.14983v1,cs.AI,Constructive Incremental Learning for Fault Diagnosis of Rolling Bearings with Ensemble Domain Adaptation,2023-08-29 02:23:58+00:00,"Given the prevalence of rolling bearing fault diagnosis as a practical issue
across various working conditions, the limited availability of samples
compounds the challenge. Additionally, the complexity of the external
environment and the structure of rolling bearings often manifests faults
characterized by randomness and fuzziness, hindering the effective extraction
of fault characteristics and restricting the accuracy of fault diagnosis. To
overcome these problems, this paper presents a novel approach termed
constructive Incremental learning-based ensemble domain adaptation (CIL-EDA)
approach. Specifically, it is implemented on stochastic configuration networks
(SCN) to constructively improve its adaptive performance in multi-domains.
Concretely, a cloud feature extraction method is employed in conjunction with
wavelet packet decomposition (WPD) to capture the uncertainty of fault
information from multiple resolution aspects. Subsequently, constructive
Incremental learning-based domain adaptation (CIL-DA) is firstly developed to
enhance the cross-domain learning capability of each hidden node through domain
matching and construct a robust fault classifier by leveraging limited labeled
data from both target and source domains. Finally, fault diagnosis results are
obtained by a majority voting of CIL-EDA which integrates CIL-DA and parallel
ensemble learning. Experimental results demonstrate that our CIL-DA outperforms
several domain adaptation methods and CIL-EDA consistently outperforms
state-of-art fault diagnosis methods in few-shot scenarios.",http://arxiv.org/pdf/2308.14983v1
2308.14976v1,astro-ph.SR,Efficient labeling of solar flux evolution videos by a deep learning model,2023-08-29 02:05:40+00:00,"Machine learning (ML) is becoming a critical tool for interrogation of large
complex data. Labeling, defined as the process of adding meaningful
annotations, is a crucial step of supervised ML. However, labeling datasets is
time consuming. Here we show that convolutional neural networks (CNNs), trained
on crudely labeled astronomical videos, can be leveraged to improve the quality
of data labeling and reduce the need for human intervention. We use videos of
the solar magnetic field, crudely labeled into two classes: emergence or
non-emergence of bipolar magnetic regions (BMRs), based on their first
detection on the solar disk. We train CNNs using crude labels, manually verify,
correct labeling vs. CNN disagreements, and repeat this process until
convergence. Traditionally, flux emergence labelling is done manually. We find
that a high-quality labeled dataset, derived through this iterative process,
reduces the necessary manual verification by 50%. Furthermore, by gradually
masking the videos and looking for maximum change in CNN inference, we locate
BMR emergence time without retraining the CNN. This demonstrates the
versatility of CNNs for simplifying the challenging task of labeling complex
dynamic events.",http://arxiv.org/pdf/2308.14976v1
2308.14972v1,cs.RO,LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks,2023-08-29 01:54:49+00:00,"This paper presents a novel approach to enhance autonomous robotic
manipulation using the Large Language Model (LLM) for logical inference,
converting high-level language commands into sequences of executable motion
functions. The proposed system combines the advantage of LLM with YOLO-based
environmental perception to enable robots to autonomously make reasonable
decisions and task planning based on the given commands. Additionally, to
address the potential inaccuracies or illogical actions arising from LLM, a
combination of teleoperation and Dynamic Movement Primitives (DMP) is employed
for action correction. This integration aims to improve the practicality and
generalizability of the LLM-based human-robot collaboration system.",http://arxiv.org/pdf/2308.14972v1
2308.14951v1,cs.CL,Robust Open-Set Spoken Language Identification and the CU MultiLang Dataset,2023-08-29 00:44:27+00:00,"Most state-of-the-art spoken language identification models are closed-set;
in other words, they can only output a language label from the set of classes
they were trained on. Open-set spoken language identification systems, however,
gain the ability to detect when an input exhibits none of the original
languages. In this paper, we implement a novel approach to open-set spoken
language identification that uses MFCC and pitch features, a TDNN model to
extract meaningful feature embeddings, confidence thresholding on softmax
outputs, and LDA and pLDA for learning to classify new unknown languages. We
present a spoken language identification system that achieves 91.76% accuracy
on trained languages and has the capability to adapt to unknown languages on
the fly. To that end, we also built the CU MultiLang Dataset, a large and
diverse multilingual speech corpus which was used to train and evaluate our
system.",http://arxiv.org/pdf/2308.14951v1
2308.14943v1,cs.AI,Transfusor: Transformer Diffusor for Controllable Human-like Generation of Vehicle Lane Changing Trajectories,2023-08-28 23:50:36+00:00,"With ongoing development of autonomous driving systems and increasing desire
for deployment, researchers continue to seek reliable approaches for ADS
systems. The virtual simulation test (VST) has become a prominent approach for
testing autonomous driving systems (ADS) and advanced driver assistance systems
(ADAS) due to its advantages of fast execution, low cost, and high
repeatability. However, the success of these simulation-based experiments
heavily relies on the realism of the testing scenarios. It is needed to create
more flexible and high-fidelity testing scenarios in VST in order to increase
the safety and reliabilityof ADS and ADAS.To address this challenge, this paper
introduces the ""Transfusor"" model, which leverages the transformer and diffusor
models (two cutting-edge deep learning generative technologies). The primary
objective of the Transfusor model is to generate highly realistic and
controllable human-like lane-changing trajectories in highway scenarios.
Extensive experiments were carried out, and the results demonstrate that the
proposed model effectively learns the spatiotemporal characteristics of humans'
lane-changing behaviors and successfully generates trajectories that closely
mimic real-world human driving. As such, the proposed model can play a critical
role of creating more flexible and high-fidelity testing scenarios in the VST,
ultimately leading to safer and more reliable ADS and ADAS.",http://arxiv.org/pdf/2308.14943v1
2308.14936v1,cs.CV,Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation,2023-08-28 23:23:53+00:00,"The Segment Anything Model (SAM) has rapidly been adopted for segmenting a
wide range of natural images. However, recent studies have indicated that SAM
exhibits subpar performance on 3D medical image segmentation tasks. In addition
to the domain gaps between natural and medical images, disparities in the
spatial arrangement between 2D and 3D images, the substantial computational
burden imposed by powerful GPU servers, and the time-consuming manual prompt
generation impede the extension of SAM to a broader spectrum of medical image
segmentation applications. To address these challenges, in this work, we
introduce a novel method, AutoSAM Adapter, designed specifically for 3D
multi-organ CT-based segmentation. We employ parameter-efficient adaptation
techniques in developing an automatic prompt learning paradigm to facilitate
the transformation of the SAM model's capabilities to 3D medical image
segmentation, eliminating the need for manually generated prompts. Furthermore,
we effectively transfer the acquired knowledge of the AutoSAM Adapter to other
lightweight models specifically tailored for 3D medical image analysis,
achieving state-of-the-art (SOTA) performance on medical image segmentation
tasks. Through extensive experimental evaluation, we demonstrate the AutoSAM
Adapter as a critical foundation for effectively leveraging the emerging
ability of foundation models in 2D natural image segmentation for 3D medical
image segmentation.",http://arxiv.org/pdf/2308.14936v1
2308.14925v1,physics.med-ph,"Patient-specific, mechanistic models of tumor growth incorporating artificial intelligence and big data",2023-08-28 22:52:17+00:00,"Despite the remarkable advances in cancer diagnosis, treatment, and
management that have occurred over the past decade, malignant tumors remain a
major public health problem. Further progress in combating cancer may be
enabled by personalizing the delivery of therapies according to the predicted
response for each individual patient. The design of personalized therapies
requires patient-specific information integrated into an appropriate
mathematical model of tumor response. A fundamental barrier to realizing this
paradigm is the current lack of a rigorous, yet practical, mathematical theory
of tumor initiation, development, invasion, and response to therapy. In this
review, we begin by providing an overview of different approaches to modeling
tumor growth and treatment, including mechanistic as well as data-driven models
based on ``big data"" and artificial intelligence. Next, we present illustrative
examples of mathematical models manifesting their utility and discussing the
limitations of stand-alone mechanistic and data-driven models. We further
discuss the potential of mechanistic models for not only predicting, but also
optimizing response to therapy on a patient-specific basis. We then discuss
current efforts and future possibilities to integrate mechanistic and
data-driven models. We conclude by proposing five fundamental challenges that
must be addressed to fully realize personalized care for cancer patients driven
by computational models.",http://arxiv.org/pdf/2308.14925v1
2308.14924v1,cs.LG,Optimal Economic Gas Turbine Dispatch with Deep Reinforcement Learning,2023-08-28 22:42:51+00:00,"Dispatching strategies for gas turbines (GTs) are changing in modern
electricity grids. A growing incorporation of intermittent renewable energy
requires GTs to operate more but shorter cycles and more frequently on partial
loads. Deep reinforcement learning (DRL) has recently emerged as a tool that
can cope with this development and dispatch GTs economically. The key
advantages of DRL are a model-free optimization and the ability to handle
uncertainties, such as those introduced by varying loads or renewable energy
production. In this study, three popular DRL algorithms are implemented for an
economic GT dispatch problem on a case study in Alberta, Canada. We highlight
the benefits of DRL by incorporating an existing thermodynamic software
provided by Siemens Energy into the environment model and by simulating
uncertainty via varying electricity prices, loads, and ambient conditions.
Among the tested algorithms and baseline methods, Deep Q-Networks (DQN)
obtained the highest rewards while Proximal Policy Optimization (PPO) was the
most sample efficient. We further propose and implement a method to assign GT
operation and maintenance cost dynamically based on operating hours and cycles.
Compared to existing methods, our approach better approximates the true cost of
modern GT dispatch and hence leads to more realistic policies.",http://arxiv.org/pdf/2308.14924v1
2308.14919v1,cs.LG,On Reward Structures of Markov Decision Processes,2023-08-28 22:29:16+00:00,"A Markov decision process can be parameterized by a transition kernel and a
reward function. Both play essential roles in the study of reinforcement
learning as evidenced by their presence in the Bellman equations. In our
inquiry of various kinds of ``costs'' associated with reinforcement learning
inspired by the demands in robotic applications, rewards are central to
understanding the structure of a Markov decision process and reward-centric
notions can elucidate important concepts in reinforcement learning.
Specifically, we studied the sample complexity of policy evaluation and
developed a novel estimator with an instance-specific error bound of
$\tilde{O}(\sqrt{\frac{\tau_s}{n}})$ for estimating a single state value. Under
the online regret minimization setting, we refined the transition-based MDP
constant, diameter, into a reward-based constant, maximum expected hitting
cost, and with it, provided a theoretical explanation for how a well-known
technique, potential-based reward shaping, could accelerate learning with
expert knowledge. In an attempt to study safe reinforcement learning, we
modeled hazardous environments with irrecoverability and proposed a
quantitative notion of safe learning via reset efficiency. In this setting, we
modified a classic algorithm to account for resets achieving promising
preliminary numerical results. Lastly, for MDPs with multiple reward functions,
we developed a planning algorithm that computationally efficiently finds Pareto
optimal stochastic policies.",http://arxiv.org/pdf/2308.14919v1
2308.14916v1,cs.IR,RecRec: Algorithmic Recourse for Recommender Systems,2023-08-28 22:26:50+00:00,"Recommender systems play an essential role in the choices people make in
domains such as entertainment, shopping, food, news, employment, and education.
The machine learning models underlying these recommender systems are often
enormously large and black-box in nature for users, content providers, and
system developers alike. It is often crucial for all stakeholders to understand
the model's rationale behind making certain predictions and recommendations.
This is especially true for the content providers whose livelihoods depend on
the recommender system. Drawing motivation from the practitioners' need, in
this work, we propose a recourse framework for recommender systems, targeted
towards the content providers. Algorithmic recourse in the recommendation
setting is a set of actions that, if executed, would modify the recommendations
(or ranking) of an item in the desired manner. A recourse suggests actions of
the form: ""if a feature changes X to Y, then the ranking of that item for a set
of users will change to Z."" Furthermore, we demonstrate that RecRec is highly
effective in generating valid, sparse, and actionable recourses through an
empirical evaluation of recommender systems trained on three real-world
datasets. To the best of our knowledge, this work is the first to conceptualize
and empirically test a generalized framework for generating recourses for
recommender systems.",http://arxiv.org/pdf/2308.14916v1
2308.14899v1,cs.CV,RobustCLEVR: A Benchmark and Framework for Evaluating Robustness in Object-centric Learning,2023-08-28 20:52:18+00:00,"Object-centric representation learning offers the potential to overcome
limitations of image-level representations by explicitly parsing image scenes
into their constituent components. While image-level representations typically
lack robustness to natural image corruptions, the robustness of object-centric
methods remains largely untested. To address this gap, we present the
RobustCLEVR benchmark dataset and evaluation framework. Our framework takes a
novel approach to evaluating robustness by enabling the specification of causal
dependencies in the image generation process grounded in expert knowledge and
capable of producing a wide range of image corruptions unattainable in existing
robustness evaluations. Using our framework, we define several causal models of
the image corruption process which explicitly encode assumptions about the
causal relationships and distributions of each corruption type. We generate
dataset variants for each causal model on which we evaluate state-of-the-art
object-centric methods. Overall, we find that object-centric methods are not
inherently robust to image corruptions. Our causal evaluation approach exposes
model sensitivities not observed using conventional evaluation processes,
yielding greater insight into robustness differences across algorithms. Lastly,
while conventional robustness evaluations view corruptions as
out-of-distribution, we use our causal framework to show that even training on
in-distribution image corruptions does not guarantee increased model
robustness. This work provides a step towards more concrete and substantiated
understanding of model performance and deterioration under complex corruption
processes of the real-world.",http://arxiv.org/pdf/2308.14899v1
2308.14898v1,cs.AI,Proceedings 39th International Conference on Logic Programming,2023-08-28 20:46:59+00:00,"This volume contains the Technical Communications presented at the 39th
International Conference on Logic Programming (ICLP 2023), held at Imperial
College London, UK from July 9 to July 15, 2023. Technical Communications
included here concern the Main Track, the Doctoral Consortium, the Application
and Systems/Demo track, the Recently Published Research Track, the
Birds-of-a-Feather track, the Thematic Tracks on Logic Programming and Machine
Learning, and Logic Programming and Explainability, Ethics, and
Trustworthiness.",http://arxiv.org/pdf/2308.14898v1
2308.14897v1,cs.LG,Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning,2023-08-28 20:46:07+00:00,"Offline reinforcement learning aims to utilize datasets of previously
gathered environment-action interaction records to learn a policy without
access to the real environment. Recent work has shown that offline
reinforcement learning can be formulated as a sequence modeling problem and
solved via supervised learning with approaches such as decision transformer.
While these sequence-based methods achieve competitive results over
return-to-go methods, especially on tasks that require longer episodes or with
scarce rewards, importance sampling is not considered to correct the policy
bias when dealing with off-policy data, mainly due to the absence of behavior
policy and the use of deterministic evaluation policies. To this end, we
propose DPE: an RL algorithm that blends offline sequence modeling and offline
reinforcement learning with Double Policy Estimation (DPE) in a unified
framework with statistically proven properties on variance reduction. We
validate our method in multiple tasks of OpenAI Gym with D4RL benchmarks. Our
method brings a performance improvements on selected methods which outperforms
SOTA baselines in several tasks, demonstrating the advantages of enabling
double policy estimation for sequence-modeled reinforcement learning.",http://arxiv.org/pdf/2308.14897v1
2308.14893v1,cs.CV,When hard negative sampling meets supervised contrastive learning,2023-08-28 20:30:10+00:00,"State-of-the-art image models predominantly follow a two-stage strategy:
pre-training on large datasets and fine-tuning with cross-entropy loss. Many
studies have shown that using cross-entropy can result in sub-optimal
generalisation and stability. While the supervised contrastive loss addresses
some limitations of cross-entropy loss by focusing on intra-class similarities
and inter-class differences, it neglects the importance of hard negative
mining. We propose that models will benefit from performance improvement by
weighting negative samples based on their dissimilarity to positive
counterparts. In this paper, we introduce a new supervised contrastive learning
objective, SCHaNe, which incorporates hard negative sampling during the
fine-tuning phase. Without requiring specialized architectures, additional
data, or extra computational resources, experimental results indicate that
SCHaNe outperforms the strong baseline BEiT-3 in Top-1 accuracy across various
benchmarks, with significant gains of up to $3.32\%$ in few-shot learning
settings and $3.41\%$ in full dataset fine-tuning. Importantly, our proposed
objective sets a new state-of-the-art for base models on ImageNet-1k, achieving
an 86.14\% accuracy. Furthermore, we demonstrate that the proposed objective
yields better embeddings and explains the improved effectiveness observed in
our experiments.",http://arxiv.org/pdf/2308.14893v1
2308.14864v1,cs.LG,NAS-X: Neural Adaptive Smoothing via Twisting,2023-08-28 19:35:43+00:00,"We present Neural Adaptive Smoothing via Twisting (NAS-X), a method for
learning and inference in sequential latent variable models based on reweighted
wake-sleep (RWS). NAS-X works with both discrete and continuous latent
variables, and leverages smoothing SMC to fit a broader range of models than
traditional RWS methods. We test NAS-X on discrete and continuous tasks and
find that it substantially outperforms previous variational and RWS-based
methods in inference and parameter recovery.",http://arxiv.org/pdf/2308.14864v1
2308.14861v1,cs.LG,Evaluation of Key Spatiotemporal Learners for Print Track Anomaly Classification Using Melt Pool Image Streams,2023-08-28 19:31:53+00:00,"Recent applications of machine learning in metal additive manufacturing (MAM)
have demonstrated significant potential in addressing critical barriers to the
widespread adoption of MAM technology. Recent research in this field emphasizes
the importance of utilizing melt pool signatures for real-time defect
prediction. While high-quality melt pool image data holds the promise of
enabling precise predictions, there has been limited exploration into the
utilization of cutting-edge spatiotemporal models that can harness the inherent
transient and sequential characteristics of the additive manufacturing process.
This research introduces and puts into practice some of the leading deep
spatiotemporal learning models that can be adapted for the classification of
melt pool image streams originating from various materials, systems, and
applications. Specifically, it investigates two-stream networks comprising
spatial and temporal streams, a recurrent spatial network, and a factorized 3D
convolutional neural network. The capacity of these models to generalize when
exposed to perturbations in melt pool image data is examined using data
perturbation techniques grounded in real-world process scenarios. The
implemented architectures demonstrate the ability to capture the spatiotemporal
features of melt pool image sequences. However, among these models, only the
Kinetics400 pre-trained SlowFast network, categorized as a two-stream network,
exhibits robust generalization capabilities in the presence of data
perturbations.",http://arxiv.org/pdf/2308.14861v1
2308.14850v1,cs.CL,Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models,2023-08-28 19:11:52+00:00,"This report introduces the Attention Visualizer package, which is crafted to
visually illustrate the significance of individual words in encoder-only
transformer-based models. In contrast to other methods that center on tokens
and self-attention scores, our approach will examine the words and their impact
on the final embedding representation. Libraries like this play a crucial role
in enhancing the interpretability and explainability of neural networks. They
offer the opportunity to illuminate their internal mechanisms, providing a
better understanding of how they operate and can be enhanced. You can access
the code and review examples on the following GitHub repository:
https://github.com/AlaFalaki/AttentionVisualizer.",http://arxiv.org/pdf/2308.14850v1
2308.14846v1,cs.HC,Trust in Construction AI-Powered Collaborative Robots: A Qualitative Empirical Analysis,2023-08-28 19:07:14+00:00,"Construction technology researchers and forward-thinking companies are
experimenting with collaborative robots (aka cobots), powered by artificial
intelligence (AI), to explore various automation scenarios as part of the
digital transformation of the industry. Intelligent cobots are expected to be
the dominant type of robots in the future of work in construction. However, the
black-box nature of AI-powered cobots and unknown technical and psychological
aspects of introducing them to job sites are precursors to trust challenges. By
analyzing the results of semi-structured interviews with construction
practitioners using grounded theory, this paper investigates the
characteristics of trustworthy AI-powered cobots in construction. The study
found that while the key trust factors identified in a systematic literature
review -- conducted previously by the authors -- resonated with the field
experts and end users, other factors such as financial considerations and the
uncertainty associated with change were also significant barriers against
trusting AI-powered cobots in construction.",http://arxiv.org/pdf/2308.14846v1
2308.14843v1,cs.RO,Robust Activity Recognition for Adaptive Worker-Robot Interaction using Transfer Learning,2023-08-28 19:03:46+00:00,"Human activity recognition (HAR) using machine learning has shown tremendous
promise in detecting construction workers' activities. HAR has many
applications in human-robot interaction research to enable robots'
understanding of human counterparts' activities. However, many existing HAR
approaches lack robustness, generalizability, and adaptability. This paper
proposes a transfer learning methodology for activity recognition of
construction workers that requires orders of magnitude less data and compute
time for comparable or better classification accuracy. The developed algorithm
transfers features from a model pre-trained by the original authors and
fine-tunes them for the downstream task of activity recognition in
construction. The model was pre-trained on Kinetics-400, a large-scale
video-based human activity recognition dataset with 400 distinct classes. The
model was fine-tuned and tested using videos captured from manual material
handling (MMH) activities found on YouTube. Results indicate that the
fine-tuned model can recognize distinct MMH tasks in a robust and adaptive
manner which is crucial for the widespread deployment of collaborative robots
in construction.",http://arxiv.org/pdf/2308.14843v1
2308.14841v1,cs.HC,Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction,2023-08-28 18:58:01+00:00,"Ergonomic efficiency is essential to the mass and prolonged adoption of VR/AR
experiences. While VR/AR head-mounted displays unlock users' natural wide-range
head movements during viewing, their neck muscle comfort is inevitably
compromised by the added hardware weight. Unfortunately, little quantitative
knowledge for understanding and addressing such an issue is available so far.
  Leveraging electromyography devices, we measure, model, and predict VR users'
neck muscle contraction levels (MCL) while they move their heads to interact
with the virtual environment. Specifically, by learning from collected
physiological data, we develop a bio-physically inspired computational model to
predict neck MCL under diverse head kinematic states. Beyond quantifying the
cumulative MCL of completed head movements, our model can also predict
potential MCL requirements with target head poses only. A series of objective
evaluations and user studies demonstrate its prediction accuracy and
generality, as well as its ability in reducing users' neck discomfort by
optimizing the layout of visual targets. We hope this research will motivate
new ergonomic-centered designs for VR/AR and interactive graphics applications.
Source code is released at:
https://github.com/NYU-ICL/xr-ergonomics-neck-comfort.",http://arxiv.org/pdf/2308.14841v1
2308.14840v1,cs.AI,Identifying and Mitigating the Security Risks of Generative AI,2023-08-28 18:51:09+00:00,"Every major technical invention resurfaces the dual-use dilemma -- the new
technology has the potential to be used for good as well as for harm.
Generative AI (GenAI) techniques, such as large language models (LLMs) and
diffusion models, have shown remarkable capabilities (e.g., in-context
learning, code-completion, and text-to-image generation and editing). However,
GenAI can be used just as well by attackers to generate new attacks and
increase the velocity and efficacy of existing attacks.
  This paper reports the findings of a workshop held at Google (co-organized by
Stanford University and the University of Wisconsin-Madison) on the dual-use
dilemma posed by GenAI. This paper is not meant to be comprehensive, but is
rather an attempt to synthesize some of the interesting findings from the
workshop. We discuss short-term and long-term goals for the community on this
topic. We hope this paper provides both a launching point for a discussion on
this important topic as well as interesting problems that the research
community can work to address.",http://arxiv.org/pdf/2308.14840v1
2308.14815v1,cs.AI,Distributionally Robust Statistical Verification with Imprecise Neural Networks,2023-08-28 18:06:24+00:00,"A particularly challenging problem in AI safety is providing guarantees on
the behavior of high-dimensional autonomous systems. Verification approaches
centered around reachability analysis fail to scale, and purely statistical
approaches are constrained by the distributional assumptions about the sampling
process. Instead, we pose a distributionally robust version of the statistical
verification problem for black-box systems, where our performance guarantees
hold over a large family of distributions. This paper proposes a novel approach
based on a combination of active learning, uncertainty quantification, and
neural network verification. A central piece of our approach is an ensemble
technique called Imprecise Neural Networks, which provides the uncertainty to
guide active learning. The active learning uses an exhaustive neural-network
verification tool Sherlock to collect samples. An evaluation on multiple
physical simulators in the openAI gym Mujoco environments with
reinforcement-learned controllers demonstrates that our approach can provide
useful and scalable guarantees for high-dimensional systems.",http://arxiv.org/pdf/2308.14815v1
2308.14752v1,cs.CY,"AI Deception: A Survey of Examples, Risks, and Potential Solutions",2023-08-28 17:59:35+00:00,"This paper argues that a range of current AI systems have learned how to
deceive humans. We define deception as the systematic inducement of false
beliefs in the pursuit of some outcome other than the truth. We first survey
empirical examples of AI deception, discussing both special-use AI systems
(including Meta's CICERO) built for specific competitive situations, and
general-purpose AI systems (such as large language models). Next, we detail
several risks from AI deception, such as fraud, election tampering, and losing
control of AI systems. Finally, we outline several potential solutions to the
problems posed by AI deception: first, regulatory frameworks should subject AI
systems that are capable of deception to robust risk-assessment requirements;
second, policymakers should implement bot-or-not laws; and finally,
policymakers should prioritize the funding of relevant research, including
tools to detect AI deception and to make AI systems less deceptive.
Policymakers, researchers, and the broader public should work proactively to
prevent AI deception from destabilizing the shared foundations of our society.",http://arxiv.org/pdf/2308.14752v1
2308.14737v1,cs.CV,Flexible Techniques for Differentiable Rendering with 3D Gaussians,2023-08-28 17:38:31+00:00,"Fast, reliable shape reconstruction is an essential ingredient in many
computer vision applications. Neural Radiance Fields demonstrated that
photorealistic novel view synthesis is within reach, but was gated by
performance requirements for fast reconstruction of real scenes and objects.
Several recent approaches have built on alternative shape representations, in
particular, 3D Gaussians. We develop extensions to these renderers, such as
integrating differentiable optical flow, exporting watertight meshes and
rendering per-ray normals. Additionally, we show how two of the recent methods
are interoperable with each other. These reconstructions are quick, robust, and
easily performed on GPU or CPU. For code and visual examples, see
https://leonidk.github.io/fmb-plus",http://arxiv.org/pdf/2308.14737v1
2308.14732v1,cs.AI,Bayesian artificial brain with ChatGPT,2023-08-28 17:34:24+00:00,"This paper aims to investigate the mathematical problem-solving capabilities
of Chat Generative Pre-Trained Transformer (ChatGPT) in case of Bayesian
reasoning. The study draws inspiration from Zhu & Gigerenzer's research in
2006, which posed the question: Can children reason the Bayesian way? In the
pursuit of answering this question, a set of 10 Bayesian reasoning problems
were presented. The results of their work revealed that children's ability to
reason effectively using Bayesian principles is contingent upon a
well-structured information representation. In this paper, we present the same
set of 10 Bayesian reasoning problems to ChatGPT. Remarkably, the results
demonstrate that ChatGPT provides the right solutions to all problems.",http://arxiv.org/pdf/2308.14732v1
2308.14731v1,cs.SE,Distilled GPT for Source Code Summarization,2023-08-28 17:34:07+00:00,"A code summary is a brief natural language description of source code.
Summaries are usually only a single sentence long, and yet form the backbone of
developer documentation. A short descriptions such as ""changes all visible
polygons to the color blue"" can give a programmer a high-level idea of what
code does without the effort of reading the code itself. Recently, products
based on Large Language Models such as ChatGPT have demonstrated a strong
ability to write these descriptions automatically. However, to use these tools,
programmers must send their code to untrusted third parties for processing
(e.g., via an API call). This loss of custody is not acceptable to many
organizations. In this paper, we present an alternative: we train an open
source model using sample output generated by GPT-3.5 in a process related to
knowledge distillation. Our model is small enough (350m parameters) to be run
on a single 16gb GPU, yet we show in our evaluation that it is large enough to
mimic GPT-3.5 on this task.",http://arxiv.org/pdf/2308.14731v1
2308.14726v1,cs.CV,PanoSwin: a Pano-style Swin Transformer for Panorama Understanding,2023-08-28 17:30:14+00:00,"In panorama understanding, the widely used equirectangular projection (ERP)
entails boundary discontinuity and spatial distortion. It severely deteriorates
the conventional CNNs and vision Transformers on panoramas. In this paper, we
propose a simple yet effective architecture named PanoSwin to learn panorama
representations with ERP. To deal with the challenges brought by
equirectangular projection, we explore a pano-style shift windowing scheme and
novel pitch attention to address the boundary discontinuity and the spatial
distortion, respectively. Besides, based on spherical distance and Cartesian
coordinates, we adapt absolute positional embeddings and relative positional
biases for panoramas to enhance panoramic geometry information. Realizing that
planar image understanding might share some common knowledge with panorama
understanding, we devise a novel two-stage learning framework to facilitate
knowledge transfer from the planar images to panoramas. We conduct experiments
against the state-of-the-art on various panoramic tasks, i.e., panoramic object
detection, panoramic classification, and panoramic layout estimation. The
experimental results demonstrate the effectiveness of PanoSwin in panorama
understanding.",http://arxiv.org/pdf/2308.14726v1
2308.14719v1,cs.AI,Hierarchical Time Series Forecasting with Bayesian Modeling,2023-08-28 17:20:47+00:00,"We encounter time series data in many domains such as finance, physics,
business, and weather. One of the main tasks of time series analysis, one that
helps to take informed decisions under uncertainty, is forecasting. Time series
are often hierarchically structured, e.g., a company sales might be broken down
into different regions, and each region into different stores. In some cases
the number of series in the hierarchy is too big to fit in a single model to
produce forecasts in relevant time, and a decentralized approach is beneficial.
  One way to do this is to train independent forecasting models for each series
and for some summary statistics series implied by the hierarchy (e.g. the sum
of all series) and to pass those models to a reconciliation algorithm to
improve those forecasts by sharing information between the series.
  In this work we focus on the reconciliation step, and propose a method to do
so from a Bayesian perspective - Bayesian forecast reconciliation. We also
define the common case of linear Gaussian reconciliation, where the forecasts
are Gaussian and the hierarchy has linear structure, and show that we can
compute reconciliation in closed form. We evaluate these methods on synthetic
and real data sets, and compare them to other work in this field.",http://arxiv.org/pdf/2308.14719v1
2308.14711v1,cs.LG,Fast Feedforward Networks,2023-08-28 17:11:41+00:00,"We break the linear link between the layer size and its inference cost by
introducing the fast feedforward (FFF) architecture, a logarithmic-time
alternative to feedforward networks.
  We show that FFFs give comparable performance to feedforward networks at an
exponential fraction of their inference cost, are quicker to deliver
performance compared to mixture-of-expert networks, and can readily take the
place of either in transformers.
  Pushing FFFs to the absolute limit, we train a vision transformer to perform
single-neuron inferences at the cost of only 5.8% performance decrease against
the full-width variant.
  Our implementation is available as a Python package; just use ""pip install
fastfeedforward"".",http://arxiv.org/pdf/2308.14711v1
2308.14710v1,cs.CV,VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation,2023-08-28 17:10:12+00:00,"Existing approaches to unsupervised video instance segmentation typically
rely on motion estimates and experience difficulties tracking small or
divergent motions. We present VideoCutLER, a simple method for unsupervised
multi-instance video segmentation without using motion-based learning signals
like optical flow or training on natural videos. Our key insight is that using
high-quality pseudo masks and a simple video synthesis method for model
training is surprisingly sufficient to enable the resulting video model to
effectively segment and track multiple instances across video frames. We show
the first competitive unsupervised learning results on the challenging
YouTubeVIS-2019 benchmark, achieving 50.7% APvideo^50 , surpassing the previous
state-of-the-art by a large margin. VideoCutLER can also serve as a strong
pretrained model for supervised video instance segmentation tasks, exceeding
DINO by 15.9% on YouTubeVIS-2019 in terms of APvideo.",http://arxiv.org/pdf/2308.14710v1
2308.14622v1,cs.IR,TRIVEA: Transparent Ranking Interpretation using Visual Explanation of Black-Box Algorithmic Rankers,2023-08-28 16:58:44+00:00,"Ranking schemes drive many real-world decisions, like, where to study, whom
to hire, what to buy, etc. Many of these decisions often come with high
consequences. For example, a university can be deemed less prestigious if not
featured in a top-k list, and consumers might not even explore products that do
not get recommended to buyers. At the heart of most of these decisions are
opaque ranking schemes, which dictate the ordering of data entities, but their
internal logic is inaccessible or proprietary. Drawing inferences about the
ranking differences is like a guessing game to the stakeholders, like, the
rankees (i.e., the entities who are ranked, like product companies) and the
decision-makers (i.e., who use the rankings, like buyers). In this paper, we
aim to enable transparency in ranking interpretation by using algorithmic
rankers that learn from available data and by enabling human reasoning about
the learned ranking differences using explainable AI (XAI) methods. To realize
this aim, we leverage the exploration-explanation paradigm of human-data
interaction to let human stakeholders explore subsets and groupings of complex
multi-attribute ranking data using visual explanations of model fit and
attribute influence on rankings. We realize this explanation paradigm for
transparent ranking interpretation in TRIVEA, a visual analytic system that is
fueled by: i) visualizations of model fit derived from algorithmic rankers that
learn the associations between attributes and rankings from available data and
ii) visual explanations derived from XAI methods that help abstract important
patterns, like, the relative influence of attributes in different ranking
ranges. Using TRIVEA, end users not trained in data science have the agency to
transparently reason about the global and local behavior of the rankings
without the need to open black-box ranking models and develop confidence in the
resulting attribute-based inferences. We demonstrate the efficacy of TRIVEA
using multiple usage scenarios and subjective feedback from researchers with
diverse domain expertise. Keywords: Visual Analytics, Learning-to-Rank,
Explainable ML, Ranking",http://arxiv.org/pdf/2308.14622v1
2308.14697v1,cs.HC,Assessing Trust in Construction AI-Powered Collaborative Robots using Structural Equation Modeling,2023-08-28 16:39:22+00:00,"This study aimed to investigate the key technical and psychological factors
that impact the architecture, engineering, and construction (AEC)
professionals' trust in collaborative robots (cobots) powered by artificial
intelligence (AI). The study employed a nationwide survey of 600 AEC industry
practitioners to gather in-depth responses and valuable insights into the
future opportunities for promoting the adoption, cultivation, and training of a
skilled workforce to leverage this technology effectively. A Structural
Equation Modeling (SEM) analysis revealed that safety and reliability are
significant factors for the adoption of AI-powered cobots in construction. Fear
of being replaced resulting from the use of cobots can have a substantial
effect on the mental health of the affected workers. A lower error rate in jobs
involving cobots, safety measurements, and security of data collected by cobots
from jobsites significantly impact reliability, while the transparency of
cobots' inner workings can benefit accuracy, robustness, security, privacy, and
communication, and results in higher levels of automation, all of which
demonstrated as contributors to trust. The study's findings provide critical
insights into the perceptions and experiences of AEC professionals towards
adoption of cobots in construction and help project teams determine the
adoption approach that aligns with the company's goals workers' welfare.",http://arxiv.org/pdf/2308.14697v1
2308.14784v1,cs.LG,Generating tabular datasets under differential privacy,2023-08-28 16:35:43+00:00,"Machine Learning (ML) is accelerating progress across fields and industries,
but relies on accessible and high-quality training data. Some of the most
important datasets are found in biomedical and financial domains in the form of
spreadsheets and relational databases. But this tabular data is often sensitive
in nature. Synthetic data generation offers the potential to unlock sensitive
data, but generative models tend to memorise and regurgitate training data,
which undermines the privacy goal. To remedy this, researchers have
incorporated the mathematical framework of Differential Privacy (DP) into the
training process of deep neural networks. But this creates a trade-off between
the quality and privacy of the resulting data. Generative Adversarial Networks
(GANs) are the dominant paradigm for synthesising tabular data under DP, but
suffer from unstable adversarial training and mode collapse, which are
exacerbated by the privacy constraints and challenging tabular data modality.
This work optimises the quality-privacy trade-off of generative models,
producing higher quality tabular datasets with the same privacy guarantees. We
implement novel end-to-end models that leverage attention mechanisms to learn
reversible tabular representations. We also introduce TableDiffusion, the first
differentially-private diffusion model for tabular data synthesis. Our
experiments show that TableDiffusion produces higher-fidelity synthetic
datasets, avoids the mode collapse problem, and achieves state-of-the-art
performance on privatised tabular data synthesis. By implementing
TableDiffusion to predict the added noise, we enabled it to bypass the
challenges of reconstructing mixed-type tabular data. Overall, the diffusion
paradigm proves vastly more data and privacy efficient than the adversarial
paradigm, due to augmented re-use of each data batch and a smoother iterative
training process.",http://arxiv.org/pdf/2308.14784v1
2308.14687v1,cs.SE,MELT: Mining Effective Lightweight Transformations from Pull Requests,2023-08-28 16:21:52+00:00,"Software developers often struggle to update APIs, leading to manual,
time-consuming, and error-prone processes. We introduce MELT, a new approach
that generates lightweight API migration rules directly from pull requests in
popular library repositories. Our key insight is that pull requests merged into
open-source libraries are a rich source of information sufficient to mine API
migration rules. By leveraging code examples mined from the library source and
automatically generated code examples based on the pull requests, we infer
transformation rules in \comby, a language for structural code search and
replace. Since inferred rules from single code examples may be too specific, we
propose a generalization procedure to make the rules more applicable to client
projects. MELT rules are syntax-driven, interpretable, and easily adaptable.
Moreover, unlike previous work, our approach enables rule inference to
seamlessly integrate into the library workflow, removing the need to wait for
client code migrations. We evaluated MELT on pull requests from four popular
libraries, successfully mining 461 migration rules from code examples in pull
requests and 114 rules from auto-generated code examples. Our generalization
procedure increases the number of matches for mined rules by 9x. We applied
these rules to client projects and ran their tests, which led to an overall
decrease in the number of warnings and fixing some test cases demonstrating
MELT's effectiveness in real-world scenarios.",http://arxiv.org/pdf/2308.14687v1
2308.14683v1,cs.CL,Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts,2023-08-28 16:18:50+00:00,"Detecting online sexual predatory behaviours and abusive language on social
media platforms has become a critical area of research due to the growing
concerns about online safety, especially for vulnerable populations such as
children and adolescents. Researchers have been exploring various techniques
and approaches to develop effective detection systems that can identify and
mitigate these risks. Recent development of large language models (LLMs) has
opened a new opportunity to address this problem more effectively. This paper
proposes an approach to detection of online sexual predatory chats and abusive
language using the open-source pretrained Llama 2 7B-parameter model, recently
released by Meta GenAI. We fine-tune the LLM using datasets with different
sizes, imbalance degrees, and languages (i.e., English, Roman Urdu and Urdu).
Based on the power of LLMs, our approach is generic and automated without a
manual search for a synergy between feature extraction and classifier design
steps like conventional methods in this domain. Experimental results show a
strong performance of the proposed approach, which performs proficiently and
consistently across three distinct datasets with five sets of experiments. This
study's outcomes indicate that the proposed method can be implemented in
real-world applications (even with non-English languages) for flagging sexual
predators, offensive or toxic content, hate speech, and discriminatory language
in online discussions and comments to maintain respectful internet or digital
communities. Furthermore, it can be employed for solving text classification
problems with other potential applications such as sentiment analysis, spam and
phishing detection, sorting legal documents, fake news detection, language
identification, user intent recognition, text-based product categorization,
medical record analysis, and resume screening.",http://arxiv.org/pdf/2308.14683v1
2308.14669v1,cs.CL,ANER: Arabic and Arabizi Named Entity Recognition using Transformer-Based Approach,2023-08-28 15:54:48+00:00,"One of the main tasks of Natural Language Processing (NLP), is Named Entity
Recognition (NER). It is used in many applications and also can be used as an
intermediate step for other tasks. We present ANER, a web-based named entity
recognizer for the Arabic, and Arabizi languages. The model is built upon BERT,
which is a transformer-based encoder. It can recognize 50 different entity
classes, covering various fields. We trained our model on the WikiFANE\_Gold
dataset which consists of Wikipedia articles. We achieved an F1 score of
88.7\%, which beats CAMeL Tools' F1 score of 83\% on the ANERcorp dataset,
which has only 4 classes. We also got an F1 score of 77.7\% on the
NewsFANE\_Gold dataset which contains out-of-domain data from News articles.
The system is deployed on a user-friendly web interface that accepts users'
inputs in Arabic, or Arabizi. It allows users to explore the entities in the
text by highlighting them. It can also direct users to get information about
entities through Wikipedia directly. We added the ability to do NER using our
model, or CAMeL Tools' model through our website. ANER is publicly accessible
at \url{http://www.aner.online}. We also deployed our model on HuggingFace at
https://huggingface.co/boda/ANER, to allow developers to test and use it.",http://arxiv.org/pdf/2308.14669v1
2308.14667v1,cs.CV,Neural Network-Based Histologic Remission Prediction In Ulcerative Colitis,2023-08-28 15:54:14+00:00,"BACKGROUND & AIMS: Histological remission (HR) is advocated and considered as
a new therapeutic target in ulcerative colitis (UC). Diagnosis of histologic
remission currently relies on biopsy; during this process, patients are at risk
for bleeding, infection, and post-biopsy fibrosis. In addition, histologic
response scoring is complex and time-consuming, and there is heterogeneity
among pathologists. Endocytoscopy (EC) is a novel ultra-high magnification
endoscopic technique that can provide excellent in vivo assessment of glands.
Based on the EC technique, we propose a neural network model that can assess
histological disease activity in UC using EC images to address the above
issues. The experiment results demonstrate that the proposed method can assist
patients in precise treatment and prognostic assessment.
  METHODS: We construct a neural network model for UC evaluation. A total of
5105 images of 154 intestinal segments from 87 patients undergoing EC treatment
at a center in China between March 2022 and March 2023 are scored according to
the Geboes score. Subsequently, 103 intestinal segments are used as the
training set, 16 intestinal segments are used as the validation set for neural
network training, and the remaining 35 intestinal segments are used as the test
set to measure the model performance together with the validation set.
  RESULTS: By treating HR as a negative category and histologic activity as a
positive category, the proposed neural network model can achieve an accuracy of
0.9, a specificity of 0.95, a sensitivity of 0.75, and an area under the curve
(AUC) of 0.81.
  CONCLUSION: We develop a specific neural network model that can distinguish
histologic remission/activity in EC images of UC, which helps to accelerate
clinical histological diagnosis.
  keywords: ulcerative colitis; Endocytoscopy; Geboes score; neural network.",http://arxiv.org/pdf/2308.14667v1
2308.14781v1,cs.LG,Conflict-Aware Active Automata Learning,2023-08-28 15:50:34+00:00,"Active automata learning algorithms cannot easily handle \emph{conflict} in
the observation data (different outputs observed for the same inputs). This
inherent inability to recover after a conflict impairs their effective
applicability in scenarios where noise is present or the system under learning
is mutating.
  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to
enable handling conflicting information during the learning process. The core
idea is to consider the so-called observation tree as a first-class citizen in
the learning process. Though this idea is explored in recent work, we take it
to its full effect by enabling its use with any existing learner and minimizing
the number of tests performed on the system under learning, specially in the
face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over
30 different realistic targets, and over 18,000 different scenarios. The
results of the evaluation show that C3AL is a suitable alternative framework
for closed-box learning that can better handle noise and mutations.targets, and
over 18,000 different scenarios. The results of the evaluation show that C3AL
is a suitable alternative framework for closed-box learning that can better
handle noise and mutations.",http://arxiv.org/pdf/2308.14781v1
2308.14657v1,cs.AI,DeepHealthNet: Adolescent Obesity Prediction System Based on a Deep Learning Framework,2023-08-28 15:40:31+00:00,"Childhood and adolescent obesity rates are a global concern because obesity
is associated with chronic diseases and long-term health risks. Artificial
intelligence technology has emerged as a promising solution to accurately
predict obesity rates and provide personalized feedback to adolescents. This
study emphasizes the importance of early identification and prevention of
obesity-related health issues. Factors such as height, weight, waist
circumference, calorie intake, physical activity levels, and other relevant
health information need to be considered for developing robust algorithms for
obesity rate prediction and delivering personalized feedback. Hence, by
collecting health datasets from 321 adolescents, we proposed an adolescent
obesity prediction system that provides personalized predictions and assists
individuals in making informed health decisions. Our proposed deep learning
framework, DeepHealthNet, effectively trains the model using data augmentation
techniques, even when daily health data are limited, resulting in improved
prediction accuracy (acc: 0.8842). Additionally, the study revealed variations
in the prediction of the obesity rate between boys (acc: 0.9320) and girls
(acc: 0.9163), allowing the identification of disparities and the determination
of the optimal time to provide feedback. The proposed system shows significant
potential in effectively addressing childhood and adolescent obesity.",http://arxiv.org/pdf/2308.14657v1
2308.15448v1,cs.CL,Vulgar Remarks Detection in Chittagonian Dialect of Bangla,2023-08-29 17:19:32+00:00,"The negative effects of online bullying and harassment are increasing with
Internet popularity, especially in social media. One solution is using natural
language processing (NLP) and machine learning (ML) methods for the automatic
detection of harmful remarks, but these methods are limited in low-resource
languages like the Chittagonian dialect of Bangla.This study focuses on
detecting vulgar remarks in social media using supervised ML and deep learning
algorithms.Logistic Regression achieved promising accuracy (0.91) while simple
RNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the
issue that NN algorithms require more data.",http://arxiv.org/pdf/2308.15448v1
2308.15419v1,cs.CL,"Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",2023-08-29 16:24:09+00:00,"How do language models learn to make predictions during pre-training? To
study this question, we extract learning curves from five autoregressive
English language model pre-training runs, for 1M tokens in context. We observe
that the language models generate short repetitive phrases before learning to
generate longer and more coherent text. We quantify the final surprisal,
within-run variability, age of acquisition, forgettability, and cross-run
variability of learning curves for individual tokens in context. More frequent
tokens reach lower final surprisals, exhibit less variability within and across
pre-training runs, are learned earlier, and are less likely to be ""forgotten""
during pre-training. Higher n-gram probabilities further accentuate these
effects. Independent of the target token, shorter and more frequent contexts
correlate with marginally more stable and quickly acquired predictions. Effects
of part-of-speech are also small, although nouns tend to be acquired later and
less stably than verbs, adverbs, and adjectives. Our work contributes to a
better understanding of language model pre-training dynamics and informs the
deployment of stable language models in practice.",http://arxiv.org/pdf/2308.15419v1
2308.15399v1,cs.CL,Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?,2023-08-29 15:57:32+00:00,"Making moral judgments is an essential step toward developing ethical AI
systems. Prevalent approaches are mostly implemented in a bottom-up manner,
which uses a large set of annotated data to train models based on crowd-sourced
opinions about morality. These approaches have been criticized for potentially
overgeneralizing a limited group of annotators' moral stances and lacking
explainability. In contrast, top-down approaches make moral judgments grounded
in a set of principles. However, it remains conceptual due to the incapability
of previous language models and the unsolved debate among moral principles. In
this study, we propose a flexible framework to steer Large Language Models
(LLMs) to perform moral reasoning with well-established moral theories from
interdisciplinary research. The theory-guided top-down framework can
incorporate various moral theories. Our experiments demonstrate the
effectiveness of the proposed framework on datasets derived from moral
theories. Furthermore, we show the alignment between different moral theories
and existing morality datasets. Our analysis exhibits the potentials and flaws
in existing resources (models and datasets) in developing explainable moral
judgment-making systems.",http://arxiv.org/pdf/2308.15399v1
2308.15363v1,cs.DB,Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation,2023-08-29 14:59:54+00:00,"Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL
task. However, the absence of a systematical benchmark inhibits the development
of designing effective, efficient and economic LLM-based Text-to-SQL solutions.
To address this challenge, in this paper, we first conduct a systematical and
extensive comparison over existing prompt engineering methods, including
question representation, example selection and example organization, and with
these experimental results, we elaborates their pros and cons. Based on these
findings, we propose a new integrated solution, named DAIL-SQL, which refreshes
the Spider leaderboard with 86.6% execution accuracy and sets a new bar.
Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize
the token efficiency in prompt engineering and compare the prior studies under
this metric. Additionally, we investigate open-source LLMs in in-context
learning, and further enhance their performance with task-specific supervised
fine-tuning. Our explorations highlight open-source LLMs' potential in
Text-to-SQL, as well as the advantages and disadvantages of the task-specific
supervised fine-tuning. We hope that our work provides a deeper understanding
of Text-to-SQL with LLMs, and inspire further investigations and broad
applications.",http://arxiv.org/pdf/2308.15363v1
2308.15352v1,cs.CL,Historical patterns of rice farming explain modern-day language use in China and Japan more than modernization and urbanization,2023-08-29 14:47:08+00:00,"We used natural language processing to analyze a billion words to study
cultural differences on Weibo, one of China's largest social media platforms.
We compared predictions from two common explanations about cultural differences
in China (economic development and urban-rural differences) against the
less-obvious legacy of rice versus wheat farming. Rice farmers had to
coordinate shared irrigation networks and exchange labor to cope with higher
labor requirements. In contrast, wheat relied on rainfall and required half as
much labor. We test whether this legacy made southern China more
interdependent. Across all word categories, rice explained twice as much
variance as economic development and urbanization. Rice areas used more words
reflecting tight social ties, holistic thought, and a cautious, prevention
orientation. We then used Twitter data comparing prefectures in Japan, which
largely replicated the results from China. This provides crucial evidence of
the rice theory in a different nation, language, and platform.",http://arxiv.org/pdf/2308.15352v1
2308.15299v1,cs.CL,TaskLAMA: Probing the Complex Task Understanding of Language Models,2023-08-29 13:36:45+00:00,"Structured Complex Task Decomposition (SCTD) is the problem of breaking down
a complex real-world task (such as planning a wedding) into a directed acyclic
graph over individual steps that contribute to achieving the task, with edges
specifying temporal dependencies between them. SCTD is an important component
of assistive planning tools, and a challenge for commonsense reasoning systems.
We probe how accurately SCTD can be done with the knowledge extracted from
Large Language Models (LLMs). We introduce a high-quality human-annotated
dataset for this problem and novel metrics to fairly assess performance of LLMs
against several baselines. Our experiments reveal that LLMs are able to
decompose complex tasks into individual steps effectively, with a relative
improvement of 15% to 280% over the best baseline. We also propose a number of
approaches to further improve their performance, with a relative improvement of
7% to 37% over the base model. However, we find that LLMs still struggle to
predict pairwise temporal dependencies, which reveals a gap in their
understanding of complex tasks.",http://arxiv.org/pdf/2308.15299v1
2308.15262v1,cs.CV,Enhancing OCR Performance through Post-OCR Models: Adopting Glyph Embedding for Improved Correction,2023-08-29 12:41:50+00:00,"The study investigates the potential of post-OCR models to overcome
limitations in OCR models and explores the impact of incorporating glyph
embedding on post-OCR correction performance. In this study, we have developed
our own post-OCR correction model. The novelty of our approach lies in
embedding the OCR output using CharBERT and our unique embedding technique,
capturing the visual characteristics of characters. Our findings show that
post-OCR correction effectively addresses deficiencies in inferior OCR models,
and glyph embedding enables the model to achieve superior results, including
the ability to correct individual words.",http://arxiv.org/pdf/2308.15262v1
2308.15246v1,cs.CL,A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation,2023-08-29 12:12:53+00:00,"Neural Machine Translation (NMT) models have been shown to be vulnerable to
adversarial attacks, wherein carefully crafted perturbations of the input can
mislead the target model. In this paper, we introduce ACT, a novel adversarial
attack framework against NMT systems guided by a classifier. In our attack, the
adversary aims to craft meaning-preserving adversarial examples whose
translations by the NMT model belong to a different class than the original
translations in the target language. Unlike previous attacks, our new approach
has a more substantial effect on the translation by altering the overall
meaning, which leads to a different class determined by a classifier. To
evaluate the robustness of NMT models to this attack, we propose enhancements
to existing black-box word-replacement-based attacks by incorporating output
translations of the target NMT model and the output logits of a classifier
within the attack process. Extensive experiments in various settings, including
a comparison with existing untargeted attacks, demonstrate that the proposed
attack is considerably more successful in altering the class of the output
translation and has more effect on the translation. This new paradigm can show
the vulnerabilities of NMT systems by focusing on the class of translation
rather than the mere translation quality as studied traditionally.",http://arxiv.org/pdf/2308.15246v1
2308.15232v1,cs.LG,Classification-Aware Neural Topic Model Combined With Interpretable Analysis -- For Conflict Classification,2023-08-29 11:40:24+00:00,"A large number of conflict events are affecting the world all the time. In
order to analyse such conflict events effectively, this paper presents a
Classification-Aware Neural Topic Model (CANTM-IA) for Conflict Information
Classification and Topic Discovery. The model provides a reliable
interpretation of classification results and discovered topics by introducing
interpretability analysis. At the same time, interpretation is introduced into
the model architecture to improve the classification performance of the model
and to allow interpretation to focus further on the details of the data.
Finally, the model architecture is optimised to reduce the complexity of the
model.",http://arxiv.org/pdf/2308.15232v1
2308.15231v1,cs.CL,"Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering",2023-08-29 11:40:03+00:00,"This paper evaluates the extent to which current Large Language Models (LLMs)
can capture task-oriented multi-party conversations (MPCs). We have recorded
and transcribed 29 MPCs between patients, their companions, and a social robot
in a hospital. We then annotated this corpus for multi-party goal-tracking and
intent-slot recognition. People share goals, answer each other's goals, and
provide other people's goals in MPCs - none of which occur in dyadic
interactions. To understand user goals in MPCs, we compared three methods in
zero-shot and few-shot settings: we fine-tuned T5, created pre-training tasks
to train DialogLM using LED, and employed prompt engineering techniques with
GPT-3.5-turbo, to determine which approach can complete this novel task with
limited data. GPT-3.5-turbo significantly outperformed the others in a few-shot
setting. The `reasoning' style prompt, when given 7% of the corpus as example
annotated conversations, was the best performing method. It correctly annotated
62.32% of the goal tracking MPCs, and 69.57% of the intent-slot recognition
MPCs. A `story' style prompt increased model hallucination, which could be
detrimental if deployed in safety-critical settings. We conclude that
multi-party conversations still challenge state-of-the-art LLMs.",http://arxiv.org/pdf/2308.15231v1
2308.15202v1,cs.CL,Benchmarking the Generation of Fact Checking Explanations,2023-08-29 10:40:46+00:00,"Fighting misinformation is a challenging, yet crucial, task. Despite the
growing number of experts being involved in manual fact-checking, this activity
is time-consuming and cannot keep up with the ever-increasing amount of Fake
News produced daily. Hence, automating this process is necessary to help curb
misinformation. Thus far, researchers have mainly focused on claim veracity
classification. In this paper, instead, we address the generation of
justifications (textual explanation of why a claim is classified as either true
or false) and benchmark it with novel datasets and advanced baselines. In
particular, we focus on summarization approaches over unstructured knowledge
(i.e. news articles) and we experiment with several extractive and abstractive
strategies. We employed two datasets with different styles and structures, in
order to assess the generalizability of our findings. Results show that in
justification production summarization benefits from the claim information,
and, in particular, that a claim-driven extractive step improves abstractive
summarization performances. Finally, we show that although cross-dataset
experiments suffer from performance degradation, a unique model trained on a
combination of the two datasets is able to retain style information in an
efficient manner.",http://arxiv.org/pdf/2308.15202v1
2308.15154v1,cs.SI,The Anatomy of Conspirators: Unveiling Traits using a Comprehensive Twitter Dataset,2023-08-29 09:35:23+00:00,"The discourse around conspiracy theories is currently thriving amidst the
rampant misinformation prevalent in online environments. Research in this field
has been focused on detecting conspiracy theories on social media, often
relying on limited datasets. In this study, we present a novel methodology for
constructing a Twitter dataset that encompasses accounts engaged in
conspiracy-related activities throughout the year 2022. Our approach centers on
data collection that is independent of specific conspiracy theories and
information operations. Additionally, our dataset includes a control group
comprising randomly selected users who can be fairly compared to the
individuals involved in conspiracy activities. This comprehensive collection
effort yielded a total of 15K accounts and 37M tweets extracted from their
timelines. We conduct a comparative analysis of the two groups across three
dimensions: topics, profiles, and behavioral characteristics. The results
indicate that conspiracy and control users exhibit similarity in terms of their
profile metadata characteristics. However, they diverge significantly in terms
of behavior and activity, particularly regarding the discussed topics, the
terminology used, and their stance on trending subjects. Interestingly, there
is no significant disparity in the presence of bot users between the two
groups, suggesting that conspiracy and automation are orthogonal concepts.
Finally, we develop a classifier to identify conspiracy users using 93
features, some of which are commonly employed in literature for troll
identification. The results demonstrate a high accuracy level (with an average
F1 score of 0.98%), enabling us to uncover the most discriminative features
associated with conspiracy-related accounts.",http://arxiv.org/pdf/2308.15154v1
2308.15122v1,cs.CL,SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT,2023-08-29 08:41:16+00:00,"Spiking neural networks (SNNs) offer a promising avenue to implement deep
neural networks in a more energy-efficient way. However, the network
architectures of existing SNNs for language tasks are too simplistic, and deep
architectures have not been fully explored, resulting in a significant
performance gap compared to mainstream transformer-based networks such as BERT.
To this end, we improve a recently-proposed spiking transformer (i.e.,
Spikformer) to make it possible to process language tasks and propose a
two-stage knowledge distillation method for training it, which combines
pre-training by distilling knowledge from BERT with a large collection of
unlabelled texts and fine-tuning with task-specific instances via knowledge
distillation again from the BERT fine-tuned on the same training examples.
Through extensive experimentation, we show that the models trained with our
method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve
comparable results to BERTs on text classification tasks for both English and
Chinese with much less energy consumption.",http://arxiv.org/pdf/2308.15122v1
2308.15118v1,cs.CL,Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills,2023-08-29 08:36:30+00:00,"While large language models have made strides in natural language processing,
their proficiency in complex reasoning tasks requiring formal language
comprehension, such as chess, remains less investigated. This paper probes the
performance of ChatGPT, a sophisticated language model by OpenAI in tackling
such complex reasoning tasks, using chess as a case study. Through robust
metrics examining both the legality and quality of moves, we assess ChatGPT's
understanding of the chessboard, adherence to chess rules, and strategic
decision-making abilities. Our evaluation identifies limitations within
ChatGPT's attention mechanism that affect its formal language comprehension and
uncovers the model's underdeveloped self-regulation abilities. Our study also
reveals ChatGPT's propensity for a coherent strategy in its gameplay and a
noticeable uptick in decision-making assertiveness when the model is presented
with a greater volume of natural language or possesses a more lucid
understanding of the state of the chessboard. These findings contribute to the
growing exploration of language models' abilities beyond natural language
processing, providing valuable information for future research towards models
demonstrating human-like cognitive abilities.",http://arxiv.org/pdf/2308.15118v1
2308.15090v1,cs.CL,Killing two birds with one stone: Can an audio captioning system also be used for audio-text retrieval?,2023-08-29 07:53:17+00:00,"Automated Audio Captioning (AAC) aims to develop systems capable of
describing an audio recording using a textual sentence. In contrast, Audio-Text
Retrieval (ATR) systems seek to find the best matching audio recording(s) for a
given textual query (Text-to-Audio) or vice versa (Audio-to-Text). These tasks
require different types of systems: AAC employs a sequence-to-sequence model,
while ATR utilizes a ranking model that compares audio and text representations
within a shared projection subspace. However, this work investigates the
relationship between AAC and ATR by exploring the ATR capabilities of an
unmodified AAC system, without fine-tuning for the new task. Our AAC system
consists of an audio encoder (ConvNeXt-Tiny) trained on AudioSet for audio
tagging, and a transformer decoder responsible for generating sentences. For
AAC, it achieves a high SPIDEr-FL score of 0.298 on Clotho and 0.472 on
AudioCaps on average. For ATR, we propose using the standard Cross-Entropy loss
values obtained for any audio/caption pair. Experimental results on the Clotho
and AudioCaps datasets demonstrate decent recall values using this simple
approach. For instance, we obtained a Text-to-Audio R@1 value of 0.382 for
Au-dioCaps, which is above the current state-of-the-art method without external
data. Interestingly, we observe that normalizing the loss values was necessary
for Audio-to-Text retrieval.",http://arxiv.org/pdf/2308.15090v1
2308.15055v1,cs.CL,Taxonomic Loss for Morphological Glossing of Low-Resource Languages,2023-08-29 06:31:21+00:00,"Morpheme glossing is a critical task in automated language documentation and
can benefit other downstream applications greatly. While state-of-the-art
glossing systems perform very well for languages with large amounts of existing
data, it is more difficult to create useful models for low-resource languages.
In this paper, we propose the use of a taxonomic loss function that exploits
morphological information to make morphological glossing more performant when
data is scarce. We find that while the use of this loss function does not
outperform a standard loss function with regards to single-label prediction
accuracy, it produces better predictions when considering the top-n predicted
labels. We suggest this property makes the taxonomic loss function useful in a
human-in-the-loop annotation setting.",http://arxiv.org/pdf/2308.15055v1
2308.15047v1,cs.LG,Large language models converge toward human-like concept organization,2023-08-29 06:09:47+00:00,"Large language models show human-like performance in knowledge extraction,
reasoning and dialogue, but it remains controversial whether this performance
is best explained by memorization and pattern matching, or whether it reflects
human-like inferential semantics and world knowledge. Knowledge bases such as
WikiData provide large-scale, high-quality representations of inferential
semantics and world knowledge. We show that large language models learn to
organize concepts in ways that are strikingly similar to how concepts are
organized in such knowledge bases. Knowledge bases model collective,
institutional knowledge, and large language models seem to induce such
knowledge from raw text. We show that bigger and better models exhibit more
human-like concept organization, across four families of language models and
three knowledge graph embeddings.",http://arxiv.org/pdf/2308.15047v1
2308.15027v1,cs.IR,Improving Neural Ranking Models with Traditional IR Methods,2023-08-29 05:18:47+00:00,"Neural ranking methods based on large transformer models have recently gained
significant attention in the information retrieval community, and have been
adopted by major commercial solutions. Nevertheless, they are computationally
expensive to create, and require a great deal of labeled data for specialized
corpora. In this paper, we explore a low resource alternative which is a
bag-of-embedding model for document retrieval and find that it is competitive
with large transformer models fine tuned on information retrieval tasks. Our
results show that a simple combination of TF-IDF, a traditional keyword
matching method, with a shallow embedding model provides a low cost path to
compete well with the performance of complex neural ranking models on 3
datasets. Furthermore, adding TF-IDF measures improves the performance of
large-scale fine tuned models on these tasks.",http://arxiv.org/pdf/2308.15027v1
2308.15010v1,cs.CL,TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification,2023-08-29 04:16:57+00:00,"Text classification is one of the most imperative tasks in natural language
processing (NLP). Recent advances with pre-trained language models (PLMs) have
shown remarkable success on this task. However, the satisfying results obtained
by PLMs heavily depend on the large amounts of task-specific labeled data,
which may not be feasible in many application scenarios due to data access and
privacy constraints. The recently-proposed prompt-based fine-tuning paradigm
improves the performance of PLMs for few-shot text classification with
task-specific templates. Yet, it is unclear how the prompting knowledge can be
transferred across tasks, for the purpose of mutual reinforcement. We propose
TransPrompt v2, a novel transferable prompting framework for few-shot learning
across similar or distant text classification tasks. For learning across
similar tasks, we employ a multi-task meta-knowledge acquisition (MMA)
procedure to train a meta-learner that captures the cross-task transferable
knowledge. For learning across distant tasks, we further inject the task type
descriptions into the prompt, and capture the intra-type and inter-type prompt
embeddings among multiple distant tasks. Additionally, two de-biasing
techniques are further designed to make the trained meta-learner more
task-agnostic and unbiased towards any tasks. After that, the meta-learner can
be adapted to each specific task with better parameters initialization.
Extensive experiments show that TransPrompt v2 outperforms single-task and
cross-task strong baselines over multiple NLP tasks and datasets. We further
show that the meta-learner can effectively improve the performance of PLMs on
previously unseen tasks. In addition, TransPrompt v2 also outperforms strong
fine-tuning baselines when learning with full training sets.",http://arxiv.org/pdf/2308.15010v1
2308.14921v1,cs.CL,Gender bias and stereotypes in Large Language Models,2023-08-28 22:32:05+00:00,"Large Language Models (LLMs) have made substantial progress in the past
several months, shattering state-of-the-art benchmarks in many domains. This
paper investigates LLMs' behavior with respect to gender stereotypes, a known
issue for prior models. We use a simple paradigm to test the presence of gender
bias, building on but differing from WinoBias, a commonly used gender bias
dataset, which is likely to be included in the training data of current LLMs.
We test four recently published LLMs and demonstrate that they express biased
assumptions about men and women's occupations. Our contributions in this paper
are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that
stereotypically aligns with a person's gender; (b) these choices align with
people's perceptions better than with the ground truth as reflected in official
job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in
perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in
sentence structure 95% of the time in our study items, but when explicitly
prompted, they recognize the ambiguity; (e) LLMs provide explanations for their
choices that are factually inaccurate and likely obscure the true reason behind
their predictions. That is, they provide rationalizations of their biased
behavior. This highlights a key property of these models: LLMs are trained on
imbalanced datasets; as such, even with the recent successes of reinforcement
learning with human feedback, they tend to reflect those imbalances back at us.
As with other types of societal biases, we suggest that LLMs must be carefully
tested to ensure that they treat minoritized individuals and communities
equitably.",http://arxiv.org/pdf/2308.14921v1
2308.14905v1,cs.CL,Neural approaches to spoken content embedding,2023-08-28 21:16:08+00:00,"Comparing spoken segments is a central operation to speech processing.
Traditional approaches in this area have favored frame-level dynamic
programming algorithms, such as dynamic time warping, because they require no
supervision, but they are limited in performance and efficiency. As an
alternative, acoustic word embeddings -- fixed-dimensional vector
representations of variable-length spoken word segments -- have begun to be
considered for such tasks as well. However, the current space of such
discriminative embedding models, training approaches, and their application to
real-world downstream tasks is limited. We start by considering ``single-view""
training losses where the goal is to learn an acoustic word embedding model
that separates same-word and different-word spoken segment pairs. Then, we
consider ``multi-view"" contrastive losses. In this setting, acoustic word
embeddings are learned jointly with embeddings of character sequences to
generate acoustically grounded embeddings of written words, or acoustically
grounded word embeddings.
  In this thesis, we contribute new discriminative acoustic word embedding
(AWE) and acoustically grounded word embedding (AGWE) approaches based on
recurrent neural networks (RNNs). We improve model training in terms of both
efficiency and performance. We take these developments beyond English to
several low-resource languages and show that multilingual training improves
performance when labeled data is limited. We apply our embedding models, both
monolingual and multilingual, to the downstream tasks of query-by-example
speech search and automatic speech recognition. Finally, we show how our
embedding approaches compare with and complement more recent self-supervised
speech models.",http://arxiv.org/pdf/2308.14905v1
2308.14903v1,cs.CL,MEMORY-VQ: Compression for Tractable Internet-Scale Memory,2023-08-28 21:11:18+00:00,"Retrieval augmentation is a powerful but expensive method to make language
models more knowledgeable about the world. Memory-based methods like LUMEN
pre-compute token representations for retrieved passages to drastically speed
up inference. However, memory also leads to much greater storage requirements
from storing pre-computed representations.
  We propose MEMORY-VQ, a new method to reduce storage requirements of
memory-augmented models without sacrificing performance. Our method uses a
vector quantization variational autoencoder (VQ-VAE) to compress token
representations. We apply MEMORY-VQ to the LUMEN model to obtain LUMEN-VQ, a
memory model that achieves a 16x compression rate with comparable performance
on the KILT benchmark. LUMEN-VQ enables practical retrieval augmentation even
for extremely large retrieval corpora.",http://arxiv.org/pdf/2308.14903v1
2308.14894v1,cs.CL,Multiscale Contextual Learning for Speech Emotion Recognition in Emergency Call Center Conversations,2023-08-28 20:31:45+00:00,"Emotion recognition in conversations is essential for ensuring advanced
human-machine interactions. However, creating robust and accurate emotion
recognition systems in real life is challenging, mainly due to the scarcity of
emotion datasets collected in the wild and the inability to take into account
the dialogue context. The CEMO dataset, composed of conversations between
agents and patients during emergency calls to a French call center, fills this
gap. The nature of these interactions highlights the role of the emotional flow
of the conversation in predicting patient emotions, as context can often make a
difference in understanding actual feelings. This paper presents a multi-scale
conversational context learning approach for speech emotion recognition, which
takes advantage of this hypothesis. We investigated this approach on both
speech transcriptions and acoustic segments. Experimentally, our method uses
the previous or next information of the targeted segment. In the text domain,
we tested the context window using a wide range of tokens (from 10 to 100) and
at the speech turns level, considering inputs from both the same and opposing
speakers. According to our tests, the context derived from previous tokens has
a more significant influence on accurate prediction than the following tokens.
Furthermore, taking the last speech turn of the same speaker in the
conversation seems useful. In the acoustic domain, we conducted an in-depth
analysis of the impact of the surrounding emotions on the prediction. While
multi-scale conversational context learning using Transformers can enhance
performance in the textual modality for emergency call recordings,
incorporating acoustic context is more challenging.",http://arxiv.org/pdf/2308.14894v1
2308.14873v1,cs.CL,CommunityFish: A Poisson-based Document Scaling With Hierarchical Clustering,2023-08-28 19:52:18+00:00,"Document scaling has been a key component in text-as-data applications for
social scientists and a major field of interest for political researchers, who
aim at uncovering differences between speakers or parties with the help of
different probabilistic and non-probabilistic approaches. Yet, most of these
techniques are either built upon the agnostically bag-of-word hypothesis or use
prior information borrowed from external sources that might embed the results
with a significant bias. If the corpus has long been considered as a collection
of documents, it can also be seen as a dense network of connected words whose
structure could be clustered to differentiate independent groups of words,
based on their co-occurrences in documents, known as communities. This paper
introduces CommunityFish as an augmented version of Wordfish based on a
hierarchical clustering, namely the Louvain algorithm, on the word space to
yield communities as semantic and independent n-grams emerging from the corpus
and use them as an input to Wordfish method, instead of considering the word
space. This strategy emphasizes the interpretability of the results, since
communities have a non-overlapping structure, hence a crucial informative power
in discriminating parties or speakers, in addition to allowing a faster
execution of the Poisson scaling model. Aside from yielding communities,
assumed to be subtopic proxies, the application of this technique outperforms
the classic Wordfish model by highlighting historical developments in the U.S.
State of the Union addresses and was found to replicate the prevailing
political stance in Germany when using the corpus of parties' legislative
manifestos.",http://arxiv.org/pdf/2308.14873v1
2308.14654v1,cs.CL,Joint Multiple Intent Detection and Slot Filling with Supervised Contrastive Learning and Self-Distillation,2023-08-28 15:36:33+00:00,"Multiple intent detection and slot filling are two fundamental and crucial
tasks in spoken language understanding. Motivated by the fact that the two
tasks are closely related, joint models that can detect intents and extract
slots simultaneously are preferred to individual models that perform each task
independently. The accuracy of a joint model depends heavily on the ability of
the model to transfer information between the two tasks so that the result of
one task can correct the result of the other. In addition, since a joint model
has multiple outputs, how to train the model effectively is also challenging.
In this paper, we present a method for multiple intent detection and slot
filling by addressing these challenges. First, we propose a bidirectional joint
model that explicitly employs intent information to recognize slots and slot
features to detect intents. Second, we introduce a novel method for training
the proposed joint model using supervised contrastive learning and
self-distillation. Experimental results on two benchmark datasets MixATIS and
MixSNIPS show that our method outperforms state-of-the-art models in both
tasks. The results also demonstrate the contributions of both bidirectional
design and the training method to the accuracy improvement. Our source code is
available at https://github.com/anhtunguyen98/BiSLU",http://arxiv.org/pdf/2308.14654v1
2308.14641v2,cs.CL,Challenges of GPT-3-based Conversational Agents for Healthcare,2023-08-28 15:12:34+00:00,"The potential to provide patients with faster information access while
allowing medical specialists to concentrate on critical tasks makes medical
domain dialog agents appealing. However, the integration of large-language
models (LLMs) into these agents presents certain limitations that may result in
serious consequences. This paper investigates the challenges and risks of using
GPT-3-based models for medical question-answering (MedQA). We perform several
evaluations contextualized in terms of standard medical principles. We provide
a procedure for manually designing patient queries to stress-test high-risk
limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to
respond adequately to these queries, generating erroneous medical information,
unsafe recommendations, and content that may be considered offensive.",http://arxiv.org/pdf/2308.14641v2
2308.14634v1,cs.CL,Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance,2023-08-28 15:04:16+00:00,"We propose the use of conversational GPT models for easy and quick few-shot
text classification in the financial domain using the Banking77 dataset. Our
approach involves in-context learning with GPT-3.5 and GPT-4, which minimizes
the technical expertise required and eliminates the need for expensive GPU
computing while yielding quick and accurate results. Additionally, we fine-tune
other pre-trained, masked language models with SetFit, a recent contrastive
learning technique, to achieve state-of-the-art results both in full-data and
few-shot settings. Our findings show that querying GPT-3.5 and GPT-4 can
outperform fine-tuned, non-generative models even with fewer examples. However,
subscription fees associated with these solutions may be considered costly for
small organizations. Lastly, we find that generative models perform better on
the given task when shown representative samples selected by a human expert
rather than when shown random ones. We conclude that a) our proposed methods
offer a practical solution for few-shot tasks in datasets with limited label
availability, and b) our state-of-the-art results can inspire future work in
the area.",http://arxiv.org/pdf/2308.14634v1
2308.14608v1,cs.LG,AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics,2023-08-28 14:23:04+00:00,"The introduction of ChatGPT and the subsequent improvement of Large Language
Models (LLMs) have prompted more and more individuals to turn to the use of
ChatBots, both for information and assistance with decision-making. However,
the information the user is after is often not formulated by these ChatBots
objectively enough to be provided with a definite, globally accepted answer.
  Controversial topics, such as ""religion"", ""gender identity"", ""freedom of
speech"", and ""equality"", among others, can be a source of conflict as partisan
or biased answers can reinforce preconceived notions or promote disinformation.
By exposing ChatGPT to such debatable questions, we aim to understand its level
of awareness and if existing models are subject to socio-political and/or
economic biases. We also aim to explore how AI-generated answers compare to
human ones. For exploring this, we use a dataset of a social media platform
created for the purpose of debating human-generated claims on polemic subjects
among users, dubbed Kialo.
  Our results show that while previous versions of ChatGPT have had important
issues with controversial topics, more recent versions of ChatGPT
(gpt-3.5-turbo) are no longer manifesting significant explicit biases in
several knowledge areas. In particular, it is well-moderated regarding economic
aspects. However, it still maintains degrees of implicit libertarian leaning
toward right-winged ideals which suggest the need for increased moderation from
the socio-political point of view. In terms of domain knowledge on
controversial topics, with the exception of the ""Philosophical"" category,
ChatGPT is performing well in keeping up with the collective human level of
knowledge. Finally, we see that sources of Bing AI have slightly more tendency
to the center when compared to human answers. All the analyses we make are
generalizable to other types of biases and domains.",http://arxiv.org/pdf/2308.14608v1
2308.14536v1,cs.CL,Spoken Language Intelligence of Large Language Models for Language Learning,2023-08-28 12:47:41+00:00,"People have long hoped for a conversational system that can assist in
real-life situations, and recent progress on large language models (LLMs) is
bringing this idea closer to reality. While LLMs are often impressive in
performance, their efficacy in real-world scenarios that demand expert
knowledge remains unclear. LLMs are believed to hold the most potential and
value in education, especially in the development of Artificial intelligence
(AI) based virtual teachers capable of facilitating language learning. Our
focus is centered on evaluating the efficacy of LLMs in the realm of education,
specifically in the areas of spoken language learning which encompass
phonetics, phonology, and second language acquisition. We introduce a new
multiple-choice question dataset to evaluate the effectiveness of LLMs in the
aforementioned scenarios, including understanding and application of spoken
language knowledge. In addition, we investigate the influence of various
prompting techniques such as zero- and few-shot method (prepending the question
with question-answer exemplars), chain-of-thought (CoT, think step-by-step),
in-domain exampler and external tools (Google, Wikipedia). We conducted
large-scale evaluation on popular LLMs (20 distinct models) using these
methods. We achieved significant performance improvements compared to the
zero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% ->
63.1%; LLaMA2-70B-Chat, 42.2% -> 48.6%). We found that models of different
sizes have good understanding of concepts in phonetics, phonology, and second
language acquisition, but show limitations in reasoning for real-world
problems. Additionally, we also explore preliminary findings on conversational
communication.",http://arxiv.org/pdf/2308.14536v1
2308.14533v1,cs.CL,A Multi-Task Semantic Decomposition Framework with Task-specific Pre-training for Few-Shot NER,2023-08-28 12:46:21+00:00,"The objective of few-shot named entity recognition is to identify named
entities with limited labeled instances. Previous works have primarily focused
on optimizing the traditional token-wise classification framework, while
neglecting the exploration of information based on NER data characteristics. To
address this issue, we propose a Multi-Task Semantic Decomposition Framework
via Joint Task-specific Pre-training (MSDP) for few-shot NER. Drawing
inspiration from demonstration-based and contrastive learning, we introduce two
novel pre-training tasks: Demonstration-based Masked Language Modeling (MLM)
and Class Contrastive Discrimination. These tasks effectively incorporate
entity boundary information and enhance entity representation in Pre-trained
Language Models (PLMs). In the downstream main task, we introduce a multi-task
joint optimization framework with the semantic decomposing method, which
facilitates the model to integrate two different semantic information for
entity classification. Experimental results of two few-shot NER benchmarks
demonstrate that MSDP consistently outperforms strong baselines by a large
margin. Extensive analyses validate the effectiveness and generalization of
MSDP.",http://arxiv.org/pdf/2308.14533v1
2308.14508v1,cs.CL,"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",2023-08-28 11:53:40+00:00,"Although large language models (LLMs) demonstrate impressive performance for
many language tasks, most of them can only handle texts a few thousand tokens
long, limiting their applications on longer sequence inputs, such as books,
reports, and codebases. Recent works have proposed methods to improve LLMs'
long context capabilities by extending context windows and more sophisticated
memory mechanisms. However, comprehensive benchmarks tailored for evaluating
long context understanding are lacking. In this paper, we introduce LongBench,
the first bilingual, multi-task benchmark for long context understanding,
enabling a more rigorous evaluation of long context understanding. LongBench
comprises 21 datasets across 6 task categories in both English and Chinese,
with an average length of 6,711 words (English) and 13,386 characters
(Chinese). These tasks cover key long-text application areas including
single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,
and code completion. All datasets in LongBench are standardized into a unified
format, allowing for effortless automatic evaluation of LLMs. Upon
comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial
model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still
struggles on longer contexts. (2) Scaled position embedding and fine-tuning on
longer sequences lead to substantial improvement on long context understanding.
(3) Context compression technique such as retrieval brings improvement for
model with weak ability on long contexts, but the performance still lags behind
models that have strong long context understanding capability. The code and
datasets are available at https://github.com/THUDM/LongBench.",http://arxiv.org/pdf/2308.14508v1
2308.14484v1,cs.CL,Multimodal Detection of Social Spambots in Twitter using Transformers,2023-08-28 10:51:11+00:00,"Although not all bots are malicious, the vast majority of them are
responsible for spreading misinformation and manipulating the public opinion
about several issues, i.e., elections and many more. Therefore, the early
detection of social spambots is crucial. Although there have been proposed
methods for detecting bots in social media, there are still substantial
limitations. For instance, existing research initiatives still extract a large
number of features and train traditional machine learning algorithms or use
GloVe embeddings and train LSTMs. However, feature extraction is a tedious
procedure demanding domain expertise. Also, language models based on
transformers have been proved to be better than LSTMs. Other approaches create
large graphs and train graph neural networks requiring in this way many hours
for training and access to computational resources. To tackle these
limitations, this is the first study employing only the user description field
and images of three channels denoting the type and content of tweets posted by
the users. Firstly, we create digital DNA sequences, transform them to 3d
images, and apply pretrained models of the vision domain, including
EfficientNet, AlexNet, VGG16, etc. Next, we propose a multimodal approach,
where we use TwHIN-BERT for getting the textual representation of the user
description field and employ VGG16 for acquiring the visual representation for
the image modality. We propose three different fusion methods, namely
concatenation, gated multimodal unit, and crossmodal attention, for fusing the
different modalities and compare their performances. Extensive experiments
conducted on the Cresci '17 dataset demonstrate valuable advantages of our
introduced approaches over state-of-the-art ones reaching Accuracy up to
99.98%.",http://arxiv.org/pdf/2308.14484v1
2308.14482v1,cs.CL,An Empirical Study of Consistency Regularization for End-to-End Speech-to-Text Translation,2023-08-28 10:44:18+00:00,"Consistency regularization methods, such as R-Drop (Liang et al., 2021) and
CrossConST (Gao et al., 2023), have achieved impressive supervised and
zero-shot performance in the neural machine translation (NMT) field. Can we
also boost end-to-end (E2E) speech-to-text translation (ST) by leveraging
consistency regularization? In this paper, we conduct empirical studies on
intra-modal and cross-modal consistency and propose two training strategies,
SimRegCR and SimZeroCR, for E2E ST in regular and zero-shot scenarios.
Experiments on the MuST-C benchmark show that our approaches achieve
state-of-the-art (SOTA) performance in most translation directions. The
analyses prove that regularization brought by the intra-modal consistency,
instead of modality gap, is crucial for the regular E2E ST, and the cross-modal
consistency could close the modality gap and boost the zero-shot E2E ST
performance.",http://arxiv.org/pdf/2308.14482v1
2308.14436v1,cs.CL,Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA,2023-08-28 09:22:02+00:00,"Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with factual information such as entities and relations in KBs.
However, traditional Pre-trained Language Models (PLMs) are directly
pre-trained on large-scale natural language corpus, which poses challenges for
them in understanding and representing complex subgraphs in structured KBs. To
bridge the gap between texts and structured KBs, we propose a Structured
Knowledge-aware Pre-training method (SKP). In the pre-training stage, we
introduce two novel structured knowledge-aware tasks, guiding the model to
effectively learn the implicit relationship and better representations of
complex subgraphs. In downstream KBQA task, we further design an efficient
linearization strategy and an interval attention mechanism, which assist the
model to better encode complex subgraphs and shield the interference of
irrelevant subgraphs during reasoning respectively. Detailed experiments and
analyses on WebQSP verify the effectiveness of SKP, especially the significant
improvement in subgraph retrieval (+4.08% H@10).",http://arxiv.org/pdf/2308.14436v1
2308.14429v1,cs.CL,Biomedical Entity Linking with Triple-aware Pre-Training,2023-08-28 09:06:28+00:00,"Linking biomedical entities is an essential aspect in biomedical natural
language processing tasks, such as text mining and question answering. However,
a difficulty of linking the biomedical entities using current large language
models (LLM) trained on a general corpus is that biomedical entities are
scarcely distributed in texts and therefore have been rarely seen during
training by the LLM. At the same time, those LLMs are not aware of high level
semantic connection between different biomedical entities, which are useful in
identifying similar concepts in different textual contexts. To cope with
aforementioned problems, some recent works focused on injecting knowledge graph
information into LLMs. However, former methods either ignore the relational
knowledge of the entities or lead to catastrophic forgetting. Therefore, we
propose a novel framework to pre-train the powerful generative LLM by a corpus
synthesized from a KG. In the evaluations we are unable to confirm the benefit
of including synonym, description or relational information.",http://arxiv.org/pdf/2308.14429v1
2308.14423v1,cs.CL,GADePo: Graph-Assisted Declarative Pooling Transformers for Document-Level Relation Extraction,2023-08-28 09:04:03+00:00,"Document-level relation extraction aims to identify relationships between
entities within a document. Current methods rely on text-based encoders and
employ various hand-coded pooling heuristics to aggregate information from
entity mentions and associated contexts. In this paper, we replace these rigid
pooling functions with explicit graph relations by leveraging the intrinsic
graph processing capabilities of the Transformer model. We propose a joint
text-graph Transformer model, and a graph-assisted declarative pooling (GADePo)
specification of the input which provides explicit and high-level instructions
for information aggregation. This allows the pooling process to be guided by
domain-specific knowledge or desired outcomes but still learned by the
Transformer, leading to more flexible and customizable pooling strategies. We
extensively evaluate our method across diverse datasets and models, and show
that our approach yields promising results that are comparable to those
achieved by the hand-coded pooling functions.",http://arxiv.org/pdf/2308.14423v1
2308.14391v1,cs.CV,FIRE: Food Image to REcipe generation,2023-08-28 08:14:20+00:00,"Food computing has emerged as a prominent multidisciplinary field of research
in recent years. An ambitious goal of food computing is to develop end-to-end
intelligent systems capable of autonomously producing recipe information for a
food image. Current image-to-recipe methods are retrieval-based and their
success depends heavily on the dataset size and diversity, as well as the
quality of learned embeddings. Meanwhile, the emergence of powerful
attention-based vision and language models presents a promising avenue for
accurate and generalizable recipe generation, which has yet to be extensively
explored. This paper proposes FIRE, a novel multimodal methodology tailored to
recipe generation in the food computing domain, which generates the food title,
ingredients, and cooking instructions based on input food images. FIRE
leverages the BLIP model to generate titles, utilizes a Vision Transformer with
a decoder for ingredient extraction, and employs the T5 model to generate
recipes incorporating titles and ingredients as inputs. We showcase two
practical applications that can benefit from integrating FIRE with large
language model prompting: recipe customization to fit recipes to user
preferences and recipe-to-code transformation to enable automated cooking
processes. Our experimental findings validate the efficacy of our proposed
approach, underscoring its potential for future advancements and widespread
adoption in food computing.",http://arxiv.org/pdf/2308.14391v1
2308.14359v1,cs.AI,Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks,2023-08-28 07:11:27+00:00,"Human emotion understanding is pivotal in making conversational technology
mainstream. We view speech emotion understanding as a perception task which is
a more realistic setting. With varying contexts (languages, demographics, etc.)
different share of people perceive the same speech segment as a non-unanimous
emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics
ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset
of multilingual speakers and multi-label regression target of 'emotion share'
or perception of that emotion. We demonstrate that the training scheme of
different foundation models dictates their effectiveness for tasks beyond
speech recognition, especially for non-semantic speech tasks like emotion
  understanding. This is a very complex task due to multilingual speakers,
variability in the target labels, and inherent imbalance in the regression
dataset. Our results show that HuBERT-Large with a self-attention-based
light-weight sequence model provides 4.6% improvement over the reported
baseline.",http://arxiv.org/pdf/2308.14359v1
2308.14353v1,cs.CL,"ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models",2023-08-28 06:56:44+00:00,"The unprecedented performance of large language models (LLMs) requires
comprehensive and accurate evaluation. We argue that for LLMs evaluation,
benchmarks need to be comprehensive and systematic. To this end, we propose the
ZhuJiu benchmark, which has the following strengths: (1) Multi-dimensional
ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions
covering 51 tasks. Especially, we also propose a new benchmark that focuses on
knowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration:
We use 3 different yet complementary evaluation methods to comprehensively
evaluate LLMs, which can ensure the authority and accuracy of the evaluation
results. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering
benchmark that fully assesses LLMs in Chinese, while also providing equally
robust evaluation abilities in English. (4) Avoiding potential data leakage: To
avoid data leakage, we construct evaluation data specifically for 37 tasks. We
evaluate 10 current mainstream LLMs and conduct an in-depth discussion and
analysis of their results. The ZhuJiu benchmark and open-participation
leaderboard are publicly released at http://www.zhujiu-benchmark.com/ and we
also provide a demo video at https://youtu.be/qypkJ89L1Ic.",http://arxiv.org/pdf/2308.14353v1
2308.14352v1,cs.LG,EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models,2023-08-28 06:56:08+00:00,"Large Language Models (LLMs) such as GPTs and LLaMa have ushered in a
revolution in machine intelligence, owing to their exceptional capabilities in
a wide range of machine learning tasks. However, the transition of LLMs from
data centers to edge devices presents a set of challenges and opportunities.
While this shift can enhance privacy and availability, it is hampered by the
enormous parameter sizes of these models, leading to impractical runtime costs.
In light of these considerations, we introduce EdgeMoE, the first on-device
inference engine tailored for mixture-of-expert (MoE) LLMs, a popular variant
of sparse LLMs that exhibit nearly constant computational complexity as their
parameter size scales. EdgeMoE achieves both memory and computational
efficiency by strategically partitioning the model across the storage
hierarchy. Specifically, non-expert weights are stored in the device's memory,
while expert weights are kept in external storage and are fetched into memory
only when they are activated. This design is underpinned by a crucial insight
that expert weights, though voluminous, are infrequently accessed due to sparse
activation patterns. To further mitigate the overhead associated with expert
I/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wise
bitwidth adaptation: This method reduces the size of expert weights with an
acceptable level of accuracy loss. (2) Expert management: It predicts the
experts that will be activated in advance and preloads them into the
compute-I/O pipeline, thus further optimizing the process. In empirical
evaluations conducted on well-established MoE LLMs and various edge devices,
EdgeMoE demonstrates substantial memory savings and performance improvements
when compared to competitive baseline solutions.",http://arxiv.org/pdf/2308.14352v1
2308.14346v1,cs.CL,DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation,2023-08-28 06:41:49+00:00,"We propose DISC-MedLLM, a comprehensive solution that leverages Large
Language Models (LLMs) to provide accurate and truthful medical response in
end-to-end conversational healthcare services. To construct high-quality
Supervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing
medical knowledge-graphs, reconstructing real-world dialogues, and
incorporating human-guided preference rephrasing. These datasets are
instrumental in training DISC-MedLLM, surpassing existing medical LLMs in both
single-turn and multi-turn consultation scenarios. Extensive experimental
results demonstrate the effectiveness of the proposed model in bridging the gap
between general language models and real-world medical consultation.
Additionally, we release the constructed dataset and model weights to further
contribute to research and development. Further details and resources can be
found at https://github.com/FudanDISC/DISC-MedLLM",http://arxiv.org/pdf/2308.14346v1
2308.14337v1,cs.AI,Cognitive Effects in Large Language Models,2023-08-28 06:30:33+00:00,"Large Language Models (LLMs) such as ChatGPT have received enormous attention
over the past year and are now used by hundreds of millions of people every
day. The rapid adoption of this technology naturally raises questions about the
possible biases such models might exhibit. In this work, we tested one of these
models (GPT-3) on a range of cognitive effects, which are systematic patterns
that are usually found in human cognitive tasks. We found that LLMs are indeed
prone to several human cognitive effects. Specifically, we show that the
priming, distance, SNARC, and size congruity effects were presented with GPT-3,
while the anchoring effect is absent. We describe our methodology, and
specifically the way we converted real-world experiments to text-based
experiments. Finally, we speculate on the possible reasons why GPT-3 exhibits
these effects and discuss whether they are imitated or reinvented.",http://arxiv.org/pdf/2308.14337v1
2308.14321v1,cs.CL,Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction,2023-08-28 06:05:18+00:00,"Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.",http://arxiv.org/pdf/2308.14321v1
2308.14306v1,cs.CL,Evaluating the Robustness to Instructions of Large Language Models,2023-08-28 04:57:07+00:00,"Recently, Instruction fine-tuning has risen to prominence as a potential
method for enhancing the zero-shot capabilities of Large Language Models (LLMs)
on novel tasks. This technique has shown an exceptional ability to boost the
performance of moderately sized LLMs, sometimes even reaching performance
levels comparable to those of much larger model variants. The focus is on the
robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an
exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional
Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction
datasets as case studies. We carried out a comprehensive evaluation of these
instruction-following LLMs which have been tuned based on open-domain
instructions and task-oriented instructions. The main discussion is their
performance and robustness towards instructions. We have observed that in most
cases, the model's performance in dealing with unfamiliar instructions tends to
worsen significantly, and the robustness of the model for RE instructions
deteriorates compared to QA. Further, we discovered that up until a certain
parameter size threshold (3B), the performance of the FLAN-T5 model improves as
the parameter count increases. The robustness of different scales of FLAN-T5
models to RE instruction is worse than the robustness to QA instruction.",http://arxiv.org/pdf/2308.14306v1
2308.14280v1,cs.CL,FonMTL: Towards Multitask Learning for the Fon Language,2023-08-28 03:26:21+00:00,"The Fon language, spoken by an average 2 million of people, is a truly
low-resourced African language, with a limited online presence, and existing
datasets (just to name but a few). Multitask learning is a learning paradigm
that aims to improve the generalization capacity of a model by sharing
knowledge across different but related tasks: this could be prevalent in very
data-scarce scenarios. In this paper, we present the first explorative approach
to multitask learning, for model capabilities enhancement in Natural Language
Processing for the Fon language. Specifically, we explore the tasks of Named
Entity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage
two language model heads as encoders to build shared representations for the
inputs, and we use linear layers blocks for classification relative to each
task. Our results on the NER and POS tasks for Fon, show competitive (or
better) performances compared to several multilingual pretrained language
models finetuned on single tasks. Additionally, we perform a few ablation
studies to leverage the efficiency of two different loss combination strategies
and find out that the equal loss weighting approach works best in our case. Our
code is open-sourced at https://github.com/bonaventuredossou/multitask_fon.",http://arxiv.org/pdf/2308.14280v1
2308.14272v1,cs.CL,Goodhart's Law Applies to NLP's Explanation Benchmarks,2023-08-28 03:03:03+00:00,"Despite the rising popularity of saliency-based explanations, the research
community remains at an impasse, facing doubts concerning their purpose,
efficacy, and tendency to contradict each other. Seeking to unite the
community's efforts around common goals, several recent works have proposed
evaluation metrics. In this paper, we critically examine two sets of metrics:
the ERASER metrics (comprehensiveness and sufficiency) and the EVAL-X metrics,
focusing our inquiry on natural language processing. First, we show that we can
inflate a model's comprehensiveness and sufficiency scores dramatically without
altering its predictions or explanations on in-distribution test inputs. Our
strategy exploits the tendency for extracted explanations and their complements
to be ""out-of-support"" relative to each other and in-distribution inputs. Next,
we demonstrate that the EVAL-X metrics can be inflated arbitrarily by a simple
method that encodes the label, even though EVAL-X is precisely motivated to
address such exploits. Our results raise doubts about the ability of current
metrics to guide explainability research, underscoring the need for a broader
reassessment of what precisely these metrics are intended to capture.",http://arxiv.org/pdf/2308.14272v1
2308.14266v1,cs.CL,SalesBot 2.0: A Human-Like Intent-Guided Chit-Chat Dataset,2023-08-28 02:48:49+00:00,"In recent research on dialogue systems and corpora, there has been a
significant focus on two distinct categories: task-oriented (TOD) and
open-domain (chit-chat) dialogues. TOD systems aim to satisfy specific user
goals, such as finding a movie to watch, whereas open-domain systems primarily
focus on generating engaging conversations. A recent study by Chiu et al.
(2022) introduced SalesBot, which provides simulators and a dataset with
one-turn transition from chit-chat to task-oriented dialogues. However, the
previously generated data solely relied on BlenderBot, which raised concerns
about its long-turn naturalness and consistency during a conversation. To
address this issue, this paper aims to build SalesBot 2.0, a revised version of
the published data, by leveraging the commonsense knowledge of large language
models (LLMs) through proper prompting. The objective is to gradually bridge
the gap between chit-chat and TOD towards better naturalness and consistency.
The newly released large-scale dataset with detailed annotations exhibits
smoother transitions between topics and is more human-like in terms of
naturalness and consistency. It can serve as a valuable resource for both
academic research and commercial applications. Furthermore, our proposed
framework can be applied to generate numerous dialogues with various target
intents.",http://arxiv.org/pdf/2308.14266v1
2308.14242v1,cs.AI,The Cultural Psychology of Large Language Models: Is ChatGPT a Holistic or Analytic Thinker?,2023-08-28 01:05:18+00:00,"The prevalent use of Large Language Models (LLMs) has necessitated studying
their mental models, yielding noteworthy theoretical and practical
implications. Current research has demonstrated that state-of-the-art LLMs,
such as ChatGPT, exhibit certain theory of mind capabilities and possess
relatively stable Big Five and/or MBTI personality traits. In addition,
cognitive process features form an essential component of these mental models.
Research in cultural psychology indicated significant differences in the
cognitive processes of Eastern and Western people when processing information
and making judgments. While Westerners predominantly exhibit analytical
thinking that isolates things from their environment to analyze their nature
independently, Easterners often showcase holistic thinking, emphasizing
relationships and adopting a global viewpoint. In our research, we probed the
cultural cognitive traits of ChatGPT. We employed two scales that directly
measure the cognitive process: the Analysis-Holism Scale (AHS) and the Triadic
Categorization Task (TCT). Additionally, we used two scales that investigate
the value differences shaped by cultural thinking: the Dialectical Self Scale
(DSS) and the Self-construal Scale (SCS). In cognitive process tests (AHS/TCT),
ChatGPT consistently tends towards Eastern holistic thinking, but regarding
value judgments (DSS/SCS), ChatGPT does not significantly lean towards the East
or the West. We suggest that the result could be attributed to both the
training paradigm and the training data in LLM development. We discuss the
potential value of this finding for AI research and directions for future
research.",http://arxiv.org/pdf/2308.14242v1
2308.14217v1,cs.DB,Generations of Knowledge Graphs: The Crazy Ideas and the Business Impact,2023-08-27 22:35:27+00:00,"Knowledge Graphs (KGs) have been used to support a wide range of
applications, from web search to personal assistant. In this paper, we describe
three generations of knowledge graphs: entity-based KGs, which have been
supporting general search and question answering (e.g., at Google and Bing);
text-rich KGs, which have been supporting search and recommendations for
products, bio-informatics, etc. (e.g., at Amazon and Alibaba); and the emerging
integration of KGs and LLMs, which we call dual neural KGs. We describe the
characteristics of each generation of KGs, the crazy ideas behind the scenes in
constructing such KGs, and the techniques developed over time to enable
industry impact. In addition, we use KGs as examples to demonstrate a recipe to
evolve research ideas from innovations to production practice, and then to the
next level of innovations, to advance both science and business.",http://arxiv.org/pdf/2308.14217v1
2308.14199v1,cs.AI,Symbolic and Language Agnostic Large Language Models,2023-08-27 20:24:33+00:00,"We argue that the relative success of large language models (LLMs) is not a
reflection on the symbolic vs. subsymbolic debate but a reflection on employing
an appropriate strategy of bottom-up reverse engineering of language at scale.
However, due to the subsymbolic nature of these models whatever knowledge these
systems acquire about language will always be buried in millions of
microfeatures (weights) none of which is meaningful on its own. Moreover, and
due to their stochastic nature, these models will often fail in capturing
various inferential aspects that are prevalent in natural language. What we
suggest here is employing the successful bottom-up strategy in a symbolic
setting, producing symbolic, language agnostic and ontologically grounded large
language models.",http://arxiv.org/pdf/2308.14199v1
2308.14186v1,cs.CL,Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations,2023-08-27 19:22:12+00:00,"The language ability of Large Language Models (LLMs) is often unbalanced
towards English because of the imbalance in the distribution of the
pre-training data. This disparity is demanded in further fine-tuning and
affecting the cross-lingual abilities of LLMs. In this paper, we propose to
empower Instructiontuned LLMs (It-LLMs) in languages other than English by
building semantic alignment between them. Hence, we propose CrossAlpaca, an
It-LLM with cross-lingual instruction-following and Translation-following
demonstrations to improve semantic alignment between languages. We validate our
approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA
and adapted versions of MMLU and BBH. Our models, tested over six different
languages, outperform the It-LLMs tuned on monolingual data. The final results
show that instruction tuning on non-English data is not enough and that
semantic alignment can be further improved by Translation-following
demonstrations.",http://arxiv.org/pdf/2308.14186v1
2308.14182v1,cs.CL,Generative AI for Business Strategy: Using Foundation Models to Create Business Strategy Tools,2023-08-27 19:03:12+00:00,"Generative models (foundation models) such as LLMs (large language models)
are having a large impact on multiple fields. In this work, we propose the use
of such models for business decision making. In particular, we combine
unstructured textual data sources (e.g., news data) with multiple foundation
models (namely, GPT4, transformer-based Named Entity Recognition (NER) models
and Entailment-based Zero-shot Classifiers (ZSC)) to derive IT (information
technology) artifacts in the form of a (sequence of) signed business networks.
We posit that such artifacts can inform business stakeholders about the state
of the market and their own positioning as well as provide quantitative
insights into improving their future outlook.",http://arxiv.org/pdf/2308.14182v1
2308.14179v1,cs.CL,Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP,2023-08-27 18:46:47+00:00,"Mechanistic interpretability seeks to understand the neural mechanisms that
enable specific behaviors in Large Language Models (LLMs) by leveraging
causality-based methods. While these approaches have identified neural circuits
that copy spans of text, capture factual knowledge, and more, they remain
unusable for multimodal models since adapting these tools to the
vision-language domain requires considerable architectural changes. In this
work, we adapt a unimodal causal tracing tool to BLIP to enable the study of
the neural mechanisms underlying image-conditioned text generation. We
demonstrate our approach on a visual question answering dataset, highlighting
the causal relevance of later layer representations for all tokens.
Furthermore, we release our BLIP causal tracing tool as open source to enable
further experimentation in vision-language mechanistic interpretability by the
community. Our code is available at
https://github.com/vedantpalit/Towards-Vision-Language-Mechanistic-Interpretability.",http://arxiv.org/pdf/2308.14179v1
2308.14149v1,cs.CL,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",2023-08-27 16:14:19+00:00,"Generative pre-trained transformer (GPT) models have revolutionized the field
of natural language processing (NLP) with remarkable performance in various
tasks and also extend their power to multimodal domains. Despite their success,
large GPT models like GPT-4 face inherent limitations such as considerable
size, high computational requirements, complex deployment processes, and closed
development loops. These constraints restrict their widespread adoption and
raise concerns regarding their responsible development and usage. The need for
user-friendly, relatively small, and open-sourced alternative GPT models arises
from the desire to overcome these limitations while retaining high performance.
In this survey paper, we provide an examination of alternative open-sourced
models of large GPTs, focusing on user-friendly and relatively small models
that facilitate easier deployment and accessibility. Through this extensive
survey, we aim to equip researchers, practitioners, and enthusiasts with a
thorough understanding of user-friendly and relatively small open-sourced
models of large GPTs, their current state, challenges, and future research
directions, inspiring the development of more efficient, accessible, and
versatile GPT models that cater to the broader scientific community and advance
the field of general artificial intelligence. The source contents are
continuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.",http://arxiv.org/pdf/2308.14149v1
2308.14132v1,cs.CL,Detecting Language Model Attacks with Perplexity,2023-08-27 15:20:06+00:00,"A novel hack involving Large Language Models (LLMs) has emerged, leveraging
adversarial suffixes to trick models into generating perilous responses. This
method has garnered considerable attention from reputable media outlets such as
the New York Times and Wired, thereby influencing public perception regarding
the security and safety of LLMs. In this study, we advocate the utilization of
perplexity as one of the means to recognize such potential attacks. The
underlying concept behind these hacks revolves around appending an unusually
constructed string of text to a harmful query that would otherwise be blocked.
This maneuver confuses the protective mechanisms and tricks the model into
generating a forbidden response. Such scenarios could result in providing
detailed instructions to a malicious user for constructing explosives or
orchestrating a bank heist. Our investigation demonstrates the feasibility of
employing perplexity, a prevalent natural language processing metric, to detect
these adversarial tactics before generating a forbidden response. By evaluating
the perplexity of queries with and without such adversarial suffixes using an
open-source LLM, we discovered that nearly 90 percent were above a perplexity
of 1000. This contrast underscores the efficacy of perplexity for detecting
this type of exploit.",http://arxiv.org/pdf/2308.14132v1
2308.14120v2,cs.LG,Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies,2023-08-27 14:28:38+00:00,"A knowledge gap persists between Machine Learning (ML) developers (e.g., data
scientists) and practitioners (e.g., clinicians), hampering the full
utilization of ML for clinical data analysis. We investigated the potential of
the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this
gap and perform ML analyses efficiently. Real-world clinical datasets and study
details from large trials across various medical specialties were presented to
chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed
state-of-the-art ML models based on the original study's training data to
predict clinical outcomes such as cancer development, cancer progression,
disease complications, or biomarkers such as pathogenic gene sequences.
Strikingly, these ML models matched or outperformed their published
counterparts. We conclude that chatGPT ADA offers a promising avenue to
democratize ML in medicine, making advanced analytics accessible to non-ML
experts and promoting broader applications in medical research and practice.",http://arxiv.org/pdf/2308.14120v2
2308.14115v1,cs.CL,Situated Natural Language Explanations,2023-08-27 14:14:28+00:00,"Natural language is among the most accessible tools for explaining decisions
to humans, and large pretrained language models (PLMs) have demonstrated
impressive abilities to generate coherent natural language explanations (NLE).
The existing NLE research perspectives do not take the audience into account.
An NLE can have high textual quality, but it might not accommodate audiences'
needs and preference. To address this limitation, we propose an alternative
perspective, situated NLE, including a situated generation framework and a
situated evaluation framework. On the generation side, we propose simple prompt
engineering methods that adapt the NLEs to situations. In human studies, the
annotators preferred the situated NLEs. On the evaluation side, we set up
automated evaluation scores in lexical, semantic, and pragmatic categories. The
scores can be used to select the most suitable prompts to generate NLEs.
Situated NLE provides a perspective to conduct further research on automatic
NLE generations.",http://arxiv.org/pdf/2308.14115v1
2308.14089v1,cs.CL,MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records,2023-08-27 12:24:39+00:00,"The ability of large language models (LLMs) to follow natural language
instructions with human-level fluency suggests many opportunities in healthcare
to reduce administrative burden and improve quality of care. However,
evaluating LLMs on realistic text generation tasks for healthcare remains
challenging. Existing question answering datasets for electronic health record
(EHR) data fail to capture the complexity of information needs and
documentation burdens experienced by clinicians. To address these challenges,
we introduce MedAlign, a benchmark dataset of 983 natural language instructions
for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes
clinician-written reference responses for 303 instructions, and provides 276
longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to
evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality
of each LLM response. We found high error rates, ranging from 35% (GPT-4) to
68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k
context lengths for GPT-4. Finally, we report correlations between clinician
rankings and automated natural language generation metrics as a way to rank
LLMs without human review. We make MedAlign available under a research data use
agreement to enable LLM evaluations on tasks aligned with clinician needs and
preferences.",http://arxiv.org/pdf/2308.14089v1
2308.14077v1,cs.FL,An Analysis of On-the-fly Determinization of Finite-state Automata,2023-08-27 11:51:27+00:00,"In this paper we establish an abstraction of on-the-fly determinization of
finite-state automata using transition monoids and demonstrate how it can be
applied to bound the asymptotics. We present algebraic and combinatorial
properties that are sufficient for a polynomial state complexity of the
deterministic automaton constructed on-the-fly. A special case of our findings
is that automata with many non-deterministic transitions almost always admit a
determinization of polynomial complexity. Furthermore, we extend our ideas to
weighted finite-state automata.",http://arxiv.org/pdf/2308.14077v1
2308.14034v1,cs.AI,Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum,2023-08-27 07:53:00+00:00,"Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to extending the capability of LLMs. Although some works
employ open-source LLMs for the tool learning task, most of them are trained in
a controlled environment in which LLMs only learn to execute the human-provided
tools. However, selecting proper tools from the large toolset is also a crucial
ability for the tool learning model to be applied in real-world applications.
Existing methods usually directly employ self-instruction methods to train the
model, which ignores differences in tool complexity. In this paper, we propose
the Confucius, a novel tool learning framework to train LLM to use complicated
tools in real-world scenarios, which contains two main phases: (1) We first
propose a multi-stage learning method to teach the LLM to use various tools
from an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative
Self-instruct from Introspective Feedback (ISIF) to dynamically construct the
dataset to improve the ability to use the complicated tool. Extensive
experiments conducted on both controlled and real-world settings demonstrate
the superiority of our tool learning framework in the real-world application
scenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based
baselines (e.g. GPT4Tools).",http://arxiv.org/pdf/2308.14034v1
2308.14763v1,eess.AS,VoiceBank-2023: A Multi-Speaker Mandarin Speech Corpus for Constructing Personalized TTS Systems for the Speech Impaired,2023-08-27 07:35:30+00:00,"Services of personalized TTS systems for the Mandarin-speaking speech
impaired are rarely mentioned. Taiwan started the VoiceBanking project in 2020,
aiming to build a complete set of services to deliver personalized Mandarin TTS
systems to amyotrophic lateral sclerosis patients. This paper reports the
corpus design, corpus recording, data purging and correction for the corpus,
and evaluations of the developed personalized TTS systems, for the VoiceBanking
project. The developed corpus is named after the VoiceBank-2023 speech corpus
because of its release year. The corpus contains 29.78 hours of utterances with
prompts of short paragraphs and common phrases spoken by 111 native Mandarin
speakers. The corpus is labeled with information about gender, degree of speech
impairment, types of users, transcription, SNRs, and speaking rates. The
VoiceBank-2023 is available by request for non-commercial use and welcomes all
parties to join the VoiceBanking project to improve the services for the speech
impaired.",http://arxiv.org/pdf/2308.14763v1
2308.13961v1,cs.CL,"Translate Meanings, Not Just Words: IdiomKB's Role in Optimizing Idiomatic Translation with Language Models",2023-08-26 21:38:31+00:00,"To translate well, machine translation (MT) systems and general-purposed
language models (LMs) need a deep understanding of both source and target
languages and cultures. Therefore, idioms, with their non-compositional nature,
pose particular challenges for Transformer-based systems, as literal
translations often miss the intended meaning. Traditional methods, which
replace idioms using existing knowledge bases (KBs), often lack scale and
context awareness. Addressing these challenges, our approach prioritizes
context awareness and scalability, allowing for offline storage of idioms in a
manageable KB size. This ensures efficient serving with smaller models and
provides a more comprehensive understanding of idiomatic expressions. We
introduce a multilingual idiom KB (IdiomKB) developed using large LMs to
address this. This KB facilitates better translation by smaller models, such as
BLOOMZ (7.1B), Alpaca (7B), and InstructGPT (6.7B), by retrieving idioms'
figurative meanings. We present a novel, GPT-4-powered metric for human-aligned
evaluation, demonstrating that IdiomKB considerably boosts model performance.
Human evaluations further validate our KB's quality.",http://arxiv.org/pdf/2308.13961v1
2308.13958v1,cs.CL,"Improving Knowledge Distillation for BERT Models: Loss Functions, Mapping Methods, and Weight Tuning",2023-08-26 20:59:21+00:00,"The use of large transformer-based models such as BERT, GPT, and T5 has led
to significant advancements in natural language processing. However, these
models are computationally expensive, necessitating model compression
techniques that reduce their size and complexity while maintaining accuracy.
This project investigates and applies knowledge distillation for BERT model
compression, specifically focusing on the TinyBERT student model. We explore
various techniques to improve knowledge distillation, including experimentation
with loss functions, transformer layer mapping methods, and tuning the weights
of attention and representation loss and evaluate our proposed techniques on a
selection of downstream tasks from the GLUE benchmark. The goal of this work is
to improve the efficiency and effectiveness of knowledge distillation, enabling
the development of more efficient and accurate models for a range of natural
language processing tasks.",http://arxiv.org/pdf/2308.13958v1
2308.13916v1,cs.CL,Exploring Large Language Models for Knowledge Graph Completion,2023-08-26 16:51:17+00:00,"Knowledge graphs play a vital role in numerous artificial intelligence tasks,
yet they frequently face the issue of incompleteness. In this study, we explore
utilizing Large Language Models (LLM) for knowledge graph completion. We
consider triples in knowledge graphs as text sequences and introduce an
innovative framework called Knowledge Graph LLM (KG-LLM) to model these
triples. Our technique employs entity and relation descriptions of a triple as
prompts and utilizes the response for predictions. Experiments on various
benchmark knowledge graphs demonstrate that our method attains state-of-the-art
performance in tasks such as triple classification and relation prediction. We
also find that fine-tuning relatively smaller models (e.g., LLaMA-7B,
ChatGLM-6B) outperforms recent ChatGPT and GPT-4.",http://arxiv.org/pdf/2308.13916v1
2308.13911v1,cs.AI,A Wide Evaluation of ChatGPT on Affective Computing Tasks,2023-08-26 16:10:30+00:00,"With the rise of foundation models, a new artificial intelligence paradigm
has emerged, by simply using general purpose foundation models with prompting
to solve problems instead of training a separate machine learning model for
each problem. Such models have been shown to have emergent properties of
solving problems that they were not initially trained on. The studies for the
effectiveness of such models are still quite limited. In this work, we widely
study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13
affective computing problems, namely aspect extraction, aspect polarity
classification, opinion extraction, sentiment analysis, sentiment intensity
ranking, emotions intensity ranking, suicide tendency detection, toxicity
detection, well-being assessment, engagement measurement, personality
assessment, sarcasm detection, and subjectivity detection. We introduce a
framework to evaluate the ChatGPT models on regression-based problems, such as
intensity ranking problems, by modelling them as pairwise ranking
classification. We compare ChatGPT against more traditional NLP methods, such
as end-to-end recurrent neural networks and transformers. The results
demonstrate the emergent abilities of the ChatGPT models on a wide range of
affective computing problems, where GPT-3.5 and especially GPT-4 have shown
strong performance on many problems, particularly the ones related to
sentiment, emotions, or toxicity. The ChatGPT models fell short for problems
with implicit signals, such as engagement measurement and subjectivity
detection.",http://arxiv.org/pdf/2308.13911v1
2308.13904v1,cs.CL,LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors,2023-08-26 15:21:47+00:00,"Prompt-tuning has emerged as an attractive paradigm for deploying large-scale
language models due to its strong downstream task performance and efficient
multitask serving ability. Despite its wide adoption, we empirically show that
prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside
in the pretrained models and can affect arbitrary downstream tasks. The
state-of-the-art backdoor detection approaches cannot defend against
task-agnostic backdoors since they hardly converge in reversing the backdoor
triggers. To address this issue, we propose LMSanitator, a novel approach for
detecting and removing task-agnostic backdoors on Transformer models. Instead
of directly inversing the triggers, LMSanitator aims to inverse the predefined
attack vectors (pretrained models' output when the input is embedded with
triggers) of the task-agnostic backdoors, which achieves much better
convergence performance and backdoor detection accuracy. LMSanitator further
leverages prompt-tuning's property of freezing the pretrained model to perform
accurate and fast output monitoring and input purging during the inference
phase. Extensive experiments on multiple language models and NLP tasks
illustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves
92.8% backdoor detection accuracy on 960 models and decreases the attack
success rate to less than 1% in most scenarios.",http://arxiv.org/pdf/2308.13904v1
2308.13844v1,cs.CL,Solving Math Word Problem with Problem Type Classification,2023-08-26 10:35:16+00:00,"Math word problems (MWPs) require analyzing text descriptions and generating
mathematical equations to derive solutions. Existing works focus on solving
MWPs with two types of solvers: tree-based solver and large language model
(LLM) solver. However, these approaches always solve MWPs by a single solver,
which will bring the following problems: (1) Single type of solver is hard to
solve all types of MWPs well. (2) A single solver will result in poor
performance due to over-fitting. To address these challenges, this paper
utilizes multiple ensemble approaches to improve MWP-solving ability. Firstly,
We propose a problem type classifier that combines the strengths of the
tree-based solver and the LLM solver. This ensemble approach leverages their
respective advantages and broadens the range of MWPs that can be solved.
Furthermore, we also apply ensemble techniques to both tree-based solver and
LLM solver to improve their performance. For the tree-based solver, we propose
an ensemble learning framework based on ten-fold cross-validation and voting
mechanism. In the LLM solver, we adopt self-consistency (SC) method to improve
answer selection. Experimental results demonstrate the effectiveness of these
ensemble approaches in enhancing MWP-solving ability. The comprehensive
evaluation showcases improved performance, validating the advantages of our
proposed approach. Our code is available at this url:
https://github.com/zhouzihao501/NLPCC2023-Shared-Task3-ChineseMWP.",http://arxiv.org/pdf/2308.13844v1
2308.13782v1,cs.CL,Planning with Logical Graph-based Language Model for Instruction Generation,2023-08-26 06:28:14+00:00,"Despite the superior performance of large language models to generate natural
language texts, it is hard to generate texts with correct logic according to a
given task, due to the difficulties for neural models to capture implied rules
from free-form texts. In this paper, we propose a novel graph-based language
model, Logical-GLM, to infuse logic into language models for more valid text
generation and interpretability. Specifically, we first capture information
from natural language instructions and construct logical bayes graphs that
generally describe domains. Next, we generate logical skeletons to guide
language model training, infusing domain knowledge into language models.
Finally, we alternately optimize the searching policy of graphs and language
models until convergence. The experimental results show that Logical-GLM is
both effective and efficient compared with traditional language models, despite
using smaller-scale training data and fewer parameters. Our approach can
generate instructional texts with more correct logic owing to the internalized
domain knowledge. Moreover, the usage of logical graphs reflects the inner
mechanism of the language models, which improves the interpretability of
black-box models.",http://arxiv.org/pdf/2308.13782v1
2308.13775v1,cs.SE,EditSum: A Retrieve-and-Edit Framework for Source Code Summarization,2023-08-26 05:48:57+00:00,"Existing studies show that code summaries help developers understand and
maintain source code. Unfortunately, these summaries are often missing or
outdated in software projects. Code summarization aims to generate natural
language descriptions automatically for source code. Code summaries are highly
structured and have repetitive patterns. Besides the patternized words, a code
summary also contains important keywords, which are the key to reflecting the
functionality of the code. However, the state-of-the-art approaches perform
poorly on predicting the keywords, which leads to the generated summaries
suffering a loss in informativeness. To alleviate this problem, this paper
proposes a novel retrieve-and-edit approach named EditSum for code
summarization. Specifically, EditSum first retrieves a similar code snippet
from a pre-defined corpus and treats its summary as a prototype summary to
learn the pattern. Then, EditSum edits the prototype automatically to combine
the pattern in the prototype with the semantic information of input code. Our
motivation is that the retrieved prototype provides a good start-point for
post-generation because the summaries of similar code snippets often have the
same pattern. The post-editing process further reuses the patternized words in
the prototype and generates keywords based on the semantic information of input
code. We conduct experiments on a large-scale Java corpus and experimental
results demonstrate that EditSum outperforms the state-of-the-art approaches by
a substantial margin. The human evaluation also proves the summaries generated
by EditSum are more informative and useful. We also verify that EditSum
performs well on predicting the patternized words and keywords.",http://arxiv.org/pdf/2308.13775v1
2308.13768v1,cs.CL,Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content,2023-08-26 05:20:58+00:00,"In this paper, we tackle the emerging challenge of unintended harmful content
generation in Large Language Models (LLMs) with a novel dual-stage optimisation
technique using adversarial fine-tuning. Our two-pronged approach employs an
adversarial model, fine-tuned to generate potentially harmful prompts, and a
judge model, iteratively optimised to discern these prompts. In this
adversarial cycle, the two models seek to outperform each other in the
prompting phase, generating a dataset of rich examples which are then used for
fine-tuning. This iterative application of prompting and fine-tuning allows
continuous refinement and improved performance. The performance of our approach
is evaluated through classification accuracy on a dataset consisting of
problematic prompts not detected by GPT-4, as well as a selection of
contentious but unproblematic prompts. We show considerable increase in
classification accuracy of the judge model on this challenging dataset as it
undergoes the optimisation process. Furthermore, we show that a rudimentary
model \texttt{ada} can achieve 13\% higher accuracy on the hold-out test set
than GPT-4 after only a few rounds of this process, and that this fine-tuning
improves performance in parallel tasks such as toxic comment identification.",http://arxiv.org/pdf/2308.13768v1
2308.13760v1,cs.AI,How Can Context Help? Exploring Joint Retrieval of Passage and Personalized Context,2023-08-26 04:49:46+00:00,"The integration of external personalized context information into
document-grounded conversational systems has significant potential business
value, but has not been well-studied. Motivated by the concept of personalized
context-aware document-grounded conversational systems, we introduce the task
of context-aware passage retrieval. We also construct a dataset specifically
curated for this purpose. We describe multiple baseline systems to address this
task, and propose a novel approach, Personalized Context-Aware Search (PCAS),
that effectively harnesses contextual information during passage retrieval.
Experimental evaluations conducted on multiple popular dense retrieval systems
demonstrate that our proposed approach not only outperforms the baselines in
retrieving the most relevant passage but also excels at identifying the
pertinent context among all the available contexts. We envision that our
contributions will serve as a catalyst for inspiring future research endeavors
in this promising direction.",http://arxiv.org/pdf/2308.13760v1
2308.13754v1,cs.SE,ZC3: Zero-Shot Cross-Language Code Clone Detection,2023-08-26 03:48:10+00:00,"Developers introduce code clones to improve programming productivity. Many
existing studies have achieved impressive performance in monolingual code clone
detection. However, during software development, more and more developers write
semantically equivalent programs with different languages to support different
platforms and help developers translate projects from one language to another.
Considering that collecting cross-language parallel data, especially for
low-resource languages, is expensive and time-consuming, how designing an
effective cross-language model that does not rely on any parallel data is a
significant problem. In this paper, we propose a novel method named ZC3 for
Zero-shot Cross-language Code Clone detection. ZC3 designs the contrastive
snippet prediction to form an isomorphic representation space among different
programming languages. Based on this, ZC3 exploits domain-aware learning and
cycle consistency learning to further constrain the model to generate
representations that are aligned among different languages meanwhile are
diacritical for different types of clones. To evaluate our approach, we conduct
extensive experiments on four representative cross-language clone detection
datasets. Experimental results show that ZC3 outperforms the state-of-the-art
baselines by 67.12%, 51.39%, 14.85%, and 53.01% on the MAP score, respectively.
We further investigate the representational distribution of different languages
and discuss the effectiveness of our method.",http://arxiv.org/pdf/2308.13754v1
2308.13738v1,math.HO,On Philomatics and Psychomatics for Combining Philosophy and Psychology with Mathematics,2023-08-26 02:52:42+00:00,"We propose the concepts of philomatics and psychomatics as hybrid
combinations of philosophy and psychology with mathematics. We explain four
motivations for this combination which are fulfilling the desire of analytical
philosophy, proposing science of philosophy, justifying mathematical algorithms
by philosophy, and abstraction in both philosophy and mathematics. We enumerate
various examples for philomatics and psychomatics, some of which are explained
in more depth. The first example is the analysis of relation between the
context principle, semantic holism, and the usage theory of meaning with the
attention mechanism in mathematics. The other example is on the relations of
Plato's theory of forms in philosophy with the holographic principle in string
theory, object-oriented programming, and machine learning. Finally, the
relation between Wittgenstein's family resemblance and clustering in
mathematics is explained. This paper opens the door of research for combining
philosophy and psychology with mathematics.",http://arxiv.org/pdf/2308.13738v1
2308.13715v1,cs.CL,A Computational Evaluation Framework for Singable Lyric Translation,2023-08-26 00:27:08+00:00,"Lyric translation plays a pivotal role in amplifying the global resonance of
music, bridging cultural divides, and fostering universal connections.
Translating lyrics, unlike conventional translation tasks, requires a delicate
balance between singability and semantics. In this paper, we present a
computational framework for the quantitative evaluation of singable lyric
translation, which seamlessly integrates musical, linguistic, and cultural
dimensions of lyrics. Our comprehensive framework consists of four metrics that
measure syllable count distance, phoneme repetition similarity, musical
structure distance, and semantic similarity. To substantiate the efficacy of
our framework, we collected a singable lyrics dataset, which precisely aligns
English, Japanese, and Korean lyrics on a line-by-line and section-by-section
basis, and conducted a comparative analysis between singable and non-singable
lyrics. Our multidisciplinary approach provides insights into the key
components that underlie the art of lyric translation and establishes a solid
groundwork for the future of computational lyric translation assessment.",http://arxiv.org/pdf/2308.13715v1
2308.13710v1,cs.CL,WellXplain: Wellness Concept Extraction and Classification in Reddit Posts for Mental Health Analysis,2023-08-25 23:50:05+00:00,"During the current mental health crisis, the importance of identifying
potential indicators of mental issues from social media content has surged.
Overlooking the multifaceted nature of mental and social well-being can have
detrimental effects on one's mental state. In traditional therapy sessions,
professionals manually pinpoint the origins and outcomes of underlying mental
challenges, a process both detailed and time-intensive. We introduce an
approach to this intricate mental health analysis by framing the identification
of wellness dimensions in Reddit content as a wellness concept extraction and
categorization challenge. We've curated a unique dataset named WELLXPLAIN,
comprising 3,092 entries and totaling 72,813 words. Drawing from Halbert L.
Dunn's well-regarded wellness theory, our team formulated an annotation
framework along with guidelines. This dataset also includes human-marked
textual segments, offering clear reasoning for decisions made in the wellness
concept categorization process. Our aim in publishing this dataset and
analyzing initial benchmarks is to spearhead the creation of advanced language
models tailored for healthcare-focused concept extraction and categorization.",http://arxiv.org/pdf/2308.13710v1
2308.13696v1,cs.CL,On the Depth between Beam Search and Exhaustive Search for Text Generation,2023-08-25 22:57:53+00:00,"Beam search and exhaustive search are two extreme ends of text decoding
algorithms with respect to the search depth. Beam search is limited in both
search width and depth, whereas exhaustive search is a global search that has
no such limitations. Surprisingly, beam search is not only computationally
cheaper but also performs better than exhaustive search despite its higher
search error. Plenty of research has investigated a range of beam widths, from
small to large, and reported that a beam width that is neither too large nor
too small is desirable. However, in terms of search depth, only the two extreme
ends, beam search and exhaustive search are studied intensively. In this paper,
we examine a range of search depths between the two extremes to discover the
desirable search depth. To this end, we introduce Lookahead Beam Search (LBS),
a multi-step lookahead search that optimizes the objective considering a fixed
number of future steps. Beam search and exhaustive search are special cases of
LBS where the lookahead depth is set to $0$ and $\infty$, respectively. We
empirically evaluate the performance of LBS and find that it outperforms beam
search overall on machine translation tasks. The result suggests there is room
for improvement in beam search by searching deeper. Inspired by the analysis,
we propose Lookbehind Heuristic Beam Search, a computationally feasible search
algorithm that heuristically simulates LBS with 1-step lookahead. The empirical
results show that the proposed method outperforms vanilla beam search on
machine translation and text summarization tasks.",http://arxiv.org/pdf/2308.13696v1
2308.13687v1,cond-mat.mtrl-sci,1.5 million materials narratives generated by chatbots,2023-08-25 22:00:53+00:00,"The advent of artificial intelligence (AI) has enabled a comprehensive
exploration of materials for various applications. However, AI models often
prioritize frequently encountered materials in the scientific literature,
limiting the selection of suitable candidates based on inherent physical and
chemical properties. To address this imbalance, we have generated a dataset of
1,494,017 natural language-material paragraphs based on combined OQMD,
Materials Project, JARVIS, COD and AFLOW2 databases, which are dominated by ab
initio calculations and tend to be much more evenly distributed on the periodic
table. The generated text narratives were then polled and scored by both human
experts and ChatGPT-4, based on three rubrics: technical accuracy, language and
structure, and relevance and depth of content, showing similar scores but with
human-scored depth of content being the most lagging. The merger of
multi-modality data sources and large language model (LLM) holds immense
potential for AI frameworks to help the exploration and discovery of
solid-state materials for specific applications.",http://arxiv.org/pdf/2308.13687v1
2308.13676v1,cs.CL,Rethinking Language Models as Symbolic Knowledge Graphs,2023-08-25 21:25:08+00:00,"Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric
applications such as search, question answering and recommendation. As
contemporary language models (LMs) trained on extensive textual data have
gained prominence, researchers have extensively explored whether the parametric
knowledge within these models can match up to that present in knowledge graphs.
Various methodologies have indicated that enhancing the size of the model or
the volume of training data enhances its capacity to retrieve symbolic
knowledge, often with minimal or no human supervision. Despite these
advancements, there is a void in comprehensively evaluating whether LMs can
encompass the intricate topological and semantic attributes of KGs, attributes
crucial for reasoning processes. In this work, we provide an exhaustive
evaluation of language models of varying sizes and capabilities. We construct
nine qualitative benchmarks that encompass a spectrum of attributes including
symmetry, asymmetry, hierarchy, bidirectionality, compositionality, paths,
entity-centricity, bias and ambiguity. Additionally, we propose novel
evaluation metrics tailored for each of these attributes. Our extensive
evaluation of various LMs shows that while these models exhibit considerable
potential in recalling factual information, their ability to capture intricate
topological and semantic traits of KGs remains significantly constrained. We
note that our proposed evaluation metrics are more reliable in evaluating these
abilities than the existing metrics. Lastly, some of our benchmarks challenge
the common notion that larger LMs (e.g., GPT-4) universally outshine their
smaller counterparts (e.g., BERT).",http://arxiv.org/pdf/2308.13676v1
2308.13646v1,cs.LG,GRASP: A Rehearsal Policy for Efficient Online Continual Learning,2023-08-25 19:34:21+00:00,"Continual learning (CL) in deep neural networks (DNNs) involves incrementally
accumulating knowledge in a DNN from a growing data stream. A major challenge
in CL is that non-stationary data streams cause catastrophic forgetting of
previously learned abilities. Rehearsal is a popular and effective way to
mitigate this problem, which is storing past observations in a buffer and
mixing them with new observations during learning. This leads to a question:
Which stored samples should be selected for rehearsal? Choosing samples that
are best for learning, rather than simply selecting them at random, could lead
to significantly faster learning. For class incremental learning, prior work
has shown that a simple class balanced random selection policy outperforms more
sophisticated methods. Here, we revisit this question by exploring a new sample
selection policy called GRASP. GRASP selects the most prototypical (class
representative) samples first and then gradually selects less prototypical
(harder) examples to update the DNN. GRASP has little additional compute or
memory overhead compared to uniform selection, enabling it to scale to large
datasets. We evaluate GRASP and other policies by conducting CL experiments on
the large-scale ImageNet-1K and Places-LT image classification datasets. GRASP
outperforms all other rehearsal policies. Beyond vision, we also demonstrate
that GRASP is effective for CL on five text classification datasets.",http://arxiv.org/pdf/2308.13646v1
2308.13517v1,cs.CL,ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection,2023-08-25 17:51:23+00:00,"Open intent detection, a crucial aspect of natural language understanding,
involves the identification of previously unseen intents in user-generated
text. Despite the progress made in this field, challenges persist in handling
new combinations of language components, which is essential for compositional
generalization. In this paper, we present a case study exploring the use of
ChatGPT as a data augmentation technique to enhance compositional
generalization in open intent detection tasks. We begin by discussing the
limitations of existing benchmarks in evaluating this problem, highlighting the
need for constructing datasets for addressing compositional generalization in
open intent detection tasks. By incorporating synthetic data generated by
ChatGPT into the training process, we demonstrate that our approach can
effectively improve model performance. Rigorous evaluation of multiple
benchmarks reveals that our method outperforms existing techniques and
significantly enhances open intent detection capabilities. Our findings
underscore the potential of large language models like ChatGPT for data
augmentation in natural language understanding tasks.",http://arxiv.org/pdf/2308.13517v1
2308.13506v2,cs.CL,Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level,2023-08-25 17:31:46+00:00,"As research on machine translation moves to translating text beyond the
sentence level, it remains unclear how effective automatic evaluation metrics
are at scoring longer translations. In this work, we first propose a method for
creating paragraph-level data for training and meta-evaluating metrics from
existing sentence-level data. Then, we use these new datasets to benchmark
existing sentence-level metrics as well as train learned metrics at the
paragraph level. Interestingly, our experimental results demonstrate that using
sentence-level metrics to score entire paragraphs is equally as effective as
using a metric designed to work at the paragraph level. We speculate this
result can be attributed to properties of the task of reference-based
evaluation as well as limitations of our datasets with respect to capturing all
types of phenomena that occur in paragraph-level translations.",http://arxiv.org/pdf/2308.13506v2
2308.13590v1,cs.IR,LSTM-based QoE Evaluation for Web Microservices' Reputation Scoring,2023-08-25 17:23:12+00:00,"Sentiment analysis is the task of mining the authors' opinions about specific
entities. It allows organizations to monitor different services in real time
and act accordingly. Reputation is what is generally said or believed about
people or things. Informally, reputation combines the measure of reliability
derived from feedback, reviews, and ratings gathered from users, which reflect
their quality of experience (QoE) and can either increase or harm the
reputation of the provided services. In this study, we propose to perform
sentiment analysis on web microservices reviews to exploit the provided
information to assess and score the microservices' reputation. Our proposed
approach uses the Long Short-Term Memory (LSTM) model to perform sentiment
analysis and the Net Brand Reputation (NBR) algorithm to assess reputation
scores for microservices. This approach is tested on a set of more than 10,000
reviews related to 15 Amazon Web microservices, and the experimental results
have shown that our approach is more accurate than existing approaches, with an
accuracy and precision of 93% obtained after applying an oversampling strategy
and a resulting reputation score of the considered microservices community of
89%.",http://arxiv.org/pdf/2308.13590v1
2308.13497v1,cs.CL,Ngambay-French Neural Machine Translation (sba-Fr),2023-08-25 17:13:20+00:00,"In Africa, and the world at large, there is an increasing focus on developing
Neural Machine Translation (NMT) systems to overcome language barriers. NMT for
Low-resource language is particularly compelling as it involves learning with
limited labelled data. However, obtaining a well-aligned parallel corpus for
low-resource languages can be challenging. The disparity between the
technological advancement of a few global languages and the lack of research on
NMT for local languages in Chad is striking. End-to-end NMT trials on
low-resource Chad languages have not been attempted. Additionally, there is a
dearth of online and well-structured data gathering for research in Natural
Language Processing, unlike some African languages. However, a guided approach
for data gathering can produce bitext data for many Chadian language
translation pairs with well-known languages that have ample data. In this
project, we created the first sba-Fr Dataset, which is a corpus of
Ngambay-to-French translations, and fine-tuned three pre-trained models using
this dataset. Our experiments show that the M2M100 model outperforms other
models with high BLEU scores on both original and original+synthetic data. The
publicly available bitext dataset can be used for research purposes.",http://arxiv.org/pdf/2308.13497v1
2308.15479v1,cs.CV,3D Adversarial Augmentations for Robust Out-of-Domain Predictions,2023-08-29 17:58:55+00:00,"Since real-world training datasets cannot properly sample the long tail of
the underlying data distribution, corner cases and rare out-of-domain samples
can severely hinder the performance of state-of-the-art models. This problem
becomes even more severe for dense tasks, such as 3D semantic segmentation,
where points of non-standard objects can be confidently associated to the wrong
class. In this work, we focus on improving the generalization to out-of-domain
data. We achieve this by augmenting the training set with adversarial examples.
First, we learn a set of vectors that deform the objects in an adversarial
fashion. To prevent the adversarial examples from being too far from the
existing data distribution, we preserve their plausibility through a series of
constraints, ensuring sensor-awareness and shapes smoothness. Then, we perform
adversarial augmentation by applying the learned sample-independent vectors to
the available objects when training a model. We conduct extensive experiments
across a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D
object detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D
semantic segmentation. Despite training on a standard single dataset, our
approach substantially improves the robustness and generalization of both 3D
object detection and 3D semantic segmentation methods to out-of-domain data.",http://arxiv.org/pdf/2308.15479v1
2308.15478v1,cs.LG,An Adaptive Tangent Feature Perspective of Neural Networks,2023-08-29 17:57:20+00:00,"In order to better understand feature learning in neural networks, we propose
a framework for understanding linear models in tangent feature space where the
features are allowed to be transformed during training. We consider linear
transformations of features, resulting in a joint optimization over parameters
and transformations with a bilinear interpolation constraint. We show that this
optimization problem has an equivalent linearly constrained optimization with
structured regularization that encourages approximately low rank solutions.
Specializing to neural network structure, we gain insights into how the
features and thus the kernel function change, providing additional nuance to
the phenomenon of kernel alignment when the target function is poorly
represented using tangent features. In addition to verifying our theoretical
observations in real neural networks on a simple regression problem, we
empirically show that an adaptive feature implementation of tangent feature
classification has an order of magnitude lower sample complexity than the fixed
tangent feature model on MNIST and CIFAR-10.",http://arxiv.org/pdf/2308.15478v1
2308.15470v1,cs.LG,Policy composition in reinforcement learning via multi-objective policy optimization,2023-08-29 17:50:27+00:00,"We enable reinforcement learning agents to learn successful behavior policies
by utilizing relevant pre-existing teacher policies. The teacher policies are
introduced as objectives, in addition to the task objective, in a
multi-objective policy optimization setting. Using the Multi-Objective Maximum
a Posteriori Policy Optimization algorithm
\citep{abdolmaleki2020distributional}, we show that teacher policies can help
speed up learning, particularly in the absence of shaping rewards. In two
domains with continuous observation and action spaces, our agents successfully
compose teacher policies in sequence and in parallel, and are also able to
further extend the policies of the teachers in order to solve the task.
  Depending on the specified combination of task and teacher(s), teacher(s) may
naturally act to limit the final performance of an agent. The extent to which
agents are required to adhere to teacher policies are determined by
hyperparameters which determine both the effect of teachers on learning speed
and the eventual performance of the agent on the task. In the {\tt humanoid}
domain \citep{deepmindcontrolsuite2018}, we also equip agents with the ability
to control the selection of teachers. With this ability, agents are able to
meaningfully compose from the teacher policies to achieve a superior task
reward on the {\tt walk} task than in cases without access to the teacher
policies. We show the resemblance of composed task policies with the
corresponding teacher policies through videos.",http://arxiv.org/pdf/2308.15470v1
2308.15466v1,cs.LG,Input margins can predict generalization too,2023-08-29 17:47:42+00:00,"Understanding generalization in deep neural networks is an active area of
research. A promising avenue of exploration has been that of margin
measurements: the shortest distance to the decision boundary for a given sample
or its representation internal to the network. While margins have been shown to
be correlated with the generalization ability of a model when measured at its
hidden representations (hidden margins), no such link between large margins and
generalization has been established for input margins. We show that while input
margins are not generally predictive of generalization, they can be if the
search space is appropriately constrained. We develop such a measure based on
input margins, which we refer to as `constrained margins'. The predictive power
of this new measure is demonstrated on the 'Predicting Generalization in Deep
Learning' (PGDL) dataset and contrasted with hidden representation margins. We
find that constrained margins achieve highly competitive scores and outperform
other margin measurements in general. This provides a novel insight on the
relationship between generalization and classification margins, and highlights
the importance of considering the data manifold for investigations of
generalization in DNNs.",http://arxiv.org/pdf/2308.15466v1
2308.15461v1,cs.CV,Canonical Factors for Hybrid Neural Fields,2023-08-29 17:38:33+00:00,"Factored feature volumes offer a simple way to build more compact, efficient,
and intepretable neural fields, but also introduce biases that are not
necessarily beneficial for real-world data. In this work, we (1) characterize
the undesirable biases that these architectures have for axis-aligned signals
-- they can lead to radiance field reconstruction differences of as high as 2
PSNR -- and (2) explore how learning a set of canonicalizing transformations
can improve representations by removing these biases. We prove in a
two-dimensional model problem that simultaneously learning these
transformations together with scene appearance succeeds with drastically
improved efficiency. We validate the resulting architectures, which we call
TILTED, using image, signed distance, and radiance field reconstruction tasks,
where we observe improvements across quality, robustness, compactness, and
runtime. Results demonstrate that TILTED can enable capabilities comparable to
baselines that are 2x larger, while highlighting weaknesses of neural field
evaluation procedures.",http://arxiv.org/pdf/2308.15461v1
2308.15434v1,cs.LG,Random feature approximation for general spectral methods,2023-08-29 16:56:03+00:00,"Random feature approximation is arguably one of the most popular techniques
to speed up kernel methods in large scale algorithms and provides a theoretical
approach to the analysis of deep neural networks. We analyze generalization
properties for a large class of spectral regularization methods combined with
random features, containing kernel methods with implicit regularization such as
gradient descent or explicit methods like Tikhonov regularization. For our
estimators we obtain optimal learning rates over regularity classes (even for
classes that are not included in the reproducing kernel Hilbert space), which
are defined through appropriate source conditions. This improves or completes
previous results obtained in related settings for specific kernel algorithms.",http://arxiv.org/pdf/2308.15434v1
2308.15410v1,astro-ph.SR,Probabilistic solar flare forecasting using historical magnetogram data,2023-08-29 16:10:20+00:00,"Solar flare forecasting research using machine learning (ML) has focused on
high resolution magnetogram data from the SDO/HMI era covering Solar Cycle 24
and the start of Solar Cycle 25, with some efforts looking back to SOHO/MDI for
data from Solar Cycle 23. In this paper, we consider over 4 solar cycles of
daily historical magnetogram data from multiple instruments. This is the first
attempt to take advantage of this historical data for ML-based flare
forecasting. We apply a convolutional neural network (CNN) to extract features
from full-disk magnetograms together with a logistic regression model to
incorporate scalar features based on magnetograms and flaring history. We use
an ensemble approach to generate calibrated probabilistic forecasts of M-class
or larger flares in the next 24 hours. Overall, we find that including
historical data improves forecasting skill and reliability. We show that single
frame magnetograms do not contain significantly more relevant information than
can be summarized in a small number of scalar features, and that flaring
history has greater predictive power than our CNN-extracted features. This
indicates the importance of including temporal information in flare forecasting
models.",http://arxiv.org/pdf/2308.15410v1
2308.15405v1,cs.LG,Robust Long-Tailed Learning via Label-Aware Bounded CVaR,2023-08-29 16:07:18+00:00,"Data in the real-world classification problems are always imbalanced or
long-tailed, wherein the majority classes have the most of the samples that
dominate the model training. In such setting, the naive model tends to have
poor performance on the minority classes. Previously, a variety of loss
modifications have been proposed to address the long-tailed leaning problem,
while these methods either treat the samples in the same class
indiscriminatingly or lack a theoretical guarantee. In this paper, we propose
two novel approaches based on CVaR (Conditional Value at Risk) to improve the
performance of long-tailed learning with a solid theoretical ground.
Specifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss
to overcome the pessimistic result of the original CVaR, and further design the
optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we
additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to
stabilize the optimization process, where we also offer the theoretical
support. Extensive experiments on real-world datasets with long-tailed label
distributions verify the superiority of our proposed methods.",http://arxiv.org/pdf/2308.15405v1
2308.15395v1,cs.LG,The CausalBench challenge: A machine learning contest for gene network inference from single-cell perturbation data,2023-08-29 15:54:15+00:00,"In drug discovery, mapping interactions between genes within cellular systems
is a crucial early step. This helps formulate hypotheses regarding molecular
mechanisms that could potentially be targeted by future medicines. The
CausalBench Challenge was an initiative to invite the machine learning
community to advance the state of the art in constructing gene-gene interaction
networks. These networks, derived from large-scale, real-world datasets of
single cells under various perturbations, are crucial for understanding the
causal mechanisms underlying disease biology. Using the framework provided by
the CausalBench benchmark, participants were tasked with enhancing the capacity
of the state of the art methods to leverage large-scale genetic perturbation
data. This report provides an analysis and summary of the methods submitted
during the challenge to give a partial image of the state of the art at the
time of the challenge. The winning solutions significantly improved performance
compared to previous baselines, establishing a new state of the art for this
critical task in biology and medicine.",http://arxiv.org/pdf/2308.15395v1
2308.15386v1,eess.IV,Shape-Margin Knowledge Augmented Network for Thyroid Nodule Segmentation and Diagnosis,2023-08-29 15:29:06+00:00,"Thyroid nodule segmentation is a crucial step in the diagnostic procedure of
physicians and computer-aided diagnosis systems. Mostly, current studies treat
segmentation and diagnosis as independent tasks without considering the
correlation between these tasks. The sequence steps of these independent tasks
in computer-aided diagnosis systems may lead to the accumulation of errors.
Therefore, it is worth combining them as a whole through exploring the
relationship between thyroid nodule segmentation and diagnosis. According to
the thyroid imaging reporting and data system (TI-RADS), the assessment of
shape and margin characteristics is the prerequisite for the discrimination of
benign and malignant thyroid nodules. These characteristics can be observed in
the thyroid nodule segmentation masks. Inspired by the diagnostic procedure of
TI-RADS, this paper proposes a shape-margin knowledge augmented network
(SkaNet) for simultaneously thyroid nodule segmentation and diagnosis. Due to
the similarity in visual features between segmentation and diagnosis, SkaNet
shares visual features in the feature extraction stage and then utilizes a
dual-branch architecture to perform thyroid nodule segmentation and diagnosis
tasks simultaneously. To enhance effective discriminative features, an
exponential mixture module is devised, which incorporates convolutional feature
maps and self-attention maps by exponential weighting. Then, SkaNet is jointly
optimized by a knowledge augmented multi-task loss function with a constraint
penalty term. It embeds shape and margin characteristics through numerical
computation and models the relationship between the thyroid nodule diagnosis
results and segmentation masks.",http://arxiv.org/pdf/2308.15386v1
2308.15370v1,stat.ML,Multi-Response Heteroscedastic Gaussian Process Models and Their Inference,2023-08-29 15:06:47+00:00,"Despite the widespread utilization of Gaussian process models for versatile
nonparametric modeling, they exhibit limitations in effectively capturing
abrupt changes in function smoothness and accommodating relationships with
heteroscedastic errors. Addressing these shortcomings, the heteroscedastic
Gaussian process (HeGP) regression seeks to introduce flexibility by
acknowledging the variability of residual variances across covariates in the
regression model. In this work, we extend the HeGP concept, expanding its scope
beyond regression tasks to encompass classification and state-space models. To
achieve this, we propose a novel framework where the Gaussian process is
coupled with a covariate-induced precision matrix process, adopting a mixture
formulation. This approach enables the modeling of heteroscedastic covariance
functions across covariates. To mitigate the computational challenges posed by
sampling, we employ variational inference to approximate the posterior and
facilitate posterior predictive modeling. Additionally, our training process
leverages an EM algorithm featuring closed-form M-step updates to efficiently
evaluate the heteroscedastic covariance function. A notable feature of our
model is its consistent performance on multivariate responses, accommodating
various types (continuous or categorical) seamlessly. Through a combination of
simulations and real-world applications in climatology, we illustrate the
model's prowess and advantages. By overcoming the limitations of traditional
Gaussian process models, our proposed framework offers a robust and versatile
tool for a wide array of applications.",http://arxiv.org/pdf/2308.15370v1
2308.15364v1,cs.LG,Heterogeneous Multi-Task Gaussian Cox Processes,2023-08-29 15:01:01+00:00,"This paper presents a novel extension of multi-task Gaussian Cox processes
for modeling multiple heterogeneous correlated tasks jointly, e.g.,
classification and regression, via multi-output Gaussian processes (MOGP). A
MOGP prior over the parameters of the dedicated likelihoods for classification,
regression and point process tasks can facilitate sharing of information
between heterogeneous tasks, while allowing for nonparametric parameter
estimation. To circumvent the non-conjugate Bayesian inference in the MOGP
modulated heterogeneous multi-task framework, we employ the data augmentation
technique and derive a mean-field approximation to realize closed-form
iterative updates for estimating model parameters. We demonstrate the
performance and inference on both 1D synthetic data as well as 2D urban data of
Vancouver.",http://arxiv.org/pdf/2308.15364v1
2308.15349v1,cs.LG,Lie-Poisson Neural Networks (LPNets): Data-Based Computing of Hamiltonian Systems with Symmetries,2023-08-29 14:45:23+00:00,"An accurate data-based prediction of the long-term evolution of Hamiltonian
systems requires a network that preserves the appropriate structure under each
time step. Every Hamiltonian system contains two essential ingredients: the
Poisson bracket and the Hamiltonian. Hamiltonian systems with symmetries, whose
paradigm examples are the Lie-Poisson systems, have been shown to describe a
broad category of physical phenomena, from satellite motion to underwater
vehicles, fluids, geophysical applications, complex fluids, and plasma physics.
The Poisson bracket in these systems comes from the symmetries, while the
Hamiltonian comes from the underlying physics. We view the symmetry of the
system as primary, hence the Lie-Poisson bracket is known exactly, whereas the
Hamiltonian is regarded as coming from physics and is considered not known, or
known approximately. Using this approach, we develop a network based on
transformations that exactly preserve the Poisson bracket and the special
functions of the Lie-Poisson systems (Casimirs) to machine precision. We
present two flavors of such systems: one, where the parameters of
transformations are computed from data using a dense neural network (LPNets),
and another, where the composition of transformations is used as building
blocks (G-LPNets). We also show how to adapt these methods to a larger class of
Poisson brackets. We apply the resulting methods to several examples, such as
rigid body (satellite) motion, underwater vehicles, a particle in a magnetic
field, and others. The methods developed in this paper are important for the
construction of accurate data-based methods for simulating the long-term
dynamics of physical systems.",http://arxiv.org/pdf/2308.15349v1
2308.15344v1,cs.LG,Imperceptible Adversarial Attack on Deep Neural Networks from Image Boundary,2023-08-29 14:41:05+00:00,"Although Deep Neural Networks (DNNs), such as the convolutional neural
networks (CNN) and Vision Transformers (ViTs), have been successfully applied
in the field of computer vision, they are demonstrated to be vulnerable to
well-sought Adversarial Examples (AEs) that can easily fool the DNNs. The
research in AEs has been active, and many adversarial attacks and explanations
have been proposed since they were discovered in 2014. The mystery of the AE's
existence is still an open question, and many studies suggest that DNN training
algorithms have blind spots. The salient objects usually do not overlap with
boundaries; hence, the boundaries are not the DNN model's attention.
Nevertheless, recent studies show that the boundaries can dominate the behavior
of the DNN models. Hence, this study aims to look at the AEs from a different
perspective and proposes an imperceptible adversarial attack that systemically
attacks the input image boundary for finding the AEs. The experimental results
have shown that the proposed boundary attacking method effectively attacks six
CNN models and the ViT using only 32% of the input image content (from the
boundaries) with an average success rate (SR) of 95.2% and an average peak
signal-to-noise ratio of 41.37 dB. Correlation analyses are conducted,
including the relation between the adversarial boundary's width and the SR and
how the adversarial boundary changes the DNN model's attention. This paper's
discoveries can potentially advance the understanding of AEs and provide a
different perspective on how AEs can be constructed.",http://arxiv.org/pdf/2308.15344v1
2308.15327v1,cs.RO,Enhancing Robot Learning through Learned Human-Attention Feature Maps,2023-08-29 14:23:44+00:00,"Robust and efficient learning remains a challenging problem in robotics, in
particular with complex visual inputs. Inspired by human attention mechanism,
with which we quickly process complex visual scenes and react to changes in the
environment, we think that embedding auxiliary information about focus point
into robot learning would enhance efficiency and robustness of the learning
process. In this paper, we propose a novel approach to model and emulate the
human attention with an approximate prediction model. We then leverage this
output and feed it as a structured auxiliary feature map into downstream
learning tasks. We validate this idea by learning a prediction model from
human-gaze recordings of manual driving in the real world. We test our approach
on two learning tasks - object detection and imitation learning. Our
experiments demonstrate that the inclusion of predicted human attention leads
to improved robustness of the trained models to out-of-distribution samples and
faster learning in low-data regime settings. Our work highlights the potential
of incorporating structured auxiliary information in representation learning
for robotics and opens up new avenues for research in this direction. All code
and data are available online.",http://arxiv.org/pdf/2308.15327v1
2308.15323v1,cs.CV,Occlusion-Aware Deep Convolutional Neural Network via Homogeneous Tanh-transforms for Face Parsing,2023-08-29 14:20:13+00:00,"Face parsing infers a pixel-wise label map for each semantic facial
component. Previous methods generally work well for uncovered faces, however
overlook the facial occlusion and ignore some contextual area outside a single
face, especially when facial occlusion has become a common situation during the
COVID-19 epidemic. Inspired by the illumination theory of image, we propose a
novel homogeneous tanh-transforms for image preprocessing, which made up of
four tanh-transforms, that fuse the central vision and the peripheral vision
together. Our proposed method addresses the dilemma of face parsing under
occlusion and compresses more information of surrounding context. Based on
homogeneous tanh-transforms, we propose an occlusion-aware convolutional neural
network for occluded face parsing. It combines the information both in
Tanh-polar space and Tanh-Cartesian space, capable of enhancing receptive
fields. Furthermore, we introduce an occlusion-aware loss to focus on the
boundaries of occluded regions. The network is simple and flexible, and can be
trained end-to-end. To facilitate future research of occluded face parsing, we
also contribute a new cleaned face parsing dataset, which is manually purified
from several academic or industrial datasets, including CelebAMask-HQ,
Short-video Face Parsing as well as Helen dataset and will make it public.
Experiments demonstrate that our method surpasses state-of-art methods of face
parsing under occlusion.",http://arxiv.org/pdf/2308.15323v1
2308.15291v1,eess.SP,"Towards quantitative precision for ECG analysis: Leveraging state space models, self-supervision and patient metadata",2023-08-29 13:25:26+00:00,"Deep learning has emerged as the preferred modeling approach for automatic
ECG analysis. In this study, we investigate three elements aimed at improving
the quantitative accuracy of such systems. These components consistently
enhance performance beyond the existing state-of-the-art, which is
predominantly based on convolutional models. Firstly, we explore more
expressive architectures by exploiting structured state space models (SSMs).
These models have shown promise in capturing long-term dependencies in time
series data. By incorporating SSMs into our approach, we not only achieve
better performance, but also gain insights into long-standing questions in the
field. Specifically, for standard diagnostic tasks, we find no advantage in
using higher sampling rates such as 500Hz compared to 100Hz. Similarly,
extending the input size of the model beyond 3 seconds does not lead to
significant improvements. Secondly, we demonstrate that self-supervised
learning using contrastive predictive coding can further improve the
performance of SSMs. By leveraging self-supervision, we enable the model to
learn more robust and representative features, leading to improved analysis
accuracy. Lastly, we depart from synthetic benchmarking scenarios and
incorporate basic demographic metadata alongside the ECG signal as input. This
inclusion of patient metadata departs from the conventional practice of relying
solely on the signal itself. Remarkably, this addition consistently yields
positive effects on predictive performance. We firmly believe that all three
components should be considered when developing next-generation ECG analysis
algorithms.",http://arxiv.org/pdf/2308.15291v1
2308.15283v1,cs.LG,Structural Node Embeddings with Homomorphism Counts,2023-08-29 13:14:53+00:00,"Graph homomorphism counts, first explored by Lov\'asz in 1967, have recently
garnered interest as a powerful tool in graph-based machine learning. Grohe
(PODS 2020) proposed the theoretical foundations for using homomorphism counts
in machine learning on graph level as well as node level tasks. By their very
nature, these capture local structural information, which enables the creation
of robust structural embeddings. While a first approach for graph level tasks
has been made by Nguyen and Maehara (ICML 2020), we experimentally show the
effectiveness of homomorphism count based node embeddings. Enriched with node
labels, node weights, and edge weights, these offer an interpretable
representation of graph data, allowing for enhanced explainability of machine
learning models.
  We propose a theoretical framework for isomorphism-invariant homomorphism
count based embeddings which lend themselves to a wide variety of downstream
tasks. Our approach capitalises on the efficient computability of graph
homomorphism counts for bounded treewidth graph classes, rendering it a
practical solution for real-world applications. We demonstrate their
expressivity through experiments on benchmark datasets. Although our results do
not match the accuracy of state-of-the-art neural architectures, they are
comparable to other advanced graph learning models. Remarkably, our approach
demarcates itself by ensuring explainability for each individual feature. By
integrating interpretable machine learning algorithms like SVMs or Random
Forests, we establish a seamless, end-to-end explainable pipeline. Our study
contributes to the advancement of graph-based techniques that offer both
performance and interpretability.",http://arxiv.org/pdf/2308.15283v1
2308.15250v1,cs.LG,The Relative Gaussian Mechanism and its Application to Private Gradient Descent,2023-08-29 12:16:57+00:00,"The Gaussian Mechanism (GM), which consists in adding Gaussian noise to a
vector-valued query before releasing it, is a standard privacy protection
mechanism. In particular, given that the query respects some L2 sensitivity
property (the L2 distance between outputs on any two neighboring inputs is
bounded), GM guarantees R\'enyi Differential Privacy (RDP). Unfortunately,
precisely bounding the L2 sensitivity can be hard, thus leading to loose
privacy bounds. In this work, we consider a Relative L2 sensitivity assumption,
in which the bound on the distance between two query outputs may also depend on
their norm. Leveraging this assumption, we introduce the Relative Gaussian
Mechanism (RGM), in which the variance of the noise depends on the norm of the
output. We prove tight bounds on the RDP parameters under relative L2
sensitivity, and characterize the privacy loss incurred by using
output-dependent noise. In particular, we show that RGM naturally adapts to a
latent variable that would control the norm of the output. Finally, we
instantiate our framework to show tight guarantees for Private Gradient
Descent, a problem that naturally fits our relative L2 sensitivity assumption.",http://arxiv.org/pdf/2308.15250v1
2308.15243v1,cs.CY,Reliability Gaps Between Groups in COMPAS Dataset,2023-08-29 12:09:22+00:00,"This paper investigates the inter-rater reliability of risk assessment
instruments (RAIs). The main question is whether different, socially salient
groups are affected differently by a lack of inter-rater reliability of RAIs,
that is, whether mistakes with respect to different groups affects them
differently. The question is investigated with a simulation study of the COMPAS
dataset. A controlled degree of noise is injected into the input data of a
predictive model; the noise can be interpreted as a synthetic rater that makes
mistakes. The main finding is that there are systematic differences in output
reliability between groups in the COMPAS dataset. The sign of the difference
depends on the kind of inter-rater statistic that is used (Cohen's Kappa,
Byrt's PABAK, ICC), and in particular whether or not a correction of
predictions prevalences of the groups is used.",http://arxiv.org/pdf/2308.15243v1
2308.15237v1,cs.CR,Assessing Cyclostationary Malware Detection via Feature Selection and Classification,2023-08-29 11:52:31+00:00,"Cyclostationarity involves periodic statistical variations in signals and
processes, commonly used in signal analysis and network security. In the
context of attacks, cyclostationarity helps detect malicious behaviors within
network traffic, such as traffic patterns in Distributed Denial of Service
(DDoS) attacks or hidden communication channels in malware. This approach
enhances security by identifying abnormal patterns and informing Network
Intrusion Detection Systems (NIDSs) to recognize potential attacks, enhancing
protection against both known and novel threats. This research focuses on
identifying cyclostationary malware behavior and its detection. The main goal
is to pinpoint essential cyclostationary features used in NIDSs. These features
are extracted using algorithms such as Boruta and Principal Component Analysis
(PCA), and then categorized to find the most significant cyclostationary
patterns. The aim of this article is to reveal periodically changing malware
behaviors through cyclostationarity. The study highlights the importance of
spotting cyclostationary malware in NIDSs by using established datasets like
KDD99, NSL-KDD, and the UGRansome dataset. The UGRansome dataset is designed
for anomaly detection research and includes both normal and abnormal network
threat categories of zero-day attacks. A comparison is made using the Random
Forest (RF) and Support Vector Machine (SVM) algorithms, while also evaluating
the effectiveness of Boruta and PCA. The findings show that PCA is more
promising than using Boruta alone for extracting cyclostationary network
feature patterns. Additionally, the analysis identifies the internet protocol
as the most noticeable cyclostationary feature pattern used by malware.
Notably, the UGRansome dataset outperforms the KDD99 and NSL-KDD, achieving 99%
accuracy in signature malware detection using the RF algorithm and 98% with the
SVM.",http://arxiv.org/pdf/2308.15237v1
2308.15223v1,cs.LG,Evaluating Explanation Methods for Multivariate Time Series Classification,2023-08-29 11:24:12+00:00,"Multivariate time series classification is an important computational task
arising in applications where data is recorded over time and over multiple
channels. For example, a smartwatch can record the acceleration and orientation
of a person's motion, and these signals are recorded as multivariate time
series. We can classify this data to understand and predict human movement and
various properties such as fitness levels. In many applications classification
alone is not enough, we often need to classify but also understand what the
model learns (e.g., why was a prediction given, based on what information in
the data). The main focus of this paper is on analysing and evaluating
explanation methods tailored to Multivariate Time Series Classification (MTSC).
We focus on saliency-based explanation methods that can point out the most
relevant channels and time series points for the classification decision. We
analyse two popular and accurate multivariate time series classifiers, ROCKET
and dResNet, as well as two popular explanation methods, SHAP and dCAM. We
study these methods on 3 synthetic datasets and 2 real-world datasets and
provide a quantitative and qualitative analysis of the explanations provided.
We find that flattening the multivariate datasets by concatenating the channels
works as well as using multivariate classifiers directly and adaptations of
SHAP for MTSC work quite well. Additionally, we also find that the popular
synthetic datasets we used are not suitable for time series analysis.",http://arxiv.org/pdf/2308.15223v1
2308.15172v1,eess.IV,Is visual explanation with Grad-CAM more reliable for deeper neural networks? a case study with automatic pneumothorax diagnosis,2023-08-29 09:54:30+00:00,"While deep learning techniques have provided the state-of-the-art performance
in various clinical tasks, explainability regarding their decision-making
process can greatly enhance the credence of these methods for safer and quicker
clinical adoption. With high flexibility, Gradient-weighted Class Activation
Mapping (Grad-CAM) has been widely adopted to offer intuitive visual
interpretation of various deep learning models' reasoning processes in
computer-assisted diagnosis. However, despite the popularity of the technique,
there is still a lack of systematic study on Grad-CAM's performance on
different deep learning architectures. In this study, we investigate its
robustness and effectiveness across different popular deep learning models,
with a focus on the impact of the networks' depths and architecture types, by
using a case study of automatic pneumothorax diagnosis in X-ray scans. Our
results show that deeper neural networks do not necessarily contribute to a
strong improvement of pneumothorax diagnosis accuracy, and the effectiveness of
GradCAM also varies among different network architectures.",http://arxiv.org/pdf/2308.15172v1
2308.15164v1,cs.LG,ABS-SGD: A Delayed Synchronous Stochastic Gradient Descent Algorithm with Adaptive Batch Size for Heterogeneous GPU Clusters,2023-08-29 09:46:52+00:00,"As the size of models and datasets grows, it has become increasingly common
to train models in parallel. However, existing distributed stochastic gradient
descent (SGD) algorithms suffer from insufficient utilization of computational
resources and poor convergence in heterogeneous clusters. In this paper, we
propose a delayed synchronous SGD algorithm with adaptive batch size (ABS-SGD)
for heterogeneous GPU clusters. In ABS-SGD, workers perform global
synchronization to accumulate delayed gradients and use the accumulated delayed
gradients to update parameters. While workers are performing global
synchronization for delayed gradients, they perform the computation of the next
batch without specifying batch size in advance, which lasts until the next
global synchronization starts, realizing the full utilization of computational
resources. Since the gradient delay is only one iteration, the stale gradient
problem can be alleviated. We theoretically prove the convergence of ABS-SGD in
heterogeneous clusters. Extensive experiments in three types of heterogeneous
clusters demonstrate that ABS-SGD can make full use of computational resources
and accelerate model convergence: When training ResNet18 network with 4
workers, ABS-SGD increases the convergence speed by 1.30x on average compared
with the best baseline algorithm.",http://arxiv.org/pdf/2308.15164v1
2308.15157v1,cs.LG,On the improvement of model-predictive controllers,2023-08-29 09:39:12+00:00,"This article investigates synthetic model-predictive control (MPC) problems
to demonstrate that an increased precision of the internal prediction model
(PM) automatially entails an improvement of the controller as a whole. In
contrast to reinforcement learning (RL), MPC uses the PM to predict subsequent
states of the controlled system (CS), instead of directly recommending suitable
actions. To assess how the precision of the PM translates into the quality of
the model-predictive controller, we compare a DNN-based PM to the optimal
baseline PM for three well-known control problems of varying complexity. The
baseline PM achieves perfect accuracy by accessing the simulation of the CS
itself. Based on the obtained results, we argue that an improvement of the PM
will always improve the controller as a whole, without considering the impact
of other components such as action selection (which, in this article, relies on
evolutionary optimization).",http://arxiv.org/pdf/2308.15157v1
2308.15141v1,eess.IV,Uncertainty Aware Training to Improve Deep Learning Model Calibration for Classification of Cardiac MR Images,2023-08-29 09:19:49+00:00,"Quantifying uncertainty of predictions has been identified as one way to
develop more trustworthy artificial intelligence (AI) models beyond
conventional reporting of performance metrics. When considering their role in a
clinical decision support setting, AI classification models should ideally
avoid confident wrong predictions and maximise the confidence of correct
predictions. Models that do this are said to be well-calibrated with regard to
confidence. However, relatively little attention has been paid to how to
improve calibration when training these models, i.e., to make the training
strategy uncertainty-aware. In this work we evaluate three novel
uncertainty-aware training strategies comparing against two state-of-the-art
approaches. We analyse performance on two different clinical applications:
cardiac resynchronisation therapy (CRT) response prediction and coronary artery
disease (CAD) diagnosis from cardiac magnetic resonance (CMR) images. The
best-performing model in terms of both classification accuracy and the most
common calibration measure, expected calibration error (ECE) was the Confidence
Weight method, a novel approach that weights the loss of samples to explicitly
penalise confident incorrect predictions. The method reduced the ECE by 17% for
CRT response prediction and by 22% for CAD diagnosis when compared to a
baseline classifier in which no uncertainty-aware strategy was included. In
both applications, as well as reducing the ECE there was a slight increase in
accuracy from 69% to 70% and 70% to 72% for CRT response prediction and CAD
diagnosis respectively. However, our analysis showed a lack of consistency in
terms of optimal models when using different calibration measures. This
indicates the need for careful consideration of performance metrics when
training and selecting models for complex high-risk applications in healthcare.",http://arxiv.org/pdf/2308.15141v1
2308.15132v1,cs.LG,Biquality Learning: a Framework to Design Algorithms Dealing with Closed-Set Distribution Shifts,2023-08-29 08:57:47+00:00,"Training machine learning models from data with weak supervision and dataset
shifts is still challenging. Designing algorithms when these two situations
arise has not been explored much, and existing algorithms cannot always handle
the most complex distributional shifts. We think the biquality data setup is a
suitable framework for designing such algorithms. Biquality Learning assumes
that two datasets are available at training time: a trusted dataset sampled
from the distribution of interest and the untrusted dataset with dataset shifts
and weaknesses of supervision (aka distribution shifts). The trusted and
untrusted datasets available at training time make designing algorithms dealing
with any distribution shifts possible. We propose two methods, one inspired by
the label noise literature and another by the covariate shift literature for
biquality learning. We experiment with two novel methods to synthetically
introduce concept drift and class-conditional shifts in real-world datasets
across many of them. We opened some discussions and assessed that developing
biquality learning algorithms robust to distributional changes remains an
interesting problem for future research.",http://arxiv.org/pdf/2308.15132v1
2308.15107v1,cs.LG,Stochastic Graph Bandit Learning with Side-Observations,2023-08-29 08:14:19+00:00,"In this paper, we investigate the stochastic contextual bandit with general
function space and graph feedback. We propose an algorithm that addresses this
problem by adapting to both the underlying graph structures and reward gaps. To
the best of our knowledge, our algorithm is the first to provide a
gap-dependent upper bound in this stochastic setting, bridging the research gap
left by the work in [35]. In comparison to [31,33,35], our method offers
improved regret upper bounds and does not require knowledge of graphical
quantities. We conduct numerical experiments to demonstrate the computational
efficiency and effectiveness of our approach in terms of regret upper bounds.
These findings highlight the significance of our algorithm in advancing the
field of stochastic contextual bandits with graph feedback, opening up avenues
for practical applications in various domains.",http://arxiv.org/pdf/2308.15107v1
2308.15096v1,cs.LG,How Faithful are Self-Explainable GNNs?,2023-08-29 08:04:45+00:00,"Self-explainable deep neural networks are a recent class of models that can
output ante-hoc local explanations that are faithful to the model's reasoning,
and as such represent a step forward toward filling the gap between
expressiveness and interpretability. Self-explainable graph neural networks
(GNNs) aim at achieving the same in the context of graph data. This begs the
question: do these models fulfill their implicit guarantees in terms of
faithfulness? In this extended abstract, we analyze the faithfulness of several
self-explainable GNNs using different measures of faithfulness, identify
several limitations -- both in the models themselves and in the evaluation
metrics -- and outline possible ways forward.",http://arxiv.org/pdf/2308.15096v1
2308.15094v1,cs.CV,Group-Conditional Conformal Prediction via Quantile Regression Calibration for Crop and Weed Classification,2023-08-29 08:02:41+00:00,"As deep learning predictive models become an integral part of a large
spectrum of precision agricultural systems, a barrier to the adoption of such
automated solutions is the lack of user trust in these highly complex, opaque
and uncertain models. Indeed, deep neural networks are not equipped with any
explicit guarantees that can be used to certify the system's performance,
especially in highly varying uncontrolled environments such as the ones
typically faced in computer vision for agriculture.Fortunately, certain methods
developed in other communities can prove to be important for agricultural
applications. This article presents the conformal prediction framework that
provides valid statistical guarantees on the predictive performance of any
black box prediction machine, with almost no assumptions, applied to the
problem of deep visual classification of weeds and crops in real-world
conditions. The framework is exposed with a focus on its practical aspects and
special attention accorded to the Adaptive Prediction Sets (APS) approach that
delivers marginal guarantees on the model's coverage. Marginal results are then
shown to be insufficient to guarantee performance on all groups of individuals
in the population as characterized by their environmental and pedo-climatic
auxiliary data gathered during image acquisition.To tackle this shortcoming,
group-conditional conformal approaches are presented: the ''classical'' method
that consists of iteratively applying the APS procedure on all groups, and a
proposed elegant reformulation and implementation of the procedure using
quantile regression on group membership indicators. Empirical results showing
the validity of the proposed approach are presented and compared to the
marginal APS then discussed.",http://arxiv.org/pdf/2308.15094v1
2308.15088v1,eess.IV,Using deep learning for an automatic detection and classification of the vascular bifurcations along the Circle of Willis,2023-08-29 07:51:36+00:00,"Most of the intracranial aneurysms (ICA) occur on a specific portion of the
cerebral vascular tree named the Circle of Willis (CoW). More particularly,
they mainly arise onto fifteen of the major arterial bifurcations constituting
this circular structure. Hence, for an efficient and timely diagnosis it is
critical to develop some methods being able to accurately recognize each
Bifurcation of Interest (BoI). Indeed, an automatic extraction of the
bifurcations presenting the higher risk of developing an ICA would offer the
neuroradiologists a quick glance at the most alarming areas. Due to the recent
efforts on Artificial Intelligence, Deep Learning turned out to be the best
performing technology for many pattern recognition tasks. Moreover, various
methods have been particularly designed for medical image analysis purposes.
This study intends to assist the neuroradiologists to promptly locate any
bifurcation presenting a high risk of ICA occurrence. It can be seen as a
Computer Aided Diagnosis scheme, where the Artificial Intelligence facilitates
the access to the regions of interest within the MRI. In this work, we propose
a method for a fully automatic detection and recognition of the bifurcations of
interest forming the Circle of Willis. Several neural networks architectures
have been tested, and we thoroughly evaluate the bifurcation recognition rate.",http://arxiv.org/pdf/2308.15088v1
2308.15074v1,cs.CV,Exploring Model Transferability through the Lens of Potential Energy,2023-08-29 07:15:57+00:00,"Transfer learning has become crucial in computer vision tasks due to the vast
availability of pre-trained deep learning models. However, selecting the
optimal pre-trained model from a diverse pool for a specific downstream task
remains a challenge. Existing methods for measuring the transferability of
pre-trained models rely on statistical correlations between encoded static
features and task labels, but they overlook the impact of underlying
representation dynamics during fine-tuning, leading to unreliable results,
especially for self-supervised models. In this paper, we present an insightful
physics-inspired approach named PED to address these challenges. We reframe the
challenge of model selection through the lens of potential energy and directly
model the interaction forces that influence fine-tuning dynamics. By capturing
the motion of dynamic representations to decline the potential energy within a
force-driven physical model, we can acquire an enhanced and more stable
observation for estimating transferability. The experimental results on 10
downstream tasks and 12 self-supervised models demonstrate that our approach
can seamlessly integrate into existing ranking techniques and enhance their
performances, revealing its effectiveness for the model selection task and its
potential for understanding the mechanism in transfer learning. Code will be
available at https://github.com/lixiaotong97/PED.",http://arxiv.org/pdf/2308.15074v1
2308.15072v1,cs.LG,Advancing Adversarial Robustness Through Adversarial Logit Update,2023-08-29 07:13:31+00:00,"Deep Neural Networks are susceptible to adversarial perturbations.
Adversarial training and adversarial purification are among the most widely
recognized defense strategies. Although these methods have different underlying
logic, both rely on absolute logit values to generate label predictions. In
this study, we theoretically analyze the logit difference around successful
adversarial attacks from a theoretical point of view and propose a new
principle, namely Adversarial Logit Update (ALU), to infer adversarial sample's
labels. Based on ALU, we introduce a new classification paradigm that utilizes
pre- and post-purification logit differences for model's adversarial robustness
boost. Without requiring adversarial or additional data for model training, our
clean data synthesis model can be easily applied to various pre-trained models
for both adversarial sample detection and ALU-based data classification.
Extensive experiments on both CIFAR-10, CIFAR-100, and tiny-ImageNet datasets
show that even with simple components, the proposed solution achieves superior
robustness performance compared to state-of-the-art methods against a wide
range of adversarial attacks. Our python implementation is submitted in our
Supplementary document and will be published upon the paper's acceptance.",http://arxiv.org/pdf/2308.15072v1
2308.15059v1,cs.LG,OEBench: Investigating Open Environment Challenges in Real-World Relational Data Streams,2023-08-29 06:43:29+00:00,"Relational datasets are widespread in real-world scenarios and are usually
delivered in a streaming fashion. This type of data stream can present unique
challenges, such as distribution drifts, outliers, emerging classes, and
changing features, which have recently been described as open environment
challenges for machine learning. While some work has been done on incremental
learning for data streams, their evaluations are mostly conducted with manually
partitioned datasets. Moreover, while several real-world streaming datasets are
available, it is uncertain whether these open environment challenges are
prevalent and how existing incremental learning algorithms perform on real
datasets. To fill this gap, we develop an Open Environment Benchmark named
OEBench to evaluate open environment challenges in relational data streams.
Specifically, we investigate 55 real-world streaming datasets and establish
that open environment scenarios are indeed widespread in real-world datasets,
which presents significant challenges for stream learning algorithms. Through
benchmarks, we find that increased data quantity may not consistently enhance
the model accuracy when applied in open environment scenarios, where machine
learning models can be significantly compromised by distribution shifts,
anomalies, or untrustworthy data within real-world data streams. The current
techniques are insufficient in effectively mitigating these challenges posed by
open environments. Thus, it is promising to conduct more researches to address
real-world new challenges of open environment scenarios.",http://arxiv.org/pdf/2308.15059v1
2308.15006v1,cs.LG,Exploiting Problem Geometry in Safe Linear Bandits,2023-08-29 03:54:53+00:00,"The safe linear bandit problem is a version of the classic linear bandit
problem where the learner's actions must satisfy an uncertain linear constraint
at all rounds. Due its applicability to many real-world settings, this problem
has received considerable attention in recent years. We find that by exploiting
the geometry of the specific problem setting, we can achieve improved regret
guarantees for both well-separated problem instances and action sets that are
finite star convex sets. Additionally, we propose a novel algorithm for this
setting that chooses problem parameters adaptively and enjoys at least as good
regret guarantees as existing algorithms. Lastly, we introduce a generalization
of the safe linear bandit setting where the constraints are convex and adapt
our algorithms and analyses to this setting by leveraging a novel
convex-analysis based approach. Simulation results show improved performance
over existing algorithms for a variety of randomly sampled settings.",http://arxiv.org/pdf/2308.15006v1
2308.14995v1,cs.CV,WSAM: Visual Explanations from Style Augmentation as Adversarial Attacker and Their Influence in Image Classification,2023-08-29 02:50:36+00:00,"Currently, style augmentation is capturing attention due to convolutional
neural networks (CNN) being strongly biased toward recognizing textures rather
than shapes. Most existing styling methods either perform a low-fidelity style
transfer or a weak style representation in the embedding vector. This paper
outlines a style augmentation algorithm using stochastic-based sampling with
noise addition to improving randomization on a general linear transformation
for style transfer. With our augmentation strategy, all models not only present
incredible robustness against image stylizing but also outperform all previous
methods and surpass the state-of-the-art performance for the STL-10 dataset. In
addition, we present an analysis of the model interpretations under different
style variations. At the same time, we compare comprehensive experiments
demonstrating the performance when applied to deep neural architectures in
training settings.",http://arxiv.org/pdf/2308.14995v1
2308.14981v1,quant-ph,Sub-universal variational circuits for combinatorial optimization problems,2023-08-29 02:16:48+00:00,"Quantum variational circuits have gained significant attention due to their
applications in the quantum approximate optimization algorithm and quantum
machine learning research. This work introduces a novel class of classical
probabilistic circuits designed for generating approximate solutions to
combinatorial optimization problems constructed using two-bit stochastic
matrices. Through a numerical study, we investigate the performance of our
proposed variational circuits in solving the Max-Cut problem on various graphs
of increasing sizes. Our classical algorithm demonstrates improved performance
for several graph types to the quantum approximate optimization algorithm. Our
findings suggest that evaluating the performance of quantum variational
circuits against variational circuits with sub-universal gate sets is a
valuable benchmark for identifying areas where quantum variational circuits can
excel.",http://arxiv.org/pdf/2308.14981v1
2308.14971v1,cs.RO,Distributed multi-agent target search and tracking with Gaussian process and reinforcement learning,2023-08-29 01:53:14+00:00,"Deploying multiple robots for target search and tracking has many practical
applications, yet the challenge of planning over unknown or partially known
targets remains difficult to address. With recent advances in deep learning,
intelligent control techniques such as reinforcement learning have enabled
agents to learn autonomously from environment interactions with little to no
prior knowledge. Such methods can address the exploration-exploitation tradeoff
of planning over unknown targets in a data-driven manner, eliminating the
reliance on heuristics typical of traditional approaches and streamlining the
decision-making pipeline with end-to-end training. In this paper, we propose a
multi-agent reinforcement learning technique with target map building based on
distributed Gaussian process. We leverage the distributed Gaussian process to
encode belief over the target locations and efficiently plan over unknown
targets. We evaluate the performance and transferability of the trained policy
in simulation and demonstrate the method on a swarm of micro unmanned aerial
vehicles with hardware experiments.",http://arxiv.org/pdf/2308.14971v1
2308.14969v1,cs.LG,Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets,2023-08-29 01:47:49+00:00,"In the era of foundation models with huge pre-training budgets, the
downstream tasks have been shifted to the narrative of efficient and fast
adaptation. For classification-based tasks in the domain of computer vision,
the two most efficient approaches have been linear probing (LP) and visual
prompting/reprogramming (VP); the former aims to learn a classifier in the form
of a linear head on the features extracted by the pre-trained model, while the
latter maps the input data to the domain of the source data on which the model
was originally pre-trained on. Although extensive studies have demonstrated the
differences between LP and VP in terms of downstream performance, we explore
the capabilities of the two aforementioned methods via the sparsity axis: (a)
Data sparsity: the impact of few-shot adaptation and (b) Model sparsity: the
impact of lottery tickets (LT). We demonstrate that LT are not universal
reprogrammers, i.e., for certain target datasets, reprogramming an LT yields
significantly lower performance than the reprogrammed dense model although
their corresponding upstream performance is similar. Further, we demonstrate
that the calibration of dense models is always superior to that of their
lottery ticket counterparts under both LP and VP regimes. Our empirical study
opens a new avenue of research into VP for sparse models and encourages further
understanding of the performance beyond the accuracy achieved by VP under
constraints of sparsity. Code and logs can be accessed at
\url{https://github.com/landskape-ai/Reprogram_LT}.",http://arxiv.org/pdf/2308.14969v1
2308.14962v1,cs.LG,Streaming Compression of Scientific Data via weak-SINDy,2023-08-29 01:29:26+00:00,"In this paper a streaming weak-SINDy algorithm is developed specifically for
compressing streaming scientific data. The production of scientific data,
either via simulation or experiments, is undergoing an stage of exponential
growth, which makes data compression important and often necessary for storing
and utilizing large scientific data sets. As opposed to classical ``offline""
compression algorithms that perform compression on a readily available data
set, streaming compression algorithms compress data ``online"" while the data
generated from simulation or experiments is still flowing through the system.
This feature makes streaming compression algorithms well-suited for scientific
data compression, where storing the full data set offline is often infeasible.
This work proposes a new streaming compression algorithm, streaming weak-SINDy,
which takes advantage of the underlying data characteristics during
compression. The streaming weak-SINDy algorithm constructs feature matrices and
target vectors in the online stage via a streaming integration method in a
memory efficient manner. The feature matrices and target vectors are then used
in the offline stage to build a model through a regression process that aims to
recover equations that govern the evolution of the data. For compressing
high-dimensional streaming data, we adopt a streaming proper orthogonal
decomposition (POD) process to reduce the data dimension and then use the
streaming weak-SINDy algorithm to compress the temporal data of the POD
expansion. We propose modifications to the streaming weak-SINDy algorithm to
accommodate the dynamically updated POD basis. By combining the built model
from the streaming weak-SINDy algorithm and a small amount of data samples, the
full data flow could be reconstructed accurately at a low memory cost, as shown
in the numerical tests.",http://arxiv.org/pdf/2308.14962v1
2308.14949v1,cs.LG,Low-bit Quantization for Deep Graph Neural Networks with Smoothness-aware Message Propagation,2023-08-29 00:25:02+00:00,"Graph Neural Network (GNN) training and inference involve significant
challenges of scalability with respect to both model sizes and number of
layers, resulting in degradation of efficiency and accuracy for large and deep
GNNs. We present an end-to-end solution that aims to address these challenges
for efficient GNNs in resource constrained environments while avoiding the
oversmoothing problem in deep GNNs. We introduce a quantization based approach
for all stages of GNNs, from message passing in training to node
classification, compressing the model and enabling efficient processing. The
proposed GNN quantizer learns quantization ranges and reduces the model size
with comparable accuracy even under low-bit quantization. To scale with the
number of layers, we devise a message propagation mechanism in training that
controls layer-wise changes of similarities between neighboring nodes. This
objective is incorporated into a Lagrangian function with constraints and a
differential multiplier method is utilized to iteratively find optimal
embeddings. This mitigates oversmoothing and suppresses the quantization error
to a bound. Significant improvements are demonstrated over state-of-the-art
quantization methods and deep GNN approaches in both full-precision and
quantized models. The proposed quantizer demonstrates superior performance in
INT2 configurations across all stages of GNN, achieving a notable level of
accuracy. In contrast, existing quantization approaches fail to generate
satisfactory accuracy levels. Finally, the inference with INT2 and INT4
representations exhibits a speedup of 5.11 $\times$ and 4.70 $\times$ compared
to full precision counterparts, respectively.",http://arxiv.org/pdf/2308.14949v1
2308.14947v1,cs.RO,Improving Reinforcement Learning Training Regimes for Social Robot Navigation,2023-08-29 00:00:18+00:00,"In order for autonomous mobile robots to navigate in human spaces, they must
abide by our social norms. Reinforcement learning (RL) has emerged as an
effective method to train robot navigation policies that are able to respect
these norms. However, a large portion of existing work in the field conducts
both RL training and testing in simplistic environments. This limits the
generalization potential of these models to unseen environments, and the
meaningfulness of their reported results. We propose a method to improve the
generalization performance of RL social navigation methods using curriculum
learning. By employing multiple environment types and by modeling pedestrians
using multiple dynamics models, we are able to progressively diversify and
escalate difficulty in training. Our results show that the use of curriculum
learning in training can be used to achieve better generalization performance
than previous training methods. We also show that results presented in many
existing state-of-the art RL social navigation works do not evaluate their
methods outside of their training environments, and thus do not reflect their
policies' failure to adequately generalize to out-of-distribution scenarios. In
response, we validate our training approach on larger and more crowded testing
environments than those used in training, allowing for more meaningful
measurements of model performance.",http://arxiv.org/pdf/2308.14947v1
2308.14946v1,cs.LG,Reinforcement Learning for Sampling on Temporal Medical Imaging Sequences,2023-08-28 23:55:23+00:00,"Accelerated magnetic resonance imaging resorts to either Fourier-domain
subsampling or better reconstruction algorithms to deal with fewer measurements
while still generating medical images of high quality. Determining the optimal
sampling strategy given a fixed reconstruction protocol often has combinatorial
complexity. In this work, we apply double deep Q-learning and REINFORCE
algorithms to learn the sampling strategy for dynamic image reconstruction. We
consider the data in the format of time series, and the reconstruction method
is a pre-trained autoencoder-typed neural network. We present a proof of
concept that reinforcement learning algorithms are effective to discover the
optimal sampling pattern which underlies the pre-trained reconstructor network
(i.e., the dynamics in the environment). The code for replicating experiments
can be found at https://github.com/zhishenhuang/RLsamp.",http://arxiv.org/pdf/2308.14946v1
2308.14945v1,stat.ML,Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals,2023-08-28 23:51:33+00:00,"We consider the problem of sampling from a distribution governed by a
potential function. This work proposes an explicit score-based MCMC method that
is deterministic, resulting in a deterministic evolution for particles rather
than a stochastic differential equation evolution. The score term is given in
closed form by a regularized Wasserstein proximal, using a kernel convolution
that is approximated by sampling. We demonstrate fast convergence on various
problems and show improved dimensional dependence of mixing time bounds for the
case of Gaussian distributions compared to the unadjusted Langevin algorithm
(ULA) and the Metropolis-adjusted Langevin algorithm (MALA). We additionally
derive closed form expressions for the distributions at each iterate for
quadratic potential functions, characterizing the variance reduction. Empirical
results demonstrate that the particles behave in an organized manner, lying on
level set contours of the potential. Moreover, the posterior mean estimator of
the proposed method is shown to be closer to the maximum a-posteriori estimator
compared to ULA and MALA, in the context of Bayesian logistic regression.",http://arxiv.org/pdf/2308.14945v1
2308.14938v1,cs.CV,Entropy-based Guidance of Deep Neural Networks for Accelerated Convergence and Improved Performance,2023-08-28 23:33:07+00:00,"Neural networks have dramatically increased our capacity to learn from large,
high-dimensional datasets across innumerable disciplines. However, their
decisions are not easily interpretable, their computational costs are high, and
building and training them are uncertain processes. To add structure to these
efforts, we derive new mathematical results to efficiently measure the changes
in entropy as fully-connected and convolutional neural networks process data,
and introduce entropy-based loss terms. Experiments in image compression and
image classification on benchmark datasets demonstrate these losses guide
neural networks to learn rich latent data representations in fewer dimensions,
converge in fewer training epochs, and achieve better test metrics.",http://arxiv.org/pdf/2308.14938v1
2308.14930v1,cs.CV,Application of Quantum Pre-Processing Filter for Binary Image Classification with Small Samples,2023-08-28 23:08:32+00:00,"Over the past few years, there has been significant interest in Quantum
Machine Learning (QML) among researchers, as it has the potential to transform
the field of machine learning. Several models that exploit the properties of
quantum mechanics have been developed for practical applications. In this
study, we investigated the application of our previously proposed quantum
pre-processing filter (QPF) to binary image classification. We evaluated the
QPF on four datasets: MNIST (handwritten digits), EMNIST (handwritten digits
and alphabets), CIFAR-10 (photographic images) and GTSRB (real-life traffic
sign images). Similar to our previous multi-class classification results, the
application of QPF improved the binary image classification accuracy using
neural network against MNIST, EMNIST, and CIFAR-10 from 98.9% to 99.2%, 97.8%
to 98.3%, and 71.2% to 76.1%, respectively, but degraded it against GTSRB from
93.5% to 92.0%. We then applied QPF in cases using a smaller number of training
and testing samples, i.e. 80 and 20 samples per class, respectively. In order
to derive statistically stable results, we conducted the experiment with 100
trials choosing randomly different training and testing samples and averaging
the results. The result showed that the application of QPF did not improve the
image classification accuracy against MNIST and EMNIST but improved it against
CIFAR-10 and GTSRB from 65.8% to 67.2% and 90.5% to 91.8%, respectively.
Further research will be conducted as part of future work to investigate the
potential of QPF to assess the scalability of the proposed approach to larger
and complex datasets.",http://arxiv.org/pdf/2308.14930v1
2308.14929v1,cs.LG,Maestro: Uncovering Low-Rank Structures via Trainable Decomposition,2023-08-28 23:08:15+00:00,"Deep Neural Networks (DNNs) have been a large driver and enabler for AI
breakthroughs in recent years. These models have been getting larger in their
attempt to become more accurate and tackle new upcoming use-cases, including
AR/VR and intelligent assistants. However, the training process of such large
models is a costly and time-consuming process, which typically yields a single
model to fit all targets. To mitigate this, various techniques have been
proposed in the literature, including pruning, sparsification or quantization
of the model weights and updates. While able to achieve high compression rates,
they often incur computational overheads or accuracy penalties. Alternatively,
factorization methods have been leveraged to incorporate low-rank compression
in the training process. Similarly, such techniques (e.g.,~SVD) frequently rely
on the computationally expensive decomposition of layers and are potentially
sub-optimal for non-linear models, such as DNNs. In this work, we take a
further step in designing efficient low-rank models and propose Maestro, a
framework for trainable low-rank layers. Instead of regularly applying a priori
decompositions such as SVD, the low-rank structure is built into the training
process through a generalized variant of Ordered Dropout. This method imposes
an importance ordering via sampling on the decomposed DNN structure. Our
theoretical analysis demonstrates that our method recovers the SVD
decomposition of linear mapping on uniformly distributed data and PCA for
linear autoencoders. We further apply our technique on DNNs and empirically
illustrate that Maestro enables the extraction of lower footprint models that
preserve model performance while allowing for graceful accuracy-latency
tradeoff for the deployment to devices of different capabilities.",http://arxiv.org/pdf/2308.14929v1
2308.14920v1,cond-mat.mtrl-sci,Matbench Discovery -- An evaluation framework for machine learning crystal stability prediction,2023-08-28 22:29:57+00:00,"Matbench Discovery simulates the deployment of machine learning (ML) energy
models in a high-throughput search for stable inorganic crystals. We address
the disconnect between (i) thermodynamic stability and formation energy and
(ii) in-domain vs out-of-distribution performance. Alongside this paper, we
publish a Python package to aid with future model submissions and a growing
online leaderboard with further insights into trade-offs between various
performance metrics. To answer the question which ML methodology performs best
at materials discovery, our initial release explores a variety of models
including random forests, graph neural networks (GNN), one-shot predictors,
iterative Bayesian optimizers and universal interatomic potentials (UIP).
Ranked best-to-worst by their test set F1 score on thermodynamic stability
prediction, we find CHGNet > M3GNet > MACE > ALIGNN > MEGNet > CGCNN > CGCNN+P
> Wrenformer > BOWSR > Voronoi tessellation fingerprints with random forest.
The top 3 models are UIPs, the winning methodology for ML-guided materials
discovery, achieving F1 scores of ~0.6 for crystal stability classification and
discovery acceleration factors (DAF) of up to 5x on the first 10k most stable
predictions compared to dummy selection from our test set. We also highlight a
sharp disconnect between commonly used global regression metrics and more
task-relevant classification metrics. Accurate regressors are susceptible to
unexpectedly high false-positive rates if those accurate predictions lie close
to the decision boundary at 0 eV/atom above the convex hull where most
materials are. Our results highlight the need to focus on classification
metrics that actually correlate with improved stability hit rate.",http://arxiv.org/pdf/2308.14920v1
2308.14909v1,cs.SD,Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech,2023-08-28 21:25:05+00:00,"For personalized speech generation, a neural text-to-speech (TTS) model must
be successfully implemented with limited data from a target speaker. To this
end, the baseline TTS model needs to be amply generalized to out-of-domain data
(i.e., target speaker's speech). However, approaches to address this
out-of-domain generalization problem in TTS have yet to be thoroughly studied.
In this work, we propose an effective pruning method for a transformer known as
sparse attention, to improve the TTS model's generalization abilities. In
particular, we prune off redundant connections from self-attention layers whose
attention weights are below the threshold. To flexibly determine the pruning
strength for searching optimal degree of generalization, we also propose a new
differentiable pruning method that allows the model to automatically learn the
thresholds. Evaluations on zero-shot multi-speaker TTS verify the effectiveness
of our method in terms of voice quality and speaker similarity.",http://arxiv.org/pdf/2308.14909v1
2308.14906v1,cs.LG,BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition,2023-08-28 21:17:12+00:00,"In real-world scenarios like traffic and energy, massive time-series data
with missing values and noises are widely observed, even sampled irregularly.
While many imputation methods have been proposed, most of them work with a
local horizon, which means models are trained by splitting the long sequence
into batches of fit-sized patches. This local horizon can make models ignore
global trends or periodic patterns. More importantly, almost all methods assume
the observations are sampled at regular time stamps, and fail to handle complex
irregular sampled time series arising from different applications. Thirdly,
most existing methods are learned in an offline manner. Thus, it is not
suitable for many applications with fast-arriving streaming data. To overcome
these limitations, we propose \ours: Bayesian Online Multivariate Time series
Imputation with functional decomposition. We treat the multivariate time series
as the weighted combination of groups of low-rank temporal factors with
different patterns. We apply a group of Gaussian Processes (GPs) with different
kernels as functional priors to fit the factors. For computational efficiency,
we further convert the GPs into a state-space prior by constructing an
equivalent stochastic differential equation (SDE), and developing a scalable
algorithm for online inference. The proposed method can not only handle
imputation over arbitrary time stamps, but also offer uncertainty
quantification and interpretability for the downstream application. We evaluate
our method on both synthetic and real-world datasets.",http://arxiv.org/pdf/2308.14906v1
2308.14904v1,cs.CV,Maturity-Aware Active Learning for Semantic Segmentation with Hierarchically-Adaptive Sample Assessment,2023-08-28 21:13:04+00:00,"Active Learning (AL) for semantic segmentation is challenging due to heavy
class imbalance and different ways of defining ""sample"" (pixels, areas, etc.),
leaving the interpretation of the data distribution ambiguous. We propose
""Maturity-Aware Distribution Breakdown-based Active Learning'' (MADBAL), an AL
method that benefits from a hierarchical approach to define a multiview data
distribution, which takes into account the different ""sample"" definitions
jointly, hence able to select the most impactful segmentation pixels with
comprehensive understanding. MADBAL also features a novel uncertainty
formulation, where AL supporting modules are included to sense the features'
maturity whose weighted influence continuously contributes to the uncertainty
detection. In this way, MADBAL makes significant performance leaps even in the
early AL stage, hence reducing the training burden significantly. It
outperforms state-of-the-art methods on Cityscapes and PASCAL VOC datasets as
verified in our extensive experiments.",http://arxiv.org/pdf/2308.14904v1
2308.14902v1,cs.IR,Ad-Rec: Advanced Feature Interactions to Address Covariate-Shifts in Recommendation Networks,2023-08-28 21:08:06+00:00,"Recommendation models are vital in delivering personalized user experiences
by leveraging the correlation between multiple input features. However, deep
learning-based recommendation models often face challenges due to evolving user
behaviour and item features, leading to covariate shifts. Effective
cross-feature learning is crucial to handle data distribution drift and
adapting to changing user behaviour. Traditional feature interaction techniques
have limitations in achieving optimal performance in this context.
  This work introduces Ad-Rec, an advanced network that leverages feature
interaction techniques to address covariate shifts. This helps eliminate
irrelevant interactions in recommendation tasks. Ad-Rec leverages masked
transformers to enable the learning of higher-order cross-features while
mitigating the impact of data distribution drift. Our approach improves model
quality, accelerates convergence, and reduces training time, as measured by the
Area Under Curve (AUC) metric. We demonstrate the scalability of Ad-Rec and its
ability to achieve superior model quality through comprehensive ablation
studies.",http://arxiv.org/pdf/2308.14902v1
2308.14895v1,cs.LG,Conformal Meta-learners for Predictive Inference of Individual Treatment Effects,2023-08-28 20:32:22+00:00,"We investigate the problem of machine learning-based (ML) predictive
inference on individual treatment effects (ITEs). Previous work has focused
primarily on developing ML-based meta-learners that can provide point estimates
of the conditional average treatment effect (CATE); these are model-agnostic
approaches for combining intermediate nuisance estimates to produce estimates
of CATE. In this paper, we develop conformal meta-learners, a general framework
for issuing predictive intervals for ITEs by applying the standard conformal
prediction (CP) procedure on top of CATE meta-learners. We focus on a broad
class of meta-learners based on two-stage pseudo-outcome regression and develop
a stochastic ordering framework to study their validity. We show that inference
with conformal meta-learners is marginally valid if their (pseudo outcome)
conformity scores stochastically dominate oracle conformity scores evaluated on
the unobserved ITEs. Additionally, we prove that commonly used CATE
meta-learners, such as the doubly-robust learner, satisfy a model- and
distribution-free stochastic (or convex) dominance condition, making their
conformal inferences valid for practically-relevant levels of target coverage.
Whereas existing procedures conduct inference on nuisance parameters (i.e.,
potential outcomes) via weighted CP, conformal meta-learners enable direct
inference on the target parameter (ITE). Numerical experiments show that
conformal meta-learners provide valid intervals with competitive efficiency
while retaining the favorable point estimation properties of CATE
meta-learners.",http://arxiv.org/pdf/2308.14895v1
2308.14845v1,cs.LG,SMOClust: Synthetic Minority Oversampling based on Stream Clustering for Evolving Data Streams,2023-08-28 19:06:03+00:00,"Many real-world data stream applications not only suffer from concept drift
but also class imbalance. Yet, very few existing studies investigated this
joint challenge. Data difficulty factors, which have been shown to be key
challenges in class imbalanced data streams, are not taken into account by
existing approaches when learning class imbalanced data streams. In this work,
we propose a drift adaptable oversampling strategy to synthesise minority class
examples based on stream clustering. The motivation is that stream clustering
methods continuously update themselves to reflect the characteristics of the
current underlying concept, including data difficulty factors. This nature can
potentially be used to compress past information without caching data in the
memory explicitly. Based on the compressed information, synthetic examples can
be created within the region that recently generated new minority class
examples. Experiments with artificial and real-world data streams show that the
proposed approach can handle concept drift involving different minority class
decomposition better than existing approaches, especially when the data stream
is severely class imbalanced and presenting high proportions of safe and
borderline minority class examples.",http://arxiv.org/pdf/2308.14845v1
2308.14838v1,cs.LG,Tackling Diverse Minorities in Imbalanced Classification,2023-08-28 18:48:34+00:00,"Imbalanced datasets are commonly observed in various real-world applications,
presenting significant challenges in training classifiers. When working with
large datasets, the imbalanced issue can be further exacerbated, making it
exceptionally difficult to train classifiers effectively. To address the
problem, over-sampling techniques have been developed to linearly interpolating
data instances between minorities and their neighbors. However, in many
real-world scenarios such as anomaly detection, minority instances are often
dispersed diversely in the feature space rather than clustered together.
Inspired by domain-agnostic data mix-up, we propose generating synthetic
samples iteratively by mixing data samples from both minority and majority
classes. It is non-trivial to develop such a framework, the challenges include
source sample selection, mix-up strategy selection, and the coordination
between the underlying model and mix-up strategies. To tackle these challenges,
we formulate the problem of iterative data mix-up as a Markov decision process
(MDP) that maps data attributes onto an augmentation strategy. To solve the
MDP, we employ an actor-critic framework to adapt the discrete-continuous
decision space. This framework is utilized to train a data augmentation policy
and design a reward signal that explores classifier uncertainty and encourages
performance improvement, irrespective of the classifier's convergence. We
demonstrate the effectiveness of our proposed framework through extensive
experiments conducted on seven publicly available benchmark datasets using
three different types of classifiers. The results of these experiments showcase
the potential and promise of our framework in addressing imbalanced datasets
with diverse minorities.",http://arxiv.org/pdf/2308.14838v1
2308.14831v1,cs.LG,Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates,2023-08-28 18:31:09+00:00,"Continual learning (CL) refers to the ability of an intelligent system to
sequentially acquire and retain knowledge from a stream of data with as little
computational overhead as possible. To this end; regularization, replay,
architecture, and parameter isolation approaches were introduced to the
literature. Parameter isolation using a sparse network which enables to
allocate distinct parts of the neural network to different tasks and also
allows to share of parameters between tasks if they are similar. Dynamic Sparse
Training (DST) is a prominent way to find these sparse networks and isolate
them for each task. This paper is the first empirical study investigating the
effect of different DST components under the CL paradigm to fill a critical
research gap and shed light on the optimal configuration of DST for CL if it
exists. Therefore, we perform a comprehensive study in which we investigate
various DST components to find the best topology per task on well-known
CIFAR100 and miniImageNet benchmarks in a task-incremental CL setup since our
primary focus is to evaluate the performance of various DST criteria, rather
than the process of mask selection. We found that, at a low sparsity level,
Erdos-Renyi Kernel (ERK) initialization utilizes the backbone more efficiently
and allows to effectively learn increments of tasks. At a high sparsity level,
however, uniform initialization demonstrates more reliable and robust
performance. In terms of growth strategy; performance is dependent on the
defined initialization strategy, and the extent of sparsity. Finally,
adaptivity within DST components is a promising way for better continual
learners.",http://arxiv.org/pdf/2308.14831v1
2308.14789v1,hep-th,Scattering with Neural Operators,2023-08-28 18:00:00+00:00,"Recent advances in machine learning establish the ability of certain
neural-network architectures called neural operators to approximate maps
between function spaces. Motivated by a prospect of employing them in
fundamental physics, we examine applications to scattering processes in quantum
mechanics. We use an iterated variant of Fourier neural operators to learn the
physics of Schr\""odinger operators, which map from the space of initial wave
functions and potentials to the final wave functions. These deep operator
learning ideas are put to test in two concrete problems: a neural operator
predicting the time evolution of a wave packet scattering off a central
potential in $1+1$ dimensions, and the double-slit experiment in $2+1$
dimensions. At inference, neural operators can become orders of magnitude more
efficient compared to traditional finite-difference solvers.",http://arxiv.org/pdf/2308.14789v1
2308.14753v1,cs.CV,Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond,2023-08-28 17:59:47+00:00,"Visual similarities discovery (VSD) is an important task with broad
e-commerce applications. Given an image of a certain object, the goal of VSD is
to retrieve images of different objects with high perceptual visual similarity.
Although being a highly addressed problem, the evaluation of proposed methods
for VSD is often based on a proxy of an identification-retrieval task,
evaluating the ability of a model to retrieve different images of the same
object. We posit that evaluating VSD methods based on identification tasks is
limited, and faithful evaluation must rely on expert annotations. In this
paper, we introduce the first large-scale fashion visual similarity benchmark
dataset, consisting of more than 110K expert-annotated image pairs. Besides
this major contribution, we share insight from the challenges we faced while
curating this dataset. Based on these insights, we propose a novel and
efficient labeling procedure that can be applied to any dataset. Our analysis
examines its limitations and inductive biases, and based on these findings, we
propose metrics to mitigate those limitations. Though our primary focus lies on
visual similarity, the methodologies we present have broader applications for
discovering and evaluating perceptual similarity across various domains.",http://arxiv.org/pdf/2308.14753v1
2308.14742v1,math.OC,Minimizing Quasi-Self-Concordant Functions by Gradient Regularization of Newton Method,2023-08-28 17:43:04+00:00,"We study the composite convex optimization problems with a
Quasi-Self-Concordant smooth component. This problem class naturally
interpolates between classic Self-Concordant functions and functions with
Lipschitz continuous Hessian. Previously, the best complexity bounds for this
problem class were associated with trust-region schemes and implementations of
a ball-minimization oracle. In this paper, we show that for minimizing
Quasi-Self-Concordant functions we can use instead the basic Newton Method with
Gradient Regularization. For unconstrained minimization, it only involves a
simple matrix inversion operation (solving a linear system) at each step. We
prove a fast global linear rate for this algorithm, matching the complexity
bound of the trust-region scheme, while our method remains especially simple to
implement. Then, we introduce the Dual Newton Method, and based on it, develop
the corresponding Accelerated Newton Scheme for this problem class, which
further improves the complexity factor of the basic method. As a direct
consequence of our results, we establish fast global linear rates of simple
variants of the Newton Method applied to several practical problems, including
Logistic Regression, Soft Maximum, and Matrix Scaling, without requiring
additional assumptions on strong or uniform convexity for the target objective.",http://arxiv.org/pdf/2308.14742v1
2308.14740v1,cs.CV,Total Selfie: Generating Full-Body Selfies,2023-08-28 17:41:14+00:00,"We present a method to generate full-body selfies -- photos that you take of
yourself, but capturing your whole body as if someone else took the photo of
you from a few feet away. Our approach takes as input a pre-captured video of
your body, a target pose photo, and a selfie + background pair for each
location. We introduce a novel diffusion-based approach to combine all of this
information into high quality, well-composed photos of you with the desired
pose and background.",http://arxiv.org/pdf/2308.14740v1
2308.14705v1,stat.ML,Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning,2023-08-28 16:58:44+00:00,"Ensembling a neural network is a widely recognized approach to enhance model
performance, estimate uncertainty, and improve robustness in deep supervised
learning. However, deep ensembles often come with high computational costs and
memory demands. In addition, the efficiency of a deep ensemble is related to
diversity among the ensemble members which is challenging for large,
over-parameterized deep neural networks. Moreover, ensemble learning has not
yet seen such widespread adoption, and it remains a challenging endeavor for
self-supervised or unsupervised representation learning. Motivated by these
challenges, we present a novel self-supervised training regime that leverages
an ensemble of independent sub-networks, complemented by a new loss function
designed to encourage diversity. Our method efficiently builds a sub-model
ensemble with high diversity, leading to well-calibrated estimates of model
uncertainty, all achieved with minimal computational overhead compared to
traditional deep self-supervised ensembles. To evaluate the effectiveness of
our approach, we conducted extensive experiments across various tasks,
including in-distribution generalization, out-of-distribution detection,
dataset corruption, and semi-supervised settings. The results demonstrate that
our method significantly improves prediction reliability. Our approach not only
achieves excellent accuracy but also enhances calibration, surpassing baseline
performance across a wide range of self-supervised architectures in computer
vision, natural language processing, and genomics data.",http://arxiv.org/pdf/2308.14705v1
2308.14785v1,stat.ML,A correlation-based fuzzy cluster validity index with secondary options detector,2023-08-28 16:40:34+00:00,"The optimal number of clusters is one of the main concerns when applying
cluster analysis. Several cluster validity indexes have been introduced to
address this problem. However, in some situations, there is more than one
option that can be chosen as the final number of clusters. This aspect has been
overlooked by most of the existing works in this area. In this study, we
introduce a correlation-based fuzzy cluster validity index known as the
Wiroonsri-Preedasawakul (WP) index. This index is defined based on the
correlation between the actual distance between a pair of data points and the
distance between adjusted centroids with respect to that pair. We evaluate and
compare the performance of our index with several existing indexes, including
Xie-Beni, Pakhira-Bandyopadhyay-Maulik, Tang, Wu-Li, generalized C, and Kwon2.
We conduct this evaluation on four types of datasets: artificial datasets,
real-world datasets, simulated datasets with ranks, and image datasets, using
the fuzzy c-means algorithm. Overall, the WP index outperforms most, if not
all, of these indexes in terms of accurately detecting the optimal number of
clusters and providing accurate secondary options. Moreover, our index remains
effective even when the fuzziness parameter $m$ is set to a large value. Our R
package called WPfuzzyCVIs used in this work is also available in
https://github.com/nwiroonsri/WPfuzzyCVIs.",http://arxiv.org/pdf/2308.14785v1
2308.15450v1,math.OC,Gauss-Newton oriented greedy algorithms for the reconstruction of operators in nonlinear dynamics,2023-08-29 17:20:59+00:00,"This paper is devoted to the development and convergence analysis of greedy
reconstruction algorithms based on the strategy presented in [Y. Maday and J.
Salomon, Joint Proceedings of the 48th IEEE Conference on Decision and Control
and the 28th Chinese Control Conference, 2009, pp. 375--379]. These procedures
allow the design of a sequence of control functions that ease the
identification of unknown operators in nonlinear dynamical systems. The
original strategy of greedy reconstruction algorithms is based on an
offline/online decomposition of the reconstruction process and an ansatz for
the unknown operator obtained by an a priori chosen set of linearly independent
matrices. In the previous work [S. Buchwald, G. Ciaramella and J. Salomon, SIAM
J. Control Optim., 59(6), pp. 4511-4537], convergence results were obtained in
the case of linear identification problems. We tackle here the more general
case of nonlinear systems. More precisely, we introduce a new greedy algorithm
based on the linearized system. Then, we show that the controls obtained with
this new algorithm lead to the local convergence of the classical Gauss-Newton
method applied to the online nonlinear identification problem. We then extend
this result to the controls obtained on nonlinear systems where a local
convergence result is also proved. The main convergence results are obtained
for the reconstruction of drift operators in dynamical systems with linear and
bilinear control structures.",http://arxiv.org/pdf/2308.15450v1
2308.15409v1,math.NA,An Incremental SVD Method for Non-Fickian Flows in Porous Media: Addressing Storage and Computational Challenges,2023-08-29 16:09:48+00:00,"It is well known that the numerical solution of the Non-Fickian flows at the
current stage depends on all previous time instances. Consequently, the storage
requirement increases linearly, while the computational complexity grows
quadratically with the number of time steps. This presents a significant
challenge for numerical simulations, and to the best of our knowledge, it
remains an unresolved issue. In this paper, we present a memory-free algorithm,
based on the incremental SVD technique, that exhibits only linear growth in
computational complexity as the number of time steps increases. We prove that
the error between the solutions generated by the conventional algorithm and our
innovative approach lies within the scope of machine error. Numerical
experiments are showcased to affirm the accuracy and efficiency gains in terms
of both memory usage and computational expenses.",http://arxiv.org/pdf/2308.15409v1
2308.15400v1,math.NA,A coupled high-accuracy phase-field fluid-structure interaction framework for Stokes fluid-filled fracture surrounded by an elastic medium,2023-08-29 15:57:38+00:00,"In this work, we couple a high-accuracy phase-field fracture reconstruction
approach iteratively to fluid-structure interaction. The key motivation is to
utilize phase-field modelling to compute the fracture path. A mesh
reconstruction allows a switch from interface-capturing to interface-tracking
in which the coupling conditions can be realized in a highly accurate fashion.
Consequently, inside the fracture, a Stokes flow can be modelled that is
coupled to the surrounding elastic medium. A fully coupled approach is obtained
by iterating between the phase-field and the fluid-structure interaction model.
The resulting algorithm is demonstrated for several numerical examples of
quasi-static brittle fractures. We consider both stationary and
quasi-stationary problems. In the latter, the dynamics arise through an
incrementally-increasing given pressure.",http://arxiv.org/pdf/2308.15400v1
2308.15375v1,math.NA,A Reduced-Order Model for Nonlinear Radiative Transfer Problems Based on Moment Equations and POD-Petrov-Galerkin Projection of the Normalized Boltzmann Transport Equation,2023-08-29 15:13:34+00:00,"A data-driven projection-based reduced-order model (ROM) for nonlinear
thermal radiative transfer (TRT) problems is presented. The TRT ROM is
formulated by (i) a hierarchy of low-order quasidiffusion (aka variable
Eddington factor) equations for moments of the radiation intensity and (ii) the
normalized Boltzmann transport equation (BTE). The multilevel system of moment
equations is derived by projection of the BTE onto a sequence of subspaces
which represent elements of the phase space of the problem. Exact closure for
the moment equations is provided by the Eddington tensor. A Petrov-Galerkin
(PG) projection of the normalized BTE is formulated using a proper orthogonal
decomposition (POD) basis representing the normalized radiation intensity over
the whole phase space and time. The Eddington tensor linearly depends on the
solution of the normalized BTE. By linear superposition of the POD basis
functions, a low-rank expansion of the Eddington tensor is constructed with
coefficients defined by the PG projected normalized BTE. The material energy
balance (MEB) equation is coupled with the effective grey low-order equations
which exist on the same dimensional scale as the MEB equation. The resulting
TRT ROM is structure and asymptotic preserving. A detailed analysis of the ROM
is performed on the classical Fleck-Cummings (F-C) TRT multigroup test problem
in 2D geometry. Numerical results are presented to demonstrate the ROM's
effectiveness in the simulation of radiation wave phenomena. The ROM is shown
to produce solutions with sufficiently high accuracy while using low-rank
approximation of the normalized BTE solution. Essential physical
characteristics of supersonic radiation wave are preserved in the ROM
solutions.",http://arxiv.org/pdf/2308.15375v1
2308.15336v1,math.OC,"Second-order methods for quartically-regularised cubic polynomials, with applications to high-order tensor methods",2023-08-29 14:30:36+00:00,"There has been growing interest in high-order tensor methods for nonconvex
optimization, with adaptive regularization, as they possess better/optimal
worst-case evaluation complexity globally and faster convergence
asymptotically. These algorithms crucially rely on repeatedly minimizing
nonconvex multivariate Taylor-based polynomial sub-problems, at least locally.
Finding efficient techniques for the solution of these sub-problems, beyond the
second-order case, has been an open question. This paper proposes a
second-order method, Quadratic Quartic Regularisation (QQR), for efficiently
minimizing nonconvex quartically-regularized cubic polynomials, such as the
AR$p$ sub-problem [3] with $p=3$. Inspired by [35], QQR approximates the
third-order tensor term by a linear combination of quadratic and quartic terms,
yielding (possibly nonconvex) local models that are solvable to global
optimality. In order to achieve accuracy $\epsilon$ in the first-order
criticality of the sub-problem, we show that the error in the QQR method
decreases either linearly or by at least $\mathcal{O}(\epsilon^{4/3})$ for
locally convex iterations, while in the sufficiently nonconvex case, by at
least $\mathcal{O}(\epsilon)$; thus improving, on these types of iterations,
the general cubic-regularization bound. Preliminary numerical experiments
indicate that two QQR variants perform competitively with state-of-the-art
approaches such as ARC (also known as AR$p$ with $p=2$), achieving either a
lower objective value or iteration counts.",http://arxiv.org/pdf/2308.15336v1
2308.15331v1,math.NA,"High-order quasi-Helmholtz Projectors: Definition, Analyses, Algorithms",2023-08-29 14:27:45+00:00,"The accuracy of the electric field integral equation (EFIE) can be
substantially improved using high-order discretizations. However, this equation
suffers from ill-conditioning and deleterious numerical effects in the
low-frequency regime, often jeopardizing its solution. This can be fixed using
quasi-Helmholtz decompositions, in which the source and testing elements are
separated into their solenoidal and non-solenoidal contributions, then rescaled
in order to avoid both the low-frequency conditioning breakdown and the loss of
numerical accuracy. However, standard quasi-Helmholtz decompositions require
handling discretized differential operators that often worsen the
mesh-refinement ill-conditioning and require the finding of the topological
cycles of the geometry, which can be expensive when modeling complex
scatterers, especially in high-order. This paper solves these drawbacks by
presenting the first extension of the quasi-Helmholtz projectors to high-order
discretizations and their application to the stabilization of the EFIE when
discretized with high-order basis functions. Our strategy will not require the
identification of the cycles and will provide constant condition numbers for
decreasing frequencies. Theoretical considerations will be accompanied by
numerical results showing the effectiveness of our method in complex scenarios.",http://arxiv.org/pdf/2308.15331v1
2308.15325v1,math.NA,Adaptivity in Local Kernel Based Methods for Approximating the Action of Linear Operators,2023-08-29 14:21:42+00:00,"Building on the successes of local kernel methods for approximating the
solutions to partial differential equations (PDE) and the evaluation of
definite integrals (quadrature/cubature), a local estimate of the error in such
approximations is developed. This estimate is useful for determining locations
in the solution domain where increased node density (equivalently, reduction in
the spacing between nodes) can decrease the error in the solution. An adaptive
procedure for adding nodes to the domain for both the approximation of
derivatives and the approximate evaluation of definite integrals is described.
This method efficiently computes the error estimate at a set of prescribed
points and adds new nodes for approximation where the error is too large.
Computational experiments demonstrate close agreement between the error
estimate and actual absolute error in the approximation. Such methods are
necessary or desirable when approximating solutions to PDE (or in the case of
quadrature/cubature), where the initial data and subsequent solution (or
integrand) exhibit localized features that require significant refinement to
resolve and where uniform increases in the density of nodes across the entire
computational domain is not possible or too burdensome.",http://arxiv.org/pdf/2308.15325v1
2308.15314v1,math.NA,Linearly convergent nonoverlapping domain decomposition methods for quasilinear parabolic equations,2023-08-29 14:01:08+00:00,"We prove linear convergence for a new family of modified Dirichlet--Neumann
methods applied to quasilinear parabolic equations, as well as the convergence
of the Robin--Robin method. Such nonoverlapping domain decomposition methods
are commonly employed for the parallelization of partial differential equation
solvers. Convergence has been extensively studied for elliptic equations, but
in the case of parabolic equations there are hardly any convergence results
that are not relying on strong regularity assumptions. Hence, we construct a
new framework for analyzing domain decomposition methods applied to quasilinear
parabolic problems, based on fractional time derivatives and time-dependent
Steklov--Poincar\'e operators. The convergence analysis is conducted without
assuming restrictive regularity assumptions on the solutions or the numerical
iterates. We also prove that these continuous convergence results extend to the
discrete case obtained when combining domain decompositions with space-time
finite elements.",http://arxiv.org/pdf/2308.15314v1
2308.15307v1,math.NA,Compositional maps for registration in complex geometries,2023-08-29 13:47:06+00:00,"We develop and analyze a parametric registration procedure for manifolds
associated with the solutions to parametric partial differential equations in
two-dimensional domains. Given the domain $\Omega \subset \mathbb{R}^2$ and the
manifold $M=\{ u_{\mu} : \mu\in P\}$ associated with the parameter domain $P
\subset \mathbb{R}^P$ and the parametric field $\mu\mapsto u_{\mu} \in
L^2(\Omega)$, our approach takes as input a set of snapshots from $M$ and
returns a parameter-dependent mapping $\Phi: \Omega \times P \to \Omega$, which
tracks coherent features (e.g., shocks, shear layers) of the solution field and
ultimately simplifies the task of model reduction. We consider mappings of the
form $\Phi=\texttt{N}(\mathbf{a})$ where $\texttt{N}:\mathbb{R}^M \to {\rm
Lip}(\Omega; \mathbb{R}^2)$ is a suitable linear or nonlinear operator; then,
we state the registration problem as an unconstrained optimization statement
for the coefficients $\mathbf{a}$. We identify minimal requirements for the
operator $\texttt{N}$ to ensure the satisfaction of the bijectivity constraint;
we propose a class of compositional maps that satisfy the desired requirements
and enable non-trivial deformations over curved boundaries of $\Omega$; we
develop a thorough analysis of the proposed ansatz for polytopal domains and we
discuss the approximation properties for general curved domains. We perform
numerical experiments for a parametric inviscid transonic compressible flow
past a cascade of turbine blades to illustrate the many features of the method.",http://arxiv.org/pdf/2308.15307v1
2308.15217v1,math.NA,Computational analysis for competition flows in arteriovenous fistulas based on non-contrast magnetic resonance imaging,2023-08-29 11:15:12+00:00,"Introduction: Characteristics of hemodynamics strongly affect the patency of
arteriovenous fistula (AVF) in hemodialysis patients. Because of pressure
balance changes among arteries after AVF construction, regurgitating flow
occurs in some patients.
  Methods: Based on phase-contrast MRI measurements, flow types around the
anastomotic site are classified to the three different types of splitting,
merging, and one-way, where merging type incorporates regurgitating flow. We
have performed computational simulations to analyze characteristic differences
among these types.
  Results: In the merging type, a characteristic spiral flow is observed in AVF
causing strong wall shear stress and large pressure drop, whereas the splitting
type shows a smooth flow and gives a smaller pressure drop. The one-way case is
intermediate between splitting and merging types.
  Conclusion: Regurgitation brings about high wall shear stress near the
anastomotic site because of instabilities induced by merging phenomena, for
which type careful follow-up examinations are regarded as necessary.",http://arxiv.org/pdf/2308.15217v1
2308.15201v1,math.NA,On Hermitian interpolation of first order data with locally generated C1-splines over triangular meshes,2023-08-29 10:39:16+00:00,"Given a system of triangles in the plane $\mathbb{R}^2$ along with given data
of function and gradient values at the vertices, we describe the general
pattern of local linear methods invoving only four smooth standard shape
functions which results in a spline function fitting the given value and
gradient data value with ${\cal C}^1$-coupling along the edges of the
triangles. We characterize their invariance properties with relavance for the
construction of interpolation surfaces over triangularizations of scanned 3D
data. %The described procedures are local linear and affine invariant. The
numerically simplest procedures among them leaving invarant all polynomials of
2-variables with degree 0 resp 1 involve only polynomials of 5-th resp. 6-th
degree, but the characteizations give rise to a huge variety of procedures with
non-polynomial shape functions.",http://arxiv.org/pdf/2308.15201v1
2308.15195v1,math.NA,MCMS-RBM: Multi-Component Multi-State Reduced Basis Method toward Efficient Transition Pathway Identification for Crystals and Quasicrystals,2023-08-29 10:22:46+00:00,"Due to quasicrystals having long-range orientational order but without
translational symmetry, traditional numerical methods usually suffer when
applied as is. In the past decade, the projection method has emerged as a
prominent solver for quasiperiodic problems. Transforming them into a higher
dimensional but periodic ones, the projection method facilitates the
application of the fast Fourier transform. However, the computational
complexity inevitably becomes high which significantly impedes e.g. the
generation of the phase diagram since a high-fidelity simulation of a problem
whose dimension is doubled must be performed for numerous times.
  To address the computational challenge of quasiperiodic problems based on the
projection method, this paper proposes a multi-component multi-state reduced
basis method (MCMS-RBM). Featuring multiple components with each providing
reduction functionality for one branch of the problem induced by one part of
the parameter domain, the MCMS-RBM does not resort to the parameter domain
configurations (e.g. phase diagrams) a priori. It enriches each component in a
greedy fashion via a phase-transition guided exploration of the multiple states
inherent to the problem. Adopting the empirical interpolation method, the
resulting online-efficient method vastly accelerates the generation of a
delicate phase diagram to a matter of minutes for a parametrized two-turn-four
dimensional Lifshitz-Petrich model with two length scales. Moreover, it
furnishes surrogate and equally accurate field variables anywhere in the
parameter domain.",http://arxiv.org/pdf/2308.15195v1
2308.15182v1,math.NA,Stabilised finite element method for Stokes problem with nonlinear slip condition,2023-08-29 10:03:25+00:00,"This work introduces a stabilised finite element formulation for the Stokes
flow problem with a nonlinear slip boundary condition of friction type. The
boundary condition is enforced with the help of an additional Lagrange
multiplier and the stabilised formulation is based on simultaneously
stabilising both the pressure and the Lagrange multiplier. We establish the
stability and the a priori error analyses, and perform a numerical convergence
study in order to verify the theory.",http://arxiv.org/pdf/2308.15182v1
2308.15145v1,math.OC,Limited memory gradient methods for unconstrained optimization,2023-08-29 09:23:25+00:00,"The limited memory steepest descent method (Fletcher, 2012) for unconstrained
optimization problems stores a few past gradients to compute multiple stepsizes
at once. We review this method and propose new variants. For strictly convex
quadratic objective functions, we study the numerical behavior of different
techniques to compute new stepsizes. In particular, we introduce a method to
improve the use of harmonic Ritz values. We also show the existence of a secant
condition associated with LMSD, where the approximating Hessian is projected
onto a low-dimensional space. In the general nonlinear case, we propose two new
alternatives to Fletcher's method: first, the addition of symmetry constraints
to the secant condition valid for the quadratic case; second, a perturbation of
the last differences between consecutive gradients, to satisfy multiple secant
equations simultaneously. We show that Fletcher's method can also be
interpreted from this viewpoint.",http://arxiv.org/pdf/2308.15145v1
2308.15106v1,math.NA,On factorization of rank-one auto-correlation matrix polynomials,2023-08-29 08:14:11+00:00,"This article characterizes the rank-one factorization of auto-correlation
matrix polynomials. We establish a sufficient and necessary uniqueness
condition for uniqueness of the factorization based on the greatest common
divisor (GCD) of multiple polynomials. In the unique case, we show that the
factorization can be carried out explicitly using GCDs. In the non-unique case,
the number of non-trivially different factorizations is given and all solutions
are enumerated.",http://arxiv.org/pdf/2308.15106v1
2308.15089v1,math.NA,Optimal error bounds on time-splitting methods for the nonlinear Schrödinger equation with low regularity potential and nonlinearity,2023-08-29 07:53:05+00:00,"We establish optimal error bounds on time-splitting methods for the nonlinear
Schr\""odinger equation with low regularity potential and typical power-type
nonlinearity $ f(\rho) = \rho^\sigma $, where $ \rho:=|\psi|^2 $ is the density
with $ \psi $ the wave function and $ \sigma > 0 $ the exponent of the
nonlinearity. For the first-order Lie-Trotter time-splitting method, optimal $
L^2 $-norm error bound is proved for $L^\infty$-potential and $ \sigma > 0 $,
and optimal $H^1$-norm error bound is obtained for $ W^{1, 4} $-potential and $
\sigma \geq 1/2 $. For the second-order Strang time-splitting method, optimal $
L^2 $-norm error bound is established for $H^2$-potential and $ \sigma \geq 1
$, and optimal $H^1$-norm error bound is proved for $H^3$-potential and $
\sigma \geq 3/2 $. Compared to those error estimates of time-splitting methods
in the literature, our optimal error bounds either improve the convergence
rates under the same regularity assumptions or significantly relax the
regularity requirements on potential and nonlinearity for optimal convergence
orders. A key ingredient in our proof is to adopt a new technique called
\textit{regularity compensation oscillation} (RCO), where low frequency modes
are analyzed by phase cancellation, and high frequency modes are estimated by
regularity of the solution. Extensive numerical results are reported to confirm
our error estimates and to demonstrate that they are sharp.",http://arxiv.org/pdf/2308.15089v1
2308.15083v1,math.AP,Hyperbolicity of a semi-Lagrangian formulation of the hydrostatic free-surface Euler system,2023-08-29 07:38:41+00:00,"By a semi-Lagrangian change of coordinates, the hydrostatic Euler equations
describing free-surface sheared flows is rewritten as a system of quasilinear
equations, where stability conditions can be determined by the analysis of its
hyperbolic structure. This new system can be written as a quasi linear system
in time and horizontal variables and involves no more vertical derivatives.
However, the coefficients in front of the horizontal derivatives include an
integral operator acting on the new vertical variable. The spectrum of these
operators is studied in detail, in particular it includes a continuous part.
Riemann invariants are then determined as conserved quantities along the
characteristic curves. Examples of solutions are provided, in particular
stationary solutions and solutions blowing-up in finite time. Eventually, we
propose an exact multi-layer $\mathbb{P}_0$-discretization, which could be used
to solve numerically this semi-Lagrangian system, and analyze the eigenvalues
of the corresponding discretized operator to investigate the hyperbolic nature
of the approximated system.",http://arxiv.org/pdf/2308.15083v1
2308.15054v1,math.OC,A Geometric Algorithm for Maximizing the Distance over an Intersection of Balls to a Given Point,2023-08-29 06:30:31+00:00,"In this paper the authors propose a polynomial algorithm which allows the
computation of the farthest in an intersection of balls to a given point under
three additional hypothesis: the farthest is unique, the distance to it is
known and its magnitude is known. As a use case the authors analyze the subset
sum problem SSP(S,T) for a given $S\in \mathbb{R}^n$ and $T \in \mathbb{R}$.
The proposed approach is to write the SSP as a distance maximization over an
intersection of balls. It was shown that the SSP has a solution if and only if
the maximum value of the distance has a predefined value. This together with
the fact that a solution is a corner of the unit hypercube, allows the authors
to apply the proposed geometry results to find a solution to the SSP under the
hypothesis that is unique.",http://arxiv.org/pdf/2308.15054v1
2308.15041v1,math.NA,Optimization via conformal Hamiltonian systems on manifolds,2023-08-29 05:50:26+00:00,"In this work we propose a method to perform optimization on manifolds. We
assume to have an objective function $f$ defined on a manifold and think of it
as the potential energy of a mechanical system. By adding a momentum-dependent
kinetic energy we define its Hamiltonian function, which allows us to write the
corresponding Hamiltonian system. We make it conformal by introducing a
dissipation term: the result is the continuous model of our scheme. We solve it
via splitting methods (Lie-Trotter and leapfrog): we combine the RATTLE scheme,
approximating the conserved flow, with the exact dissipated flow. The result is
a conformal symplectic method for constant stepsizes. We also propose an
adaptive stepsize version of it. We test it on an example, the minimization of
a function defined on a sphere, and compare it with the usual gradient descent
method.",http://arxiv.org/pdf/2308.15041v1
2308.14958v1,math.NA,Robust topology optimisation of lattice structures with spatially correlated uncertainties,2023-08-29 01:16:42+00:00,"The uncertainties in material and other properties of structures are usually
spatially correlated. We introduce an efficient technique for representing and
processing spatially correlated random fields in robust topology optimisation
of lattice structures. Robust optimisation considers the statistics of the
structural response to obtain a design whose performance is less sensitive to
the specific realisation of the random field. We represent Gaussian random
fields on lattices by leveraging the established link between random fields and
stochastic partial differential equations (SPDEs). It is known that the
precision matrix, i.e. the inverse of the covariance matrix, of a random field
with Mat\'ern covariance is equal to the finite element stiffness matrix of a
possibly fractional PDE with a second-order elliptic operator. We consider the
discretisation of the PDE on the lattice to obtain a random field which, by
design, considers its geometry and connectivity. The so-obtained random field
can be interpreted as a physics-informed prior by the hypothesis that the
elliptic SPDE models the physical processes occurring during manufacturing,
like heat and mass diffusion. Although the proposed approach is general, we
demonstrate its application to lattices modelled as pin-jointed trusses with
uncertainties in member Young's moduli. We consider as a cost function the
weighted sum of the expectation and standard deviation of the structural
compliance. To compute the expectation and standard deviation and their
gradients with respect to member cross-sections we use a first-order Taylor
series approximation. The cost function and its gradient are computed using
only sparse matrix operations. We demonstrate the efficiency of the proposed
approach using several lattice examples with isotropic, anisotropic and
non-stationary random fields and up to eighty thousand random and optimisation
variables.",http://arxiv.org/pdf/2308.14958v1
2308.14933v1,math.NA,A hybridizable discontinuous Galerkin method for the dual-porosity-Stokes problem,2023-08-28 23:14:57+00:00,"We introduce and analyze a hybridizable discontinuous Galerkin (HDG) method
for the dual-porosity-Stokes problem. This coupled problem describes the
interaction between free flow in macrofractures/conduits, governed by the
Stokes equations, and flow in microfractures/matrix, governed by a
dual-porosity model. We prove that the HDG method is strongly conservative,
well-posed, and give an a priori error analysis showing dependence on the
problem parameters. Our theoretical findings are corroborated by numerical
examples",http://arxiv.org/pdf/2308.14933v1
2308.14872v1,math.NA,Consistency and convergence of flux-corrected finite element methods for nonlinear hyperbolic problems,2023-08-28 19:49:28+00:00,"We investigate the consistency and convergence of flux-corrected finite
element approximations in the context of nonlinear hyperbolic conservation
laws. In particular, we focus on a monolithic convex limiting approach and
prove a Lax--Wendroff-type theorem for the corresponding semi-discrete problem.
A key component of our analysis is the use of a weak estimate on bounded
variation, which follows from the semi-discrete entropy stability property of
the method under investigation. For the Euler equations of gas dynamics, we
prove the weak convergence of the flux-corrected finite element scheme to a
dissipative weak solution. If a strong solution exists, the sequence of
numerical approximations converges strongly to the strong solution.",http://arxiv.org/pdf/2308.14872v1
2308.14639v1,math.NA,A Rational Krylov Subspace Method for the Computation of the Matrix Exponential Operator,2023-08-28 15:09:13+00:00,"The computation of approximating e^tA B, where A is a large sparse matrix and
B is a rectangular matrix, serves as a crucial element in numerous scientific
and engineering calculations. A powerful way to consider this problem is to use
Krylov subspace methods. The purpose of this work is to approximate the matrix
exponential and some Cauchy-Stieltjes functions on a block vectors B of R^n*p
using a rational block Lanczos algorithm. We also derive some error estimates
and error bound for the convergence of the rational approximation and finally
numerical results attest to the computational efficiency of the proposed
method.",http://arxiv.org/pdf/2308.14639v1
2308.14573v1,math.NA,Linearizing Anhysteretic Magnetization Curves: A Novel Algorithm for Finding Simulation Parameters and Magnetic Moments,2023-08-28 13:37:43+00:00,"This paper proposes a new method for determining the simulation parameters of
the Jiles-Atherton Model used to simulate the first magnetization curve and
hysteresis loop in ferromagnetic materials. The Jiles-Atherton Model is an
important tool in engineering applications due to its relatively simple
differential formulation. However, determining the simulation parameters for
the anhysteretic curve is challenging. Several methods have been proposed,
primarily based on mathematical aspects of the anhysteretic and first
magnetization curves and hysteresis loops. This paper focuses on finding the
magnetic moments of the material, which are used to define the simulation
parameters for its anhysteretic curve. The proposed method involves using the
susceptibility of the material and a linear approximation of a paramagnet to
find the magnetic moments. The simulation parameters can then be found based on
the magnetic moments. The method is validated theoretically and experimentally
and offers a more physical approach to finding simulation parameters for the
anhysteretic curve and a simplified way of determining the magnetic moments of
the material.",http://arxiv.org/pdf/2308.14573v1
2308.14537v1,math.NA,Solving parametric elliptic interface problems via interfaced operator network,2023-08-28 12:49:08+00:00,"Learning operator mapping between infinite-dimensional Banach spaces via
neural networks has attracted a considerable amount of attention in recent
years. In this work, we propose an interfaced operator network (IONet) to solve
parametric elliptic interface PDEs, where different coefficients, source terms
and boundary conditions are considered as input features. To capture the
discontinuities of both input functions and output solutions across the
interface, IONet divides the entire domain into several separate sub-domains
according to the interface, and leverages multiple branch networks and truck
networks. Each branch network extracts latent representations of input
functions at a fixed number of sensors on a specific sub-domain, and each truck
network is responsible for output solutions on one sub-domain. In addition,
tailored physics-informed loss of IONet is proposed to ensure physical
consistency, which greatly reduces the requirement for training datasets and
makes IONet effective without any paired input-output observations in the
interior of the computational domain. Extensive numerical studies show that
IONet outperforms existing state-of-the-art deep operator networks in terms of
accuracy, efficiency, and versatility.",http://arxiv.org/pdf/2308.14537v1
2308.14490v1,math.NA,Efficient least squares approximation and collocation methods using radial basis functions,2023-08-28 11:07:38+00:00,"We describe an efficient method for the approximation of functions using
radial basis functions (RBFs), and extend this to a solver for boundary value
problems on irregular domains. The method is based on RBFs with centers on a
regular grid defined on a bounding box, with some of the centers outside the
computational domain. The equation is discretized using collocation with
oversampling, with collocation points inside the domain only, resulting in a
rectangular linear system to be solved in a least squares sense. The goal of
this paper is the efficient solution of that rectangular system. We show that
the least squares problem splits into a regular part, which can be expedited
with the FFT, and a low rank perturbation, which is treated separately with a
direct solver. The rank of the perturbation is influenced by the irregular
shape of the domain and by the weak enforcement of boundary conditions at
points along the boundary. The solver extends the AZ algorithm which was
previously proposed for function approximation involving frames and other
overcomplete sets. The solver has near optimal log-linear complexity for
univariate problems, and loses optimality for higher-dimensional problems but
remains faster than a direct solver.",http://arxiv.org/pdf/2308.14490v1
2308.14487v1,math.NA,Deep multi-step mixed algorithm for high dimensional non-linear PDEs and associated BSDEs,2023-08-28 11:00:32+00:00,"We propose a new multistep deep learning-based algorithm for the resolution
of moderate to high dimensional nonlinear backward stochastic differential
equations (BSDEs) and their corresponding parabolic partial differential
equations (PDE). Our algorithm relies on the iterated time discretisation of
the BSDE and approximates its solution and gradient using deep neural networks
and automatic differentiation at each time step. The approximations are
obtained by sequential minimisation of local quadratic loss functions at each
time step through stochastic gradient descent. We provide an analysis of
approximation error in the case of a network architecture with weight
constraints requiring only low regularity conditions on the generator of the
BSDE. The algorithm increases accuracy from its single step parent model and
has reduced complexity when compared to similar models in the literature.",http://arxiv.org/pdf/2308.14487v1
2308.14479v1,math.NA,A convergent interacting particle method for computing KPP front speeds in random flows,2023-08-28 10:38:31+00:00,"We aim to efficiently compute spreading speeds of
reaction-diffusion-advection (RDA) fronts in divergence free random flows under
the Kolmogorov-Petrovsky-Piskunov (KPP) nonlinearity. We study a stochastic
interacting particle method (IPM) for the reduced principal eigenvalue
(Lyapunov exponent) problem of an associated linear advection-diffusion
operator with spatially random coefficients. The Fourier representation of the
random advection field and the Feynman-Kac (FK) formula of the principal
eigenvalue (Lyapunov exponent) form the foundation of our method implemented as
a genetic evolution algorithm. The particles undergo advection-diffusion, and
mutation/selection through a fitness function originated in the FK semigroup.
We analyze convergence of the algorithm based on operator splitting, present
numerical results on representative flows such as 2D cellular flow and 3D
Arnold-Beltrami-Childress (ABC) flow under random perturbations. The 2D
examples serve as a consistency check with semi-Lagrangian computation. The 3D
results demonstrate that IPM, being mesh free and self-adaptive, is simple to
implement and efficient for computing front spreading speeds in the
advection-dominated regime for high-dimensional random flows on unbounded
domains where no truncation is needed.",http://arxiv.org/pdf/2308.14479v1
2308.14431v1,math.NA,Two-Scale Finite Element Approximation of a Homogenized Plate Model,2023-08-28 09:09:34+00:00,"This paper studies the discretization of a homogenization and dimension
reduction model for the elastic deformation of microstructured thin plates
proposed by Hornung, Neukamm, and Vel\v{c}i\'c in 2014. Thereby, a nonlinear
bending energy is based on a homogenized quadratic form which acts on the
second fundamental form associated with the elastic deformation. Convergence is
proven for a multi-affine finite element discretization of the involved
three-dimensional microscopic cell problems and a discrete Kirchhoff triangle
discretization of the two-dimensional isometry-constrained macroscopic problem.
Finally, the convergence properties are numerically verified in selected test
cases and qualitatively compared with deformation experiments for
microstructured sheets of paper.",http://arxiv.org/pdf/2308.14431v1
2308.14405v1,math.NA,Simulation of Permittivity and Conductivity Graded Materials for HVDC GIL for Different Voltage Forms,2023-08-28 08:40:21+00:00,"Functionally graded materials (FGM) are applied in HVDC gas insulated lines
(GIL) to control the electric field within the DC insulation system. In HVDC
GIL, FGM with a spatial distribution of the electric conductivity
(conductivity-FGM) is applied to control the electric field under DC steady
state condition. However, besides DC steady state, different DC conditions
occur, e.g. DC-on process, polarity reversal and lightning impulse. Under these
conditions conductivity-FGM is not sufficient to control the electric field,
since these conditions result in transient capacitive fields, where the
permittivity is decisive for the electric field. In this paper, we suggest
combining conductivity-FGM and a spatial distribution of permittivity
(permittivity-FGM) in the spacer material to control the electric field around
DC-GIL spacer for various DC-conditions, considering nonlinear material models
for the insulating gas and the epoxy spacer. A variation of the spatial
distribution of permittivity and conductivity in the spacer is investigated in
this paper for an effective field reduction. The results show a reduction of
the electric field intensity up to 65.8 %, when conductivity/permittivity-FGM
is applied.",http://arxiv.org/pdf/2308.14405v1
2308.14222v1,math.NA,Accurate complex Jacobi rotations,2023-08-27 22:46:18+00:00,"This note shows how to compute, to high relative accuracy under mild
assumptions, complex Jacobi rotations for diagonalization of Hermitian matrices
of order two, using the correctly rounded functions $\mathtt{cr\_hypot}$ and
$\mathtt{cr\_rsqrt}$, proposed for standardization in the C programming
language as recommended by the IEEE-754 floating-point standard. The rounding
to nearest (ties to even) and the non-stop arithmetic are assumed. The
numerical examples compare the observed with theoretical bounds on the relative
errors in the rotations' elements, and show that the maximal observed departure
of the rotations' determinants from unity is smaller than that of the
transformations computed by LAPACK.",http://arxiv.org/pdf/2308.14222v1
2308.14188v1,math.NA,Bayesian deep operator learning for homogenized to fine-scale maps for multiscale PDE,2023-08-27 19:36:53+00:00,"We present a new framework for computing fine-scale solutions of multiscale
Partial Differential Equations (PDEs) using operator learning tools. Obtaining
fine-scale solutions of multiscale PDEs can be challenging, but there are many
inexpensive computational methods for obtaining coarse-scale solutions.
Additionally, in many real-world applications, fine-scale solutions can only be
observed at a limited number of locations. In order to obtain approximations or
predictions of fine-scale solutions over general regions of interest, we
propose to learn the operator mapping from coarse-scale solutions to fine-scale
solutions using a limited number (and possibly noisy) observations of the
fine-scale solutions. The approach is to train multi-fidelity homogenization
maps using mathematically motivated neural operators. The operator learning
framework can efficiently obtain the solution of multiscale PDEs at any
arbitrary point, making our proposed framework a mesh-free solver. We verify
our results on multiple numerical examples showing that our approach is an
efficient mesh-free solver for multiscale PDEs.",http://arxiv.org/pdf/2308.14188v1
2308.14143v1,math.OC,Ensemble-localized Kernel Density Estimation with Applications to the Ensemble Gaussian Mixture Filter,2023-08-27 15:54:17+00:00,"The ensemble Gaussian mixture filter (EnGMF) is a powerful filter for highly
non-Gaussian and non-linear models that has practical utility in the case of a
small number of samples, and theoretical convergence to full Bayesian inference
in the ensemble limit. We aim to increase the utility of the EnGMF by
introducing an ensemble-local notion of covariance into the kernel density
estimation (KDE) step for the prior distribution. We prove that in the Gaussian
case, our new ensemble-localized KDE technique is exactly the same as more
traditional KDE techniques. We also show an example of a non-Gaussian
distribution that can fail to be approximated by canonical KDE methods, but can
be approximated exactly by our new KDE technique. We showcase our new KDE
technique on a simple bivariate problem, showing that it has nice qualitative
and quantitative properties, and significantly improves the estimate of the
prior and posterior distributions for all ensemble sizes tested. We
additionally show the utility of the proposed methodology for sequential
filtering for the Lorenz '63 equations, achieving a significant reduction in
error, and less conservative behavior in the uncertainty estimate with respect
to traditional techniques.",http://arxiv.org/pdf/2308.14143v1
2308.14127v1,math.NA,Information geometric regularization of the barotropic Euler equation,2023-08-27 15:03:32+00:00,"A key numerical difficulty in compressible fluid dynamics is the formation of
shock waves. Shock waves feature jump discontinuities in the velocity and
density of the fluid and thus preclude the existence of classical solutions to
the compressible Euler equations. Weak ""entropy"" solutions are commonly defined
by viscous regularization, but even small amounts of viscosity can
substantially change the long-term behavior of the solution. In this work, we
propose an inviscid regularization based on ideas from semidefinite programming
and information geometry. From a Lagrangian perspective, shock formation in
entropy solutions amounts to inelastic collisions of fluid particles. Their
trajectories are akin to that of projected gradient descent on a feasible set
of nonintersecting paths. We regularize these trajectories by replacing them
with solution paths of interior point methods based on log determinantal
barrier functions. These paths are geodesic curves with respect to the
information geometry induced by the barrier function. Thus, our regularization
amounts to replacing the Euclidean geometry of phase space with a suitable
information geometry. We extend this idea to infinite families of paths by
viewing Euler's equations as a dynamical system on a diffeomorphism manifold.
Our regularization embeds this manifold into an information geometric ambient
space, equipping it with a geodesically complete geometry. Expressing the
resulting Lagrangian equations in Eulerian form, we derive a regularized Euler
equation in conservation form. Numerical experiments on one and two-dimensional
problems show its promise as a numerical tool.",http://arxiv.org/pdf/2308.14127v1
2308.14080v1,math.OC,The Global R-linear Convergence of Nesterov's Accelerated Gradient Method with Unknown Strongly Convex Parameter,2023-08-27 11:56:20+00:00,"The Nesterov accelerated gradient (NAG) method is an important
extrapolation-based numerical algorithm that accelerates the convergence of the
gradient descent method in convex optimization. When dealing with an objective
function that is $\mu$-strongly convex, selecting extrapolation coefficients
dependent on $\mu$ enables global R-linear convergence. In cases $\mu$ is
unknown, a commonly adopted approach is to set the extrapolation coefficient
using the original NAG method, referred to as NAG-c. This choice allows for
achieving the optimal iteration complexity among first-order methods for
general convex problems. However, it remains an open question whether the NAG-c
method exhibits global R-linear convergence for strongly convex problems. In
this work, we answer this question positively by establishing the Q-linear
convergence of certain constructed Lyapunov sequences. Furthermore, we extend
our result to the global R-linear convergence of the accelerated proximal
gradient method, which is employed for solving strongly convex composite
optimization problems with nonsmooth terms in the objective function.
Interestingly, these results contradict the findings of the continuous
counterpart of the NAG-c method in [Su, Boyd, and Cand\'es, J. Mach. Learn.
Res., 2016, 17(153), 1-43], where the convergence rate by the suggested
ordinary differential equation cannot exceed $O(1/{\tt poly}(k))$ for strongly
convex functions.",http://arxiv.org/pdf/2308.14080v1
2308.13999v1,math.NA,A Milstein-type method for highly non-linear non-autonomous time-changed stochastic differential equations,2023-08-27 04:02:38+00:00,"A Milstein-type method is proposed for some highly non-linear non-autonomous
time-changed stochastic differential equations (SDEs). The spatial variables in
the coefficients of the time-changed SDEs satisfy the super-linear growth
condition and the temporal variables obey some H\""older's continuity condition.
The strong convergence in the finite time is studied and the convergence order
is obtained.",http://arxiv.org/pdf/2308.13999v1
2308.13986v1,math.NA,A Deep Learning Method for Computing Eigenvalues of the Fractional Schrödinger Operator,2023-08-27 02:17:07+00:00,"We present a novel deep learning method for computing eigenvalues of the
fractional Schr\""odinger operator. Our approach combines a newly developed loss
function with an innovative neural network architecture that incorporates prior
knowledge of the problem. These improvements enable our method to handle both
high-dimensional problems and problems posed on irregular bounded domains. We
successfully compute up to the first 30 eigenvalues for various fractional
Schr\""odinger operators. As an application, we share a conjecture to the
fractional order isospectral problem that has not yet been studied.",http://arxiv.org/pdf/2308.13986v1
2308.13840v1,math.NA,Optimal Transport-inspired Deep Learning Framework for Slow-Decaying Problems: Exploiting Sinkhorn Loss and Wasserstein Kernel,2023-08-26 10:24:43+00:00,"Reduced order models (ROMs) are widely used in scientific computing to tackle
high-dimensional systems. However, traditional ROM methods may only partially
capture the intrinsic geometric characteristics of the data. These
characteristics encompass the underlying structure, relationships, and
essential features crucial for accurate modeling.
  To overcome this limitation, we propose a novel ROM framework that integrates
optimal transport (OT) theory and neural network-based methods. Specifically,
we investigate the Kernel Proper Orthogonal Decomposition (kPOD) method
exploiting the Wasserstein distance as the custom kernel, and we efficiently
train the resulting neural network (NN) employing the Sinkhorn algorithm. By
leveraging an OT-based nonlinear reduction, the presented framework can capture
the geometric structure of the data, which is crucial for accurate learning of
the reduced solution manifold. When compared with traditional metrics such as
mean squared error or cross-entropy, exploiting the Sinkhorn divergence as the
loss function enhances stability during training, robustness against
overfitting and noise, and accelerates convergence.
  To showcase the approach's effectiveness, we conduct experiments on a set of
challenging test cases exhibiting a slow decay of the Kolmogorov n-width. The
results show that our framework outperforms traditional ROM methods in terms of
accuracy and computational efficiency.",http://arxiv.org/pdf/2308.13840v1
2308.13819v1,cs.LG,Guaranteed Stable Quadratic Models and their applications in SINDy and Operator Inference,2023-08-26 09:00:31+00:00,"Scientific machine learning for learning dynamical systems is a powerful tool
that combines data-driven modeling models, physics-based modeling, and
empirical knowledge. It plays an essential role in an engineering design cycle
and digital twinning. In this work, we primarily focus on an operator inference
methodology that builds dynamical models, preferably in low-dimension, with a
prior hypothesis on the model structure, often determined by known physics or
given by experts. Then, for inference, we aim to learn the operators of a model
by setting up an appropriate optimization problem. One of the critical
properties of dynamical systems is{stability. However, such a property is not
guaranteed by the inferred models. In this work, we propose inference
formulations to learn quadratic models, which are stable by design. Precisely,
we discuss the parameterization of quadratic systems that are locally and
globally stable. Moreover, for quadratic systems with no stable point yet
bounded (e.g., Chaotic Lorenz model), we discuss an attractive trapping region
philosophy and a parameterization of such systems. Using those
parameterizations, we set up inference problems, which are then solved using a
gradient-based optimization method. Furthermore, to avoid numerical derivatives
and still learn continuous systems, we make use of an integration form of
differential equations. We present several numerical examples, illustrating the
preservation of stability and discussing its comparison with the existing
state-of-the-art approach to infer operators. By means of numerical examples,
we also demonstrate how proposed methods are employed to discover governing
equations and energy-preserving models.",http://arxiv.org/pdf/2308.13819v1
2308.13709v1,cs.IT,Fast and Low-Memory Compressive Sensing Algorithms for Low Tucker-Rank Tensor Approximation from Streamed Measurements,2023-08-25 23:44:47+00:00,"In this paper we consider the problem of recovering a low-rank Tucker
approximation to a massive tensor based solely on structured random compressive
measurements. Crucially, the proposed random measurement ensembles are both
designed to be compactly represented (i.e., low-memory), and can also be
efficiently computed in one-pass over the tensor. Thus, the proposed
compressive sensing approach may be used to produce a low-rank factorization of
a huge tensor that is too large to store in memory with a total memory
footprint on the order of the much smaller desired low-rank factorization. In
addition, the compressive sensing recovery algorithm itself (which takes the
compressive measurements as input, and then outputs a low-rank factorization)
also runs in a time which principally depends only on the size of the sought
factorization, making its runtime sub-linear in the size of the large tensor
one is approximating. Finally, unlike prior works related to (streaming)
algorithms for low-rank tensor approximation from such compressive
measurements, we present a unified analysis of both Kronecker and Khatri-Rao
structured measurement ensembles culminating in error guarantees comparing the
error of our recovery algorithm's approximation of the input tensor to the best
possible low-rank Tucker approximation error achievable for the tensor by any
possible algorithm. We further include an empirical study of the proposed
approach that verifies our theoretical findings and explores various trade-offs
of parameters of interest.",http://arxiv.org/pdf/2308.13709v1
2308.13476v1,math.NA,Stand-alone Multigrid for Helmholtz Revisited: Towards Convergence Using Standard Components,2023-08-25 16:28:50+00:00,"Getting standard multigrid to work efficiently for the high-frequency
Helmholtz equation has been an open problem in applied mathematics for years.
Much effort has been dedicated to finding solution methods which can use
multigrid components to obtain solvers with a linear time complexity. In this
work we present one among the first stand-alone multigrid solvers for the 2D
Helmholtz equation using both a constant and non-constant wavenumber model
problem. We use standard smoothing techniques and do not impose any
restrictions on the number of grid points per wavelength on the coarse-grid. As
a result we are able to obtain a full V- and W-cycle algorithm. The key
features of the algorithm are the use of higher-order inter-grid transfer
operators combined with a complex constant in the coarsening process. Using
weighted-Jacobi smoothing, we obtain a solver which is $h-$independent and
scales linearly with the wavenumber $k$. Numerical results using 1 to 5
GMRES(3) smoothing steps approach $k-$ and $h-$ independent convergence, when
combined with the higher-order inter-grid transfer operators and a small or
even zero complex shift. The proposed algorithm provides an important step
towards the perpetuating branch of research in finding scalable solvers for
challenging wave propagation problems.",http://arxiv.org/pdf/2308.13476v1
2308.13333v1,cs.RO,Small Celestial Body Exploration with CubeSat Swarms,2023-08-25 12:09:31+00:00,"This work presents a large-scale simulation study investigating the
deployment and operation of distributed swarms of CubeSats for interplanetary
missions to small celestial bodies. Utilizing Taylor numerical integration and
advanced collision detection techniques, we explore the potential of large
CubeSat swarms in capturing gravity signals and reconstructing the internal
mass distribution of a small celestial body while minimizing risks and Delta V
budget. Our results offer insight into the applicability of this approach for
future deep space exploration missions.",http://arxiv.org/pdf/2308.13333v1
2308.13295v1,math.NA,Resolution-independent generative models based on operator learning for physics-constrained Bayesian inverse problems,2023-08-25 10:41:00+00:00,"The Bayesian inference approach is widely used to tackle inverse problems due
to its versatile and natural ability to handle ill-posedness. However, it often
faces challenges when dealing with situations involving continuous fields or
large-resolution discrete representations (high-dimensional). Moreover, the
prior distribution of unknown parameters is commonly difficult to be
determined. In this study, an Operator Learning-based Generative Adversarial
Network (OL-GAN) is proposed and integrated into the Bayesian inference
framework to handle these issues. Unlike most Bayesian approaches, the
distinctive characteristic of the proposed method is to learn the joint
distribution of parameters and responses. By leveraging the trained generative
model, the posteriors of the unknown parameters can theoretically be
approximated by any sampling algorithm (e.g., Markov Chain Monte Carlo, MCMC)
in a low-dimensional latent space shared by the components of the joint
distribution. The latent space is typically a simple and easy-to-sample
distribution (e.g., Gaussian, uniform), which significantly reduces the
computational cost associated with the Bayesian inference while avoiding prior
selection concerns. Furthermore, incorporating operator learning enables
resolution-independent in the generator. Predictions can be obtained at desired
coordinates, and inversions can be performed even if the observation data are
misaligned with the training data. Finally, the effectiveness of the proposed
method is validated through several numerical experiments.",http://arxiv.org/pdf/2308.13295v1
2308.13224v1,math.PR,Exponential Euler method for stiff stochastic differential equations with additive fractional Brownian noise,2023-08-25 07:41:59+00:00,"We discuss a system of stochastic differential equations with a stiff linear
term and additive noise driven by fractional Brownian motions (fBms) with Hurst
parameter H>1/2, which arise e. g., from spatial approximations of stochastic
partial differential equations. For their numerical approximation, we present
an exponential Euler scheme and show that it converges in the strong sense with
an exact rate close to the Hurst parameter H. Further, based on [2], we
conclude the existence of a unique stationary solution of the exponential Euler
scheme that is pathwise asymptotically stable.",http://arxiv.org/pdf/2308.13224v1
2308.13214v1,math.NA,Gl-QFOM and Gl-QGMRES: two efficient algorithms for quaternion linear systems with multiple right-hand sides,2023-08-25 07:24:53+00:00,"In this paper, we propose the global quaternion full orthogonalization
(Gl-QFOM) and global quaternion generalized minimum residual (Gl-QGMRES)
methods, which are built upon global orthogonal and oblique projections onto a
quaternion matrix Krylov subspace, for solving quaternion linear systems with
multiple right-hand sides. We first develop the global quaternion Arnoldi
procedure to preserve the quaternion Hessenberg form during the iterations. We
then establish the convergence analysis of the proposed methods, and show how
to apply them to solve the Sylvester quaternion matrix equation. Numerical
examples are provided to illustrate the effectiveness of our methods compared
with the traditional Gl-FOM and Gl-GMRES iterations for the real
representations of the original linear systems.",http://arxiv.org/pdf/2308.13214v1
2308.13195v1,math.NA,Preconditioning for Generalized Jacobians with the $ω$-Condition Number,2023-08-25 06:20:20+00:00,"Preconditioning is essential in iterative methods for solving linear systems
of equations. We study a nonclassic matrix condition number, the
$\omega$-condition number, in the context of optimal conditioning for low rank
updating of positive definite matrices. For a positive definite matrix, this
condition measure is the ratio of the arithmetic and geometric means of the
eigenvalues. In particular, we concentrate on linear systems with low rank
updates of positive definite matrices which are close to singular. These
systems arise in the contexts of nonsmooth Newton methods using generalized
Jacobians. We derive an explicit formula for the optimal
$\omega$-preconditioned update in this framework.
  Evaluating or estimating the classical condition number $\kappa$ can be
expensive. We show that the $\omega$-condition number can be evaluated exactly
following a Cholesky or LU factorization and it estimates the actual condition
of a linear system significantly better. Moreover, our empirical results show a
significant decrease in the number of iterations required for a requested
accuracy in the residual during an iterative method, i.e., these results
confirm the efficacy of using the $\omega$-condition number compared to the
classical condition number.",http://arxiv.org/pdf/2308.13195v1
2308.13152v1,math.NA,The time dimensional reduction method to determine the initial conditions without the knowledge of damping coefficients,2023-08-25 03:11:30+00:00,"This paper aims to reconstruct the initial condition of a hyperbolic equation
with an unknown damping coefficient. Our approach involves approximating the
hyperbolic equation's solution by its truncated Fourier expansion in the time
domain and using a polynomial-exponential basis. This truncation process
facilitates the elimination of the time variable, consequently, yielding a
system of quasi-linear elliptic equations. To globally solve the system without
needing an accurate initial guess, we employ the Carleman contraction
principle. We provide several numerical examples to illustrate the efficacy of
our method. The method not only delivers precise solutions but also showcases
remarkable computational efficiency.",http://arxiv.org/pdf/2308.13152v1
2308.13123v1,cs.CE,Multiscale modeling of thermal properties in Polyurethane incorporated with phase change materials composites: A case study,2023-08-25 00:29:56+00:00,"Polyurethane (PU) is an ideal thermal insulation material due to its
excellent thermal properties. The incorporation of Phase Change Materials
(PCMs) capsules into Polyurethane (PU) has been shown to be effective in
building envelopes. This design can significantly increase the stability of the
indoor thermal environment and reduce the fluctuation of indoor air
temperature. We develop a multiscale model of a PU-PCM foam composite and study
the thermal conductivity of this material. Later, the design of materials can
be optimized by obtaining thermal conductivity. We conduct a case study based
on the performance of this optimized material to fully consider the thermal
comfort of the occupants of a building envelope with the application of PU-PCMs
composites in a single room. At the same time, we also predict the energy
consumption of this case. All the outcomes show that this design is promising,
enabling the passive design of building energy and significantly improving
occupants' comfort.",http://arxiv.org/pdf/2308.13123v1
2308.13055v1,physics.flu-dyn,On the dynamics of the boundary vorticity for incompressible viscous flows,2023-08-24 19:42:54+00:00,"The dynamical equation of the boundary vorticity has been obtained, which
shows that the viscosity at a solid wall is doubled as if the fluid became more
viscous at the boundary. For certain viscous flows the boundary vorticity can
be determined via the dynamical equation up to bounded errors for all time,
without the need of knowing the details of the main stream flows. We then
validate the dynamical equation by carrying out stochastic direct numerical
simulations (i.e. the random vortex method for wall-bounded incompressible
viscous flows) by two different means of updating the boundary vorticity, one
using mollifiers of the Biot-Savart singular integral kernel, another using the
dynamical equations.",http://arxiv.org/pdf/2308.13055v1
2308.12939v1,cs.LG,Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries,2023-08-24 17:29:57+00:00,"Recently deep learning surrogates and neural operators have shown promise in
solving partial differential equations (PDEs). However, they often require a
large amount of training data and are limited to bounded domains. In this work,
we present a novel physics-informed neural operator method to solve
parametrized boundary value problems without labeled data. By reformulating the
PDEs into boundary integral equations (BIEs), we can train the operator network
solely on the boundary of the domain. This approach reduces the number of
required sample points from $O(N^d)$ to $O(N^{d-1})$, where $d$ is the domain's
dimension, leading to a significant acceleration of the training process.
Additionally, our method can handle unbounded problems, which are unattainable
for existing physics-informed neural networks (PINNs) and neural operators. Our
numerical experiments show the effectiveness of parametrized complex geometries
and unbounded problems.",http://arxiv.org/pdf/2308.12939v1
2308.12907v1,math.NA,New time domain decomposition methods for parabolic control problems I: Dirichlet-Neumann and Neumann-Dirichlet algorithms,2023-08-24 16:31:01+00:00,"We present new Dirichlet-Neumann and Neumann-Dirichlet algorithms with a time
domain decomposition applied to unconstrained parabolic optimal control
problems. After a spatial semi-discretization, we use the Lagrange multiplier
approach to derive a coupled forward-backward optimality system, which can then
be solved using a time domain decomposition. Due to the forward-backward
structure of the optimality system, three variants can be found for the
Dirichlet-Neumann and Neumann-Dirichlet algorithms. We analyze their
convergence behavior and determine the optimal relaxation parameter for each
algorithm. Our analysis reveals that the most natural algorithms are actually
only good smoothers, and there are better choices which lead to efficient
solvers. We illustrate our analysis with numerical experiments.",http://arxiv.org/pdf/2308.12907v1
2308.12891v1,math.NA,A class of Discontinuous Galerkin methods for nonlinear variational problems,2023-08-24 16:11:23+00:00,"In the context of Discontinuous Galerkin methods, we study approximations of
nonlinear variational problems associated with convex energies. We propose
element-wise nonconforming finite element methods to discretize the continuous
minimisation problem. Using $\Gamma$-convergence arguments we show that the
discrete minimisers converge to the unique minimiser of the continuous problem
as the mesh parameter tends to zero, under the additional contribution of
appropriately defined penalty terms at the level of the discrete energies. We
finally substantiate the feasibility of our methods by numerical examples.",http://arxiv.org/pdf/2308.12891v1
2308.12886v1,math.NA,Linear implicit approximations of invariant measures of semi-linear SDEs with non-globally Lipschitz coefficients,2023-08-24 16:03:40+00:00,"This article investigates the weak approximation towards the invariant
measure of semi-linear stochastic differential equations (SDEs) under
non-globally Lipschitz coefficients. For this purpose, we propose a
linear-theta-projected Euler (LTPE) scheme, which also admits an invariant
measure, to handle the potential influence of the linear stiffness. Under
certain assumptions, both the SDE and the corresponding LTPE method are shown
to converge exponentially to the underlying invariant measures, respectively.
Moreover, with time-independent regularity estimates for the corresponding
Kolmogorov equation, the weak error between the numerical invariant measure and
the original one can be guaranteed with an order one. Numerical experiments are
provided to verify our theoretical findings.",http://arxiv.org/pdf/2308.12886v1
2308.12884v1,math.NA,A second-order length-preserving and unconditionally energy stable rotational discrete gradient method for Oseen-Frank gradient flows,2023-08-24 16:01:25+00:00,"We present a second-order strictly length-preserving and unconditionally
energy-stable rotational discrete gradient (Rdg) scheme for the numerical
approximation of the Oseen-Frank gradient flows with anisotropic elastic energy
functional. Two essential ingredients of the Rdg method are reformulation of
the length constrained gradient flow into an unconstrained rotational form and
discrete gradient discretization for the energy variation. Besides the
well-known mean-value and Gonzalez discrete gradients, we propose a novel
Oseen-Frank discrete gradient, specifically designed for the solution of
Oseen-Frank gradient flow. We prove that the proposed Oseen-Frank discrete
gradient satisfies the energy difference relation, thus the resultant Rdg
scheme is energy stable. Numerical experiments demonstrate the efficiency and
accuracy of the proposed Rdg method and its capability for providing reliable
simulation results with highly disparate elastic coefficients.",http://arxiv.org/pdf/2308.12884v1
2308.12865v1,math.NA,A highly efficient and accurate divergence-free spectral method for curl-curl equation in two and three dimensions,2023-08-24 15:43:12+00:00,"In this paper, we present a fast divergence-free spectral algorithm (FDSA)
for the curl-curl problem. Divergence-free bases in two and three dimensions
are constructed by using the generalized Jacobi polynomials. An accurate
spectral method with exact preservation of the divergence-free constraint
point-wisely is then proposed, and its corresponding error estimate is
established. We then present a highly efficient solution algorithm based on a
combination of matrix-free preconditioned Krylov subspace iterative method and
a fully diagonalizable auxiliary problem, which is derived from the spectral
discretisations of generalized eigenvalue problems of Laplace and biharmonic
operators. We rigorously prove that the dimensions of the invariant subspace of
the preconditioned linear system resulting from the divergence-free spectral
method with respect to the dominate eigenvalue $1$, are $(N-3)^2$ and
$2(N-3)^3$ for two- and three-dimensional problems with $(N-1)^2$ and
$2(N-1)^3$ unknowns, respectively. Thus, the proposed method usually takes only
several iterations to converge, and astonishingly, as the problem size
(polynomial order) increases, the number of iterations will decrease, even for
highly indefinite system and oscillatory solutions. As a result, the
computational cost of the solution algorithm is only a small multiple of $N^3$
and $N^4$ floating number operations for 2D and 3D problems, respectively.
Plenty of numerical examples for solving the curl-curl problem with both
constant and variable coefficients in two and three dimensions are presented to
demonstrate the accuracy and efficiency of the proposed method.",http://arxiv.org/pdf/2308.12865v1
2308.12864v1,cs.LG,Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution,2023-08-24 15:39:01+00:00,"In this article, we present a novel data assimilation strategy in pore-scale
imaging and demonstrate that this makes it possible to robustly address
reactive inverse problems incorporating Uncertainty Quantification (UQ).
Pore-scale modeling of reactive flow offers a valuable opportunity to
investigate the evolution of macro-scale properties subject to dynamic
processes. Yet, they suffer from imaging limitations arising from the
associated X-ray microtomography (X-ray microCT) process, which induces
discrepancies in the properties estimates. Assessment of the kinetic parameters
also raises challenges, as reactive coefficients are critical parameters that
can cover a wide range of values. We account for these two issues and ensure
reliable calibration of pore-scale modeling, based on dynamical microCT images,
by integrating uncertainty quantification in the workflow.
  The present method is based on a multitasking formulation of reactive inverse
problems combining data-driven and physics-informed techniques in calcite
dissolution. This allows quantifying morphological uncertainties on the
porosity field and estimating reactive parameter ranges through prescribed PDE
models with a latent concentration field and dynamical microCT. The data
assimilation strategy relies on sequential reinforcement incorporating
successively additional PDE constraints. We guarantee robust and unbiased
uncertainty quantification by straightforward adaptive weighting of Bayesian
Physics-Informed Neural Networks (BPINNs), ensuring reliable micro-porosity
changes during geochemical transformations. We demonstrate successful Bayesian
Inference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT
images with meaningful posterior distribution on the reactive parameters and
dimensionless numbers.",http://arxiv.org/pdf/2308.12864v1
2308.12807v1,math.NA,Denoising Particle-In-Cell Data via Smoothness-Increasing Accuracy-Conserving Filters with Application to Bohm Speed Computation,2023-08-24 14:10:05+00:00,"The simulation of plasma physics is computationally expensive because the
underlying physical system is of high dimensions, requiring three spatial
dimensions and three velocity dimensions. One popular numerical approach is
Particle-In-Cell (PIC) methods owing to its ease of implementation and
favorable scalability in high-dimensional problems. An unfortunate drawback of
the method is the introduction of statistical noise resulting from the use of
finitely many particles. In this paper we examine the application of the
Smoothness-Increasing Accuracy-Conserving (SIAC) family of convolution kernel
filters as denoisers for moment data arising from PIC simulations. We show that
SIAC filtering is a promising tool to denoise PIC data in the physical space as
well as capture the appropriate scales in the Fourier space. Furthermore, we
demonstrate how the application of the SIAC technique reduces the amount of
information necessary in the computation of quantities of interest in plasma
physics such as the Bohm speed.",http://arxiv.org/pdf/2308.12807v1
2308.12781v1,math.NA,A Riemannian optimization method to compute the nearest singular pencil,2023-08-24 13:37:35+00:00,"Given a square pencil $A+ \lambda B$, where $A$ and $B$ are complex matrices,
we consider the problem of finding the singular pencil nearest to it in the
Frobenius distance. This problem is known to be very difficult, and the few
algorithms available in the literature can only deal efficiently with pencils
of very small size. We show that the problem is equivalent to minimizing a
certain objective function over the Riemannian manifold $SU(n) \times SU(n)$,
where $SU(n)$ denotes the special unitary group. With minor modifications, the
same approach extends to the case of finding a nearest singular pencil with a
specified minimal index. This novel perspective is based on the generalized
Schur form of pencils, and yields a competitive numerical method, by pairing it
with an algorithm capable of doing optimization on a Riemannian manifold. We
provide numerical experiments that show that the resulting method allows us to
deal with pencils of much larger size than alternative techniques, yielding
candidate minimizers of comparable or better quality. In the course of our
analysis, we also obtain a number of new theoretical results related to the
generalized Schur form of a (regular or singular) square pencil and to the
minimal index of a singular square pencil whose nullity is $1$.",http://arxiv.org/pdf/2308.12781v1
2308.12764v1,math.NA,Dirichlet-Neumann and Neumann-Neumann Methods for Elliptic Control Problems,2023-08-24 13:12:56+00:00,"We present the Dirichlet-Neumann (DN) and Neumann-Neumann (NN) methods
applied to the optimal control problems arising from elliptic partial
differential equations (PDEs) under the $H^{-1}$ regularization. We use the
Lagrange multiplier approach to derive a forward-backward optimality system
with the $L^2$ regularization, and a singular perturbed Poisson equation with
the $H^{-1}$ regularization. The $H^{-1}$ regularization thus avoids solving a
coupled bi-Laplacian problem, yet the solutions are less regular. The singular
perturbed Poisson equation is then solved by using the DN and NN methods, and a
detailed analysis is given both in the one-dimensional and two-dimensional
case. Finally, we provide some numerical experiments with conclusions.",http://arxiv.org/pdf/2308.12764v1
2308.12716v1,math.NA,Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks,2023-08-24 11:31:24+00:00,"This paper explores the ability of physics-informed neural networks (PINNs)
to solve forward and inverse problems of contact mechanics for small
deformation elasticity. We deploy PINNs in a mixed-variable formulation
enhanced by output transformation to enforce Dirichlet and Neumann boundary
conditions as hard constraints. Inequality constraints of contact problems,
namely Karush-Kuhn-Tucker (KKT) type conditions, are enforced as soft
constraints by incorporating them into the loss function during network
training. To formulate the loss function contribution of KKT constraints,
existing approaches applied to elastoplasticity problems are investigated and
we explore a nonlinear complementarity problem (NCP) function, namely
Fischer-Burmeister, which possesses advantageous characteristics in terms of
optimization. Based on the Hertzian contact problem, we show that PINNs can
serve as pure partial differential equation (PDE) solver, as data-enhanced
forward model, as inverse solver for parameter identification, and as
fast-to-evaluate surrogate model. Furthermore, we demonstrate the importance of
choosing proper hyperparameters, e.g. loss weights, and a combination of Adam
and L-BFGS-B optimizers aiming for better results in terms of accuracy and
training time.",http://arxiv.org/pdf/2308.12716v1
2308.12683v1,math.NA,The key to the enhanced performance of slab-like topologically interlocked structures with non-planar blocks,2023-08-24 09:48:38+00:00,"Topologically interlocked structures are assemblies of interlocking blocks
that hold together solely through contact. Such structures have been shown to
exhibit high strength, energy dissipation, and crack arrest properties. Recent
studies on beam-like topologically interlocked structures have shown that, with
non-planar blocks, it is possible to reach levels of strength and
work-to-failure which are otherwise possible only with unrealistically high
friction coefficients. While non-planar blocks have been extensively used for
slab-like assemblies, many questions in that context are still not fully
understood. Specifically, it is unclear what are the exact characteristics of
non-planar surface morphologies which can potentially improve the enhanced
mechanical response of slab-like assemblies. In addition, it is unclear if
slab-like structures with non-planar surface blocks can reach a saturated
response with realistic friction coefficient values, as is the case with
beam-like ones. Here, we investigate such fundamental questions using numerical
simulations. We show that, by using non-planar blocks, it is possible to reach
saturation to the response capacity of the structure with a realistic friction
coefficient. Furthermore, we show that the key morphology parameter responsible
for the enhanced performance is the local angle of inclination at the top of
the loaded block. Lastly, we show that non-planar morphologies lead to improved
work-to-failure and ultimate deflection, which cannot be attained with
planar-faced blocks. These findings shed new light on topologically interlocked
structures with non-planar blocks, allowing for a better understanding of their
strengths and potential applications.",http://arxiv.org/pdf/2308.12683v1
2308.12393v1,cs.LG,Machine learning in parameter estimation of nonlinear systems,2023-08-23 19:20:24+00:00,"Accurately estimating parameters in complex nonlinear systems is crucial
across scientific and engineering fields. We present a novel approach for
parameter estimation using a neural network with the Huber loss function. This
method taps into deep learning's abilities to uncover parameters governing
intricate behaviors in nonlinear equations. We validate our approach using
synthetic data and predefined functions that model system dynamics. By training
the neural network with noisy time series data, it fine-tunes the Huber loss
function to converge to accurate parameters. We apply our method to damped
oscillators, Van der Pol oscillators, Lotka-Volterra systems, and Lorenz
systems under multiplicative noise. The trained neural network accurately
estimates parameters, evident from closely matching latent dynamics. Comparing
true and estimated trajectories visually reinforces our method's precision and
robustness. Our study underscores the Huber loss-guided neural network as a
versatile tool for parameter estimation, effectively uncovering complex
relationships in nonlinear systems. The method navigates noise and uncertainty
adeptly, showcasing its adaptability to real-world challenges.",http://arxiv.org/pdf/2308.12393v1
2308.12279v1,cs.LG,On-Manifold Projected Gradient Descent,2023-08-23 17:50:50+00:00,"This work provides a computable, direct, and mathematically rigorous
approximation to the differential geometry of class manifolds for
high-dimensional data, along with nonlinear projections from input space onto
these class manifolds. The tools are applied to the setting of neural network
image classifiers, where we generate novel, on-manifold data samples, and
implement a projected gradient descent algorithm for on-manifold adversarial
training. The susceptibility of neural networks (NNs) to adversarial attack
highlights the brittle nature of NN decision boundaries in input space.
Introducing adversarial examples during training has been shown to reduce the
susceptibility of NNs to adversarial attack; however, it has also been shown to
reduce the accuracy of the classifier if the examples are not valid examples
for that class. Realistic ""on-manifold"" examples have been previously generated
from class manifolds in the latent of an autoencoder. Our work explores these
phenomena in a geometric and computational setting that is much closer to the
raw, high-dimensional input space than can be provided by VAE or other black
box dimensionality reductions. We employ conformally invariant diffusion maps
(CIDM) to approximate class manifolds in diffusion coordinates, and develop the
Nystr\""{o}m projection to project novel points onto class manifolds in this
setting. On top of the manifold approximation, we leverage the spectral
exterior calculus (SEC) to determine geometric quantities such as tangent
vectors of the manifold. We use these tools to obtain adversarial examples that
reside on a class manifold, yet fool a classifier. These misclassifications
then become explainable in terms of human-understandable manipulations within
the data, by expressing the on-manifold adversary in the semantic basis on the
manifold.",http://arxiv.org/pdf/2308.12279v1
2308.12255v1,math.NA,Absorbing boundary conditions for the Helmholtz equation using Gauss-Legendre quadrature reduced integrations,2023-08-23 17:15:20+00:00,"We introduce a new class of absorbing boundary conditions (ABCs) for the
Helmholtz equation. The proposed ABCs can be derived from a certain simple
class of perfectly matched layers using $L$ discrete layers and using the $Q_N$
Lagrange finite element in conjunction with the $N$-point Gauss-Legendre
quadrature reduced integration rule. The proposed ABCs are classified by a
tuple $(L,N)$, and achieve reflection error of order $O(R^{2LN})$ for some
$R<1$. The new ABCs generalise the perfectly matched discrete layers proposed
by Guddati and Lim [Int. J. Numer. Meth. Engng 66 (6) (2006) 949-977],
including them as type $(L,1)$. An analysis of the proposed ABCs is performed
motivated by the work of Ainsworth [J. Comput. Phys. 198 (1) (2004) 106-130].
The new ABCs facilitate numerical implementations of the Helmholtz problem with
ABCs if $Q_N$ finite elements are used in the physical domain. Moreover, giving
more insight, the analysis presented in this work potentially aids with
developing ABCs in related areas.",http://arxiv.org/pdf/2308.12255v1
2308.12164v1,math.NA,A robust family of exponential attractors for a linear time discretization of the Cahn-Hilliard equation with a source term,2023-08-23 14:28:21+00:00,"We consider a linear implicit-explicit (IMEX) time discretization of the
Cahn-Hilliard equation with a source term, endowed with Dirichlet boundary
conditions. For every time step small enough, we build an exponential attractor
of the discrete-in-time dynamical system associated to the discretization. We
prove that, as the time step tends to 0, this attractor converges for the
symmmetric Hausdorff distance to an exponential attractor of the
continuous-in-time dynamical system associated with the PDE. We also prove that
the fractal dimension of the exponential attractor (and consequently, of the
global attractor) is bounded by a constant independent of the time step. The
results also apply to the classical Cahn-Hilliard equation with Neumann
boundary conditions.",http://arxiv.org/pdf/2308.12164v1
2308.12145v1,math.NA,Modeling excitable cells with the EMI equations: spectral analysis and iterative solution strategy,2023-08-23 14:01:07+00:00,"In this work, we are interested in solving large linear systems stemming from
the Extra-Membrane-Intra (EMI) model, which is employed for simulating
excitable tissues at a cellular scale. After setting the related systems of
partial differential equations (PDEs) equipped with proper boundary conditions,
we provide numerical approximation schemes for the EMI PDEs and focus on the
resulting large linear systems. We first give a relatively complete spectral
analysis using tools from the theory of Generalized Locally Toeplitz matrix
sequences. The obtained spectral information is used for designing appropriate
(preconditioned) Krylov solvers. We show, through numerical experiments, that
the presented solution strategy is robust w.r.t. problem and discretization
parameters, efficient and scalable.",http://arxiv.org/pdf/2308.12145v1
2308.12130v1,math.NA,Space-time hybridizable discontinuous Galerkin method for advection-diffusion on deforming domains: The advection-dominated regime,2023-08-23 13:36:12+00:00,"We analyze a space-time hybridizable discontinuous Galerkin method to solve
the time-dependent advection-diffusion equation on deforming domains. We prove
stability of the discretization in the advection-dominated regime by using
weighted test functions and derive a priori space-time error estimates. A
numerical example illustrates the theoretical results.",http://arxiv.org/pdf/2308.12130v1
2308.12104v1,math.NA,Computational Modeling of Coupled Interactions of Fluid Membranes with Embedded Filaments,2023-08-23 12:47:26+00:00,"In this work, we present a computational formulation based on continuum
mechanics to study the interaction of fluid membranes embedded with
semiflexible filaments. This is motivated by systems in membrane biology, such
as cytoskeletal networks and protein filaments aiding the cell fission process.
We model the membrane as a fluid shell via the Helfrich-Canham energy and the
filament as a one-dimensional Cosserat continuum. We assume the filament to be
tethered to the surface of the membrane in a way that it is allowed to float on
the surface freely. The novel filament-membrane coupling, which is anticipated
to yield interesting physics, also gives rise to unique computational
challenges, which we address in this work. We present validation results and
apply the formulation to certain problems inspired by cellular biology.",http://arxiv.org/pdf/2308.12104v1
2308.11964v1,math.NA,On the Computation of the Logarithm of the Modified Bessel Function of the Second Kind,2023-08-23 07:09:03+00:00,"The modified Bessel function of the second kind K$\nu$ appears in a wide
variety of applied scientific fields. While its use is greatly facilitated by
an implementation in most numerical libraries, overflow issues can be
encountered especially for large value of $\nu$. After giving some necessary
and sufficient conditions for their occurrences, this technical note shows that
they can mostly be avoided by directly computing the logarithm of K$\nu$ thanks
to a simple and stable forward recursion. A statistical examples based on the
Gil-Pelaez inversion formula is given to illustrate the recursive method.",http://arxiv.org/pdf/2308.11964v1
2308.11925v1,math.OC,Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks,2023-08-23 05:18:19+00:00,"In this work, we present and analyze a numerical solver for optimal control
problems (without / with box constraint) for linear and semilinear second-order
elliptic problems. The approach is based on a coupled system derived from the
first-order optimality system of the optimal control problem, and applies
physics informed neural networks (PINNs) to solve the coupled system. We
present an error analysis of the numerical scheme, and provide $L^2(\Omega)$
error bounds on the state, control and adjoint state in terms of deep neural
network parameters (e.g., depth, width, and parameter bounds) and the number of
sampling points in the domain and on the boundary. The main tools in the
analysis include offset Rademacher complexity and boundedness and Lipschitz
continuity of neural network functions. We present several numerical examples
to illustrate the approach and compare it with three existing approaches.",http://arxiv.org/pdf/2308.11925v1
2308.11892v1,physics.comp-ph,Constrained Pressure-Temperature Residual (CPTR) Preconditioner Performance for Large-Scale Thermal CO2 Injection Simulation,2023-08-23 03:40:26+00:00,"This work studies the performance of a novel preconditioner, designed for
thermal reservoir simulation cases and recently introduced in Roy et al. (2020)
and Cremon et al. (2020), on large-scale thermal CO2 injection cases. For
Carbon Capture and Sequestration (CCS) projects, injecting CO2 under
supercritical conditions is typically tens of degrees colder than the reservoir
temperature. Thermal effects can have a significant impact on the simulation
results, but they also add many challenges for the solvers. More specifically,
the usual combination of an iterative linear solver (such as GMRES) and the
Constrained Pressure Residual (CPR) physics-based block-preconditioner is known
to perform rather poorly or fail to converge when thermal effects play a
significant role. The Constrained Pressure-Temperature Residual (CPTR)
preconditioner retains the 2x2 block structure (elliptic/hyperbolic) of CPR but
includes the temperature in the elliptic subsystem. The elliptic subsystem is
now formed by two equations, and is dealt with by the system-solver of
BoomerAMG (from the HYPRE library). Then a global smoother, ILU(0), is applied
to the full system to handle the local, hyperbolic temperature fronts. We
implemented CPTR in the multi-physics solver GEOS and present results on
various large-scale thermal CCS simulation cases, including both Cartesian and
fully unstructured meshes, up to tens of millions of degrees of freedom. The
CPTR preconditioner severely reduces the number of GMRES iterations and the
runtime, with cases timing out in 24h with CPR now requiring a few hours with
CPTR. We present strong scaling results using hundreds of CPU cores for
multiple cases, and show close to linear scaling. CPTR is also virtually
insensitive to the thermal Peclet number (which compares advection and
diffusion effects) and is suitable to any thermal regime.",http://arxiv.org/pdf/2308.11892v1
2308.11882v1,math.NA,The macroscopic finite-difference scheme and modified equations of the general propagation multiple-relaxation-time lattice Boltzmann model,2023-08-23 02:59:04+00:00,"In this paper, we first present the general propagation
multiple-relaxation-time lattice Boltzmann (GPMRT-LB) model and obtain the
corresponding macroscopic finite-difference (GPMFD) scheme on conservative
moments. Then based on the Maxwell iteration method, we conduct the analysis on
the truncation errors and modified equations (MEs) of the GPMRT-LB model and
GPMFD scheme at both diffusive and acoustic scalings. For the nonlinear
anisotropic convection-diffusion equation (NACDE) and Navier-Stokes equations
(NSEs), we also derive the first- and second-order MEs of the GPMRT-LB model
and GPMFD scheme. In particular, for the one-dimensional convection-diffusion
equation (CDE) with the constant velocity and diffusion coefficient, we can
develop a fourth-order GPMRT-LB (F-GPMRT-LB) model and the corresponding
fourth-order GPMFD (F-GPMFD) scheme at the diffusive scaling. Finally, two
benchmark problems, Gauss hill problem and Poiseuille flow in two-dimensional
space, are used to test the GPMRT-LB model and GPMFD scheme, and it is found
that the numerical results are not only in good agreement with corresponding
analytical solutions, but also have a second-order convergence rate in space.
Additionally, a numerical study on one-dimensional CDE also demonstrates that
the F-GPMRT-LB model and F-GPMFD scheme can achieve a fourth-order accuracy in
space, which is consistent with our theoretical analysis.",http://arxiv.org/pdf/2308.11882v1
2308.11821v1,math.NA,Multi-temporal decomposition for elastoplastic ratcheting solids,2023-08-22 23:01:11+00:00,"This paper presents a multi-temporal formulation for simulating elastoplastic
solids under cyclic loading. We leverage the proper generalized decomposition
(PGD) to decompose the displacements into multiple time scales, separating the
spatial and intra-cyclic dependence from the inter-cyclic variation. In
contrast with the standard incremental approach, which solves the (non-linear
and computationally intensive) mechanical balance equations at every time step,
the proposed PGD approach allows the mechanical balance equations to be solved
exclusively for the small-time intra-cyclic response, while the large-time
inter-cyclic response is described by simple scalar algebraic equations.
Numerical simulations exhibiting complex cyclic responses, including a 2D
problem and an application to a monopile foundation, demonstrate that PGD
solutions with a limited number of space-time degrees of freedom may be
obtained numerically, only requiring a few modes to accurately capture the
reference response.",http://arxiv.org/pdf/2308.11821v1
2308.11581v1,math.NA,Dynamically Orthogonal Approximation for Stochastic Differential Equations,2023-08-22 17:30:12+00:00,"In this paper, we set the mathematical foundations of the Dynamical Low Rank
Approximation (DLRA) method for high-dimensional stochastic differential
equations. DLRA aims at approximating the solution as a linear combination of a
small number of basis vectors with random coefficients (low rank format) with
the peculiarity that both the basis vectors and the random coefficients vary in
time. While the formulation and properties of DLRA are now well understood for
random/parametric equations, the same cannot be said for SDEs and this work
aims to fill this gap. We start by rigorously formulating a Dynamically
Orthogonal (DO) approximation (an instance of DLRA successfully used in
applications) for SDEs, which we then generalize to define a parametrization
independent DLRA for SDEs. We show local well-posedness of the DO equations and
their equivalence with the DLRA formulation. We also characterize the explosion
time of the DO solution by a loss of linear independence of the random
coefficients defining the solution expansion and give sufficient conditions for
global existence.",http://arxiv.org/pdf/2308.11581v1
2308.11580v1,physics.comp-ph,NIPG-DG schemes for transformed master equations modeling open quantum systems,2023-08-22 17:28:51+00:00,"This work presents a numerical analysis of a master equation modeling the
interaction of a system with a noisy environment in the particular context of
open quantum systems. It is shown that our transformed master equation has a
reduced computational cost in comparison to a Wigner-Fokker-Planck model of the
same system for the general case of any potential. Specifics of a NIPG-DG
numerical scheme adequate for the convection-diffusion system obtained are then
presented. This will let us solve computationally the transformed system of
interest modeling our open quantum system. A benchmark problem, the case of a
harmonic potential, is then presented, for which the numerical results are
compared against the analytical steady-state solution of this problem.",http://arxiv.org/pdf/2308.11580v1
2308.11533v1,math.NA,Lifting Sylvester equations: singular value decay for non-normal coefficients,2023-08-22 15:59:41+00:00,"We aim to find conditions on two Hilbert space operators $A$ and $B$ under
which the expression $AX-XB$ having low rank forces the operator $X$ itself to
admit a good low rank approximation. It is known that this can be achieved when
$A$ and $B$ are normal and have well-separated spectra. In this paper, we relax
this normality condition, using the idea of operator dilations. The basic
problem then becomes the lifting of Sylvester equations, which is reminiscent
of the classical commutant lifting theorem and its variations. Our approach
also allows us to show that the (factored) alternating direction implicit
method for solving Sylvester equaftions $AX-XB=C$ does not require too many
iterations, even without requiring $A$ to be normal.",http://arxiv.org/pdf/2308.11533v1
2308.11503v1,math.NA,Multi-level Neural Networks for Accurate Solutions of Boundary-Value Problems,2023-08-22 15:24:29+00:00,"The solution to partial differential equations using deep learning approaches
has shown promising results for several classes of initial and boundary-value
problems. However, their ability to surpass, particularly in terms of accuracy,
classical discretization methods such as the finite element methods, remains a
significant challenge. Deep learning methods usually struggle to reliably
decrease the error in their approximate solution. A new methodology to better
control the error for deep learning methods is presented here. The main idea
consists in computing an initial approximation to the problem using a simple
neural network and in estimating, in an iterative manner, a correction by
solving the problem for the residual error with a new network of increasing
complexity. This sequential reduction of the residual of the partial
differential equation allows one to decrease the solution error, which, in some
cases, can be reduced to machine precision. The underlying explanation is that
the method is able to capture at each level smaller scales of the solution
using a new network. Numerical examples in 1D and 2D are presented to
demonstrate the effectiveness of the proposed approach. This approach applies
not only to physics informed neural networks but to other neural network
solvers based on weak or strong formulations of the residual.",http://arxiv.org/pdf/2308.11503v1
2308.11469v1,math.NA,An iterative method for Helmholtz boundary value problems arising in wave propagation,2023-08-22 14:31:39+00:00,"The complex Helmholtz equation $(\Delta + k^2)u=f$ (where $k\in{\mathbb
R},u(\cdot),f(\cdot)\in{\mathbb C}$) is a mainstay of computational wave
simulation. Despite its apparent simplicity, efficient numerical methods are
challenging to design and, in some applications, regarded as an open problem.
Two sources of difficulty are the large number of degrees of freedom and the
indefiniteness of the matrices arising after discretisation. Seeking to meet
them within the novel framework of probabilistic domain decomposition, we set
out to rewrite the Helmholtz equation into a form amenable to the Feynman-Kac
formula for elliptic boundary value problems. We consider two typical
scenarios, the scattering of a plane wave and the propagation inside a cavity,
and recast them as a sequence of Poisson equations. By means of stochastic
arguments, we find a sufficient and simulatable condition for the convergence
of the iterations. Upon discretisation a necessary condition for convergence
can be derived by adding up the iterates using the harmonic series for the
matrix inverse -- we illustrate the procedure in the case of finite
differences.
  From a practical point of view, our results are ultimately of limited scope.
Nonetheless, this unexpected -- even paradoxical -- new direction of attack on
the Helmholtz equation proposed by this work offers a fresh perspective on this
classical and difficult problem. Our results show that there indeed exists a
predictable range $k<k_{max}$ in which this new ansatz works with $k_{max}$
being far below the challenging situation.",http://arxiv.org/pdf/2308.11469v1
2308.15407v1,physics.flu-dyn,Bounded flows of dense gases,2023-08-29 16:08:12+00:00,"Numerical solutions of the Enskog equation obtained employing a
Finite-Difference Lattice Boltzmann (FDLB) and a Direct Simulation Monte Carlo
(DSMC)-like particle method (PM) are systematically compared to determine the
range of applicability of the simplified Enskog collision operator implemented
in the Lattice Boltzmann framework. Three types of bounded flows of dense gases
- namely the Fourier, the Couette, and the Poiseuille flows - are investigated
for a wide range of input parameters. For low to moderate reduced density, the
proposed FDLB model exhibits commendable accuracy for all bounded flows tested
in this study, with substantially lower computational cost than the PM method.",http://arxiv.org/pdf/2308.15407v1
2308.15301v1,physics.data-an,Convexity constraints on linear background models for electron energy-loss spectra,2023-08-29 13:40:15+00:00,"In this paper convexity constraints are derived for a background model of
electron energy loss spectra (EELS) that is linear in the fitting parameters.
The model outperforms a power-law both on experimental and simulated
backgrounds, especially for wide energy ranges, and thus improves elemental
quantification results. Owing to the model's linearity, the constraints can be
imposed through fitting by quadratic programming. This has important advantages
over conventional nonlinear power-law fitting such as high speed, not requiring
initial parameters, and a guaranteed unique solution. As such, the need for
user input is significantly reduced, which is essential for unsupervised
treatment of large data sets. This is demonstrated on a demanding spectrum
image of a semiconductor device sample with a high number of elements over a
wide energy range.",http://arxiv.org/pdf/2308.15301v1
2308.14948v1,astro-ph.IM,Machine Learning for Mini-EUSO Telescope Data Analysis,2023-08-29 00:04:19+00:00,"Neural networks as well as other methods of machine learning (ML) are known
to be highly efficient in different classification tasks, including
classification of images and videos. Mini- EUSO is a wide-field-of-view imaging
telescope that operates onboard the International Space Station since 2019
collecting data on miscellaneous processes that take place in the atmosphere of
Earth in the UV range. Here we briefly present our results on the development
of ML-based approaches for recognition and classification of track-like signals
in the Mini-EUSO data, among them meteors, space debris and signals the light
curves and kinematics of which are similar to those expected from extensive air
showers generated by ultra-high-energy cosmic rays. We show that even simple
neural networks demonstrate impressive performance in solving these tasks.",http://arxiv.org/pdf/2308.14948v1
2308.14890v1,physics.chem-ph,QCMATH: Mathematica modules for electronic structure calculations,2023-08-28 20:25:32+00:00,"We introduce \textsc{qcmath}, a user-friendly quantum chemistry software
tailored for electronic structure calculations, implemented using the Wolfram
Mathematica language and available at \url{https://github.com/LCPQ/qcmath}.
This software, designed with accessibility in mind, takes advantage of the
symbolic capabilities intrinsic to Mathematica. Its primary goal is to provide
a supportive environment for newcomers to the field of quantum chemistry,
enabling them to easily conceptualize, develop, and test their own ideas. The
functionalities of \textsc{qcmath} encompass a broad spectrum of methods,
catering to both ground- and excited-state calculations. We provide a
comprehensive overview of these capabilities, complemented by essential
theoretical insights. To facilitate ease of use, we offer an exhaustive
blueprint of the software's architecture. Furthermore, we provide users with
comprehensive guides, addressing both the operational aspects and the more
intricate programming facets of \textsc{qcmath}.",http://arxiv.org/pdf/2308.14890v1
2308.14885v1,cond-mat.stat-mech,Inferring phase transitions and critical exponents from limited observations with Thermodynamic Maps,2023-08-28 20:13:39+00:00,"Phase transitions are ubiquitous across life, yet hard to quantify and
describe accurately. In this work, we develop an approach for characterizing
generic attributes of phase transitions from very limited observations made
deep within different phases' domains of stability. Our approach is called
Thermodynamic Maps, which combines statistical mechanics and molecular
simulations with score-based generative models. Thermodynamic Maps enable
learning the temperature dependence of arbitrary thermodynamic observables
across a wide range of temperatures. We show its usefulness by calculating
phase transition attributes such as melting temperature, temperature-dependent
heat capacities, and critical exponents. For instance, we demonstrate the
ability of thermodynamic maps to infer the ferromagnetic phase transition of
the Ising model, including temperature-dependent heat capacity and critical
exponents, despite never having seen samples from the transition region. In
addition, we efficiently characterize the temperature-dependent conformational
ensemble and compute melting curves of the two RNA systems GCAA tetraloop and
HIV-TAR, which are notoriously hard to sample due to glassy-like landscapes.",http://arxiv.org/pdf/2308.14885v1
2308.14788v1,quant-ph,Floquet Topology Stabilized with Non-Hermitian Driving,2023-08-28 17:55:27+00:00,"This study presents a mechanism that enables the stabilization of Floquet
systems indefintely; albeit in a manner that allows for noise during each
Floquet cycle. This is due to the fact that external qubits are added after
each Floquet cycle and these external qubits obtain information about the
Floquet system (in this case the Floquet system is the Anomalous
Floquet-Anderson Insulator). This information is used to correct the system for
noise after which these qubits are removed. The fact that these external qubits
are added and then removed after performing operations on the system is what
allows for this process to be referred to as non-Hermitian driving. The
external qubits effectively act to carry away entropy of the system and
therefore allow for the Floquet system to be cooled. In addition, another
mechanism is found where the periodic implementation of entanglement for every
site of the system at a high frequency during the normal time evolution allows
for the system to be highly localized in a manner similar to Anderson
localization.",http://arxiv.org/pdf/2308.14788v1
2308.14618v1,physics.chem-ph,Seniority and Hierarchy Configuration Interaction for Radicals and Excited States,2023-08-28 14:42:13+00:00,"Hierarchy configuration interaction (hCI) has been recently introduced as an
alternative configuration interaction (CI) route combining excitation degree
and seniority number, which showed to efficiently recover both dynamic and
static correlations for closed-shell molecular systems
[\href{https://doi.org/10.1021/acs.jpclett.2c00730}{\textit{J.~Phys.~Chem.~Lett.}~\textbf{2022},
\textit{13}, 4342}]. Here, we generalize hCI for an arbitrary reference
determinant, allowing calculations for radicals and for excited states in a
state-specific way. We gauge this route against excitation-based CI (eCI) and
seniority-based CI (sCI) by evaluating how different ground-state properties of
radicals converge to the full CI limit. We find that hCI outperforms or matches
eCI, whereas sCI is far less accurate, in line with previous observations for
closed-shell molecules. Employing the second-order Epstein-Nesbet perturbation
theory as a correction significantly accelerates the convergence of hCI and
eCI. We further explore various hCI and sCI models to calculate excitation
energies of closed- and open-shell systems. Our results underline that both the
choice of the reference determinant and the set of orbitals drive the fine
balance between correlation of ground and excited states. State-specific hCI2
and higher order models perform similarly to their eCI counterparts, whereas
lower orders of hCI deliver poor results. In turn, sCI1 produces decent
excitation energies for radicals, encouraging the development of related
seniority-based coupled cluster methods.",http://arxiv.org/pdf/2308.14618v1
2308.14239v1,quant-ph,Quantum Next Generation Reservoir Computing: An Efficient Quantum Algorithm for Forecasting Quantum Dynamics,2023-08-28 00:34:40+00:00,"Next Generation Reservoir Computing (NG-RC) is a modern class of model-free
machine learning that enables an accurate forecasting of time series data
generated by dynamical systems. We demonstrate that NG-RC can accurately
predict full many-body quantum dynamics, instead of merely concentrating on the
dynamics of observables, which is the conventional application of reservoir
computing. In addition, we apply a technique which we refer to as skipping
ahead to predict far future states accurately without the need to extract
information about the intermediate states. However, adopting a classical NG-RC
for many-body quantum dynamics prediction is computationally prohibitive due to
the large Hilbert space of sample input data. In this work, we propose an
end-to-end quantum algorithm for many-body quantum dynamics forecasting with a
quantum computational speedup via the block-encoding technique. This proposal
presents an efficient model-free quantum scheme to forecast quantum dynamics
coherently, bypassing inductive biases incurred in a model-based approach.",http://arxiv.org/pdf/2308.14239v1
2308.13863v1,physics.comp-ph,Full-scale ab initio simulations of laser-driven atomistic dynamics,2023-08-26 12:46:45+00:00,"The coupling of excited states and ionic dynamics is the basic and
challenging point for the materials response at extreme conditions. In
laboratory, the intense laser produces transient nature and complexity with
highly nonequilibrium states, making it extremely difficult and interesting for
both experimental measurements and theoretical methods. With the inclusion of
laser-excited states, we extended ab initio method into the direct simulations
of whole laser-driven microscopic dynamics from solid to liquid. We constructed
the framework of combining the electron-temperaturedependent deep neural
network potential energy surface with hybrid atomistic-continuum approach,
controlling non-adiabatic energy exchange and atomistic dynamics, which enables
consistent interpretation of experimental data. By large scale ab inito
simulations, we demonstrate that the nonthermal effects introduced by hot
electrons play a dominant role in modulating the lattice dynamics,
thermodynamic pathway, and structural transformation. We highlight that the
present work provides a path to realistic computational studies of laser-driven
processes, thus bridging the gap between experiments and simulations.",http://arxiv.org/pdf/2308.13863v1
2308.13727v1,physics.plasm-ph,Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: II. dynamics forecasting,2023-08-26 01:48:29+00:00,"In part I of the article, we demonstrated that a variant of the Dynamic Mode
Decomposition (DMD) algorithm based on variable projection optimization, called
Optimized DMD (OPT-DMD), enables a robust identification of the dominant
spatiotemporally coherent modes underlying the data across various test cases
representing different physical parameters in an ExB simulation configuration.
As the OPT-DMD can be constrained to produce stable reduced-order models (ROMs)
by construction, in this paper, we extend the application of the OPT-DMD and
investigate the capabilities of the linear ROM from this algorithm toward
forecasting in time of the plasma dynamics in configurations representative of
the radial-azimuthal and axial-azimuthal cross-sections of a Hall thruster and
over a range of simulation parameters in each test case. The predictive
capacity of the OPT-DMD ROM is assessed primarily in terms of short-term
dynamics forecast or, in other words, for large ratios of training-to-test
data. However, the utility of the ROM for long-term dynamics forecasting is
also presented for an example case in the radial-azimuthal configuration. The
model's predictive performance is heterogeneous across various test cases.
Nonetheless, a remarkable predictiveness is observed in the test cases that do
not exhibit highly transient behaviors. Moreover, in all investigated cases,
the error between the ground-truth and the reconstructed data from the OPT-DMD
ROM remains bounded over time within both the training and the test window. As
a result, despite its limitation in terms of generalized applicability to all
plasma conditions, the OPT-DMD is proven as a reliable method to develop low
computational cost and highly predictive data-driven reduced-order models in
systems with a quasi-periodic global evolution of the plasma state.",http://arxiv.org/pdf/2308.13727v1
2308.13726v1,physics.plasm-ph,Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: I. Extraction of spatiotemporally coherent patterns,2023-08-26 01:37:52+00:00,"In this two-part article, we evaluate the utility and the generalizability of
the Dynamic Mode Decomposition (DMD) algorithm for data-driven analysis and
reduced-order modelling of plasma dynamics in cross-field ExB configurations.
The DMD algorithm is an interpretable data-driven method that finds a best-fit
linear model describing the time evolution of spatiotemporally coherent
structures (patterns) in data. We have applied the DMD to extensive
high-fidelity datasets generated using a particle-in-cell (PIC) code based on a
cost-efficient reduced-order PIC scheme. In this part, we first provide an
overview of the concept of DMD and its underpinning Proper Orthogonal and
Singular Value Decomposition methods. Two of the main DMD variants are next
introduced. We then present and discuss the results of the DMD application in
terms of the identification and extraction of the dominant spatiotemporal modes
from high-fidelity data over a range of simulation conditions. We demonstrate
that the DMD variant based on variable projection optimization (OPT-DMD)
outperforms the basic DMD method in identification of the modes underlying the
data, leading to notably more reliable reconstruction of the ground-truth.
Furthermore, we show in multiple test cases that the discrete frequency
spectrum of OPT-DMD-extracted modes is consistent with the temporal spectrum
from the Fast Fourier Transform of the data. This observation implies that the
OPT-DMD augments the conventional spectral analyses by being able to uniquely
reveal the spatial structure of the dominant modes in the frequency spectra,
thus, yielding more accessible, comprehensive information on the spatiotemporal
characteristics of the plasma phenomena.",http://arxiv.org/pdf/2308.13726v1
2308.13692v1,cond-mat.mtrl-sci,Enhanced Spin Hall Ratio in Two-Dimensional III-V Semiconductors,2023-08-25 22:21:27+00:00,"Spin Hall effect plays a critical role in spintronics since it can convert
charge current to spin current. Using state-of-the-art ab initio calculations
including quadrupole and spin-orbit coupling, the charge and spin transports
have been investigated in pristine and doped two-dimensional III-V
semiconductors. Valence bands induce a strong scattering which limits charge
conductivity in the hole-doped system, where spin Hall conductivity is enhanced
by the spin-orbit splitting, yielding an ultrahigh spin Hall ratio
$\xi\approx0.9$ in GaAs monolayer at room temperature.",http://arxiv.org/pdf/2308.13692v1
2308.13432v1,physics.acc-ph,Dephasingless laser wakefield acceleration in the bubble regime,2023-08-25 15:25:17+00:00,"Laser wakefield accelerators (LWFAs) have electric fields that are orders of
magnitude larger than those of conventional accelerators, promising an
attractive, small-scale alternative for next-generation light sources and
lepton colliders. The maximum energy gain in a single-stage LWFA is limited by
dephasing, which occurs when the trapped particles outrun the accelerating
phase of the wakefield. Here, we demonstrate that a single space-time
structured laser pulse can be used for ionization injection and electron
acceleration over many dephasing lengths in the bubble regime. Simulations of a
dephasingless laser wakefield accelerator driven by a 6.2-J laser pulse show 25
pC of injected charge accelerated over 20 dephasing lengths (1.3 cm) to a
maximum energy of 2.1 GeV. The space-time structured laser pulse features an
ultrashort, programmable-trajectory focus. Accelerating the focus, reducing the
focused spot-size variation, and mitigating unwanted self-focusing stabilize
the electron acceleration, which improves beam quality and leads to projected
energy gains of 125 GeV in a single, sub-meter stage driven by a 500-J pulse.",http://arxiv.org/pdf/2308.13432v1
2308.13299v1,physics.flu-dyn,Microstructure-based prediction of hydrodynamic forces in stationary particle assemblies,2023-08-25 10:50:17+00:00,"In the work, we derive novel hydrodynamic force models to describe the
interaction of a flow with particles in an assembly when only an averaged
resolution of the flow is available. These force models are able to predict the
average drag on the particle assembly, as well as the deviations from the
average drag force and the lift force for each individual particle in the
assembly. To achieve this, PR-DNS of various particle assemblies and flow
regimes are carried out, varying the particle volume fraction up to 0.6, and
the mean particle flow Reynolds number up to 300. To characterize the structure
of the particles in the assembly, a Voronoi tessellation is carried out, and a
number of scalars, vectors and tensors are defined based upon this
tessellation. The microstructure informed hydrodynamic force models are based
on symbolic regressions of these quantities derived from the Voronoi
tessellation, the global particle volume fraction of the particle assembly and
the flow regime represented by the Reynolds number, and the forces on the
individual particles in the assembly.
  The resulting hydrodynamic force models are single expressions can be
directly employed in a Lagrangian particle tracking (LPT) or computational
fluid dynamics/discrete element model (CFD/DEM) framework. By comparing the
results of the newly proposed hydrodynamic force models with an averaged force
model, as is usually adopted in Lagrangian particle tracking simulations, we
show that a significant increase in accuracy can be achieved, without
significantly increasing the cost of the simulation.",http://arxiv.org/pdf/2308.13299v1
2308.13280v1,physics.ao-ph,AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning,2023-08-25 10:02:26+00:00,"The atmosphere affects humans in a multitude of ways, from loss of life due
to adverse weather effects to long-term social and economic impacts on
societies. Computer simulations of atmospheric dynamics are, therefore, of
great importance for the well-being of our and future generations. Here, we
propose AtmoRep, a novel, task-independent stochastic computer model of
atmospheric dynamics that can provide skillful results for a wide range of
applications. AtmoRep uses large-scale representation learning from artificial
intelligence to determine a general description of the highly complex,
stochastic dynamics of the atmosphere from the best available estimate of the
system's historical trajectory as constrained by observations. This is enabled
by a novel self-supervised learning objective and a unique ensemble that
samples from the stochastic model with a variability informed by the one in the
historical record. The task-independent nature of AtmoRep enables skillful
results for a diverse set of applications without specifically training for
them and we demonstrate this for nowcasting, temporal interpolation, model
correction, and counterfactuals. We also show that AtmoRep can be improved with
additional data, for example radar observations, and that it can be extended to
tasks such as downscaling. Our work establishes that large-scale neural
networks can provide skillful, task-independent models of atmospheric dynamics.
With this, they provide a novel means to make the large record of atmospheric
observations accessible for applications and for scientific inquiry,
complementing existing simulations based on first principles.",http://arxiv.org/pdf/2308.13280v1
2308.13222v1,physics.comp-ph,Bayesian Reasoning for Physics Informed Neural Networks,2023-08-25 07:38:50+00:00,"Physics informed neural network (PINN) approach in Bayesian formulation is
presented. We adopt the Bayesian neural network framework formulated by MacKay
(Neural Computation 4 (3) (1992) 448). The posterior densities are obtained
from Laplace approximation. For each model (fit), the so-called evidence is
computed. It is a measure that classifies the hypothesis. The most optimal
solution has the maximal value of the evidence. The Bayesian framework allows
us to control the impact of the boundary contribution to the total loss.
Indeed, the relative weights of loss components are fine-tuned by the Bayesian
algorithm. We solve heat, wave, and Burger's equations. The obtained results
are in good agreement with the exact solutions. All solutions are provided with
the uncertainties computed within the Bayesian framework.",http://arxiv.org/pdf/2308.13222v1
2308.13096v1,cond-mat.mtrl-sci,Electronic Structure Prediction of Multi-million Atom Systems Through Uncertainty Quantification Enabled Transfer Learning,2023-08-24 21:41:29+00:00,"The ground state electron density - obtainable using Kohn-Sham Density
Functional Theory (KS-DFT) simulations - contains a wealth of material
information, making its prediction via machine learning (ML) models attractive.
However, the computational expense of KS-DFT scales cubically with system size
which tends to stymie training data generation, making it difficult to develop
quantifiably accurate ML models that are applicable across many scales and
system configurations. Here, we address these fundamental challenges using
Bayesian neural networks and employ transfer learning to leverage the
multi-scale nature of the training data. Our ML models employ descriptors
involving simple scalar products, comprehensively sample system configurations
through thermalization, and quantify uncertainty in electron density
predictions. We show that our models incur significantly lower data generation
costs while allowing confident - and when verifiable, accurate - predictions
for a wide variety of bulk systems well beyond training, including systems with
defects, different alloy compositions, and at unprecedented, multi-million-atom
scales.",http://arxiv.org/pdf/2308.13096v1
2308.12754v1,cond-mat.soft,PyMembrane: A flexible framework for efficient simulations of elastic and liquid membranes,2023-08-24 13:01:12+00:00,"PyMembrane is a software package for simulating liquid and elastic membranes
using a discretisation of the continuum description based on unstructured
triangulated two-dimensional meshes embedded in three-dimensional space. The
package is written in C++, with a flexible and intuitive Python interface,
allowing for a quick setup, execution and analysis of complex simulations.
PyMembrane follows modern software engineering principles and features a
modular design that allows for straightforward implementation of custom
extensions while ensuring consistency and enabling inexpensive maintenance. A
hallmark feature of this design is the use of a standardized C++ interface
which streamlines adding new functionalities. Furthermore, PyMembrane uses data
structures optimised for unstructured meshes, ensuring efficient mesh
operations and force calculations. By providing several templates for typical
simulations supplemented by extensive documentation, the users can seamlessly
set up and run research-level simulations and extend the package to integrate
additional features, underscoring PyMembrane's commitment to user-centric
design.",http://arxiv.org/pdf/2308.12754v1
2308.13555v1,cond-mat.stat-mech,Probabilistic description of dissipative chaotic scattering,2023-08-24 12:19:32+00:00,"We investigate the extent to which the probabilistic properties of a chaotic
scattering system with dissipation can be understood from the properties of the
dissipation-free system. For large energies $E$, a fully chaotic scattering
leads to an exponential decay of the survival probability $P(t) \sim e^{-\kappa
t}$ with an escape rate $\kappa$ that decreases with $E$. Dissipation
$\gamma>0$ leads to the appearance of different finite-time regimes in $P(t)$.
We show how these different regimes can be understood for small $\gamma\ll 1$
and $t\gg 1/\kappa_0$ from the effective escape rate
$\kappa_\gamma(t)=\kappa_0(E(t))$ (including the non-hyperbolic regime) until
the energy reaches a critical value $E_c$ at which no escape is possible. More
generally, we argue that for small dissipation $\gamma$ and long times $t$ the
surviving trajectories in the dissipative system are distributed according to
the conditionally invariant measure of the conservative system at the
corresponding energy $E(t)<E(0)$. Quantitative predictions of our general
theory are compared with numerical simulations in the Henon-Heiles model.",http://arxiv.org/pdf/2308.13555v1
2308.12717v1,physics.chem-ph,Erfonium: A Hooke Atom with Soft Interaction Potential,2023-08-24 11:31:55+00:00,"Properties of erfonium, a Hooke atom with the Coulomb interaction potential
$1/r$ replaced by a non-singular $\text{erf}(\mu r)/r$ potential are
investigated. The structure of the Hooke atom potential and properties of its
energy spectrum, relative to the ones of the spherical harmonic oscillator and
of harmonium, are analyzed. It is shown, that at a certain value of $\mu$ the
system changes its behavior from a harmonium-like regime to a
harmonic-oscillator-like regime.",http://arxiv.org/pdf/2308.12717v1
2308.12358v1,cond-mat.str-el,variPEPS -- a versatile tensor network library for variational ground state simulations in two spatial dimensions,2023-08-23 18:03:14+00:00,"Tensor networks capture large classes of ground states of phases of quantum
matter faithfully and efficiently. Their manipulation and contraction has
remained a challenge over the years, however. For most of the history, ground
state simulations of two-dimensional quantum lattice systems using (infinite)
projected entangled pair states have relied on what is called a time-evolving
block decimation. In recent years, multiple proposals for the variational
optimization of the quantum state have been put forward, overcoming accuracy
and convergence problems of previously known methods. The incorporation of
automatic differentiation in tensor networks algorithms has ultimately enabled
a new, flexible way for variational simulation of ground states and excited
states. In this work, we review the state of the art of the variational iPEPS
framework. We present and explain the functioning of an efficient,
comprehensive and general tensor network library for the simulation of infinite
two-dimensional systems using iPEPS, with support for flexible unit cells and
different lattice geometries.",http://arxiv.org/pdf/2308.12358v1
2308.12206v1,cond-mat.mtrl-sci,"Plastic deformation mechanisms during nanoindentation of W, Mo, V body-centered cubic single crystals and their corresponding W-Mo, W-V equiatomic random solid solutions",2023-08-23 15:43:21+00:00,"Deformation plasticity mechanisms in alloys and compounds may unveil the
material capacity towards optimal mechanical properties. We conduct a series of
molecular dynamics (MD) simulations to investigate plasticity mechanisms due to
nanoindentation in pure tungsten, molybdenum and vanadium body-centered cubic
single crystals, as well as the also body-centered cubic, equiatomic, random
solid solutions (RSS) of tungsten--molybdenum and tungsten--vanadium alloys.
Our analysis focuses on a thorough, side-by-side comparison of dynamic
deformation processes, defect nucleation, and evolution, along with
corresponding stress--strain curves. We also check the surface morphology of
indented samples through atomic shear strain mapping. As expected, the presence
of Mo and V atoms in W matrices introduces lattice strain and distortion,
increasing material resistance to deformation and slowing down dislocation
mobility of dislocation loops with a Burgers vector of 1/2 $\langle 111
\rangle$. Our side-by-side comparison displays a remarkable suppression of the
plastic zone size in equiatomic W--V RSS, but not in equiatomic W--Mo RSS
alloys, displaying a clear prediction for optimal hardening response equiatomic
W--V RSS alloys. If the small-depth nanoindentation plastic response is
indicative of overall mechanical performance, it is possible to conceive a
novel MD-based pathway towards material design for mechanical applications in
complex, multi-component alloys.",http://arxiv.org/pdf/2308.12206v1
2308.12002v1,cs.LG,Neural oscillators for magnetic hysteresis modeling,2023-08-23 08:41:24+00:00,"Hysteresis is a ubiquitous phenomenon in science and engineering; its
modeling and identification are crucial for understanding and optimizing the
behavior of various systems. We develop an ordinary differential equation-based
recurrent neural network (RNN) approach to model and quantify the hysteresis,
which manifests itself in sequentiality and history-dependence. Our neural
oscillator, HystRNN, draws inspiration from coupled-oscillatory RNN and
phenomenological hysteresis models to update the hidden states. The performance
of HystRNN is evaluated to predict generalized scenarios, involving first-order
reversal curves and minor loops. The findings show the ability of HystRNN to
generalize its behavior to previously untrained regions, an essential feature
that hysteresis models must have. This research highlights the advantage of
neural oscillators over the traditional RNN-based methods in capturing complex
hysteresis patterns in magnetic materials, where traditional rate-dependent
methods are inadequate to capture intrinsic nonlinearity.",http://arxiv.org/pdf/2308.12002v1
2308.12312v1,physics.comp-ph,Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas,2023-08-23 07:00:56+00:00,"The Vlasov-Poisson system is employed in its reduced form version (1D1V) as a
test bed for the applicability of Physics Informed Neural Network (PINN) to the
wave-particle resonance. Two examples are explored: the Landau damping and the
bump-on-tail instability. PINN is first tested as a compression method for the
solution of the Vlasov-Poisson system and compared to the standard neural
networks. Second, the application of PINN to solving the Vlasov-Poisson system
is also presented with the special emphasis on the integral part, which
motivates the implementation of a PINN variant, called Integrable PINN
(I-PINN), based on the automatic-differentiation to solve the partial
differential equation and on the automatic-integration to solve the integral
equation.",http://arxiv.org/pdf/2308.12312v1
2308.11865v1,physics.plasm-ph,Resistive Hose modes in Tokamak Runaway Electron Beams,2023-08-23 01:56:17+00:00,"Beams of energetic runaway electrons are generated during disruptions in
tokamaks, and fluid models are used to study their effects on macroscale
dynamics. Linear computations of a massless, runaway electron beam coupled to
MHD plasma show that resistive hose instabilities grow faster than tearing
modes at large resistivity. Eigenvalue results with reduced models of the
resistive hose instability are compared with results from the full MHD and beam
system, showing that the resistive hose decouples from any plasma response. An
estimate of plasma temperature at which growth of the resistive hose dominates
tearing for post-disruption DIII-D plasma parameters is in a physically
relevant regime",http://arxiv.org/pdf/2308.11865v1
2308.11724v1,physics.comp-ph,MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations,2023-08-22 18:30:53+00:00,"Molecular Dynamics (MD) simulations are ubiquitous in cutting-edge
physio-chemical research. They provide critical insights into how a physical
system evolves over time given a model of interatomic interactions.
Understanding a system's evolution is key to selecting the best candidates for
new drugs, materials for manufacturing, and countless other practical
applications. With today's technology, these simulations can encompass millions
of unit transitions between discrete molecular structures, spanning up to
several milliseconds of real time. Attempting to perform a brute-force analysis
with data-sets of this size is not only computationally impractical, but would
not shed light on the physically-relevant features of the data. Moreover, there
is a need to analyze simulation ensembles in order to compare similar processes
in differing environments. These problems call for an approach that is
analytically transparent, computationally efficient, and flexible enough to
handle the variety found in materials based research. In order to address these
problems, we introduce MolSieve, a progressive visual analytics system that
enables the comparison of multiple long-duration simulations. Using MolSieve,
analysts are able to quickly identify and compare regions of interest within
immense simulations through its combination of control charts, data-reduction
techniques, and highly informative visual components. A simple programming
interface is provided which allows experts to fit MolSieve to their needs. To
demonstrate the efficacy of our approach, we present two case studies of
MolSieve and report on findings from domain collaborators.",http://arxiv.org/pdf/2308.11724v1
2308.15326v1,physics.soc-ph,Dynamical heterogeneity and universality of power-grids,2023-08-29 14:21:58+00:00,"While weak, tuned asymmetry can improve, strong heterogeneity destroys
synchronization in the electric power system. We study the level of
heterogeneity, by comparing large high voltage (HV) power-grids of Europe and
North America. We provide an analysis of power capacities and loads of various
energy sources from the databases and found heavy tailed distributions with
similar characteristics. Graph topological measures, community structures also
exhibit strong similarities, while the cable admittance distributions can be
well fitted with the same power-laws (PL), related to the length distributions.
The community detection analysis shows the level of synchronization in
different domains of the European HV power grids, by solving a set of swing
equations. We provide numerical evidence for frustrated synchronization and
Chimera states and point out the relation of topology and level of
synchronization in the subsystems. We also provide empirical data analysis of
the frequency heterogeneities within the Hungarian HV network and find
q-Gaussian distributions related to super-statistics of time-lagged
fluctuations, which agree well with former results on the Nordic Grid.",http://arxiv.org/pdf/2308.15326v1
2308.15290v1,hep-ex,A flexible and efficient approach for missing transverse momentum reconstruction,2023-08-29 13:25:24+00:00,"Missing transverse momentum is a crucial observable for physics at hadron
colliders, being the only constraint on the kinematics of ""invisible"" objects
such as neutrinos and hypothetical dark matter particles. Computing missing
transverse momentum at the highest possible precision, particularly in
experiments at the energy frontier, can be a challenging procedure due to
ambiguities in the distribution of energy and momentum between many
reconstructed particle candidates. This paper describes a novel solution for
efficiently encoding information required for the computation of missing
transverse momentum given arbitrary selection criteria for the constituent
reconstructed objects. Pileup suppression using information from both the
calorimeter and the inner detector is an integral component of the
reconstruction procedure. Energy calibration and systematic variations are
naturally supported. Following this strategy, the ATLAS Collaboration has been
able to optimise the use of missing transverse momentum in diverse analyses
throughout Runs 2 and 3 of the Large Hadron Collider and for future analyses.",http://arxiv.org/pdf/2308.15290v1
2308.14407v1,physics.optics,Identifying topology of leaky photonic lattices with machine learning,2023-08-28 08:42:06+00:00,"We show how machine learning techniques can be applied for the classification
of topological phases in leaky photonic lattices using limited measurement
data. We propose an approach based solely on bulk intensity measurements, thus
exempt from the need for complicated phase retrieval procedures. In particular,
we design a fully connected neural network that accurately determines
topological properties from the output intensity distribution in dimerized
waveguide arrays with leaky channels, after propagation of a spatially
localized initial excitation at a finite distance, in a setting that closely
emulates realistic experimental conditions.",http://arxiv.org/pdf/2308.14407v1
2308.13636v1,cond-mat.stat-mech,Non-parametric learning critical behavior in Ising partition functions: PCA entropy and intrinsic dimension,2023-08-25 19:06:22+00:00,"We provide and critically analyze a framework to learn critical behavior in
classical partition functions through the application of non-parametric methods
to data sets of thermal configurations. We illustrate our approach in phase
transitions in 2D and 3D Ising models. First, we extend previous studies on the
intrinsic dimension of 2D partition function data sets, by exploring the effect
of volume in 3D Ising data. We find that as opposed to 2D systems for which
this quantity has been successfully used in unsupervised characterizations of
critical phenomena, in the 3D case its estimation is far more challenging. To
circumvent this limitation, we then use the principal component analysis (PCA)
entropy, a ``Shannon entropy'' of the normalized spectrum of the covariance
matrix. We find a striking qualitative similarity to the thermodynamic entropy,
which the PCA entropy approaches asymptotically. The latter allows us to
extract -- through a conventional finite-size scaling analysis with modest
lattice sizes -- the critical temperature with less than $1\%$ error for both
2D and 3D models while being computationally efficient. The PCA entropy can
readily be applied to characterize correlations and critical phenomena in a
huge variety of many-body problems and suggests a (direct) link between
easy-to-compute quantities and entropies.",http://arxiv.org/pdf/2308.13636v1
2308.13604v1,cond-mat.dis-nn,Network science Ising states of matter,2023-08-25 18:01:03+00:00,"Network science provides very powerful tools for extracting information from
interacting data. Although recently the unsupervised detection of phases of
matter using machine learning has raised significant interest, the full
prediction power of network science has not yet been systematically explored in
this context. Here we fill this gap by providing an in-depth statistical,
combinatorial, geometrical and topological characterization of 2D Ising
snapshot networks (IsingNets) extracted from Monte Carlo simulations of the 2D
Ising model at different temperatures, going across the phase transition. Our
analysis reveals the complex organization properties of IsingNets in both the
ferromagnetic and paramagnetic phases and demonstrates the significant
deviations of the IsingNets with respect to randomized null models. In
particular percolation properties of the IsingNets reflect the existence of the
symmetry between configurations with opposite magnetization below the critical
temperature and the very compact nature of the two emerging giant clusters
revealed by our persistent homology analysis of the IsingNets. Moreover, the
IsingNets display a very broad degree distribution and significant
degree-degree correlations and weight-degree correlations demonstrating that
they encode relevant information present in the configuration space of the 2D
Ising model. The geometrical organization of the critical IsingNets is
reflected in their spectral properties deviating from the one of the null
model. This work reveals the important insights that network science can bring
to the characterization of phases of matter. The set of tools described hereby
can be applied as well to numerical and experimental data.",http://arxiv.org/pdf/2308.13604v1
2308.13117v1,physics.data-an,Probabilistic Mixture Model-Based Spectral Unmixing,2023-08-24 23:46:16+00:00,"Identifying pure components in mixtures is a common yet challenging problem.
This unmixing process requires that mixing preserves the identity of the
components (endmembers), i.e., mixing is linear, and the endmembers must be
spectrally distinct. Even with these requirements met, extracting the
endmembers from a single mixture may be impossible; an ensemble of mixtures
with sufficient diversity is needed. Several spectral unmixing approaches have
been proposed, many of which are connected to hyperspectral imaging. However,
most of them assume highly diverse collections of mixtures and extremely
low-loss spectroscopic measurements. Additionally, these frameworks do not
incorporate the uncertainty inherent in unmixing. We propose a probabilistic
inference approach that explicitly incorporates noise and uncertainty, enabling
us to unmix endmembers in collections of mixtures with limited diversity. We
use a Bayesian mixture model to jointly extract endmember spectra and mixing
parameters while explicitly modeling observation noise and the resulting
inference uncertainties. We obtain approximate distributions over endmember
coordinates for each set of observed spectra while remaining robust to
inference biases from the lack of pure observations and presence of
non-isotropic Gaussian noise. Access to reliable uncertainties on the unmixing
solutions would enable robust solutions as well as informed decision making.",http://arxiv.org/pdf/2308.13117v1
2308.13028v1,quant-ph,Training Neural Networks with Universal Adiabatic Quantum Computing,2023-08-24 18:51:50+00:00,"The training of neural networks (NNs) is a computationally intensive task
requiring significant time and resources. This paper presents a novel approach
to NN training using Adiabatic Quantum Computing (AQC), a paradigm that
leverages the principles of adiabatic evolution to solve optimisation problems.
We propose a universal AQC method that can be implemented on gate quantum
computers, allowing for a broad range of Hamiltonians and thus enabling the
training of expressive neural networks. We apply this approach to various
neural networks with continuous, discrete, and binary weights. Our results
indicate that AQC can very efficiently find the global minimum of the loss
function, offering a promising alternative to classical training methods.",http://arxiv.org/pdf/2308.13028v1
2308.13027v1,quant-ph,Efficient characterization of blinking quantum emitters from scarce data sets via machine learning,2023-08-24 18:51:30+00:00,"Single photon emitters are core building blocks of quantum technologies, with
established and emerging applications ranging from quantum computing and
communication to metrology and sensing. Regardless of their nature, quantum
emitters universally display fluorescence intermittency or photoblinking:
interaction with the environment can cause the emitters to undergo quantum
jumps between on and off states that correlate with higher and lower
photoemission events, respectively. Understanding and quantifying the mechanism
and dynamics of photoblinking is important for both fundamental and practical
reasons. However, the analysis of blinking time traces is often afflicted by
data scarcity. Blinking emitters can photo-bleach and cease to fluoresce over
time scales that are too short for their photodynamics to be captured by
traditional statistical methods. Here, we demonstrate two approaches based on
machine learning that directly address this problem. We present a multi-feature
regression algorithm and a genetic algorithm that allow for the extraction of
blinking on/off switching rates with >85% accuracy, and with >10x less data and
>20x higher precision than traditional methods based on statistical inference.
Our algorithms effectively extend the range of surveyable blinking systems and
trapping dynamics to those that would otherwise be considered too short-lived
to be investigated. They are therefore a powerful tool to help gain a better
understanding of the physical mechanism of photoblinking, with practical
benefits for applications based on quantum emitters that rely on either
mitigating or harnessing the phenomenon.",http://arxiv.org/pdf/2308.13027v1
2308.12724v1,hep-ex,Jet energy calibration with deep learning as a Kubeflow pipeline,2023-08-24 12:02:09+00:00,"Precise measurements of the energy of jets emerging from particle collisions
at the LHC are essential for a vast majority of physics searches at the CMS
experiment. In this study, we leverage well-established deep learning models
for point clouds and CMS open data to improve the energy calibration of
particle jets. To enable production-ready machine learning based jet energy
calibration an end-to-end pipeline is built on the Kubeflow cloud platform. The
pipeline allowed us to scale up our hyperparameter tuning experiments on cloud
resources, and serve optimal models as REST endpoints. We present the results
of the parameter tuning process and analyze the performance of the served
models in terms of inference time and overhead, providing insights for future
work in this direction. The study also demonstrates improvements in both flavor
dependence and resolution of the energy response when compared to the standard
jet energy corrections baseline.",http://arxiv.org/pdf/2308.12724v1
2308.11952v1,physics.med-ph,iGLU 4.0: A continuous glucose monitoring and balancing paradigm with physiological parameters,2023-08-23 06:49:32+00:00,"The conventional method of glucose measurement such as pricking blood from
the body is prevalent which brings pain and trauma. Invasive methods of
measurement sometimes raise the risk of blood infection to the patient.
Sometimes, some of the physiological parameters such as body temperature and
systolic blood pressure (SBP) are responsible for blood glucose level
fluctuations. Moreover, diabetes for a long duration usually becomes a critical
issue. In such situation, patients need to consult diabetologist frequently,
which is not possible in normal life. Therefore, it is required to develop
non-invasive glucose balancing paradigm, which measures blood glucose without
pricking blood along with physiological parameters measurement and decision
model. The proposed paradigm helps to doctor, who is even available at remote
location. There will not be any need to consult frequently. In the way of
optimized non-invasive system design, an NIRS technique with specific
wavelengths along with physiological parameters is taken to predict the precise
glucose value. The all parameters (glucose, Blood pressure and body
temperature), food intake and insulin levels are parts of decision model, which
would help to the doctor to take decision related to the further medicine doses
and diet plan. The patients would have suggestions according to maintain their
blood glucose level. The proposed system demonstrated an accurate model with
MARD and AvgE 12.50% and 12.10% respectively using DNN model. Coefficient of
determination R2 has been found 0.97.",http://arxiv.org/pdf/2308.11952v1
2308.11763v1,physics.data-an,An efficient set-theoretic algorithm for high-order Forman-Ricci curvature,2023-08-22 20:09:53+00:00,"Differential geometric approaches are ubiquitous in several fields of
mathematics, physics and engineering, and their discretizations enable the
development of network-based mathematical and computational frameworks, which
are essential for large-scale data science. The Forman-Ricci curvature (FRC) -
a statistical measure based on Riemannian geometry and designed for networks -
is known for its high capacity for extracting geometric information from
complex networks. However, extracting information from dense networks is still
challenging due to the combinatorial explosion of high-order network
structures. Motivated by this challenge we sought a set-theoretic
representation theory for high-order network cells and FRC, as well as their
associated concepts and properties, which together provide an alternative and
efficient formulation for computing high-order FRC in complex networks. We
provide a pseudo-code, a software implementation coined FastForman, as well as
a benchmark comparison with alternative implementations. Crucially, our
representation theory reveals previous computational bottlenecks and also
accelerates the computation of FRC. As a consequence, our findings open new
research possibilities in complex systems where higher-order geometric
computations are required.",http://arxiv.org/pdf/2308.11763v1
2308.11700v1,physics.ins-det,SuperCalo: Calorimeter shower super-resolution,2023-08-22 18:00:00+00:00,"Calorimeter shower simulation is a major bottleneck in the Large Hadron
Collider computational pipeline. There have been recent efforts to employ
deep-generative surrogate models to overcome this challenge. However, many of
best performing models have training and generation times that do not scale
well to high-dimensional calorimeter showers. In this work, we introduce
SuperCalo, a flow-based super-resolution model, and demonstrate that
high-dimensional fine-grained calorimeter showers can be quickly upsampled from
coarse-grained showers. This novel approach presents a way to reduce
computational cost, memory requirements and generation time associated with
fast calorimeter simulation models. Additionally, we show that the showers
upsampled by SuperCalo possess a high degree of variation. This allows a large
number of high-dimensional calorimeter showers to be upsampled from much fewer
coarse showers with high-fidelity, which results in additional reduction in
generation time.",http://arxiv.org/pdf/2308.11700v1
2308.15127v1,physics.med-ph,Nanozyme-based biosensing for clinical diagnosis of COVID-19: A mini review,2023-08-29 08:51:38+00:00,"Several clinical methods had been utilized for diagnosis of COVID-19 for
instance, real-time reverse transcription-polymerase chain reaction (rRT-PCR),
hematology examination, polymerase chain reaction (PCR), diagnostic guidelines
based on clinical features, and Chest CT scans. However, the accurate current
methods are time-consuming and expensive and other methods are inaccurate. To
solve these drawbacks, nanozyme-based sensors have been developed for the
reliable, accurate, and rapid detection of SARS-CoV-2. The main basis of these
sensors is the detection of color variation of a nanozyme-mediated oxidation
reaction in the presence and the absence of antigens of COVID-19. Besides, some
of methods are based on probing the fluorescence of these systems as the
clinical signal toward detection of SARS-CoV-2. This mini review focused on
overviewing the nanozymes-based methods toward COVID-19 diagnosis. The
historical background of COVID-19 was reviewed. Thereafter, the biomedical
applications of nanozymes was discussed and finally, the recent progress of
early diagnosis of COVID-19 based on nanozymatic systems was briefly reviewed.",http://arxiv.org/pdf/2308.15127v1
2308.14942v1,q-bio.QM,Woolf et als GWAS by subtraction is not useful for cross-generational Mendelian randomization studies,2023-08-28 23:49:51+00:00,"Mendelian randomization (MR) is an epidemiological method that can be used to
strengthen causal inference regarding the relationship between a modifiable
environmental exposure and a medically relevant trait and to estimate the
magnitude of this relationship1. Recently, there has been considerable interest
in using MR to examine potential causal relationships between parental
phenotypes and outcomes amongst their offspring. In a recent issue of BMC
Research Notes, Woolf et al (2023) present a new method, GWAS by subtraction,
to derive genome-wide summary statistics for paternal smoking and other
paternal phenotypes with the goal that these estimates can then be used in
downstream (including two sample) MR studies. Whilst a potentially useful goal,
Woolf et al. (2023) focus on the wrong parameter of interest for useful
genome-wide association studies (GWAS) and downstream cross-generational MR
studies, and the estimator that they derive is neither efficient nor
appropriate for such use.",http://arxiv.org/pdf/2308.14942v1
2308.14869v1,q-bio.MN,PROSO Toolbox: a unified protein-constrained genome-scale modelling framework for strain designing and optimization,2023-08-28 19:44:55+00:00,"The genome-scale metabolic model with protein constraint (PC-model) has been
increasingly popular for microbial metabolic simulations. We present PROSO
Toolbox, a unified and simple-to-use PC-model toolbox that takes any
high-quality genome-scale metabolic reconstruction as the input. The toolbox
can construct a PC-model automatically, apply various algorithms for
computational strain design and simulation, and help unveil metabolism from
gene expression data through a state-of-the-art OVERLAY workflow. It also has
detailed tutorials and documentation for maximum accessibility to researchers
from diverse backgrounds. PROSO Toolbox, tutorials, and documentation are
freely available online: https://github.com/QCSB/PROSO-Toolbox.",http://arxiv.org/pdf/2308.14869v1
2308.14549v1,q-bio.QM,Computational modelling of peritoneal dialysis: an overview,2023-08-28 13:08:35+00:00,"Peritoneal dialysis (PD) is becoming more popular as a result of a rising
interest in home dialysis, lower intrusion in social life and longer
preservation of residual kidney function. However, PD has several important
drawbacks: small solute clearance is relatively low compared to hemodialysis
and technique survival is limited. Application of continuous flow,
sorbent-based dialysate regeneration and novel glucose-sparing PD solutions are
some solutions proposed to address the limitations of PD. To optimize and
personalize current and novel PD therapies, patient peritoneal characteristics
interacting with PD techniques need to be studied together and separately as
they interplay. However, considering the multitude of parameters, it would be
difficult, expensive, and time consuming to optimize all parameter settings
only with the help of clinical trials. Mathematical modelling is an exciting
tool to dissect these interacting processes and comprehend PD techniques better
at a patient specific level. In this review, we look at the history of
computational PD models, explore the many ways a computational PD model can be
constructed and review the various existing PD models that can be used to
optimize and personalize PD treatment.",http://arxiv.org/pdf/2308.14549v1
2308.14774v1,eess.AS,EEG-Derived Voice Signature for Attended Speaker Detection,2023-08-28 10:39:03+00:00,"\textit{Objective:} Conventional EEG-based auditory attention detection (AAD)
is achieved by comparing the time-varying speech stimuli and the elicited EEG
signals. However, in order to obtain reliable correlation values, these methods
necessitate a long decision window, resulting in a long detection latency.
Humans have a remarkable ability to recognize and follow a known speaker,
regardless of the spoken content. In this paper, we seek to detect the attended
speaker among the pre-enrolled speakers from the elicited EEG signals. In this
manner, we avoid relying on the speech stimuli for AAD at run-time. In doing
so, we propose a novel EEG-based attended speaker detection (E-ASD) task.
\textit{Methods:} We encode a speaker's voice with a fixed dimensional vector,
known as speaker embedding, and project it to an audio-derived voice signature,
which characterizes the speaker's unique voice regardless of the spoken
content. We hypothesize that such a voice signature also exists in the
listener's brain that can be decoded from the elicited EEG signals, referred to
as EEG-derived voice signature. By comparing the audio-derived voice signature
and the EEG-derived voice signature, we are able to effectively detect the
attended speaker in the listening brain. \textit{Results:} Experiments show
that E-ASD can effectively detect the attended speaker from the 0.5s EEG
decision windows, achieving 99.78\% AAD accuracy, 99.94\% AUC, and 0.27\% EER.
\textit{Conclusion:} We conclude that it is possible to derive the attended
speaker's voice signature from the EEG signals so as to detect the attended
speaker in a listening brain. \textit{Significance:} We present the first proof
of concept for detecting the attended speaker from the elicited EEG signals in
a cocktail party environment. The successful implementation of E-ASD marks a
non-trivial, but crucial step towards smart hearing aids.",http://arxiv.org/pdf/2308.14774v1
2308.13891v1,cs.LG,Drug Interaction Vectors Neural Network: DrIVeNN,2023-08-26 14:24:41+00:00,"Polypharmacy, the concurrent use of multiple drugs to treat a single
condition, is common in patients managing multiple or complex conditions.
However, as more drugs are added to the treatment plan, the risk of adverse
drug events (ADEs) rises rapidly. Many serious ADEs associated with
polypharmacy only become known after the drugs are in use. It is impractical to
test every possible drug combination during clinical trials. This issue is
particularly prevalent among older adults with cardiovascular disease (CVD)
where polypharmacy and ADEs are commonly observed. In this research, our
primary objective was to identify key drug features to build and evaluate a
model for modeling polypharmacy ADEs. Our secondary objective was to assess our
model on a domain-specific case study. We developed a two-layer neural network
that incorporated drug features such as molecular structure, drug-protein
interactions, and mono drug side effects (DrIVeNN). We assessed DrIVeNN using
publicly available side effect databases and determined Principal Component
Analysis (PCA) with a variance threshold of 0.95 as the most effective feature
selection method. DrIVeNN performed moderately better than state-of-the-art
models like RESCAL, DEDICOM, DeepWalk, Decagon, DeepDDI, KGDDI, and KGNN in
terms of AUROC for the drug-drug interaction prediction task. We also conducted
a domain-specific case study centered on the treatment of cardiovascular
disease (CVD). When the best performing model architecture was applied to the
CVD treatment cohort, there was a significant increase in performance from the
general model. We observed an average AUROC for CVD drug pair prediction
increasing from 0.826 (general model) to 0.975 (CVD specific model). Our
findings indicate the strong potential of domain-specific models for improving
the accuracy of drug-drug interaction predictions.",http://arxiv.org/pdf/2308.13891v1
2308.13304v1,eess.IV,Bang and the Artefacts are Gone! Rapid Artefact Removal and Tissue Segmentation in Haematoxylin and Eosin Stained Biopsies,2023-08-25 11:04:35+00:00,"We present H&E Otsu thresholding, a scheme for rapidly detecting tissue in
whole-slide images (WSIs) that eliminates a wide range of undesirable artefacts
such as pen marks and scanning artefacts. Our method involves obtaining a
bid-modal representation of a low-magnification RGB overview image which
enables simple Otsu thresholding to separate tissue from background and
artefacts. We demonstrate our method on WSIs prepared from a wide range of
institutions and WSI digital scanners, each containing substantial artefacts
that cause other methods to fail. The beauty of our approach lies in its
simplicity: manipulating RGB colour space and using Otsu thresholding allows
for the rapid removal of artefacts and segmentation of tissue.",http://arxiv.org/pdf/2308.13304v1
2308.13182v1,cs.CV,Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon,2023-08-25 05:24:23+00:00,"With the advent of digital scanners and deep learning, diagnostic operations
may move from a microscope to a desktop. Hematoxylin and Eosin (H&E) staining
is one of the most frequently used stains for disease analysis, diagnosis, and
grading, but pathologists do need different immunohistochemical (IHC) stains to
analyze specific structures or cells. Obtaining all of these stains (H&E and
different IHCs) on a single specimen is a tedious and time-consuming task.
Consequently, virtual staining has emerged as an essential research direction.
Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for
synthesizing IHC stains from H&E images, and vice versa. Our method expressly
incorporates structural information in the form of edges (in addition to color
data) and employs attention modules exclusively in the decoder of the proposed
generator model. This integration enhances feature localization and preserves
contextual information during the generation process. In addition, a structural
loss is incorporated to ensure accurate structure alignment between the
generated and input markers. To demonstrate the efficacy of the proposed model,
experiments are conducted with two IHC markers emphasizing distinct structures
of glands in the colon: the nucleus of epithelial cells (CDX2) and the
cytoplasm (CK818). Quantitative metrics such as FID and SSIM are frequently
used for the analysis of generative models, but they do not correlate
explicitly with higher-quality virtual staining results. Therefore, we propose
two new quantitative metrics that correlate directly with the virtual staining
specificity of IHC markers.",http://arxiv.org/pdf/2308.13182v1
2308.13171v1,quant-ph,Q-Drug: a Framework to bring Drug Design into Quantum Space using Deep Learning,2023-08-25 04:26:02+00:00,"Optimizing the properties of molecules (materials or drugs) for stronger
toughness, lower toxicity, or better bioavailability has been a long-standing
challenge. In this context, we propose a molecular optimization framework
called Q-Drug (Quantum-inspired optimization algorithm for Drugs) that
leverages quantum-inspired algorithms to optimize molecules on discrete binary
domain variables. The framework begins by encoding the molecules into binary
embeddings using a discrete VAE. The binary embeddings are then used to
construct an Ising energy-like objective function, over which the
state-of-the-art quantum-inspired optimization algorithm is adopted to find the
optima. The binary embeddings corresponding to the optima are decoded to obtain
the optimized molecules. We have tested the framework for optimizing drug
molecule properties and have found that it outperforms other molecular
optimization methods, finding molecules with better properties in 1/20th to
1/10th of the time previously required. The framework can also be deployed
directly on various quantum computing equipment, such as laser pulses CIMs,
FPGA Ising Machines, and quantum computers based on quantum annealing, among
others. Our work demonstrates a new paradigm that leverages the advantages of
quantum computing and AI to solve practically useful problems.",http://arxiv.org/pdf/2308.13171v1
2308.13066v1,cs.LG,Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE,2023-08-24 20:22:22+00:00,"Variational autoencoder (VAE) is a popular method for drug discovery and
various architectures and pipelines have been proposed to improve its
performance. However, VAE approaches are known to suffer from poor manifold
recovery when the data lie on a low-dimensional manifold embedded in a higher
dimensional ambient space [Dai and Wipf, 2019]. The consequences of it in drug
discovery are somewhat under-explored. In this paper, we explore applying a
multi-stage VAE approach, that can improve manifold recovery on a synthetic
dataset, to the field of drug discovery. We experimentally evaluate our
multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability
to improve the property statistics of generated molecules substantially from
pre-existing methods without incorporating property predictors into the
training pipeline. We further fine-tune our models on two curated and much
smaller molecule datasets that target different proteins. Our experiments show
an increase in the number of active molecules generated by the multi-stage VAE
in comparison to their one-stage equivalent. For each of the two tasks, our
baselines include methods that use learned property predictors to incorporate
target metrics directly into the training objective and we discuss
complications that arise with this methodology.",http://arxiv.org/pdf/2308.13066v1
2308.13035v1,q-bio.QM,The intersection of video capsule endoscopy and artificial intelligence: addressing unique challenges using machine learning,2023-08-24 19:00:26+00:00,"Introduction: Technical burdens and time-intensive review processes limit the
practical utility of video capsule endoscopy (VCE). Artificial intelligence
(AI) is poised to address these limitations, but the intersection of AI and VCE
reveals challenges that must first be overcome. We identified five challenges
to address. Challenge #1: VCE data are stochastic and contains significant
artifact. Challenge #2: VCE interpretation is cost-intensive. Challenge #3: VCE
data are inherently imbalanced. Challenge #4: Existing VCE AIMLT are
computationally cumbersome. Challenge #5: Clinicians are hesitant to accept
AIMLT that cannot explain their process.
  Methods: An anatomic landmark detection model was used to test the
application of convolutional neural networks (CNNs) to the task of classifying
VCE data. We also created a tool that assists in expert annotation of VCE data.
We then created more elaborate models using different approaches including a
multi-frame approach, a CNN based on graph representation, and a few-shot
approach based on meta-learning.
  Results: When used on full-length VCE footage, CNNs accurately identified
anatomic landmarks (99.1%), with gradient weighted-class activation mapping
showing the parts of each frame that the CNN used to make its decision. The
graph CNN with weakly supervised learning (accuracy 89.9%, sensitivity of
91.1%), the few-shot model (accuracy 90.8%, precision 91.4%, sensitivity
90.9%), and the multi-frame model (accuracy 97.5%, precision 91.5%, sensitivity
94.8%) performed well. Discussion: Each of these five challenges is addressed,
in part, by one of our AI-based models. Our goal of producing high performance
using lightweight models that aim to improve clinician confidence was achieved.",http://arxiv.org/pdf/2308.13035v1
2308.12780v1,physics.bio-ph,The motility-matrix production switch in Bacillus subtilis -- a modeling perspective,2023-08-24 13:34:01+00:00,"Phenotype switching can be triggered by external stimuli and by intrinsic
stochasticity. Here, we focus on the motility-matrix production switch in
Bacillus subtilis. We use modeling to describe the SinR-SlrR bistable switch
its regulation by SinI, and to distinguish different sources of stochasticity.
Our simulations indicate that intrinsic fluctuations in the synthesis of SinI
are insufficient to drive spontaneous switching and suggest that switching is
triggered by upstream noise from the Spo0A phosphorelay.",http://arxiv.org/pdf/2308.12780v1
2308.12740v1,cs.AI,Human Comprehensible Active Learning of Genome-Scale Metabolic Networks,2023-08-24 12:42:00+00:00,"An important application of Synthetic Biology is the engineering of the host
cell system to yield useful products. However, an increase in the scale of the
host system leads to huge design space and requires a large number of
validation trials with high experimental costs. A comprehensible machine
learning approach that efficiently explores the hypothesis space and guides
experimental design is urgently needed for the Design-Build-Test-Learn (DBTL)
cycle of the host cell system. We introduce a novel machine learning framework
ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive
logical reasoning and actively learns from training examples. In contrast to
numerical models, ILP-iML1515 is built on comprehensible logical
representations of a genome-scale metabolic model and can update the model by
learning new logical structures from auxotrophic mutant trials. The ILP-iML1515
framework 1) allows high-throughput simulations and 2) actively selects
experiments that reduce the experimental cost of learning gene functions in
comparison to randomly selected experiments.",http://arxiv.org/pdf/2308.12740v1
2308.12416v1,eess.IV,Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach,2023-08-23 20:33:22+00:00,"Deep learning models have achieved state-of-the-art results in estimating
brain age, which is an important brain health biomarker, from magnetic
resonance (MR) images. However, most of these models only provide a global age
prediction, and rely on techniques, such as saliency maps to interpret their
results. These saliency maps highlight regions in the input image that were
significant for the model's predictions, but they are hard to be interpreted,
and saliency map values are not directly comparable across different samples.
In this work, we reframe the age prediction problem from MR images to an
image-to-image regression problem where we estimate the brain age for each
brain voxel in MR images. We compare voxel-wise age prediction models against
global age prediction models and their corresponding saliency maps. The results
indicate that voxel-wise age prediction models are more interpretable, since
they provide spatial information about the brain aging process, and they
benefit from being quantitative.",http://arxiv.org/pdf/2308.12416v1
2308.12224v1,q-bio.QM,Enhancing cardiovascular risk prediction through AI-enabled calcium-omics,2023-08-23 16:05:14+00:00,"Background. Coronary artery calcium (CAC) is a powerful predictor of major
adverse cardiovascular events (MACE). Traditional Agatston score simply sums
the calcium, albeit in a non-linear way, leaving room for improved
calcification assessments that will more fully capture the extent of disease.
  Objective. To determine if AI methods using detailed calcification features
(i.e., calcium-omics) can improve MACE prediction.
  Methods. We investigated additional features of calcification including
assessment of mass, volume, density, spatial distribution, territory, etc. We
used a Cox model with elastic-net regularization on 2457 CT calcium score
(CTCS) enriched for MACE events obtained from a large no-cost CLARIFY program
(ClinicalTri-als.gov Identifier: NCT04075162). We employed sampling techniques
to enhance model training. We also investigated Cox models with selected
features to identify explainable high-risk characteristics.
  Results. Our proposed calcium-omics model with modified synthetic down
sampling and up sampling gave C-index (80.5%/71.6%) and two-year AUC
(82.4%/74.8%) for (80:20, training/testing), respectively (sampling was applied
to the training set only). Results compared favorably to Agatston which gave
C-index (71.3%/70.3%) and AUC (71.8%/68.8%), respectively. Among calcium-omics
features, numbers of calcifications, LAD mass, and diffusivity (a measure of
spatial distribution) were important determinants of increased risk, with dense
calcification (>1000HU) associated with lower risk. The calcium-omics model
reclassified 63% of MACE patients to the high risk group in a held-out test.
The categorical net-reclassification index was NRI=0.153.
  Conclusions. AI analysis of coronary calcification can lead to improved
results as compared to Agatston scoring. Our findings suggest the utility of
calcium-omics in improved prediction of risk.",http://arxiv.org/pdf/2308.12224v1
2308.12325v1,q-bio.QM,Predicting Drug Solubility Using Different Machine Learning Methods -- Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network,2023-08-23 15:35:20+00:00,"Predicting the solubility of given molecules is an important task in the
pharmaceutical industry, and consequently this is a well-studied topic. In this
research, we revisited this problem with the advantage of modern computing
resources. We applied two machine learning models, a linear regression model
and a graph convolutional neural network model, on multiple experimental
datasets. Both methods can make reasonable predictions while the GCNN model had
the best performance. However, the current GCNN model is a black box, while
feature importance analysis from the linear regression model offers more
insights into the underlying chemical influences. Using the linear regression
model, we show how each functional group affects the overall solubility.
Ultimately, knowing how chemical structure influences chemical properties is
crucial when designing new drugs. Future work should aim to combine the high
performance of GCNNs with the interpretability of linear regression, unlocking
new advances in next generation high throughput screening.",http://arxiv.org/pdf/2308.12325v1
2308.12188v1,cs.LG,Development and external validation of a lung cancer risk estimation tool using gradient-boosting,2023-08-23 15:25:17+00:00,"Lung cancer is a significant cause of mortality worldwide, emphasizing the
importance of early detection for improved survival rates. In this study, we
propose a machine learning (ML) tool trained on data from the PLCO Cancer
Screening Trial and validated on the NLST to estimate the likelihood of lung
cancer occurrence within five years. The study utilized two datasets, the PLCO
(n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk
factors, clinical measurements, and outcomes related to lung cancer. Data
preprocessing involved removing patients who were not current or former smokers
and those who had died of causes unrelated to lung cancer. Additionally, a
focus was placed on mitigating bias caused by censored data. Feature selection,
hyper-parameter optimization, and model calibration were performed using
XGBoost, an ensemble learning algorithm that combines gradient boosting and
decision trees. The ML model was trained on the pre-processed PLCO dataset and
tested on the NLST dataset. The model incorporated features such as age,
gender, smoking history, medical diagnoses, and family history of lung cancer.
The model was well-calibrated (Brier score=0.044). ROC-AUC was 82% on the PLCO
dataset and 70% on the NLST dataset. PR-AUC was 29% and 11% respectively. When
compared to the USPSTF guidelines for lung cancer screening, our model provided
the same recall with a precision of 13.1% vs. 9.3% on the PLCO dataset and 3.2%
vs. 3.1% on the NLST dataset. The developed ML tool provides a freely available
web application for estimating the likelihood of developing lung cancer within
five years. By utilizing risk factors and clinical data, individuals can assess
their risk and make informed decisions regarding lung cancer screening. This
research contributes to the efforts in early detection and prevention
strategies, aiming to reduce lung cancer-related mortality rates.",http://arxiv.org/pdf/2308.12188v1
2308.11969v1,eess.IV,Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification,2023-08-23 07:30:16+00:00,"The burden of liver tumors is important, ranking as the fourth leading cause
of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation
of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is
performed to guide the treatment strategy. As this task is time-consuming,
needs high expertise and could be subject to inter-observer variability there
is a strong need for automatic tools. However, challenges arise from the lack
of available training data, as well as the high variability in terms of image
resolution and MRI sequence. In this work we propose to compare two different
pipelines based on anisotropic models to obtain the segmentation of the liver
and tumors. The first pipeline corresponds to a baseline multi-class model that
performs the simultaneous segmentation of the liver and tumor classes. In the
second approach, we train two distinct binary models, one segmenting the liver
only and the other the tumors. Our results show that both pipelines exhibit
different strengths and weaknesses. Moreover we propose an uncertainty
quantification strategy allowing the identification of potential false positive
tumor lesions. Both solutions were submitted to the MICCAI 2023 Atlas challenge
regarding liver and tumor segmentation.",http://arxiv.org/pdf/2308.11969v1
2308.11927v1,q-bio.QM,Recovering a Molecule's 3D Dynamics from Liquid-phase Electron Microscopy Movies,2023-08-23 05:26:27+00:00,"The dynamics of biomolecules are crucial for our understanding of their
functioning in living systems. However, current 3D imaging techniques, such as
cryogenic electron microscopy (cryo-EM), require freezing the sample, which
limits the observation of their conformational changes in real time. The
innovative liquid-phase electron microscopy (liquid-phase EM) technique allows
molecules to be placed in the native liquid environment, providing a unique
opportunity to observe their dynamics. In this paper, we propose TEMPOR, a
Temporal Electron MicroscoPy Object Reconstruction algorithm for liquid-phase
EM that leverages an implicit neural representation (INR) and a dynamical
variational auto-encoder (DVAE) to recover time series of molecular structures.
We demonstrate its advantages in recovering different motion dynamics from two
simulated datasets, 7bcq and Cas9. To our knowledge, our work is the first
attempt to directly recover 3D structures of a temporally-varying particle from
liquid-phase EM movies. It provides a promising new approach for studying
molecules' 3D dynamics in structural biology.",http://arxiv.org/pdf/2308.11927v1
2308.11846v1,nlin.PS,A Data-Driven Approach to Morphogenesis under Structural Instability,2023-08-23 00:51:43+00:00,"Morphological development into evolutionary patterns under structural
instability is ubiquitous in living systems and often of vital importance for
engineering structures. Here we propose a data-driven approach to understand
and predict their spatiotemporal complexities. A machine-learning framework is
proposed based on the physical modeling of morphogenesis triggered by internal
or external forcing. Digital libraries of structural patterns are constructed
from the simulation data, which are then used to recognize the abnormalities,
predict their development, and assist in risk assessment and prognosis. The
capabilities to identify the key bifurcation characteristics and predict the
history-dependent development from the global and local features are
demonstrated by examples of brain growth and aerospace structural design, which
offer guidelines for disease diagnosis/prognosis and instability-tolerant
design.",http://arxiv.org/pdf/2308.11846v1
2308.11773v1,cs.CL,Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model,2023-08-22 20:30:59+00:00,"Language use has been shown to correlate with depression, but large-scale
validation is needed. Traditional methods like clinic studies are expensive.
So, natural language processing has been employed on social media to predict
depression, but limitations remain-lack of validated labels, biased user
samples, and no context. Our study identified 29 topics in 3919
smartphone-collected speech recordings from 265 participants using the Whisper
tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal
to 10 were regarded as risk topics for depression: No Expectations, Sleep,
Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic
emergence and associations with depression, we compared behavioral (from
wearables) and linguistic characteristics across identified topics. The
correlation between topic shifts and changes in depression severity over time
was also investigated, indicating the importance of longitudinally monitoring
language use. We also tested the BERTopic model on a similar smaller dataset
(356 speech recordings from 57 participants), obtaining some consistent
results. In summary, our findings demonstrate specific speech topics may
indicate depression severity. The presented data-driven workflow provides a
practical approach to collecting and analyzing large-scale speech data from
real-world settings for digital health research.",http://arxiv.org/pdf/2308.11773v1
2308.15443v1,q-fin.ST,Combining predictive distributions of electricity prices: Does minimizing the CRPS lead to optimal decisions in day-ahead bidding?,2023-08-29 17:10:38+00:00,"Probabilistic price forecasting has recently gained attention in power
trading because decisions based on such predictions can yield significantly
higher profits than those made with point forecasts alone. At the same time,
methods are being developed to combine predictive distributions, since no model
is perfect and averaging generally improves forecasting performance. In this
article we address the question of whether using CRPS learning, a novel
weighting technique minimizing the continuous ranked probability score (CRPS),
leads to optimal decisions in day-ahead bidding. To this end, we conduct an
empirical study using hourly day-ahead electricity prices from the German EPEX
market. We find that increasing the diversity of an ensemble can have a
positive impact on accuracy. At the same time, the higher computational cost of
using CRPS learning compared to an equal-weighted aggregation of distributions
is not offset by higher profits, despite significantly more accurate
predictions.",http://arxiv.org/pdf/2308.15443v1
2308.14952v1,stat.CO,Stochastic Variational Inference for GARCH Models,2023-08-29 00:49:47+00:00,"Stochastic variational inference algorithms are derived for fitting various
heteroskedastic time series models. We examine Gaussian, t, and skew-t response
GARCH models and fit these using Gaussian variational approximating densities.
We implement efficient stochastic gradient ascent procedures based on the use
of control variates or the reparameterization trick and demonstrate that the
proposed implementations provide a fast and accurate alternative to Markov
chain Monte Carlo sampling. Additionally, we present sequential updating
versions of our variational algorithms, which are suitable for efficient
portfolio construction and dynamic asset allocation.",http://arxiv.org/pdf/2308.14952v1
2308.14830v1,stat.AP,COVID anomaly in the correlation analysis of S&P 500 market states,2023-08-28 18:29:48+00:00,"Analyzing market states of the S&P 500 components on a time horizon January
3, 2006 to August 10, 2023, we found the appearance of a new market state not
previously seen and we shall discuss its possible implications as an isolated
state or as a beginning of a new general market condition. We study this in
terms of the Pearson correlation matrix and relative correlation with respect
to the S&P 500 index. In both cases the anomaly shows strongly.",http://arxiv.org/pdf/2308.14830v1
2308.14671v1,stat.ME,A generalized Bayesian stochastic block model for microbiome community detection,2023-08-28 15:57:59+00:00,"Advances in next-generation sequencing technology have enabled the
high-throughput profiling of metagenomes and accelerated the microbiome study.
Recently, there has been a rise in quantitative studies that aim to decipher
the microbiome co-occurrence network and its underlying community structure
based on metagenomic sequence data. Uncovering the complex microbiome community
structure is essential to understanding the role of the microbiome in disease
progression and susceptibility. Taxonomic abundance data generated from
metagenomic sequencing technologies are high-dimensional and compositional,
suffering from uneven sampling depth, over-dispersion, and zero-inflation.
These characteristics often challenge the reliability of the current methods
for microbiome community detection. To this end, we propose a Bayesian
stochastic block model to study the microbiome co-occurrence network based on
the recently developed modified centered-log ratio transformation tailored for
microbiome data analysis. Our model allows us to incorporate taxonomic tree
information using a Markov random field prior. The model parameters are jointly
inferred by using Markov chain Monte Carlo sampling techniques. Our simulation
study showed that the proposed approach performs better than competing methods
even when taxonomic tree information is non-informative. We applied our
approach to a real urinary microbiome dataset from postmenopausal women, the
first time the urinary microbiome co-occurrence network structure has been
studied. In summary, this statistical methodology provides a new tool for
facilitating advanced microbiome studies.",http://arxiv.org/pdf/2308.14671v1
2308.14106v1,stat.CO,Diffusion Schrödinger Bridges for Bayesian Computation,2023-08-27 13:22:55+00:00,"Denoising diffusion models are a novel class of generative models that have
recently become extremely popular in machine learning. In this paper, we
describe how such ideas can also be used to sample from posterior distributions
and, more generally, any target distribution whose density is known up to a
normalizing constant. The key idea is to consider a forward ``noising''
diffusion initialized at the target distribution which ``transports'' this
latter to a normal distribution for long diffusion times. The time-reversal of
this process, the ``denoising'' diffusion, thus ``transports'' the normal
distribution to the target distribution and can be approximated so as to sample
from the target. To accelerate simulation, we show how one can introduce and
approximate a Schr\""{o}dinger bridge between these two distributions, i.e. a
diffusion which transports the normal to the target in finite time.",http://arxiv.org/pdf/2308.14106v1
2308.14048v1,stat.ML,A Bayesian Non-parametric Approach to Generative Models: Integrating Variational Autoencoder and Generative Adversarial Networks using Wasserstein and Maximum Mean Discrepancy,2023-08-27 08:58:31+00:00,"Generative models have emerged as a promising technique for producing
high-quality images that are indistinguishable from real images. Generative
adversarial networks (GANs) and variational autoencoders (VAEs) are two of the
most prominent and widely studied generative models. GANs have demonstrated
excellent performance in generating sharp realistic images and VAEs have shown
strong abilities to generate diverse images. However, GANs suffer from ignoring
a large portion of the possible output space which does not represent the full
diversity of the target distribution, and VAEs tend to produce blurry images.
To fully capitalize on the strengths of both models while mitigating their
weaknesses, we employ a Bayesian non-parametric (BNP) approach to merge GANs
and VAEs. Our procedure incorporates both Wasserstein and maximum mean
discrepancy (MMD) measures in the loss function to enable effective learning of
the latent space and generate diverse and high-quality samples. By fusing the
discriminative power of GANs with the reconstruction capabilities of VAEs, our
novel model achieves superior performance in various generative tasks, such as
anomaly detection and data augmentation. Furthermore, we enhance the model's
capability by employing an extra generator in the code space, which enables us
to explore areas of the code space that the VAE might have overlooked. With a
BNP perspective, we can model the data distribution using an
infinite-dimensional space, which provides greater flexibility in the model and
reduces the risk of overfitting. By utilizing this framework, we can enhance
the performance of both GANs and VAEs to create a more robust generative model
suitable for various applications.",http://arxiv.org/pdf/2308.14048v1
2308.13928v2,stat.ME,A flexible Bayesian tool for CoDa mixed models: logistic-normal distribution with Dirichlet covariance,2023-08-26 18:02:15+00:00,"Compositional Data Analysis (CoDa) has gained popularity in recent years.
This type of data consists of values from disjoint categories that sum up to a
constant. Both Dirichlet regression and logistic-normal regression have become
popular as CoDa analysis methods. However, fitting this kind of multivariate
models presents challenges, especially when structured random effects are
included in the model, such as temporal or spatial effects.
  To overcome these challenges, we propose the logistic-normal Dirichlet Model
(LNDM). We seamlessly incorporate this approach into the R-INLA package,
facilitating model fitting and model prediction within the framework of Latent
Gaussian Models (LGMs). Moreover, we explore metrics like Deviance Information
Criteria (DIC), Watanabe Akaike information criterion (WAIC), and
cross-validation measure conditional predictive ordinate (CPO) for model
selection in R-INLA for CoDa.
  Illustrating LNDM through a simple simulated example and with an ecological
case study on Arabidopsis thaliana in the Iberian Peninsula, we underscore its
potential as an effective tool for managing CoDa and large CoDa databases.",http://arxiv.org/pdf/2308.13928v2
2308.13630v1,stat.ME,Degrees of Freedom: Search Cost and Self-consistency,2023-08-25 18:55:10+00:00,"Model degrees of freedom ($\df$) is a fundamental concept in statistics
because it quantifies the flexibility of a fitting procedure and is
indispensable in model selection. The $\df$ is often intuitively equated with
the number of independent variables in the fitting procedure. But for adaptive
regressions that perform variable selection (e.g., the best subset
regressions), the model $\df$ is larger than the number of selected variables.
The excess part has been defined as the \emph{search degrees of freedom}
($\sdf$) to account for model selection. However, this definition is limited
since it does not consider fitting procedures in augmented space, such as
splines and regression trees; and it does not use the same fitting procedure
for $\sdf$ and $\df$. For example, the lasso's $\sdf$ is defined through the
\emph{relaxed} lasso's $\df$ instead of the lasso's $\df$.
  Here we propose a \emph{modified search degrees of freedom} ($\msdf$) to
directly account for the cost of searching in the original or augmented space.
Since many fitting procedures can be characterized by a linear operator, we
define the search cost as the effort to determine such a linear operator. When
we construct a linear operator for the lasso via the iterative ridge
regression, $\msdf$ offers a new perspective for its search cost. For some
complex procedures such as the multivariate adaptive regression splines (MARS),
the search cost needs to be pre-determined to serve as a tuning parameter for
the procedure itself, but it might be inaccurate. To investigate the inaccurate
pre-determined search cost, we develop two concepts, \emph{nominal} $\df$ and
\emph{actual} $\df$, and formulate a property named \emph{self-consistency}
when there is no gap between the \emph{nominal} $\df$ and the \emph{actual}
$\df$.",http://arxiv.org/pdf/2308.13630v1
2308.13564v1,econ.EM,SGMM: Stochastic Approximation to Generalized Method of Moments,2023-08-25 00:22:45+00:00,"We introduce a new class of algorithms, Stochastic Generalized Method of
Moments (SGMM), for estimation and inference on (overidentified) moment
restriction models. Our SGMM is a novel stochastic approximation alternative to
the popular Hansen (1982) (offline) GMM, and offers fast and scalable
implementation with the ability to handle streaming datasets in real time. We
establish the almost sure convergence, and the (functional) central limit
theorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we
propose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that
can be seamlessly integrated within the SGMM framework. Extensive Monte Carlo
simulations show that as the sample size increases, the SGMM matches the
standard (offline) GMM in terms of estimation accuracy and gains over
computational efficiency, indicating its practical value for both large-scale
and online datasets. We demonstrate the efficacy of our approach by a proof of
concept using two well known empirical examples with large sample sizes.",http://arxiv.org/pdf/2308.13564v1
2308.13068v1,cs.LG,Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology,2023-08-24 20:24:12+00:00,"Multivariate Time Series (MVTS) anomaly detection is a long-standing and
challenging research topic that has attracted tremendous research effort from
both industry and academia recently. However, a careful study of the literature
makes us realize that 1) the community is active but not as organized as other
sibling machine learning communities such as Computer Vision (CV) and Natural
Language Processing (NLP), and 2) most proposed solutions are evaluated using
either inappropriate or highly flawed protocols, with an apparent lack of
scientific foundation. So flawed is one very popular protocol, the so-called
\pa protocol, that a random guess can be shown to systematically outperform
\emph{all} algorithms developed so far. In this paper, we review and evaluate
many recent algorithms using more robust protocols and discuss how a normally
good protocol may have weaknesses in the context of MVTS anomaly detection and
how to mitigate them. We also share our concerns about benchmark datasets,
experiment design and evaluation methodology we observe in many works.
Furthermore, we propose a simple, yet challenging, baseline algorithm based on
Principal Components Analysis (PCA) that surprisingly outperforms many recent
Deep Learning (DL) based approaches on popular benchmark datasets. The main
objective of this work is to stimulate more effort towards important aspects of
the research such as data, experiment design, evaluation methodology and result
interpretability, instead of putting the highest weight on the design of
increasingly more complex and ""fancier"" algorithms.",http://arxiv.org/pdf/2308.13068v1
2308.13033v1,stat.CO,"A Strength and Sparsity Preserving Algorithm for Generating Weighted, Directed Networks with Predetermined Assortativity",2023-08-24 18:59:20+00:00,"Degree-preserving rewiring is a widely used technique for generating
unweighted networks with given assortativity, but for weighted networks, it is
unclear how an analog would preserve the strengths and other critical network
features such as sparsity level. This study introduces a novel approach for
rewiring weighted networks to achieve desired directed assortativity. The
method utilizes a mixed integer programming framework to establish a target
network with predetermined assortativity coefficients, followed by an efficient
rewiring algorithm termed ""strength and sparsity preserving rewiring"" (SSPR).
SSPR retains the node strength distributions and network sparsity after
rewiring. It is also possible to accommodate additional properties like edge
weight distribution with extra computational cost. The optimization scheme can
be used to determine feasible assortativity ranges for an initial network. The
effectiveness of the proposed SSPR algorithm is demonstrated through its
application to two classes of popular network models.",http://arxiv.org/pdf/2308.13033v1
2308.12944v1,quant-ph,Parallel-in-time quantum simulation via Page and Wootters quantum time,2023-08-24 17:32:41+00:00,"In the past few decades, researchers have created a veritable zoo of quantum
algorithm by drawing inspiration from classical computing, information theory,
and even from physical phenomena. Here we present quantum algorithms for
parallel-in-time simulations that are inspired by the Page and Wooters
formalism. In this framework, and thus in our algorithms, the classical
time-variable of quantum mechanics is promoted to the quantum realm by
introducing a Hilbert space of ""clock"" qubits which are then entangled with the
""system"" qubits. We show that our algorithms can compute temporal properties
over $N$ different times of many-body systems by only using $\log(N)$ clock
qubits. As such, we achieve an exponential trade-off between time and spatial
complexities. In addition, we rigorously prove that the entanglement created
between the system qubits and the clock qubits has operational meaning, as it
encodes valuable information about the system's dynamics. We also provide a
circuit depth estimation of all the protocols, showing an exponential advantage
in computation times over traditional sequential in time algorithms. In
particular, for the case when the dynamics are determined by the Aubry-Andre
model, we present a hybrid method for which our algorithms have a depth that
only scales as $\mathcal{O}(\log(N)n)$. As a by product we can relate the
previous schemes to the problem of equilibration of an isolated quantum system,
thus indicating that our framework enable a new dimension for studying
dynamical properties of many-body systems.",http://arxiv.org/pdf/2308.12944v1
2308.12470v1,stat.ME,Scalable Estimation of Multinomial Response Models with Uncertain Consideration Sets,2023-08-23 23:48:47+00:00,"A standard assumption in the fitting of unordered multinomial response models
for J mutually exclusive nominal categories, on cross-sectional or longitudinal
data, is that the responses arise from the same set of J categories between
subjects. However, when responses measure a choice made by the subject, it is
more appropriate to assume that the distribution of multinomial responses is
conditioned on a subject-specific consideration set, where this consideration
set is drawn from the power set of {1,2,...,J}. Because the cardinality of this
power set is exponential in J, estimation is infeasible in general. In this
paper, we provide an approach to overcoming this problem. A key step in the
approach is a probability model over consideration sets, based on a general
representation of probability distributions on contingency tables. Although the
support of this distribution is exponentially large, the posterior distribution
over consideration sets given parameters is typically sparse, and is easily
sampled as part of an MCMC scheme that iterates sampling of subject-specific
consideration sets given parameters, followed by parameters given consideration
sets. The effectiveness of the procedure is documented in simulated
longitudinal data sets with J=100 categories and real data from the cereal
market with J=73 brands.",http://arxiv.org/pdf/2308.12470v1
2308.11495v1,stat.AP,Evaluating the accuracy of Gaussian approximations in VSWIR imaging spectroscopy retrievals,2023-08-22 15:17:33+00:00,"The joint retrieval of surface reflectances and atmospheric parameters in
VSWIR imaging spectroscopy is a computationally challenging high-dimensional
problem. Using NASA's Surface Biology and Geology mission as the motivational
context, the uncertainty associated with the retrievals is crucial for further
application of the retrieved results for environmental applications. Although
Markov chain Monte Carlo (MCMC) is a Bayesian method ideal for uncertainty
quantification, the full-dimensional implementation of MCMC for the retrieval
is computationally intractable.
  In this work, we developed a block Metropolis MCMC algorithm for the
high-dimensional VSWIR surface reflectance retrieval that leverages the
structure of the forward radiative transfer model to enable tractable fully
Bayesian computation. We use the posterior distribution from this MCMC
algorithm to assess the limitations of optimal estimation, the state-of-the-art
Bayesian algorithm in operational retrievals which is more computationally
efficient but uses a Gaussian approximation to characterize the posterior.
Analyzing the differences in the posterior computed by each method, the MCMC
algorithm was shown to give more physically sensible results and reveals the
non-Gaussian structure of the posterior, specifically in the atmospheric
aerosol optical depth parameter and the low-wavelength surface reflectances.",http://arxiv.org/pdf/2308.11495v1
