<!DOCTYPE html>
    <html>
    <body style="background-color: #FFFFFF; color: #A2A2F5">
    
    <div class="feed-item" style="border: 1px solid #ccc;
    padding: 15px;
    margin: 15px;
    border-radius: 8px;">
    <h2 class="title" style=" font-size: 24px;
    font-weight: bold;
    margin-bottom: 5px; color: #262730;">Chain-of-Verification Reduces Hallucination in Large Language Models</h2>
    <h4 class="author" style="font-size: 14px;
    font-weight: bold;
    margin-bottom: 10px; color:#262730;">Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston...</h4>
    <div class="metadata" style="font-size: 14px;
    color: #262730;
    margin-bottom: 10px;">
        <span class="id">Entry Id: 2309.11495v1</span> | 
        <span class="category">cs.CL</span> | 
        <span class="published">Published on 09-20-2023</span>
    </div>
    <div class="abstract" style="font-size: 16px;
    margin-bottom: 10px; color: #262730;">
        Generation of plausible yet incorrect factual information, termed
hallucination, is an unsolved issue in large language models. We study the
ability of language models to deliberate on the responses they give in order to
correct their mistakes. We develop the Chain-of-Verification (CoVe) method
whereby the model first (i) drafts an initial response; then (ii) plans
verification questions to fact-check its draft; (iii) answers those questions
independently so the answers are not biased by other r...
    </div>
    <a href="http://arxiv.org/pdf/2309.11495v1" target="_blank" style="display: inline-block;
    background-color: #A2A2F5;
    color: white;
    padding: 8px 16px;
    border-radius: 4px;
    text-decoration: none;">Read More</a>
    </div>
    
    <div class="feed-item" style="border: 1px solid #ccc;
    padding: 15px;
    margin: 15px;
    border-radius: 8px;">
    <h2 class="title" style=" font-size: 24px;
    font-weight: bold;
    margin-bottom: 5px; color: #262730;">CPLLM: Clinical Prediction with Large Language Models</h2>
    <h4 class="author" style="font-size: 14px;
    font-weight: bold;
    margin-bottom: 10px; color:#262730;">Ofir Ben Shoham, Nadav Rappoport...</h4>
    <div class="metadata" style="font-size: 14px;
    color: #262730;
    margin-bottom: 10px;">
        <span class="id">Entry Id: 2309.11295v1</span> | 
        <span class="category">cs.CL</span> | 
        <span class="published">Published on 09-20-2023</span>
    </div>
    <div class="abstract" style="font-size: 16px;
    margin-bottom: 10px; color: #262730;">
        We present Clinical Prediction with Large Language Models (CPLLM), a method
that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical
disease prediction. We utilized quantization and fine-tuned the LLM using
prompts, with the task of predicting whether patients will be diagnosed with a
target disease during their next visit or in the subsequent diagnosis,
leveraging their historical diagnosis records. We compared our results versus
various baselines, including Logistic Regr...
    </div>
    <a href="http://arxiv.org/pdf/2309.11295v1" target="_blank" style="display: inline-block;
    background-color: #A2A2F5;
    color: white;
    padding: 8px 16px;
    border-radius: 4px;
    text-decoration: none;">Read More</a>
    </div>
    
    <div class="feed-item" style="border: 1px solid #ccc;
    padding: 15px;
    margin: 15px;
    border-radius: 8px;">
    <h2 class="title" style=" font-size: 24px;
    font-weight: bold;
    margin-bottom: 5px; color: #262730;">Sequence-to-Sequence Spanish Pre-trained Language Models</h2>
    <h4 class="author" style="font-size: 14px;
    font-weight: bold;
    margin-bottom: 10px; color:#262730;">Vladimir Araujo, Maria Mihaela Trusca, Rodrigo Tufi√±o, Marie-Francine Moens...</h4>
    <div class="metadata" style="font-size: 14px;
    color: #262730;
    margin-bottom: 10px;">
        <span class="id">Entry Id: 2309.11259v1</span> | 
        <span class="category">cs.CL</span> | 
        <span class="published">Published on 09-20-2023</span>
    </div>
    <div class="abstract" style="font-size: 16px;
    margin-bottom: 10px; color: #262730;">
        In recent years, substantial advancements in pre-trained language models have
paved the way for the development of numerous non-English language versions,
with a particular focus on encoder-only and decoder-only architectures. While
Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited
prowess in natural language understanding and generation, there remains a
scarcity of encoder-decoder models designed for sequence-to-sequence tasks
involving input-output pairs. This paper br...
    </div>
    <a href="http://arxiv.org/pdf/2309.11259v1" target="_blank" style="display: inline-block;
    background-color: #A2A2F5;
    color: white;
    padding: 8px 16px;
    border-radius: 4px;
    text-decoration: none;">Read More</a>
    </div>
    
    <div class="feed-item" style="border: 1px solid #ccc;
    padding: 15px;
    margin: 15px;
    border-radius: 8px;">
    <h2 class="title" style=" font-size: 24px;
    font-weight: bold;
    margin-bottom: 5px; color: #262730;">Is GPT4 a Good Trader?</h2>
    <h4 class="author" style="font-size: 14px;
    font-weight: bold;
    margin-bottom: 10px; color:#262730;">Bingzhe Wu...</h4>
    <div class="metadata" style="font-size: 14px;
    color: #262730;
    margin-bottom: 10px;">
        <span class="id">Entry Id: 2309.10982v1</span> | 
        <span class="category">cs.AI</span> | 
        <span class="published">Published on 09-20-2023</span>
    </div>
    <div class="abstract" style="font-size: 16px;
    margin-bottom: 10px; color: #262730;">
        Recently, large language models (LLMs), particularly GPT-4, have demonstrated
significant capabilities in various planning and reasoning tasks
\cite{cheng2023gpt4,bubeck2023sparks}. Motivated by these advancements, there
has been a surge of interest among researchers to harness the capabilities of
GPT-4 for the automated design of quantitative factors that do not overlap with
existing factor libraries, with an aspiration to achieve alpha returns
\cite{webpagequant}. In contrast to these work, th...
    </div>
    <a href="http://arxiv.org/pdf/2309.10982v1" target="_blank" style="display: inline-block;
    background-color: #A2A2F5;
    color: white;
    padding: 8px 16px;
    border-radius: 4px;
    text-decoration: none;">Read More</a>
    </div>
    
    <div class="feed-item" style="border: 1px solid #ccc;
    padding: 15px;
    margin: 15px;
    border-radius: 8px;">
    <h2 class="title" style=" font-size: 24px;
    font-weight: bold;
    margin-bottom: 5px; color: #262730;">AceGPT, Localizing Large Language Models in Arabic</h2>
    <h4 class="author" style="font-size: 14px;
    font-weight: bold;
    margin-bottom: 10px; color:#262730;">Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Ziche Liu, Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, Jinchao Xu...</h4>
    <div class="metadata" style="font-size: 14px;
    color: #262730;
    margin-bottom: 10px;">
        <span class="id">Entry Id: 2309.12053v1</span> | 
        <span class="category">cs.CL</span> | 
        <span class="published">Published on 09-21-2023</span>
    </div>
    <div class="abstract" style="font-size: 16px;
    margin-bottom: 10px; color: #262730;">
        This paper explores the imperative need and methodology for developing a
localized Large Language Model (LLM) tailored for Arabic, a language with
unique cultural characteristics that are not adequately addressed by current
mainstream models like ChatGPT. Key concerns additionally arise when
considering cultural sensitivity and local values. To this end, the paper
outlines a packaged solution, including further pre-training with Arabic texts,
supervised fine-tuning (SFT) using native Arabic inst...
    </div>
    <a href="http://arxiv.org/pdf/2309.12053v1" target="_blank" style="display: inline-block;
    background-color: #A2A2F5;
    color: white;
    padding: 8px 16px;
    border-radius: 4px;
    text-decoration: none;">Read More</a>
    </div>
     </body>
    </html>