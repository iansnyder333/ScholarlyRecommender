Id,Category,Title,Published,Abstract,URL,Author
2309.11400v1,q-fin.TR,Transformers versus LSTMs for electronic trading,2023-09-20 15:25:43+00:00,"With the rapid development of artificial intelligence, long short term memory
(LSTM), one kind of recurrent neural network (RNN), has been widely applied in
time series prediction.
  Like RNN, Transformer is designed to handle the sequential data. As
Transformer achieved great success in Natural Language Processing (NLP),
researchers got interested in Transformer's performance on time series
prediction, and plenty of Transformer-based solutions on long time series
forecasting have come out recently. However, when it comes to financial time
series prediction, LSTM is still a dominant architecture. Therefore, the
question this study wants to answer is: whether the Transformer-based model can
be applied in financial time series prediction and beat LSTM.
  To answer this question, various LSTM-based and Transformer-based models are
compared on multiple financial prediction tasks based on high-frequency limit
order book data. A new LSTM-based model called DLSTM is built and new
architecture for the Transformer-based model is designed to adapt for financial
prediction. The experiment result reflects that the Transformer-based model
only has the limited advantage in absolute price sequence prediction. The
LSTM-based models show better and more robust performance on difference
sequence prediction, such as price difference and price movement.",http://arxiv.org/pdf/2309.11400v1,"['Paul Bilokon', 'Yitao Qiu']"
2309.10982v1,cs.AI,Is GPT4 a Good Trader?,2023-09-20 00:47:52+00:00,"Recently, large language models (LLMs), particularly GPT-4, have demonstrated
significant capabilities in various planning and reasoning tasks
\cite{cheng2023gpt4,bubeck2023sparks}. Motivated by these advancements, there
has been a surge of interest among researchers to harness the capabilities of
GPT-4 for the automated design of quantitative factors that do not overlap with
existing factor libraries, with an aspiration to achieve alpha returns
\cite{webpagequant}. In contrast to these work, this study aims to examine the
fidelity of GPT-4's comprehension of classic trading theories and its
proficiency in applying its code interpreter abilities to real-world trading
data analysis. Such an exploration is instrumental in discerning whether the
underlying logic GPT-4 employs for trading is intrinsically reliable.
Furthermore, given the acknowledged interpretative latitude inherent in most
trading theories, we seek to distill more precise methodologies of deploying
these theories from GPT-4's analytical process, potentially offering invaluable
insights to human traders.
  To achieve this objective, we selected daily candlestick (K-line) data from
specific periods for certain assets, such as the Shanghai Stock Index. Through
meticulous prompt engineering, we guided GPT-4 to analyze the technical
structures embedded within this data, based on specific theories like the
Elliott Wave Theory. We then subjected its analytical output to manual
evaluation, assessing its interpretative depth and accuracy vis-\`a-vis these
trading theories from multiple dimensions. The results and findings from this
study could pave the way for a synergistic amalgamation of human expertise and
AI-driven insights in the realm of trading.",http://arxiv.org/pdf/2309.10982v1,['Bingzhe Wu']
2309.11495v1,cs.CL,Chain-of-Verification Reduces Hallucination in Large Language Models,2023-09-20 17:50:55+00:00,"Generation of plausible yet incorrect factual information, termed
hallucination, is an unsolved issue in large language models. We study the
ability of language models to deliberate on the responses they give in order to
correct their mistakes. We develop the Chain-of-Verification (CoVe) method
whereby the model first (i) drafts an initial response; then (ii) plans
verification questions to fact-check its draft; (iii) answers those questions
independently so the answers are not biased by other responses; and (iv)
generates its final verified response. In experiments, we show CoVe decreases
hallucinations across a variety of tasks, from list-based questions from
Wikidata, closed book MultiSpanQA and longform text generation.",http://arxiv.org/pdf/2309.11495v1,"['Shehzaad Dhuliawala', 'Mojtaba Komeili', 'Jing Xu', 'Roberta Raileanu', 'Xian Li', 'Asli Celikyilmaz', 'Jason Weston']"
2309.11830v1,cs.CL,A Chinese Prompt Attack Dataset for LLMs with Evil Content,2023-09-21 07:07:49+00:00,"Large Language Models (LLMs) present significant priority in text
understanding and generation. However, LLMs suffer from the risk of generating
harmful contents especially while being employed to applications. There are
several black-box attack methods, such as Prompt Attack, which can change the
behaviour of LLMs and induce LLMs to generate unexpected answers with harmful
contents. Researchers are interested in Prompt Attack and Defense with LLMs,
while there is no publicly available dataset to evaluate the abilities of
defending prompt attack. In this paper, we introduce a Chinese Prompt Attack
Dataset for LLMs, called CPAD. Our prompts aim to induce LLMs to generate
unexpected outputs with several carefully designed prompt attack approaches and
widely concerned attacking contents. Different from previous datasets involving
safety estimation, We construct the prompts considering three dimensions:
contents, attacking methods and goals, thus the responses can be easily
evaluated and analysed. We run several well-known Chinese LLMs on our dataset,
and the results show that our prompts are significantly harmful to LLMs, with
around 70% attack success rate. We will release CPAD to encourage further
studies on prompt attack and defense.",http://arxiv.org/pdf/2309.11830v1,"['Chengyuan Liu', 'Fubang Zhao', 'Lizhi Qing', 'Yangyang Kang', 'Changlong Sun', 'Kun Kuang', 'Fei Wu']"
2309.11688v1,cs.CL,LLM Guided Inductive Inference for Solving Compositional Problems,2023-09-20 23:44:16+00:00,"While large language models (LLMs) have demonstrated impressive performance
in question-answering tasks, their performance is limited when the questions
require knowledge that is not included in the model's training data and can
only be acquired through direct observation or interaction with the real world.
Existing methods decompose reasoning tasks through the use of modules invoked
sequentially, limiting their ability to answer deep reasoning tasks. We
introduce a method, Recursion based extensible LLM (REBEL), which handles
open-world, deep reasoning tasks by employing automated reasoning techniques
like dynamic planning and forward-chaining strategies. REBEL allows LLMs to
reason via recursive problem decomposition and utilization of external tools.
The tools that REBEL uses are specified only by natural language description.
We further demonstrate REBEL capabilities on a set of problems that require a
deeply nested use of external tools in a compositional and conversational
setting.",http://arxiv.org/pdf/2309.11688v1,"['Abhigya Sodani', 'Lauren Moos', 'Matthew Mirman']"
