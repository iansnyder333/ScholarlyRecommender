Id,Category,Title,Published,Abstract,URL
2309.12312v1,cs.RO,ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals,2023-09-21 17:59:50+00:00,"We present ForceSight, a system for text-guided mobile manipulation that
predicts visual-force goals using a deep neural network. Given a single RGBD
image combined with a text prompt, ForceSight determines a target end-effector
pose in the camera frame (kinematic goal) and the associated forces (force
goal). Together, these two components form a visual-force goal. Prior work has
demonstrated that deep models outputting human-interpretable kinematic goals
can enable dexterous manipulation by real robots. Forces are critical to
manipulation, yet have typically been relegated to lower-level execution in
these systems. When deployed on a mobile manipulator equipped with an
eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps,
drawer opening, and object handovers with an 81% success rate in unseen
environments with object instances that differed significantly from the
training data. In a separate experiment, relying exclusively on visual servoing
and ignoring force goals dropped the success rate from 90% to 45%,
demonstrating that force goals can significantly enhance performance. The
appendix, videos, code, and trained models are available at
https://force-sight.github.io/.",http://arxiv.org/pdf/2309.12312v1
2309.12311v1,cs.CV,LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent,2023-09-21 17:59:45+00:00,"3D visual grounding is a critical skill for household robots, enabling them
to navigate, manipulate objects, and answer questions based on their
environment. While existing approaches often rely on extensive labeled data or
exhibit limitations in handling complex language queries, we propose
LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model
(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to
decompose complex natural language queries into semantic constituents and
employs a visual grounding tool, such as OpenScene or LERF, to identify objects
in a 3D scene. The LLM then evaluates the spatial and commonsense relations
among the proposed objects to make a final grounding decision. Our method does
not require any labeled training data and can generalize to novel 3D scenes and
arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and
demonstrate state-of-the-art zero-shot grounding accuracy. Our findings
indicate that LLMs significantly improve the grounding capability, especially
for complex language queries, making LLM-Grounder an effective approach for 3D
vision-language tasks in robotics. Videos and interactive demos can be found on
the project website https://chat-with-nerf.github.io/ .",http://arxiv.org/pdf/2309.12311v1
2309.12309v1,cs.HC,Rehearsal: Simulating Conflict to Teach Conflict Resolution,2023-09-21 17:59:20+00:00,"Interpersonal conflict is an uncomfortable but unavoidable fact of life.
Navigating conflict successfully is a skill -- one that can be learned through
deliberate practice -- but few have access to effective training or feedback.
To expand this access, we introduce Rehearsal, a system that allows users to
rehearse conflicts with a believable simulated interlocutor, explore
counterfactual ""what if?"" scenarios to identify alternative conversational
paths, and learn through feedback on how and when to apply specific conflict
strategies. Users can utilize Rehearsal to practice handling a variety of
predefined conflict scenarios, from office disputes to relationship issues, or
they can choose to create their own. To enable Rehearsal, we develop IRP
prompting, a method of conditioning output of a large language model on the
influential Interest-Rights-Power (IRP) theory from conflict resolution.
Rehearsal uses IRP to generate utterances grounded in conflict resolution
theory, guiding users towards counterfactual conflict resolution strategies
that help de-escalate difficult conversations. In a between-subjects
evaluation, 40 participants engaged in an actual conflict with a confederate
after training. Compared to a control group with lecture material covering the
same IRP theory, participants with simulated training from Rehearsal
significantly improved their performance in the unaided conflict: they reduced
their use of escalating competitive strategies by an average of 67%, while
doubling their use of cooperative strategies. Overall, Rehearsal highlights the
potential effectiveness of language models as tools for learning and practicing
interpersonal skills.",http://arxiv.org/pdf/2309.12309v1
2309.12307v1,cs.CL,LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models,2023-09-21 17:59:11+00:00,"We present LongLoRA, an efficient fine-tuning approach that extends the
context sizes of pre-trained large language models (LLMs), with limited
computation cost. Typically, training LLMs with long context sizes is
computationally expensive, requiring extensive training hours and GPU
resources. For example, training on the context length of 8192 needs 16x
computational costs in self-attention layers as that of 2048. In this paper, we
speed up the context extension of LLMs in two aspects. On the one hand,
although dense global attention is needed during inference, fine-tuning the
model can be effectively and efficiently done by sparse local attention. The
proposed shift short attention effectively enables context extension, leading
to non-trivial computation saving with similar performance to fine-tuning with
vanilla attention. Particularly, it can be implemented with only two lines of
code in training, while being optional in inference. On the other hand, we
revisit the parameter-efficient fine-tuning regime for context expansion.
Notably, we find that LoRA for context extension works well under the premise
of trainable embedding and normalization. LongLoRA demonstrates strong
empirical results on various tasks on LLaMA2 models from 7B/13B to 70B.
LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a
single 8x A100 machine. LongLoRA extends models' context while retaining their
original architectures, and is compatible with most existing techniques, like
FlashAttention-2. In addition, to make LongLoRA practical, we collect a
dataset, LongQA, for supervised fine-tuning. It contains more than 3k long
context question-answer pairs.",http://arxiv.org/pdf/2309.12307v1
2309.12301v1,cs.LG,Environment-biased Feature Ranking for Novelty Detection Robustness,2023-09-21 17:58:26+00:00,"We tackle the problem of robust novelty detection, where we aim to detect
novelties in terms of semantic content while being invariant to changes in
other, irrelevant factors. Specifically, we operate in a setup with multiple
environments, where we determine the set of features that are associated more
with the environments, rather than to the content relevant for the task. Thus,
we propose a method that starts with a pretrained embedding and a multi-env
setup and manages to rank the features based on their environment-focus. First,
we compute a per-feature score based on the feature distribution variance
between envs. Next, we show that by dropping the highly scored ones, we manage
to remove spurious correlations and improve the overall performance by up to
6%, both in covariance and sub-population shift cases, both for a real and a
synthetic benchmark, that we introduce for this task.",http://arxiv.org/pdf/2309.12301v1
2309.12300v1,cs.RO,See to Touch: Learning Tactile Dexterity through Visual Incentives,2023-09-21 17:58:13+00:00,"Equipping multi-fingered robots with tactile sensing is crucial for achieving
the precise, contact-rich, and dexterous manipulation that humans excel at.
However, relying solely on tactile sensing fails to provide adequate cues for
reasoning about objects' spatial configurations, limiting the ability to
correct errors and adapt to changing situations. In this paper, we present
Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances
tactile-based dexterity by optimizing dexterous policies using vision-based
rewards. First, we use a contrastive-based objective to learn visual
representations. Next, we construct a reward function using these visual
representations through optimal-transport based matching on one human
demonstration. Finally, we use online reinforcement learning on our robot to
optimize tactile-based policies that maximize the visual reward. On six
challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping
slender objects, TAVI achieves a success rate of 73% using our four-fingered
Allegro robot hand. The increase in performance is 108% higher than policies
using tactile and vision-based rewards and 135% higher than policies without
tactile observational input. Robot videos are best viewed on our project
website: https://see-to-touch.github.io/.",http://arxiv.org/pdf/2309.12300v1
2309.12295v1,cs.CV,Learning to Drive Anywhere,2023-09-21 17:55:36+00:00,"Human drivers can seamlessly adapt their driving decisions across
geographical locations with diverse conditions and rules of the road, e.g.,
left vs. right-hand traffic. In contrast, existing models for autonomous
driving have been thus far only deployed within restricted operational domains,
i.e., without accounting for varying driving behaviors across locations or
model scalability. In this work, we propose AnyD, a single geographically-aware
conditional imitation learning (CIL) model that can efficiently learn from
heterogeneous and globally distributed data with dynamic environmental,
traffic, and social characteristics. Our key insight is to introduce a
high-capacity geo-location-based channel attention mechanism that effectively
adapts to local nuances while also flexibly modeling similarities among regions
in a data-driven manner. By optimizing a contrastive imitation objective, our
proposed approach can efficiently scale across inherently imbalanced data
distributions and location-dependent events. We demonstrate the benefits of our
AnyD agent across multiple datasets, cities, and scalable deployment paradigms,
i.e., centralized, semi-supervised, and distributed agent training.
Specifically, AnyD outperforms CIL baselines by over 14% in open-loop
evaluation and 30% in closed-loop testing on CARLA.",http://arxiv.org/pdf/2309.12295v1
2309.12288v1,cs.CL,"The Reversal Curse: LLMs trained on ""A is B"" fail to learn ""B is A""",2023-09-21 17:52:19+00:00,"We expose a surprising failure of generalization in auto-regressive large
language models (LLMs). If a model is trained on a sentence of the form ""A is
B"", it will not automatically generalize to the reverse direction ""B is A"".
This is the Reversal Curse. For instance, if a model is trained on ""Olaf Scholz
was the ninth Chancellor of Germany"", it will not automatically be able to
answer the question, ""Who was the ninth Chancellor of Germany?"". Moreover, the
likelihood of the correct answer (""Olaf Scholz"") will not be higher than for a
random name. Thus, models exhibit a basic failure of logical deduction and do
not generalize a prevalent pattern in their training set (i.e. if ""A is B''
occurs, ""B is A"" is more likely to occur). We provide evidence for the Reversal
Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as ""Uriah
Hawthorne is the composer of 'Abyssal Melodies'"" and showing that they fail to
correctly answer ""Who composed 'Abyssal Melodies?'"". The Reversal Curse is
robust across model sizes and model families and is not alleviated by data
augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about
real-world celebrities, such as ""Who is Tom Cruise's mother? [A: Mary Lee
Pfeiffer]"" and the reverse ""Who is Mary Lee Pfeiffer's son?"". GPT-4 correctly
answers questions like the former 79% of the time, compared to 33% for the
latter. This shows a failure of logical deduction that we hypothesize is caused
by the Reversal Curse. Code is available at
https://github.com/lukasberglund/reversal_curse.",http://arxiv.org/pdf/2309.12288v1
2309.12284v1,cs.CL,MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models,2023-09-21 17:45:42+00:00,"Large language models (LLMs) have pushed the limits of natural language
understanding and exhibited excellent problem-solving ability. Despite the
great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away
from satisfactory for solving mathematical problem due to the complex reasoning
procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned
language model that specializes in mathematical reasoning. Specifically, we
start by bootstrapping mathematical questions by rewriting the question from
multiple perspectives without extra knowledge, which results in a new dataset
called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA.
Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for
mathematical reasoning demonstrate that MetaMath outperforms a suite of
open-source LLMs by a significant margin. Our MetaMath-7B model achieves
$66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models
of the same size by $11.5\%$ and $8.7\%$. Particularly, {MetaMath-70B} achieves
an accuracy of $82.3\%$ on {GSM8K}, slightly better than {GPT-3.5-Turbo}. We
release the {MetaMathQA} dataset, the {MetaMath} models with different model
sizes and the training code for public use.",http://arxiv.org/pdf/2309.12284v1
2309.12276v1,cs.HC,LLMR: Real-time Prompting of Interactive Worlds using Large Language Models,2023-09-21 17:37:01+00:00,"We present Large Language Model for Mixed Reality (LLMR), a framework for the
real-time creation and modification of interactive Mixed Reality experiences
using LLMs. LLMR leverages novel strategies to tackle difficult cases where
ideal training data is scarce, or where the design goal requires the synthesis
of internal dynamics, intuitive analysis, or advanced interactivity. Our
framework relies on text interaction and the Unity game engine. By
incorporating techniques for scene understanding, task planning,
self-debugging, and memory management, LLMR outperforms the standard GPT-4 by
4x in average error rate. We demonstrate LLMR's cross-platform interoperability
with several example worlds, and evaluate it on a variety of creation and
modification tasks to show that it can produce and edit diverse objects, tools,
and scenes. Finally, we conducted a usability study (N=11) with a diverse set
that revealed participants had positive experiences with the system and would
use it again.",http://arxiv.org/pdf/2309.12276v1
2309.12267v1,cs.CR,Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications,2023-09-21 17:17:28+00:00,"Federated Learning (FL) has revolutionized how we train deep neural networks
by enabling decentralized collaboration while safeguarding sensitive data and
improving model performance. However, FL faces two crucial challenges: the
diverse nature of data held by individual clients and the vulnerability of the
FL system to security breaches. This paper introduces an innovative solution
named Estimated Mean Aggregation (EMA) that not only addresses these challenges
but also provides a fundamental reference point as a $\mathsf{baseline}$ for
advanced aggregation techniques in FL systems. EMA's significance lies in its
dual role: enhancing model security by effectively handling malicious outliers
through trimmed means and uncovering data heterogeneity to ensure that trained
models are adaptable across various client datasets. Through a wealth of
experiments, EMA consistently demonstrates high accuracy and area under the
curve (AUC) compared to alternative methods, establishing itself as a robust
baseline for evaluating the effectiveness and security of FL aggregation
methods. EMA's contributions thus offer a crucial step forward in advancing the
efficiency, security, and versatility of decentralized deep learning in the
context of FL.",http://arxiv.org/pdf/2309.12267v1
2309.12253v1,cs.LG,SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning,2023-09-21 16:57:09+00:00,"We introduce an extension to the CLRS algorithmic learning benchmark,
prioritizing scalability and the utilization of sparse representations. Many
algorithms in CLRS require global memory or information exchange, mirrored in
its execution model, which constructs fully connected (not sparse) graphs based
on the underlying problem. Despite CLRS's aim of assessing how effectively
learned algorithms can generalize to larger instances, the existing execution
model becomes a significant constraint due to its demanding memory requirements
and runtime (hard to scale). However, many important algorithms do not demand a
fully connected graph; these algorithms, primarily distributed in nature, align
closely with the message-passing paradigm employed by Graph Neural Networks.
Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark
specifically with scalability and sparseness in mind. Our approach includes
adapted algorithms from the original CLRS benchmark and introduces new problems
from distributed and randomized algorithms. Moreover, we perform a thorough
empirical evaluation of our benchmark. Code is publicly available at
https://github.com/jkminder/SALSA-CLRS.",http://arxiv.org/pdf/2309.12253v1
2309.12247v1,cs.CL,"Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",2023-09-21 16:47:30+00:00,"Detecting fake news requires both a delicate sense of diverse clues and a
profound understanding of the real-world background, which remains challenging
for detectors based on small language models (SLMs) due to their knowledge and
capability limitations. Recent advances in large language models (LLMs) have
shown remarkable performance in various tasks, but whether and how LLMs could
help with fake news detection remains underexplored. In this paper, we
investigate the potential of LLMs in fake news detection. First, we conduct an
empirical study and find that a sophisticated LLM such as GPT 3.5 could
generally expose fake news and provide desirable multi-perspective rationales
but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis
attributes such a gap to the LLM's inability to select and integrate rationales
properly to conclude. Based on these findings, we propose that current LLMs may
not substitute fine-tuned SLMs in fake news detection but can be a good advisor
for SLMs by providing multi-perspective instructive rationales. To instantiate
this proposal, we design an adaptive rationale guidance network for fake news
detection (ARG), in which SLMs selectively acquire insights on news analysis
from the LLMs' rationales. We further derive a rationale-free version of ARG by
distillation, namely ARG-D, which services cost-sensitive scenarios without
inquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and
ARG-D outperform three types of baseline methods, including SLM-based,
LLM-based, and combinations of small and large language models.",http://arxiv.org/pdf/2309.12247v1
2309.12244v1,cs.HC,ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events,2023-09-21 16:43:17+00:00,"Children typically learn to identify and express emotions through sharing
their stories and feelings with others, particularly their family. However, it
is challenging for parents or siblings to have emotional communication with
children since children are still developing their communication skills. We
present ChaCha, a chatbot that encourages and guides children to share personal
events and associated emotions. ChaCha combines a state machine and large
language models (LLMs) to keep the dialogue on track while carrying on
free-form conversations. Through an exploratory study with 20 children (aged
8-12), we examine how ChaCha prompts children to share personal events and
guides them to describe associated emotions. Participants perceived ChaCha as a
close friend and shared their stories on various topics, such as family trips
and personal achievements. Based on the quantitative and qualitative findings,
we discuss opportunities for leveraging LLMs to design child-friendly chatbots
to support children in sharing their emotions.",http://arxiv.org/pdf/2309.12244v1
2309.12237v1,cs.CR,t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators,2023-09-21 16:30:40+00:00,"Presentation attack (spoofing) detection (PAD) typically operates alongside
biometric verification to improve reliablity in the face of spoofing attacks.
Even though the two sub-systems operate in tandem to solve the single task of
reliable biometric verification, they address different detection tasks and are
hence typically evaluated separately. Evidence shows that this approach is
suboptimal. We introduce a new metric for the joint evaluation of PAD solutions
operating in situ with biometric verification. In contrast to the tandem
detection cost function proposed recently, the new tandem equal error rate
(t-EER) is parameter free. The combination of two classifiers nonetheless leads
to a \emph{set} of operating points at which false alarm and miss rates are
equal and also dependent upon the prevalence of attacks. We therefore introduce
the \emph{concurrent} t-EER, a unique operating point which is invariable to
the prevalence of attacks. Using both modality (and even application) agnostic
simulated scores, as well as real scores for a voice biometrics application, we
demonstrate application of the t-EER to a wide range of biometric system
evaluations under attack. The proposed approach is a strong candidate metric
for the tandem evaluation of PAD systems and biometric comparators.",http://arxiv.org/pdf/2309.12237v1
2309.12177v1,cs.AI,Explainable Artificial Intelligence for Drug Discovery and Development -- A Comprehensive Survey,2023-09-21 15:36:06+00:00,"The field of drug discovery has experienced a remarkable transformation with
the advent of artificial intelligence (AI) and machine learning (ML)
technologies. However, as these AI and ML models are becoming more complex,
there is a growing need for transparency and interpretability of the models.
Explainable Artificial Intelligence (XAI) is a novel approach that addresses
this issue and provides a more interpretable understanding of the predictions
made by machine learning models. In recent years, there has been an increasing
interest in the application of XAI techniques to drug discovery. This review
article provides a comprehensive overview of the current state-of-the-art in
XAI for drug discovery, including various XAI methods, their application in
drug discovery, and the challenges and limitations of XAI techniques in drug
discovery. The article also covers the application of XAI in drug discovery,
including target identification, compound design, and toxicity prediction.
Furthermore, the article suggests potential future research directions for the
application of XAI in drug discovery. The aim of this review article is to
provide a comprehensive understanding of the current state of XAI in drug
discovery and its potential to transform the field.",http://arxiv.org/pdf/2309.12177v1
2309.12161v1,cs.CL,Code Soliloquies for Accurate Calculations in Large Language Models,2023-09-21 15:16:58+00:00,"High-quality conversational datasets are integral to the successful
development of Intelligent Tutoring Systems (ITS) that employ a Large Language
Model (LLM) backend. These datasets, when used to fine-tune the LLM backend,
significantly enhance the quality of interactions between students and ITS. A
common strategy for developing these datasets involves generating synthetic
student-teacher dialogues using advanced GPT-4 models. However, challenges
arise when these dialogues demand complex calculations, common in subjects like
physics. Despite its advanced capabilities, GPT-4's performance falls short in
reliably handling even simple multiplication tasks, marking a significant
limitation in its utility for these subjects. To address these challenges, this
paper introduces an innovative stateful prompt design. Our approach generates a
mock conversation between a student and a tutorbot, both roles simulated by
GPT-4. Each student response triggers a soliloquy (an inner monologue) in the
GPT-tutorbot, which assesses whether its response would necessitate
calculations. If so, it proceeds to script the required code in Python and then
uses the resulting output to construct its response to the student. Our
approach notably enhances the quality of synthetic conversation datasets,
especially for subjects that are calculation-intensive. Our findings show that
our Higgs model -- a LLaMA finetuned with datasets generated through our novel
stateful prompt design -- proficiently utilizes Python for computations.
Consequently, finetuning with our datasets enriched with code soliloquies
enhances not just the accuracy but also the computational reliability of Higgs'
responses.",http://arxiv.org/pdf/2309.12161v1
2309.12148v1,cs.NE,Neural Modelling of Dynamic Systems with Time Delays Based on an Adjusted NEAT Algorithm,2023-09-21 15:04:42+00:00,"A problem related to the development of an algorithm designed to find an
architecture of artificial neural network used for black-box modelling of
dynamic systems with time delays has been addressed in this paper. The proposed
algorithm is based on a well-known NeuroEvolution of Augmenting Topologies
(NEAT) algorithm. The NEAT algorithm has been adjusted by allowing additional
connections within an artificial neural network and developing original
specialised evolutionary operators. This resulted in a compromise between the
size of neural network and its accuracy in capturing the response of the
mathematical model under which it has been learnt. The research involved an
extended validation study based on data generated from a mathematical model of
an exemplary system as well as the fast processes occurring in a pressurised
water nuclear reactor. The obtaining simulation results demonstrate the high
effectiveness of the devised neural (black-box) models of dynamic systems with
time delays.",http://arxiv.org/pdf/2309.12148v1
2309.12146v1,astro-ph.CO,Search for a possible quasi-periodic structure based on data of the SDSS DR12 LOWZ,2023-09-21 15:04:35+00:00,"We carry out a statistical analysis of the spatial distribution of galaxies
at cosmological redshifts $0.16 \leq z \leq 0.47$ based on the SDSS\ DR12\ LOWZ
catalogue. Our aim is to search and study possible large-scale quasi-regular
structures embedded in the {\it cosmic web}. We calculate projections of the
Cartesian galaxy coordinates on different axes (directions) densely covering
certain regions in the sky to look for special directions along which
one-dimensional distributions of the projections contain significant
quasi-periodic components. These components appear as peaks in the power
spectra and lie in a narrow range of wave numbers $0.05 < k < 0.07$. Particular
attention is paid to the evaluation of the significance of the peaks. It is
found that the significance of the dominant peaks for some selected directions
exceeds $(4 - 5)\sigma$. In order to reduce possible selection effects, we
create a mock homogeneous catalogue of spatial distribution of galaxies by
adding a random set of artificial objects (points) to the real galaxies under
study. The power spectrum of this cumulative model data also demonstrates
significant peak corresponding to approximately the same scale. As a result we
assume the existence of an anisotropic cosmological quasi-periodic structure
with characteristic scale $(116 \pm 10)~h^{-1}$~Mpc.",http://arxiv.org/pdf/2309.12146v1
2309.12140v1,cs.CV,Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features,2023-09-21 15:00:31+00:00,"The rapid development of 3D object detection systems for self-driving cars
has significantly improved accuracy. However, these systems struggle to
generalize across diverse driving environments, which can lead to
safety-critical failures in detecting traffic participants. To address this, we
propose a method that utilizes unlabeled repeated traversals of multiple
locations to adapt object detectors to new driving environments. By
incorporating statistics computed from repeated LiDAR scans, we guide the
adaptation process effectively. Our approach enhances LiDAR-based detection
models using spatial quantized historical features and introduces a lightweight
regression head to leverage the statistics for feature regularization.
Additionally, we leverage the statistics for a novel self-training process to
stabilize the training. The framework is detector model-agnostic and
experiments on real-world datasets demonstrate significant improvements,
achieving up to a 20-point performance gain, especially in detecting
pedestrians and distant objects. Code is available at
https://github.com/zhangtravis/Hist-DA.",http://arxiv.org/pdf/2309.12140v1
2309.12139v1,cs.RO,"On the relationship between Benchmarking, Standards and Certification in Robotics and AI",2023-09-21 14:59:36+00:00,"Benchmarking, standards and certification are closely related processes.
Standards can provide normative requirements that robotics and AI systems may
or may not conform to. Certification generally relies upon conformance with one
or more standards as the key determinant of granting a certificate to operate.
And benchmarks are sets of standardised tests against which robots and AI
systems can be measured. Benchmarks therefore can be thought of as informal
standards. In this paper we will develop these themes with examples from
benchmarking, standards and certification, and argue that these three linked
processes are not only useful but vital to the broader practice of Responsible
Innovation.",http://arxiv.org/pdf/2309.12139v1
2309.12137v1,cs.CL,OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media,2023-09-21 14:58:50+00:00,"While resources for English language are fairly sufficient to understand
content on social media, similar resources in Arabic are still immature. The
main reason that the resources in Arabic are insufficient is that Arabic has
many dialects in addition to the standard version (MSA). Arabs do not use MSA
in their daily communications; rather, they use dialectal versions.
Unfortunately, social users transfer this phenomenon into their use of social
media platforms, which in turn has raised an urgent need for building suitable
AI models for language-dependent applications. Existing machine translation
(MT) systems designed for MSA fail to work well with Arabic dialects. In light
of this, it is necessary to adapt to the informal nature of communication on
social networks by developing MT systems that can effectively handle the
various dialects of Arabic. Unlike for MSA that shows advanced progress in MT
systems, little effort has been exerted to utilize Arabic dialects for MT
systems. While few attempts have been made to build translation datasets for
dialectal Arabic, they are domain dependent and are not OSN cultural-language
friendly. In this work, we attempt to alleviate these limitations by proposing
an online social network-based multidialect Arabic dataset that is crafted by
contextually translating English tweets into four Arabic dialects: Gulf,
Yemeni, Iraqi, and Levantine. To perform the translation, we followed our
proposed guideline framework for content translation, which could be
universally applicable for translation between foreign languages and local
dialects. We validated the authenticity of our proposed dataset by developing
neural MT models for four Arabic dialects. Our results have shown a superior
performance of our NMT models trained using our dataset. We believe that our
dataset can reliably serve as an Arabic multidialectal translation dataset for
informal MT tasks.",http://arxiv.org/pdf/2309.12137v1
2309.12132v1,cs.AI,A knowledge representation approach for construction contract knowledge modeling,2023-09-21 14:53:36+00:00,"The emergence of large language models (LLMs) presents an unprecedented
opportunity to automate construction contract management, reducing human errors
and saving significant time and costs. However, LLMs may produce convincing yet
inaccurate and misleading content due to a lack of domain expertise. To address
this issue, expert-driven contract knowledge can be represented in a structured
manner to constrain the automatic contract management process. This paper
introduces the Nested Contract Knowledge Graph (NCKG), a knowledge
representation approach that captures the complexity of contract knowledge
using a nested structure. It includes a nested knowledge representation
framework, a NCKG ontology built on the framework, and an implementation
method. Furthermore, we present the LLM-assisted contract review pipeline
enhanced with external knowledge in NCKG. Our pipeline achieves a promising
performance in contract risk reviewing, shedding light on the combination of
LLM and KG towards more reliable and interpretable contract management.",http://arxiv.org/pdf/2309.12132v1
2309.12113v1,cs.AI,Incentivizing Massive Unknown Workers for Budget-Limited Crowdsensing: From Off-Line and On-Line Perspectives,2023-09-21 14:30:42+00:00,"Although the uncertainties of the workers can be addressed by the standard
Combinatorial Multi-Armed Bandit (CMAB) framework in existing proposals through
a trade-off between exploration and exploitation, we may not have sufficient
budget to enable the trade-off among the individual workers, especially when
the number of the workers is huge while the budget is limited. Moreover, the
standard CMAB usually assumes the workers always stay in the system, whereas
the workers may join in or depart from the system over time, such that what we
have learnt for an individual worker cannot be applied after the worker leaves.
To address the above challenging issues, in this paper, we first propose an
off-line Context-Aware CMAB-based Incentive (CACI) mechanism. We innovate in
leveraging the exploration-exploitation trade-off in a elaborately partitioned
context space instead of the individual workers, to effectively incentivize the
massive unknown workers with very limited budget. We also extend the above
basic idea to the on-line setting where unknown workers may join in or depart
from the systems dynamically, and propose an on-line version of the CACI
mechanism. Specifically, by the exploitation-exploration trade-off in the
context space, we learn to estimate the sensing ability of any unknown worker
(even it never appeared in the system before) according to its context
information. We perform rigorous theoretical analysis to reveal the upper
bounds on the regrets of our CACI mechanisms and to prove their truthfulness
and individual rationality, respectively. Extensive experiments on both
synthetic and real datasets are also conducted to verify the efficacy of our
mechanisms.",http://arxiv.org/pdf/2309.12113v1
2309.12109v1,cs.CL,PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models,2023-09-21 14:29:23+00:00,"In this era of large language models (LLMs), the traditional training of
models has become increasingly unimaginable for regular users and institutions.
The exploration of efficient fine-tuning for high-resource languages on these
models is an undeniable trend that is gradually gaining popularity. However,
there has been very little exploration for various low-resource languages, such
as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While
there is currently no existing large language model for Tibetan due to its
low-resource nature, that day will undoubtedly arrive. Therefore, research on
efficient fine-tuning for low-resource language models like Tibetan is highly
necessary. Our research can serve as a reference to fill this crucial gap.
Efficient fine-tuning strategies for pre-trained language models (PLMs) in
Tibetan have seen minimal exploration. We conducted three types of efficient
fine-tuning experiments on the publicly available TNCC-title dataset:
""prompt-tuning,"" ""Adapter lightweight fine-tuning,"" and ""prompt-tuning +
Adapter fine-tuning."" The experimental results demonstrate significant
improvements using these methods, providing valuable insights for advancing
Tibetan language applications in the context of pre-trained models.",http://arxiv.org/pdf/2309.12109v1
2309.12089v1,cs.RO,HiCRISP: A Hierarchical Closed-Loop Robotic Intelligent Self-Correction Planner,2023-09-21 13:58:26+00:00,"The integration of Large Language Models (LLMs) into robotics has
revolutionized human-robot interactions and autonomous task planning. However,
these systems are often unable to self-correct during the task execution, which
hinders their adaptability in dynamic real-world environments. To address this
issue, we present a Hierarchical Closed-loop Robotic Intelligent
Self-correction Planner (HiCRISP), an innovative framework that enables robots
to correct errors within individual steps during the task execution. HiCRISP
actively monitors and adapts the task execution process, addressing both
high-level planning and low-level action errors. Extensive benchmark
experiments, encompassing virtual and real-world scenarios, showcase HiCRISP's
exceptional performance, positioning it as a promising solution for robotic
task planning with LLMs.",http://arxiv.org/pdf/2309.12089v1
2309.12081v1,eess.SY,A Framework on Fully Distributed State Estimation and Cooperative Stabilization of LTI Plants,2023-09-21 13:53:12+00:00,"How to realize high-level autonomy of individuals is one of key technical
issues to promote swarm intelligence of multi-agent (node) systems with
collective tasks, while the fully distributed design is a potential way to
achieve this goal. This paper works on the fully distributed state estimation
and cooperative stabilization problem of linear time-invariant (LTI) plants
with multiple nodes communicating over general directed graphs, and is aimed to
provide a fully distributed framework for each node to perform cooperative
stabilization tasks. First, by incorporating a novel adaptive law, a
consensus-based estimator is designed for each node to obtain the plant state
based on its local measurement and local interaction with neighbors, without
using any global information of the communication topology. Subsequently, a
local controller is developed for each node to stabilize the plant
collaboratively with performance guaranteed under mild conditions.
Specifically, the proposed method only requires that the communication graph be
strongly connected, and the plant be collectively controllable and observable.
Further, the proposed method can be applied to pure fully distributed state
estimation scenarios and modified for noise-bounded LTI plants. Finally, two
numerical examples are provided to show the effectiveness of the theoretical
results.",http://arxiv.org/pdf/2309.12081v1
2309.12075v1,cs.CL,Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models,2023-09-21 13:45:32+00:00,"Prompt Tuning is emerging as a scalable and cost-effective method to
fine-tune Pretrained Language Models (PLMs). This study benchmarks the
performance and computational efficiency of Prompt Tuning and baseline methods
on a multi-label text classification task. This is applied to the use case of
classifying companies into an investment firm's proprietary industry taxonomy,
supporting their thematic investment strategy. Text-to-text classification with
PLMs is frequently reported to outperform classification with a classification
head, but has several limitations when applied to a multi-label classification
problem where each label consists of multiple tokens: (a) Generated labels may
not match any label in the industry taxonomy; (b) During fine-tuning, multiple
labels must be provided in an arbitrary order; (c) The model provides a binary
decision for each label, rather than an appropriate confidence score.
Limitation (a) is addressed by applying constrained decoding using Trie Search,
which slightly improves classification performance. All limitations (a), (b),
and (c) are addressed by replacing the PLM's language head with a
classification head. This improves performance significantly, while also
reducing computational costs during inference. The results indicate the
continuing need to adapt state-of-the-art methods to domain-specific tasks,
even in the era of PLMs with strong generalization abilities.",http://arxiv.org/pdf/2309.12075v1
2309.12071v1,cs.AI,Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam,2023-09-21 13:39:54+00:00,"Although Large Language Models (LLMs) represent a revolution in the way we
interact with computers, allowing the construction of complex questions and the
ability to reason over a sequence of statements, their use is restricted due to
the need for dedicated hardware for execution. In this study, we evaluate the
performance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a
quantization process and run on home hardware. The models considered were
Alpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we
developed a database containing 1,006 questions from the ENEM (Brazilian
National Secondary School Exam). Our analysis revealed that the best performing
models achieved an accuracy of approximately 46% for the original texts of the
Portuguese questions and 49% on their English translations. In addition, we
evaluated the computational efficiency of the models by measuring the time
required for execution. On average, the 7 and 13 billion LLMs took
approximately 20 and 50 seconds, respectively, to process the queries on a
machine equipped with an AMD Ryzen 5 3600x processor",http://arxiv.org/pdf/2309.12071v1
2309.12067v1,cs.CV,"Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives",2023-09-21 13:36:57+00:00,"Action scene understanding in soccer is a challenging task due to the complex
and dynamic nature of the game, as well as the interactions between players.
This article provides a comprehensive overview of this task divided into action
recognition, spotting, and spatio-temporal action localization, with a
particular emphasis on the modalities used and multimodal methods. We explore
the publicly available data sources and metrics used to evaluate models'
performance. The article reviews recent state-of-the-art methods that leverage
deep learning techniques and traditional methods. We focus on multimodal
methods, which integrate information from multiple sources, such as video and
audio data, and also those that represent one source in various ways. The
advantages and limitations of methods are discussed, along with their potential
for improving the accuracy and robustness of models. Finally, the article
highlights some of the open research questions and future directions in the
field of soccer action recognition, including the potential for multimodal
methods to advance this field. Overall, this survey provides a valuable
resource for researchers interested in the field of action scene understanding
in soccer.",http://arxiv.org/pdf/2309.12067v1
2309.12058v1,cs.LG,An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM,2023-09-21 13:25:11+00:00,"Anticancer peptides (ACPs) are a group of peptides that exhibite
antineoplastic properties. The utilization of ACPs in cancer prevention can
present a viable substitute for conventional cancer therapeutics, as they
possess a higher degree of selectivity and safety. Recent scientific
advancements generate an interest in peptide-based therapies which offer the
advantage of efficiently treating intended cells without negatively impacting
normal cells. However, as the number of peptide sequences continues to increase
rapidly, developing a reliable and precise prediction model becomes a
challenging task. In this work, our motivation is to advance an efficient model
for categorizing anticancer peptides employing the consolidation of word
embedding and deep learning models. First, Word2Vec and FastText are evaluated
as word embedding techniques for the purpose of extracting peptide sequences.
Then, the output of word embedding models are fed into deep learning approaches
CNN, LSTM, BiLSTM. To demonstrate the contribution of proposed framework,
extensive experiments are carried on widely-used datasets in the literature,
ACPs250 and Independent. Experiment results show the usage of proposed model
enhances classification accuracy when compared to the state-of-the-art studies.
The proposed combination, FastText+BiLSTM, exhibits 92.50% of accuracy for
ACPs250 dataset, and 96.15% of accuracy for Independent dataset, thence
determining new state-of-the-art.",http://arxiv.org/pdf/2309.12058v1
2309.12056v1,cs.AI,BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision,2023-09-21 13:24:01+00:00,"This paper presents BELT, a novel model and learning framework for the
pivotal topic of brain-to-language translation research. The translation from
noninvasive brain signals into readable natural language has the potential to
promote the application scenario as well as the development of brain-computer
interfaces (BCI) as a whole. The critical problem in brain signal decoding or
brain-to-language translation is the acquisition of semantically appropriate
and discriminative EEG representation from a dataset of limited scale and
quality. The proposed BELT method is a generic and efficient framework that
bootstraps EEG representation learning using off-the-shelf large-scale
pretrained language models (LMs). With a large LM's capacity for understanding
semantic information and zero-shot generalization, BELT utilizes large LMs
trained on Internet-scale datasets to bring significant improvements to the
understanding of EEG signals.
  In particular, the BELT model is composed of a deep conformer encoder and a
vector quantization encoder. Semantical EEG representation is achieved by a
contrastive learning step that provides natural language supervision. We
achieve state-of-the-art results on two featuring brain decoding tasks
including the brain-to-language translation and zero-shot sentiment
classification. Specifically, our model surpasses the baseline model on both
tasks by 5.45% and over 10% and archives a 42.31% BLEU-1 score and 67.32%
precision on the main evaluation metrics for translation and zero-shot
sentiment classification respectively.",http://arxiv.org/pdf/2309.12056v1
2309.12052v1,cs.NI,Optimizing V2V Unicast Communication Transmission with Reinforcement Learning and Vehicle Clustering,2023-09-21 13:17:50+00:00,"Efficient routing algorithms based on vehicular ad hoc networks (VANETs) play
an important role in emerging intelligent transportation systems. This highly
dynamic topology faces a number of wireless communication service challenges.
In this paper, we propose a protocol based on reinforcement learning and
vehicle node clustering, the protocol is called Qucts, solve
vehicle-to-fixed-destination or V2V messaging problems. Improve message
delivery rates with minimal hops and latency, link stability is also taken into
account. The agreement is divided into three levels, first cluster the
vehicles, each cluster head broadcasts its own coordinates and speed, to get
more cluster members. Also when a cluster member receives another cluster head
broadcast message, the cluster head generates a list of surrounding clusters,
find the best cluster to the destination as the next cluster during message
passing. Second, the protocol constructs a Q-value table based on the state
after clustering, used to participate in the selection of messaging clusters.
Finally, we introduce parameters that express the stability of the vehicle
within the cluster, for communication node selection. This protocol hierarchy
makes Qucts an offline and online solution. In order to distinguish unstable
nodes within a cluster, Coding of each road, will have vehicles with planned
routes, For example, car hailing and public bus. Compare the overlap with other
planned paths vehicles in the cluster, low overlap is labeled as unstable
nodes. Vehicle path overlap rate without a planned path is set to the mean
value. Comparing Qucts with existing routing protocols through simulation, Our
proposed Qucts scheme provides large improvements in both data delivery rate
and end-to-end delay reduction.",http://arxiv.org/pdf/2309.12052v1
2309.12038v1,cs.RO,Uncertainty-driven Exploration Strategies for Online Grasp Learning,2023-09-21 13:06:03+00:00,"Existing grasp prediction approaches are mostly based on offline learning,
while, ignored the exploratory grasp learning during online adaptation to new
picking scenarios, i.e., unseen object portfolio, camera and bin settings etc.
In this paper, we present a novel method for online learning of grasp
predictions for robotic bin picking in a principled way. Existing grasp
prediction approaches are mostly based on offline learning, while, ignored the
exploratory grasp learning during online adaptation to new picking scenarios,
i.e., unseen object portfolio, camera and bin settings etc. In this paper, we
present a novel method for online learning of grasp predictions for robotic bin
picking in a principled way. Specifically, the online learning algorithm with
an effective exploration strategy can significantly improve its adaptation
performance to unseen environment settings. To this end, we first propose to
formulate online grasp learning as a RL problem that will allow to adapt both
grasp reward prediction and grasp poses. We propose various uncertainty
estimation schemes based on Bayesian Uncertainty Quantification and
Distributional Ensembles. We carry out evaluations on real-world bin picking
scenes of varying difficulty. The objects in the bin have various challenging
physical and perceptual characteristics that can be characterized by semi- or
total transparency, and irregular or curved surfaces. The results of our
experiments demonstrate a notable improvement in the suggested approach
compared to conventional online learning methods which incorporate only naive
exploration strategies.",http://arxiv.org/pdf/2309.12038v1
2309.12028v1,cs.LG,Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting,2023-09-21 12:44:55+00:00,"This paper studies the problem of traffic flow forecasting, which aims to
predict future traffic conditions on the basis of road networks and traffic
conditions in the past. The problem is typically solved by modeling complex
spatio-temporal correlations in traffic data using spatio-temporal graph neural
networks (GNNs). However, the performance of these methods is still far from
satisfactory since GNNs usually have limited representation capacity when it
comes to complex traffic networks. Graphs, by nature, fall short in capturing
non-pairwise relations. Even worse, existing methods follow the paradigm of
message passing that aggregates neighborhood information linearly, which fails
to capture complicated spatio-temporal high-order interactions. To tackle these
issues, in this paper, we propose a novel model named Dynamic Hypergraph
Structure Learning (DyHSL) for traffic flow prediction. To learn non-pairwise
relationships, our DyHSL extracts hypergraph structural information to model
dynamics in the traffic networks, and updates each node representation by
aggregating messages from its associated hyperedges. Additionally, to capture
high-order spatio-temporal relations in the road network, we introduce an
interactive graph convolution block, which further models the neighborhood
interaction for each node. Finally, we integrate these two views into a
holistic multi-scale correlation extraction module, which conducts temporal
pooling with different scales to model different temporal patterns. Extensive
experiments on four popular traffic benchmark datasets demonstrate the
effectiveness of our proposed DyHSL compared with a broad range of competing
baselines.",http://arxiv.org/pdf/2309.12028v1
2309.12022v1,cs.AI,Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification,2023-09-21 12:39:36+00:00,"In the film industry, movie posters have been an essential part of
advertising and marketing for many decades, and continue to play a vital role
even today in the form of digital posters through online, social media and OTT
platforms. Typically, movie posters can effectively promote and communicate the
essence of a film, such as its genre, visual style/ tone, vibe and storyline
cue/ theme, which are essential to attract potential viewers. Identifying the
genres of a movie often has significant practical applications in recommending
the film to target audiences. Previous studies on movie genre identification
are limited to subtitles, plot synopses, and movie scenes that are mostly
accessible after the movie release. Posters usually contain pre-release
implicit information to generate mass interest. In this paper, we work for
automated multi-label genre identification only from movie poster images,
without any aid of additional textual/meta-data information about movies, which
is one of the earliest attempts of its kind. Here, we present a deep
transformer network with a probabilistic module to identify the movie genres
exclusively from the poster. For experimental analysis, we procured 13882
number of posters of 13 genres from the Internet Movie Database (IMDb), where
our model performances were encouraging and even outperformed some major
contemporary architectures.",http://arxiv.org/pdf/2309.12022v1
2309.12004v1,cs.LG,Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling Based on Energy Consumption,2023-09-21 12:22:11+00:00,"This paper presents a Hierarchical Reinforcement Learning methodology
tailored for optimizing CubeSat task scheduling in Low Earth Orbits (LEO).
Incorporating a high-level policy for global task distribution and a low-level
policy for real-time adaptations as a safety mechanism, our approach integrates
the Similarity Attention-based Encoder (SABE) for task prioritization and an
MLP estimator for energy consumption forecasting. Integrating this mechanism
creates a safe and fault-tolerant system for CubeSat task scheduling.
Simulation results validate the Hierarchical Reinforcement Learning superior
convergence and task success rate, outperforming both the MADDPG model and
traditional random scheduling across multiple CubeSat configurations.",http://arxiv.org/pdf/2309.12004v1
2309.12002v1,cond-mat.mtrl-sci,Lateral Solid Phase Epitaxy of Yttrium Iron Garnet,2023-09-21 12:17:34+00:00,"Solid phase epitaxy is a crystallization technique used to produce high
quality thin films. Lateral solid phase epitaxy furthermore enables the
realization of non-planar structures, which are interesting, e.g., in the field
of spintronics. Here, we demonstrate lateral solid phase epitaxy of yttrium
iron garnet over an artificial edge, such that the crystallization direction is
perpendicular to the initial seed. We use single crystalline garnet seed
substrates partially covered by a \ch{SiO_x} film to study the lateral
crystallization over the \ch{SiO_x} mesa. The yttrium iron garnet layer retains
the crystal orientation of the substrate not only when in direct contact with
the substrate, but also across the edge on top of the \ch{SiO_x} mesa. By
controlling the crystallization dynamics it is possible to almost completely
suppress the formation of polycrystals and to enable epitaxial growth of single
crystalline yttrium iron garnet on top of mesas made from arbitrary materials.
From a series of annealing experiments, we extract an activation energy of
\SI{2.8}{eV} and a velocity prefactor of \SI{5.1e13}{nm/s} for the lateral
epitaxial crystallization along the <$100$> direction. Our results pave the way
to engineer single crystalline non-planar yttrium iron garnet structures with
controlled crystal orientation.",http://arxiv.org/pdf/2309.12002v1
2309.11998v1,cs.CL,LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset,2023-09-21 12:13:55+00:00,"Studying how people interact with large language models (LLMs) in real-world
scenarios is increasingly important due to their widespread use in various
applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset
containing one million real-world conversations with 25 state-of-the-art LLMs.
This dataset is collected from 210K unique IP addresses in the wild on our
Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's
content, including its curation process, basic statistics, and topic
distribution, highlighting its diversity, originality, and scale. We
demonstrate its versatility through four use cases: developing content
moderation models that perform similarly to GPT-4, building a safety benchmark,
training instruction-following models that perform similarly to Vicuna, and
creating challenging benchmark questions. We believe that this dataset will
serve as a valuable resource for understanding and advancing LLM capabilities.
The dataset is publicly available at
\url{https://huggingface.co/datasets/lmsys/lmsys-chat-1m}.",http://arxiv.org/pdf/2309.11998v1
2309.11991v1,cs.MA,Quantifying Feature Importance of Games and Strategies via Shapley Values,2023-09-21 12:03:13+00:00,"Recent advances in game informatics have enabled us to find strong strategies
across a diverse range of games. However, these strategies are usually
difficult for humans to interpret. On the other hand, research in Explainable
Artificial Intelligence (XAI) has seen a notable surge in scholarly activity.
Interpreting strong or near-optimal strategies or the game itself can provide
valuable insights. In this paper, we propose two methods to quantify the
feature importance using Shapley values: one for the game itself and another
for individual AIs. We empirically show that our proposed methods yield
intuitive explanations that resonate with and augment human understanding.",http://arxiv.org/pdf/2309.11991v1
2309.11987v1,cs.LG,Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis,2023-09-21 11:54:20+00:00,"Post-hoc explainability methods aim to clarify predictions of black-box
machine learning models. However, it is still largely unclear how well users
comprehend the provided explanations and whether these increase the users
ability to predict the model behavior. We approach this question by conducting
a user study to evaluate comprehensibility and predictability in two widely
used tools: LIME and SHAP. Moreover, we investigate the effect of
counterfactual explanations and misclassifications on users ability to
understand and predict the model behavior. We find that the comprehensibility
of SHAP is significantly reduced when explanations are provided for samples
near a model's decision boundary. Furthermore, we find that counterfactual
explanations and misclassifications can significantly increase the users
understanding of how a machine learning model is making decisions. Based on our
findings, we also derive design recommendations for future post-hoc
explainability methods with increased comprehensibility and predictability.",http://arxiv.org/pdf/2309.11987v1
2309.11984v1,cs.RO,Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study,2023-09-21 11:41:22+00:00,"Choosing an appropriate representation of the environment for the underlying
decision-making process of the \gls{RL} agent is not always straightforward.
The state representation should be inclusive enough to allow the agent to
informatively decide on its actions and compact enough to increase sample
efficiency for policy training. Given this outlook, this work examines the
effect of various state representations in incentivizing the agent to solve a
specific robotic task: antipodal and planar object grasping. A continuum of
state representation abstractions is defined, starting from a model-based
approach with complete system knowledge, through hand-crafted numerical, to
image-based representations with decreasing level of induced task-specific
knowledge. We examine the effects of each representation in the ability of the
agent to solve the task in simulation and the transferability of the learned
policy to the real robot. The results show that RL agents using numerical
states can perform on par with non-learning baselines. Furthermore, we find
that agents using image-based representations from pre-trained environment
embedding vectors perform better than end-to-end trained agents, and
hypothesize that task-specific knowledge is necessary for achieving convergence
and high success rates in robot control. Supplementary material can be found at
the project webpage: https://github.com/PetropoulakisPanagiotis/igae.",http://arxiv.org/pdf/2309.11984v1
2309.11981v1,cs.CL,Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics,2023-09-21 11:34:52+00:00,"In the burgeoning field of artificial intelligence (AI), the unprecedented
progress of large language models (LLMs) in natural language processing (NLP)
offers an opportunity to revisit the entire approach of traditional metrics of
machine intelligence, both in form and content. As the realm of machine
cognitive evaluation has already reached Imitation, the next step is an
efficient Language Acquisition and Understanding. Our paper proposes a paradigm
shift from the established Turing Test towards an all-embracing framework that
hinges on language acquisition, taking inspiration from the recent advancements
in LLMs. The present contribution is deeply tributary of the excellent work
from various disciplines, point out the need to keep interdisciplinary bridges
open, and delineates a more robust and sustainable approach.",http://arxiv.org/pdf/2309.11981v1
2309.11975v1,cs.AI,Inferring Capabilities from Task Performance with Bayesian Triangulation,2023-09-21 11:19:26+00:00,"As machine learning models become more general, we need to characterise them
in richer, more meaningful ways. We describe a method to infer the cognitive
profile of a system from diverse experimental data. To do so, we introduce
measurement layouts that model how task-instance features interact with system
capabilities to affect performance. These features must be triangulated in
complex ways to be able to infer capabilities from non-populational data -- a
challenge for traditional psychometric and inferential tools. Using the
Bayesian probabilistic programming library PyMC, we infer different cognitive
profiles for agents in two scenarios: 68 actual contestants in the AnimalAI
Olympics and 30 synthetic agents for O-PIAAGETS, an object permanence battery.
We showcase the potential for capability-oriented evaluation.",http://arxiv.org/pdf/2309.11975v1
2309.11973v1,math.NA,A review of troubled cell indicators for discontinuous Galerkin method,2023-09-21 11:10:53+00:00,"In this paper, eight different troubled cell indicators (shock detectors) are
reviewed for the solution of nonlinear hyperbolic conservation laws using
discontinuous Galerkin (DG) method and a WENO limiter. Extensive simulations
using one-dimensional and two-dimensional problems for various orders on the
hyperbolic system of Euler equations are used to compare these troubled cell
indicators. For one-dimensional problems, the performance of Fu and Shu
indicator and the modified KXRCF indicator is better than other indicators. For
two-dimensional problems, the performance of the artificial neural network
(ANN) indicator of Ray and Hesthaven is quite good and the Fu and Shu and the
modified KXRCF indicators are also good. These three indicators are suitable
candidates for applications of DGM using WENO limiters though it should be
noted that the ANN indicator is quite expensive and requires a lot of training.",http://arxiv.org/pdf/2309.11973v1
2309.11960v1,cs.AI,A Comprehensive Review on Financial Explainable AI,2023-09-21 10:30:49+00:00,"The success of artificial intelligence (AI), and deep learning models in
particular, has led to their widespread adoption across various industries due
to their ability to process huge amounts of data and learn complex patterns.
However, due to their lack of explainability, there are significant concerns
regarding their use in critical sectors, such as finance and healthcare, where
decision-making transparency is of paramount importance. In this paper, we
provide a comparative survey of methods that aim to improve the explainability
of deep learning models within the context of finance. We categorize the
collection of explainable AI methods according to their corresponding
characteristics, and we review the concerns and challenges of adopting
explainable AI methods, together with future directions we deemed appropriate
and important.",http://arxiv.org/pdf/2309.11960v1
2309.11957v1,cs.HC,Continuous Multi-user Activity Tracking via Room-Scale mmWave Sensing,2023-09-21 10:15:43+00:00,"Continuous detection of human activities and presence is essential for
developing a pervasive interactive smart space. Existing literature lacks
robust wireless sensing mechanisms capable of continuously monitoring multiple
users' activities without prior knowledge of the environment. Developing such a
mechanism requires simultaneous localization and tracking of multiple subjects.
In addition, it requires identifying their activities at various scales, some
being macro-scale activities like walking, squats, etc., while others are
micro-scale activities like typing or sitting, etc. In this paper, we develop a
holistic system called MARS using a single Commercial off the-shelf (COTS)
Millimeter Wave (mmWave) radar, which employs an intelligent model to sense
both macro and micro activities. In addition, it uses a dynamic spatial time
sharing approach to sense different subjects simultaneously. A thorough
evaluation of MARS shows that it can infer activities continuously with a
weighted F1-Score of > 94% and an average response time of approx 2 sec, with 5
subjects and 19 different activities.",http://arxiv.org/pdf/2309.11957v1
2309.11937v1,cs.AI,On the Definition of Appropriate Trust and the Tools that Come with it,2023-09-21 09:52:06+00:00,"Evaluating the efficiency of human-AI interactions is challenging, including
subjective and objective quality aspects. With the focus on the human
experience of the explanations, evaluations of explanation methods have become
mostly subjective, making comparative evaluations almost impossible and highly
linked to the individual user. However, it is commonly agreed that one aspect
of explanation quality is how effectively the user can detect if the
predictions are trustworthy and correct, i.e., if the explanations can increase
the user's appropriate trust in the model. This paper starts with the
definitions of appropriate trust from the literature. It compares the
definitions with model performance evaluation, showing the strong similarities
between appropriate trust and model performance evaluation. The paper's main
contribution is a novel approach to evaluating appropriate trust by taking
advantage of the likenesses between definitions. The paper offers several
straightforward evaluation methods for different aspects of user performance,
including suggesting a method for measuring uncertainty and appropriate trust
in regression.",http://arxiv.org/pdf/2309.11937v1
2309.11932v1,cs.LG,A Machine Learning-oriented Survey on Tiny Machine Learning,2023-09-21 09:47:12+00:00,"The emergence of Tiny Machine Learning (TinyML) has positively revolutionized
the field of Artificial Intelligence by promoting the joint design of
resource-constrained IoT hardware devices and their learning-based software
architectures. TinyML carries an essential role within the fourth and fifth
industrial revolutions in helping societies, economies, and individuals employ
effective AI-infused computing technologies (e.g., smart cities, automotive,
and medical robotics). Given its multidisciplinary nature, the field of TinyML
has been approached from many different angles: this comprehensive survey
wishes to provide an up-to-date overview focused on all the learning algorithms
within TinyML-based solutions. The survey is based on the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow,
allowing for a systematic and complete literature survey. In particular,
firstly we will examine the three different workflows for implementing a
TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly,
we propose a taxonomy that covers the learning panorama under the TinyML lens,
examining in detail the different families of model optimization and design, as
well as the state-of-the-art learning techniques. Thirdly, this survey will
present the distinct features of hardware devices and software tools that
represent the current state-of-the-art for TinyML intelligent edge
applications. Finally, we discuss the challenges and future directions.",http://arxiv.org/pdf/2309.11932v1
2309.11928v1,cs.CV,Video Scene Location Recognition with Neural Networks,2023-09-21 09:42:39+00:00,"This paper provides an insight into the possibility of scene recognition from
a video sequence with a small set of repeated shooting locations (such as in
television series) using artificial neural networks. The basic idea of the
presented approach is to select a set of frames from each scene, transform them
by a pre-trained singleimage pre-processing convolutional network, and classify
the scene location with subsequent layers of the neural network. The considered
networks have been tested and compared on a dataset obtained from The Big Bang
Theory television series. We have investigated different neural network layers
to combine individual frames, particularly AveragePooling, MaxPooling, Product,
Flatten, LSTM, and Bidirectional LSTM layers. We have observed that only some
of the approaches are suitable for the task at hand.",http://arxiv.org/pdf/2309.11928v1
2309.11918v1,eess.SP,Multi-Passive/Active-IRS Enhanced Wireless Coverage: Deployment Optimization and Cost-Performance Trade-off,2023-09-21 09:30:49+00:00,"Both passive and active intelligent reflecting surfaces (IRSs) can be
deployed in complex environments to enhance wireless network coverage by
creating multiple blockage-free cascaded line-of-sight (LoS) links. In this
paper, we study a multi-passive/active-IRS (PIRS/AIRS) aided wireless network
with a multi-antenna base station (BS) in a given region. First, we divide the
region into multiple non-overlapping cells, each of which may contain one
candidate location that can be deployed with a single PIRS or AIRS. Then, we
show several trade-offs between minimizing the total IRS deployment cost and
enhancing the signal-to-noise ratio (SNR) performance over all cells via
direct/cascaded LoS transmission with the BS. To reconcile these trade-offs, we
formulate a joint multi-PIRS/AIRS deployment problem to select an optimal
subset of all candidate locations for deploying IRS and also optimize the
number of passive/active reflecting elements deployed at each selected location
to satisfy a given SNR target over all cells, such that the total deployment
cost is minimized. However, due to the combinatorial optimization involved, the
formulated problem is difficult to be solved optimally. To tackle this
difficulty, we first optimize the reflecting element numbers with given
PIRS/AIRS deployed locations via sequential refinement, followed by a partial
enumeration to determine the PIRS/AIRS locations. Simulation results show that
our proposed algorithm achieves better cost-performance trade-offs than other
baseline deployment strategies.",http://arxiv.org/pdf/2309.11918v1
2309.11907v1,cs.AI,Learning to Recover for Safe Reinforcement Learning,2023-09-21 09:17:38+00:00,"Safety controllers is widely used to achieve safe reinforcement learning.
Most methods that apply a safety controller are using handcrafted safety
constraints to construct the safety controller. However, when the environment
dynamics are sophisticated, handcrafted safety constraints become unavailable.
Therefore, it worth to research on constructing safety controllers by learning
algorithms. We propose a three-stage architecture for safe reinforcement
learning, namely TU-Recovery Architecture. A safety critic and a recovery
policy is learned before task training. They form a safety controller to ensure
safety in task training. Then a phenomenon induced by disagreement between task
policy and recovery policy, called adversarial phenomenon, which reduces
learning efficiency and model performance, is described. Auxiliary reward is
proposed to mitigate adversarial phenomenon, while help the task policy to
learn to recover from high-risk states. A series of experiments are conducted
in a robot navigation environment. Experiments demonstrate that TU-Recovery
outperforms unconstrained counterpart in both reward gaining and constraint
violations during task training, and auxiliary reward further improve
TU-Recovery in reward-to-cost ratio by significantly reduce constraint
violations.",http://arxiv.org/pdf/2309.11907v1
2309.11904v1,astro-ph.EP,SELENA: Semi-analytical Integrator for Lunar Artificial Satellites,2023-09-21 09:15:54+00:00,"The present report summarizes the main theory and implementation steps
associated with SELENA (SEmi-anaLytical intEgrator for a luNar Artificial
satellite), i.e. the semi-analytical propagator for lunar satellite orbits
developed in the framework of the the R&T R-S20/BS-0005-062 CNES research
activity in collaboration between the University of Padova (UniPd), and the
Aristotle University of Thessaloniki (AUTH), both acting as contractors with
CNES.
  A detailed account of the method, algorithms and symbolic manipulations
employed in the derivation of the final theory are described in detail in this
report: they invoke the use of canonical perturbation theory in the form of Lie
series computed in `closed form', i.e., without expansions in the satellite's
orbital eccentricity. These algorithms are provided in the form of a symbolic
package accompanying the present report. The package contains symbolic algebra
programs, as well as explicit data files containing the final Hamiltonian,
equations of motion and transformations (i.e. the coefficients and exponents of
each variable in each term) leading to the averaging of the short-periodic
terms in the satellite's equations of motion.",http://arxiv.org/pdf/2309.11904v1
2309.11899v1,cs.CV,Unlocking the Heart Using Adaptive Locked Agnostic Networks,2023-09-21 09:06:36+00:00,"Supervised training of deep learning models for medical imaging applications
requires a significant amount of labeled data. This is posing a challenge as
the images are required to be annotated by medical professionals. To address
this limitation, we introduce the Adaptive Locked Agnostic Network (ALAN), a
concept involving self-supervised visual feature extraction using a large
backbone model to produce anatomically robust semantic self-segmentation. In
the ALAN methodology, this self-supervised training occurs only once on a large
and diverse dataset. Due to the intuitive interpretability of the segmentation,
downstream models tailored for specific tasks can be easily designed using
white-box models with few parameters. This, in turn, opens up the possibility
of communicating the inner workings of a model with domain experts and
introducing prior knowledge into it. It also means that the downstream models
become less data-hungry compared to fully supervised approaches. These
characteristics make ALAN particularly well-suited for resource-scarce
scenarios, such as costly clinical trials and rare diseases. In this paper, we
apply the ALAN approach to three publicly available echocardiography datasets:
EchoNet-Dynamic, CAMUS, and TMED-2. Our findings demonstrate that the
self-supervised backbone model robustly identifies anatomical subregions of the
heart in an apical four-chamber view. Building upon this, we design two
downstream models, one for segmenting a target anatomical region, and a second
for echocardiogram view classification.",http://arxiv.org/pdf/2309.11899v1
2309.11895v1,cs.SD,Audio Contrastive based Fine-tuning,2023-09-21 08:59:13+00:00,"Audio classification plays a crucial role in speech and sound processing
tasks with a wide range of applications. There still remains a challenge of
striking the right balance between fitting the model to the training data
(avoiding overfitting) and enabling it to generalise well to a new domain.
Leveraging the transferability of contrastive learning, we introduce Audio
Contrastive-based Fine-tuning (AudioConFit), an efficient approach
characterised by robust generalisability. Empirical experiments on a variety of
audio classification tasks demonstrate the effectiveness and robustness of our
approach, which achieves state-of-the-art results in various settings.",http://arxiv.org/pdf/2309.11895v1
2309.11893v1,cs.IT,On the Performance Analysis of RIS-Empowered Communications Over Nakagami-m Fading,2023-09-21 08:56:49+00:00,"In this paper, we study the performance of wireless communications empowered
by Reconfigurable Intelligent Surface (RISs) over Nakagami-m fading channels.
We consider two phase configuration designs for the RIS, one random and another
one based on coherent phase shifting. For both phase configuration cases, we
present single-integral expressions for the outage probability and the bit
error rate of binary modulation schemes, which can be efficiently evaluated
numerically. In addition, we propose accurate closed-form approximations for
the ergodic capacity of the considered system. For all considered metrics, we
have also derived simple analytical expressions that become tight for large
numbers of RIS reflecting elements. Numerically evaluated results compared with
Monte Carlo simulations are presented in order to verify the correctness of the
proposed analysis and showcase the impact of various system settings.",http://arxiv.org/pdf/2309.11893v1
2309.11876v1,cs.CV,Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training,2023-09-21 08:22:44+00:00,"Contrastive learning, which is a powerful technique for learning image-level
representations from unlabeled data, leads a promising direction to dealing
with the dilemma between large-scale pre-training and limited labeled data.
However, most existing contrastive learning strategies are designed mainly for
downstream tasks of natural images, therefore they are sub-optimal and even
worse than learning from scratch when directly applied to medical images whose
downstream tasks are usually segmentation. In this work, we propose a novel
asymmetric contrastive learning framework named JCL for medical image
segmentation with self-supervised pre-training. Specifically, (1) A novel
asymmetric contrastive learning strategy is proposed to pre-train both encoder
and decoder simultaneously in one-stage to provide better initialization for
segmentation models. (2) A multi-level contrastive loss is designed to take the
correspondence among feature-level, image-level and pixel-level projections,
respectively into account to make sure multi-level representations can be
learned by the encoder and decoder during pre-training. (3) Experiments on
multiple medical image datasets indicate our JCL framework outperforms existing
SOTA contrastive learning strategies.",http://arxiv.org/pdf/2309.11876v1
2309.11875v1,cs.LG,Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes,2023-09-21 08:22:12+00:00,"Machine learning models trained with structural health monitoring data have
become a powerful tool for system identification. This paper presents a
physics-informed Gaussian process (GP) model for Timoshenko beam elements. The
model is constructed as a multi-output GP with covariance and cross-covariance
kernels analytically derived based on the differential equations for
deflections, rotations, strains, bending moments, shear forces and applied
loads. Stiffness identification is performed in a Bayesian format by maximising
a posterior model through a Markov chain Monte Carlo method, yielding a
stochastic model for the structural parameters. The optimised GP model is
further employed for probabilistic predictions of unobserved responses.
Additionally, an entropy-based method for physics-informed sensor placement
optimisation is presented, exploiting heterogeneous sensor position information
and structural boundary conditions built into the GP model. Results demonstrate
that the proposed approach is effective at identifying structural parameters
and is capable of fusing data from heterogeneous and multi-fidelity sensors.
Probabilistic predictions of structural responses and internal forces are in
closer agreement with measured data. We validate our model with an experimental
setup and discuss the quality and uncertainty of the obtained results. The
proposed approach has potential applications in the field of structural health
monitoring (SHM) for both mechanical and structural systems.",http://arxiv.org/pdf/2309.11875v1
2309.11858v1,cs.CV,OSNet & MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios,2023-09-21 07:59:58+00:00,"Recently, linear computed tomography (LCT) systems have actively attracted
attention. To weaken projection truncation and image the region of interest
(ROI) for LCT, the backprojection filtration (BPF) algorithm is an effective
solution. However, in BPF for LCT, it is difficult to achieve stable interior
reconstruction, and for differentiated backprojection (DBP) images of LCT,
multiple rotation-finite inversion of Hilbert transform (Hilbert
filtering)-inverse rotation operations will blur the image. To satisfy multiple
reconstruction scenarios for LCT, including interior ROI, complete object, and
exterior region beyond field-of-view (FOV), and avoid the rotation operations
of Hilbert filtering, we propose two types of reconstruction architectures. The
first overlays multiple DBP images to obtain a complete DBP image, then uses a
network to learn the overlying Hilbert filtering function, referred to as the
Overlay-Single Network (OSNet). The second uses multiple networks to train
different directional Hilbert filtering models for DBP images of multiple
linear scannings, respectively, and then overlays the reconstructed results,
i.e., Multiple Networks Overlaying (MNetO). In two architectures, we introduce
a Swin Transformer (ST) block to the generator of pix2pixGAN to extract both
local and global features from DBP images at the same time. We investigate two
architectures from different networks, FOV sizes, pixel sizes, number of
projections, geometric magnification, and processing time. Experimental results
show that two architectures can both recover images. OSNet outperforms BPF in
various scenarios. For the different networks, ST-pix2pixGAN is superior to
pix2pixGAN and CycleGAN. MNetO exhibits a few artifacts due to the differences
among the multiple models, but any one of its models is suitable for imaging
the exterior edge in a certain direction.",http://arxiv.org/pdf/2309.11858v1
2309.11853v1,cs.CL,BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework,2023-09-21 07:55:54+00:00,"Relation triple extraction (RTE) is an essential task in information
extraction and knowledge graph construction. Despite recent advancements,
existing methods still exhibit certain limitations. They just employ
generalized pre-trained models and do not consider the specificity of RTE
tasks. Moreover, existing tagging-based approaches typically decompose the RTE
task into two subtasks, initially identifying subjects and subsequently
identifying objects and relations. They solely focus on extracting relational
triples from subject to object, neglecting that once the extraction of a
subject fails, it fails in extracting all triples associated with that subject.
To address these issues, we propose BitCoin, an innovative Bidirectional
tagging and supervised Contrastive learning based joint relational triple
extraction framework. Specifically, we design a supervised contrastive learning
method that considers multiple positives per anchor rather than restricting it
to just one positive. Furthermore, a penalty term is introduced to prevent
excessive similarity between the subject and object. Our framework implements
taggers in two directions, enabling triples extraction from subject to object
and object to subject. Experimental results show that BitCoin achieves
state-of-the-art results on the benchmark datasets and significantly improves
the F1 score on Normal, SEO, EPO, and multiple relation extraction tasks.",http://arxiv.org/pdf/2309.11853v1
2309.11850v1,cs.IT,Joint Beamforming for RIS Aided Full-Duplex Integrated Sensing and Uplink Communication,2023-09-21 07:48:56+00:00,"This paper studies integrated sensing and communication (ISAC) technology in
a full-duplex (FD) uplink communication system. As opposed to the half-duplex
system, where sensing is conducted in a first-emit-then-listen manner, FD ISAC
system emits and listens simultaneously and hence conducts uninterrupted target
sensing. Besides, impressed by the recently emerging reconfigurable intelligent
surface (RIS) technology, we also employ RIS to improve the self-interference
(SI) suppression and signal processing gain. As will be seen, the joint
beamforming, RIS configuration and mobile users' power allocation is a
difficult optimization problem. To resolve this challenge, via leveraging the
cutting-the-edge majorization-minimization (MM) and penalty-dual-decomposition
(PDD) methods, we develop an iterative solution that optimizes all variables
via using convex optimization techniques. Numerical results demonstrate the
effectiveness of our proposed solution and the great benefit of employing RIS
in the FD ISAC system.",http://arxiv.org/pdf/2309.11850v1
2309.11839v1,cs.CV,MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation,2023-09-21 07:30:21+00:00,"Multi-modal unsupervised domain adaptation (MM-UDA) for 3D semantic
segmentation is a practical solution to embed semantic understanding in
autonomous systems without expensive point-wise annotations. While previous
MM-UDA methods can achieve overall improvement, they suffer from significant
class-imbalanced performance, restricting their adoption in real applications.
This imbalanced performance is mainly caused by: 1) self-training with
imbalanced data and 2) the lack of pixel-wise 2D supervision signals. In this
work, we propose Multi-modal Prior Aided (MoPA) domain adaptation to improve
the performance of rare objects. Specifically, we develop Valid Ground-based
Insertion (VGI) to rectify the imbalance supervision signals by inserting prior
rare objects collected from the wild while avoiding introducing artificial
artifacts that lead to trivial solutions. Meanwhile, our SAM consistency loss
leverages the 2D prior semantic masks from SAM as pixel-wise supervision
signals to encourage consistent predictions for each object in the semantic
mask. The knowledge learned from modal-specific prior is then shared across
modalities to achieve better rare object segmentation. Extensive experiments
show that our method achieves state-of-the-art performance on the challenging
MM-UDA benchmark. Code will be available at https://github.com/AronCao49/MoPA.",http://arxiv.org/pdf/2309.11839v1
2309.11838v1,cs.CL,Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues,2023-09-21 07:28:03+00:00,"In this paper, we investigate the use of large language models (LLMs) like
ChatGPT for document-grounded response generation in the context of
information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus
of task-oriented dialogues in four social service domains previously used in
the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded
in multiple documents providing relevant information. We generate dialogue
completion responses by prompting a ChatGPT model, using two methods:
Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT
model pretraining while LlamaIndex also extracts relevant information from
documents. Observing that document-grounded response generation via LLMs cannot
be adequately assessed by automatic evaluation metrics as they are
significantly more verbose, we perform a human evaluation where annotators rate
the output of the shared task winning system, the two Chat-GPT variants
outputs, and human responses. While both ChatGPT variants are more likely to
include information not present in the relevant segments, possibly including a
presence of hallucinations, they are rated higher than both the shared task
winning system and human responses.",http://arxiv.org/pdf/2309.11838v1
2309.11811v1,eess.SP,Multimodal Transformers for Wireless Communications: A Case Study in Beam Prediction,2023-09-21 06:29:38+00:00,"Wireless communications at high-frequency bands with large antenna arrays
face challenges in beam management, which can potentially be improved by
multimodality sensing information from cameras, LiDAR, radar, and GPS. In this
paper, we present a multimodal transformer deep learning framework for
sensing-assisted beam prediction. We employ a convolutional neural network to
extract the features from a sequence of images, point clouds, and radar raw
data sampled over time. At each convolutional layer, we use transformer
encoders to learn the hidden relations between feature tokens from different
modalities and time instances over abstraction space and produce encoded
vectors for the next-level feature extraction. We train the model on a
combination of different modalities with supervised learning. We try to enhance
the model over imbalanced data by utilizing focal loss and exponential moving
average. We also evaluate data processing and augmentation techniques such as
image enhancement, segmentation, background filtering, multimodal data
flipping, radar signal transformation, and GPS angle calibration. Experimental
results show that our solution trained on image and GPS data produces the best
distance-based accuracy of predicted beams at 78.44%, with effective
generalization to unseen day scenarios near 73% and night scenarios over 84%.
This outperforms using other modalities and arbitrary data processing
techniques, which demonstrates the effectiveness of transformers with feature
fusion in performing radio beam prediction from images and GPS. Furthermore,
our solution could be pretrained from large sequences of multimodality wireless
data, on fine-tuning for multiple downstream radio network tasks.",http://arxiv.org/pdf/2309.11811v1
2309.11810v1,astro-ph.CO,Extragalactic Test of General Relativity from Strong Gravitational Lensing by using Artificial Neural Networks,2023-09-21 06:28:39+00:00,"This study aims to test the validity of general relativity (GR) on kiloparsec
scales by employing a newly compiled galaxy-scale strong gravitational lensing
(SGL) sample. We utilize the distance sum rule within the
Friedmann-Lema\^{\i}tre-Robertson-Walker metric to obtain cosmology-independent
constraints on both the parameterized post-Newtonian parameter $\gamma_{\rm
PPN}$ and the spatial curvature $\Omega_{k}$, which overcomes the circularity
problem induced by the presumption of a cosmological model grounded in GR. To
calibrate the distances in the SGL systems, we introduce a novel nonparametric
approach, Artificial Neural Network (ANN), to reconstruct a smooth
distance--redshift relation from the Pantheon+ sample of type Ia supernovae.
Our results show that $\gamma_{\rm PPN}=1.16_{-0.12}^{+0.15}$ and
$\Omega_k=0.89_{-1.00}^{+1.97}$, indicating a spatially flat universe with the
conservation of GR (i.e., $\Omega_k=0$ and $\gamma_{\rm PPN}=1$) is basically
supported within $1\sigma$ confidence level. Assuming a zero spatial curvature,
we find $\gamma_{\rm PPN}=1.09_{-0.10}^{+0.11}$, representing an agreement with
the prediction of 1 from GR to a 9.6\% precision. If we instead assume GR holds
(i.e., $\gamma_{\rm PPN}=1$), the curvature parameter constraint can be further
improved to be $\Omega_k=0.11_{-0.47}^{+0.78}$. These resulting constraints
demonstrate the effectiveness of our method in testing GR on galactic scales by
combining observations of strong lensing and the distance--redshift relation
reconstructed by ANN.",http://arxiv.org/pdf/2309.11810v1
2309.11805v1,cs.AI,JobRecoGPT -- Explainable job recommendations using LLMs,2023-09-21 06:25:28+00:00,"In today's rapidly evolving job market, finding the right opportunity can be
a daunting challenge. With advancements in the field of AI, computers can now
recommend suitable jobs to candidates. However, the task of recommending jobs
is not same as recommending movies to viewers. Apart from must-have criteria,
like skills and experience, there are many subtle aspects to a job which can
decide if it is a good fit or not for a given candidate. Traditional approaches
can capture the quantifiable aspects of jobs and candidates, but a substantial
portion of the data that is present in unstructured form in the job
descriptions and resumes is lost in the process of conversion to structured
format. As of late, Large Language Models (LLMs) have taken over the AI field
by storm with extraordinary performance in fields where text-based data is
available. Inspired by the superior performance of LLMs, we leverage their
capability to understand natural language for capturing the information that
was previously getting lost during the conversion of unstructured data to
structured form. To this end, we compare performance of four different
approaches for job recommendations namely, (i) Content based deterministic,
(ii) LLM guided, (iii) LLM unguided, and (iv) Hybrid. In this study, we present
advantages and limitations of each method and evaluate their performance in
terms of time requirements.",http://arxiv.org/pdf/2309.11805v1
2309.11795v1,math.NA,An optimal control deep learning method to design artificial viscosities for Discontinuous Galerkin schemes,2023-09-21 05:43:15+00:00,"In this paper, we propose a method for constructing a neural network
viscosity in order to reduce the non-physical oscillations generated by
high-order Discontiuous Galerkin (DG) methods. To this end, the problem is
reformulated as an optimal control problem for which the control is the
viscosity function and the cost function involves comparison with a reference
solution after several compositions of the scheme. The learning process is
strongly based on gradient backpropagation tools. Numerical simulations show
that the artificial viscosities constructed in this way are just as good or
better than those used in the literatur",http://arxiv.org/pdf/2309.11795v1
2309.11793v1,quant-ph,Quantum Circuits for Stabilizer Error Correcting Codes: A Tutorial,2023-09-21 05:42:04+00:00,"Quantum computers have the potential to provide exponential speedups over
their classical counterparts. Quantum principles are being applied to fields
such as communications, information processing, and artificial intelligence to
achieve quantum advantage. However, quantum bits are extremely noisy and prone
to decoherence. Thus, keeping the qubits error free is extremely important
toward reliable quantum computing. Quantum error correcting codes have been
studied for several decades and methods have been proposed to import classical
error correcting codes to the quantum domain. However, circuits for such
encoders and decoders haven't been explored in depth. This paper serves as a
tutorial on designing and simulating quantum encoder and decoder circuits for
stabilizer codes. We present encoding and decoding circuits for five-qubit code
and Steane code, along with verification of these circuits using IBM Qiskit. We
also provide nearest neighbour compliant encoder and decoder circuits for the
five-qubit code.",http://arxiv.org/pdf/2309.11793v1
2309.11782v1,cs.CV,DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning,2023-09-21 05:12:55+00:00,"Self-supervised learning (SSL) has gained remarkable success, for which
contrastive learning (CL) plays a key role. However, the recent development of
new non-CL frameworks has achieved comparable or better performance with high
improvement potential, prompting researchers to enhance these frameworks
further. Assimilating CL into non-CL frameworks has been thought to be
beneficial, but empirical evidence indicates no visible improvements. In view
of that, this paper proposes a strategy of performing CL along the dimensional
direction instead of along the batch direction as done in conventional
contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL
aims to enhance the feature diversity, and it can serve as a regularizer to
prior SSL frameworks. DimCL has been found to be effective, and the
hardness-aware property is identified as a critical reason for its success.
Extensive experimental results reveal that assimilating DimCL into SSL
frameworks leads to performance improvement by a non-trivial margin on various
datasets and backbone architectures.",http://arxiv.org/pdf/2309.11782v1
2309.11755v1,cs.CV,2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation on Point Cloud,2023-09-21 03:32:22+00:00,"Recently, multi-modality models have been introduced because of the
complementary information from different sensors such as LiDAR and cameras. It
requires paired data along with precise calibrations for all modalities, the
complicated calibration among modalities hugely increases the cost of
collecting such high-quality datasets, and hinder it from being applied to
practical scenarios. Inherit from the previous works, we not only fuse the
information from multi-modality without above issues, and also exhaust the
information in the RGB modality. We introduced the 2D Detection Annotations
Transmittable Aggregation(\textbf{2DDATA}), designing a data-specific branch,
called \textbf{Local Object Branch}, which aims to deal with points in a
certain bounding box, because of its easiness of acquiring 2D bounding box
annotations. We demonstrate that our simple design can transmit bounding box
prior information to the 3D encoder model, proving the feasibility of large
multi-modality models fused with modality-specific data.",http://arxiv.org/pdf/2309.11755v1
2309.11753v1,cs.AI,Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language,2023-09-21 03:25:35+00:00,"Reinforcement learning is a powerful technique for learning from trial and
error, but it often requires a large number of interactions to achieve good
performance. In some domains, such as sparse-reward tasks, an oracle that can
provide useful feedback or guidance to the agent during the learning process is
really of great importance. However, querying the oracle too frequently may be
costly or impractical, and the oracle may not always have a clear answer for
every situation. Therefore, we propose a novel method for interacting with the
oracle in a selective and efficient way, using a retrieval-based approach. We
assume that the interaction can be modeled as a sequence of templated questions
and answers, and that there is a large corpus of previous interactions
available. We use a neural network to encode the current state of the agent and
the oracle, and retrieve the most relevant question from the corpus to ask the
oracle. We then use the oracle's answer to update the agent's policy and value
function. We evaluate our method on an object manipulation task. We show that
our method can significantly improve the efficiency of RL by reducing the
number of interactions needed to reach a certain level of performance, compared
to baselines that do not use the oracle or use it in a naive way.",http://arxiv.org/pdf/2309.11753v1
2309.11751v1,cs.CV,How Robust is Google's Bard to Adversarial Image Attacks?,2023-09-21 03:24:30+00:00,"Multimodal Large Language Models (MLLMs) that integrate text and other
modalities (especially vision) have achieved unprecedented performance in
various multimodal tasks. However, due to the unsolved adversarial robustness
problem of vision models, MLLMs can have more severe safety and security risks
by introducing the vision inputs. In this work, we study the adversarial
robustness of Google's Bard, a competitive chatbot to ChatGPT that released its
multimodal capability recently, to better understand the vulnerabilities of
commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs,
the generated adversarial examples can mislead Bard to output wrong image
descriptions with a 22% success rate based solely on the transferability. We
show that the adversarial examples can also attack other MLLMs, e.g., a 26%
attack success rate against Bing Chat and a 86% attack success rate against
ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face
detection and toxicity detection of images. We design corresponding attacks to
evade these defenses, demonstrating that the current defenses of Bard are also
vulnerable. We hope this work can deepen our understanding on the robustness of
MLLMs and facilitate future research on defenses. Our code is available at
https://github.com/thu-ml/Attack-Bard.",http://arxiv.org/pdf/2309.11751v1
2309.11748v1,cs.IT,Deep Learning Meets Swarm Intelligence for UAV-Assisted IoT Coverage in Massive MIMO,2023-09-21 03:03:06+00:00,"This study considers a UAV-assisted multi-user massive multiple-input
multiple-output (MU-mMIMO) systems, where a decode-and-forward (DF) relay in
the form of an unmanned aerial vehicle (UAV) facilitates the transmission of
multiple data streams from a base station (BS) to multiple Internet-of-Things
(IoT) users. A joint optimization problem of hybrid beamforming (HBF), UAV
relay positioning, and power allocation (PA) to multiple IoT users to maximize
the total achievable rate (AR) is investigated. The study adopts a
geometry-based millimeter-wave (mmWave) channel model for both links and
proposes three different swarm intelligence (SI)-based algorithmic solutions to
optimize: 1) UAV location with equal PA; 2) PA with fixed UAV location; and 3)
joint PA with UAV deployment. The radio frequency (RF) stages are designed to
reduce the number of RF chains based on the slow time-varying angular
information, while the baseband (BB) stages are designed using the
reduced-dimension effective channel matrices. Then, a novel deep learning
(DL)-based low-complexity joint hybrid beamforming, UAV location and power
allocation optimization scheme (J-HBF-DLLPA) is proposed via fully-connected
deep neural network (DNN), consisting of an offline training phase, and an
online prediction of UAV location and optimal power values for maximizing the
AR. The illustrative results show that the proposed algorithmic solutions can
attain higher capacity and reduce average delay for delay-constrained
transmissions in a UAV-assisted MU-mMIMO IoT systems. Additionally, the
proposed J-HBF-DLLPA can closely approach the optimal capacity while
significantly reducing the runtime by 99%, which makes the DL-based solution a
promising implementation for real-time online applications in UAV-assisted
MU-mMIMO IoT systems.",http://arxiv.org/pdf/2309.11748v1
2309.11737v1,cs.AI,Choice-75: A Dataset on Decision Branching in Script Learning,2023-09-21 02:23:44+00:00,"Script learning studies how daily events unfold. Previous works tend to
consider a script as a linear sequence of events while ignoring the potential
branches that arise due to people's circumstantial choices. We hence propose
Choice-75, the first benchmark that challenges intelligent systems to predict
decisions given descriptive scenarios, containing 75 scripts and more than 600
scenarios. While large language models demonstrate overall decent performances,
there is still notable room for improvement in many hard scenarios.",http://arxiv.org/pdf/2309.11737v1
2309.11729v1,astro-ph.EP,The possibility of detecting our solar system through astrometry,2023-09-21 02:03:05+00:00,"Searching for exoplanets with different methods has always been the focus of
astronomers over the past few years. Among multiple planet detection
techniques, astrometry stands out for its capability to accurately determine
the orbital parameters of exoplanets. In this study, we examine the likelihood
of extraterrestrial intelligent civilizations detecting planets in our solar
system using the astrometry method. By conducting injection-recovery
simulations, we investigate the detectability of the four giant planets in our
solar system under different observing baselines and observational errors. Our
findings indicate that extraterrestrial intelligence could detect and
characterize all four giant planets, provided they are observed for a minimum
of 90 years with signal-noise ratios exceeding 1. For individual planets such
as Jupiter, Saturn, and Neptune, a baseline that surpasses half of their
orbital periods is necessary for detection. However, Uranus requires longer
observing baselines since its orbital period is roughly half of that of
Neptune. If the astrometry precision is equal to or better than 10 $\mu$as, all
8,707 stars located within 30 pcs of our solar system possess the potential to
detect the four giant planets within 100 years. Additionally, our prediction
suggests that over 300 stars positioned within 10 pcs from our solar system
could detect our Earth if they achieve an astrometry precision of 0.3 $\mu$as.",http://arxiv.org/pdf/2309.11729v1
2309.11725v1,cs.SD,FluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency,2023-09-21 01:58:01+00:00,"Text-based speech editing (TSE) techniques are designed to enable users to
edit the output audio by modifying the input text transcript instead of the
audio itself. Despite much progress in neural network-based TSE techniques, the
current techniques have focused on reducing the difference between the
generated speech segment and the reference target in the editing region,
ignoring its local and global fluency in the context and original utterance. To
maintain the speech fluency, we propose a fluency speech editing model, termed
\textit{FluentEditor}, by considering fluency-aware training criterion in the
TSE training. Specifically, the \textit{acoustic consistency constraint} aims
to smooth the transition between the edited region and its neighboring acoustic
segments consistent with the ground truth, while the \textit{prosody
consistency constraint} seeks to ensure that the prosody attributes within the
edited regions remain consistent with the overall style of the original
utterance. The subjective and objective experimental results on VCTK
demonstrate that our \textit{FluentEditor} outperforms all advanced baselines
in terms of naturalness and fluency. The audio samples and code are available
at \url{https://github.com/Ai-S2-Lab/FluentEditor}.",http://arxiv.org/pdf/2309.11725v1
2309.11724v1,cs.AI,Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech,2023-09-21 01:51:10+00:00,"Prosodic phrasing is crucial to the naturalness and intelligibility of
end-to-end Text-to-Speech (TTS). There exist both linguistic and emotional
prosody in natural speech. As the study of prosodic phrasing has been
linguistically motivated, prosodic phrasing for expressive emotion rendering
has not been well studied. In this paper, we propose an emotion-aware prosodic
phrasing model, termed \textit{EmoPP}, to mine the emotional cues of utterance
accurately and predict appropriate phrase breaks. We first conduct objective
observations on the ESD dataset to validate the strong correlation between
emotion and prosodic phrasing. Then the objective and subjective evaluations
show that the EmoPP outperforms all baselines and achieves remarkable
performance in terms of emotion expressiveness. The audio samples and the code
are available at \url{https://github.com/AI-S2-Lab/EmoPP}.",http://arxiv.org/pdf/2309.11724v1
2309.11714v1,eess.SP,A Dynamic Domain Adaptation Deep Learning Network for EEG-based Motor Imagery Classification,2023-09-21 01:34:00+00:00,"There is a correlation between adjacent channels of electroencephalogram
(EEG), and how to represent this correlation is an issue that is currently
being explored. In addition, due to inter-individual differences in EEG
signals, this discrepancy results in new subjects need spend a amount of
calibration time for EEG-based motor imagery brain-computer interface. In order
to solve the above problems, we propose a Dynamic Domain Adaptation Based Deep
Learning Network (DADL-Net). First, the EEG data is mapped to the
three-dimensional geometric space and its temporal-spatial features are learned
through the 3D convolution module, and then the spatial-channel attention
mechanism is used to strengthen the features, and the final convolution module
can further learn the spatial-temporal information of the features. Finally, to
account for inter-subject and cross-sessions differences, we employ a dynamic
domain-adaptive strategy, the distance between features is reduced by
introducing a Maximum Mean Discrepancy loss function, and the classification
layer is fine-tuned by using part of the target domain data. We verify the
performance of the proposed method on BCI competition IV 2a and OpenBMI
datasets. Under the intra-subject experiment, the accuracy rates of 70.42% and
73.91% were achieved on the OpenBMI and BCIC IV 2a datasets.",http://arxiv.org/pdf/2309.11714v1
2309.11691v1,cs.AI,RAI4IoE: Responsible AI for Enabling the Internet of Energy,2023-09-20 23:45:54+00:00,"This paper plans to develop an Equitable and Responsible AI framework with
enabling techniques and algorithms for the Internet of Energy (IoE), in short,
RAI4IoE. The energy sector is going through substantial changes fueled by two
key drivers: building a zero-carbon energy sector and the digital
transformation of the energy infrastructure. We expect to see the convergence
of these two drivers resulting in the IoE, where renewable distributed energy
resources (DERs), such as electric cars, storage batteries, wind turbines and
photovoltaics (PV), can be connected and integrated for reliable energy
distribution by leveraging advanced 5G-6G networks and AI technology. This
allows DER owners as prosumers to participate in the energy market and derive
economic incentives. DERs are inherently asset-driven and face equitable
challenges (i.e., fair, diverse and inclusive). Without equitable access,
privileged individuals, groups and organizations can participate and benefit at
the cost of disadvantaged groups. The real-time management of DER resources not
only brings out the equity problem to the IoE, it also collects highly
sensitive location, time, activity dependent data, which requires to be handled
responsibly (e.g., privacy, security and safety), for AI-enhanced predictions,
optimization and prioritization services, and automated management of flexible
resources. The vision of our project is to ensure equitable participation of
the community members and responsible use of their data in IoE so that it could
reap the benefits of advances in AI to provide safe, reliable and sustainable
energy services.",http://arxiv.org/pdf/2309.11691v1
2309.11688v1,cs.CL,LLM Guided Inductive Inference for Solving Compositional Problems,2023-09-20 23:44:16+00:00,"While large language models (LLMs) have demonstrated impressive performance
in question-answering tasks, their performance is limited when the questions
require knowledge that is not included in the model's training data and can
only be acquired through direct observation or interaction with the real world.
Existing methods decompose reasoning tasks through the use of modules invoked
sequentially, limiting their ability to answer deep reasoning tasks. We
introduce a method, Recursion based extensible LLM (REBEL), which handles
open-world, deep reasoning tasks by employing automated reasoning techniques
like dynamic planning and forward-chaining strategies. REBEL allows LLMs to
reason via recursive problem decomposition and utilization of external tools.
The tools that REBEL uses are specified only by natural language description.
We further demonstrate REBEL capabilities on a set of problems that require a
deeply nested use of external tools in a compositional and conversational
setting.",http://arxiv.org/pdf/2309.11688v1
2309.11682v1,cs.LG,Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework,2023-09-20 23:25:28+00:00,"While training fair machine learning models has been studied extensively in
recent years, most developed methods rely on the assumption that the training
and test data have similar distributions. In the presence of distribution
shifts, fair models may behave unfairly on test data. There have been some
developments for fair learning robust to distribution shifts to address this
shortcoming. However, most proposed solutions are based on the assumption of
having access to the causal graph describing the interaction of different
features. Moreover, existing algorithms require full access to data and cannot
be used when small batches are used (stochastic/batch implementation). This
paper proposes the first stochastic distributionally robust fairness framework
with convergence guarantees that do not require knowledge of the causal graph.
More specifically, we formulate the fair inference in the presence of the
distribution shift as a distributionally robust optimization problem under
$L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual
Information (ERMI) as the measure of fairness violation. We then discuss how
the proposed method can be implemented in a stochastic fashion. We have
evaluated the presented framework's performance and efficiency through
extensive experiments on real datasets consisting of distribution shifts.",http://arxiv.org/pdf/2309.11682v1
2309.11680v1,cs.LG,Federated Learning with Neural Graphical Models,2023-09-20 23:24:22+00:00,"Federated Learning (FL) addresses the need to create models based on
proprietary data in such a way that multiple clients retain exclusive control
over their data, while all benefit from improved model accuracy due to pooled
resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic
Graphical models that utilize the expressive power of neural networks to learn
complex non-linear dependencies between the input features. They learn to
capture the underlying data distribution and have efficient algorithms for
inference and sampling. We develop a FL framework which maintains a global NGM
model that learns the averaged information from the local NGM models while
keeping the training data within the client's environment. Our design, FedNGMs,
avoids the pitfalls and shortcomings of neuron matching frameworks like
Federated Matched Averaging that suffers from model parameter explosion. Our
global model size remains constant throughout the process. In the cases where
clients have local variables that are not part of the combined global
distribution, we propose a `Stitching' algorithm, which personalizes the global
NGM models by merging the additional variables using the client's data. FedNGM
is robust to data heterogeneity, large number of participants, and limited
communication bandwidth.",http://arxiv.org/pdf/2309.11680v1
2309.11672v1,cs.AI,Generative AI in Mafia-like Game Simulation,2023-09-20 22:38:34+00:00,"In this research, we explore the efficacy and potential of Generative AI
models, specifically focusing on their application in role-playing simulations
exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's
advanced capabilities, the study aimed to showcase the model's potential in
understanding, decision-making, and interaction during game scenarios.
Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo,
demonstrated GPT-4's enhanced adaptability to the game environment, with
significant improvements in posing relevant questions and forming human-like
responses. However, challenges such as the model;s limitations in bluffing and
predicting opponent moves emerged. Reflections on game development, financial
constraints, and non-verbal limitations of the study were also discussed. The
findings suggest that while GPT-4 exhibits promising advancements over earlier
models, there remains potential for further development, especially in
instilling more human-like attributes in AI.",http://arxiv.org/pdf/2309.11672v1
2309.11665v1,eess.SP,Channel Reciprocity Attacks Using Intelligent Surfaces with Non-Diagonal Phase Shifts,2023-09-20 22:16:21+00:00,"While reconfigurable intelligent surface (RIS) technology has been shown to
provide numerous benefits to wireless systems, in the hands of an adversary
such technology can also be used to disrupt communication links. This paper
describes and analyzes an RIS-based attack on multi-antenna wireless systems
that operate in time-division duplex mode under the assumption of channel
reciprocity. In particular, we show how an RIS with a non-diagonal (ND) phase
shift matrix (referred to here as an ND-RIS) can be deployed to maliciously
break the channel reciprocity and hence degrade the downlink network
performance. Such an attack is entirely passive and difficult to detect. We
provide a theoretical analysis of the degradation in the sum ergodic rate that
results when an arbitrary malicious ND-RIS is deployed and design an approach
based on the genetic algorithm for optimizing the ND structure under partial
knowledge of the available channel state information. Our simulation results
validate the analysis and demonstrate that an ND-RIS channel reciprocity attack
can dramatically reduce the downlink throughput.",http://arxiv.org/pdf/2309.11665v1
2309.11653v1,cs.HC,"""It's a Fair Game'', or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents",2023-09-20 21:34:36+00:00,"The widespread use of Large Language Model (LLM)-based conversational agents
(CAs), especially in high-stakes domains, raises many privacy concerns.
Building ethical LLM-based CAs that respect user privacy requires an in-depth
understanding of the privacy risks that concern users the most. However,
existing research, primarily model-centered, does not provide insight into
users' perspectives. To bridge this gap, we analyzed sensitive disclosures in
real-world ChatGPT conversations and conducted semi-structured interviews with
19 LLM-based CA users. We found that users are constantly faced with trade-offs
between privacy, utility, and convenience when using LLM-based CAs. However,
users' erroneous mental models and the dark patterns in system design limited
their awareness and comprehension of the privacy risks. Additionally, the
human-like interactions encouraged more sensitive disclosures, which
complicated users' ability to navigate the trade-offs. We discuss practical
design guidelines and the needs for paradigmatic shifts to protect the privacy
of LLM-based CA users.",http://arxiv.org/pdf/2309.11653v1
2309.11648v1,cs.CV,Orbital AI-based Autonomous Refuelling Solution,2023-09-20 21:25:52+00:00,"Cameras are rapidly becoming the choice for on-board sensors towards space
rendezvous due to their small form factor and inexpensive power, mass, and
volume costs. When it comes to docking, however, they typically serve a
secondary role, whereas the main work is done by active sensors such as lidar.
This paper documents the development of a proposed AI-based (artificial
intelligence) navigation algorithm intending to mature the use of on-board
visible wavelength cameras as a main sensor for docking and on-orbit servicing
(OOS), reducing the dependency on lidar and greatly reducing costs.
Specifically, the use of AI enables the expansion of the relative navigation
solution towards multiple classes of scenarios, e.g., in terms of targets or
illumination conditions, which would otherwise have to be crafted on a
case-by-case manner using classical image processing methods. Multiple
convolutional neural network (CNN) backbone architectures are benchmarked on
synthetically generated data of docking manoeuvres with the International Space
Station (ISS), achieving position and attitude estimates close to 1%
range-normalised and 1 deg, respectively. The integration of the solution with
a physical prototype of the refuelling mechanism is validated in laboratory
using a robotic arm to simulate a berthing procedure.",http://arxiv.org/pdf/2309.11648v1
2309.11646v1,cs.LG,Early diagnosis of autism spectrum disorder using machine learning approaches,2023-09-20 21:23:37+00:00,"Autistic Spectrum Disorder (ASD) is a neurological disease characterized by
difficulties with social interaction, communication, and repetitive activities.
The severity of these difficulties varies, and those with this diagnosis face
unique challenges. While its primary origin lies in genetics, identifying and
addressing it early can contribute to the enhancement of the condition. In
recent years, machine learning-driven intelligent diagnosis has emerged as a
supplement to conventional clinical approaches, aiming to address the potential
drawbacks of time-consuming and costly traditional methods. In this work, we
utilize different machine learning algorithms to find the most significant
traits responsible for ASD and to automate the diagnostic process. We study six
classification models to see which model works best to identify ASD and also
study five popular clustering methods to get a meaningful insight of these ASD
datasets. To find the best classifier for these binary datasets, we evaluate
the models using accuracy, precision, recall, specificity, F1-score, AUC, kappa
and log loss metrics. Our evaluation demonstrates that five out of the six
selected models perform exceptionally, achieving a 100% accuracy rate on the
ASD datasets when hyperparameters are meticulously tuned for each model. As
almost all classification models are able to get 100% accuracy, we become
interested in observing the underlying insights of the datasets by implementing
some popular clustering algorithms on these datasets. We calculate Normalized
Mutual Information (NMI), Adjusted Rand Index (ARI) & Silhouette Coefficient
(SC) metrics to select the best clustering models. Our evaluation finds that
spectral clustering outperforms all other benchmarking clustering models in
terms of NMI & ARI metrics and it also demonstrates comparability to the
optimal SC achieved by k-means.",http://arxiv.org/pdf/2309.11646v1
2309.11641v1,cs.CV,Attentive VQ-VAE,2023-09-20 21:11:36+00:00,"We present a novel approach to enhance the capabilities of VQVAE models
through the integration of an Attentive Residual Encoder (AREN) and a Residual
Pixel Attention layer. The objective of our research is to improve the
performance of VQVAE while maintaining practical parameter levels. The AREN
encoder is designed to operate effectively at multiple levels, accommodating
diverse architectural complexities. The key innovation is the integration of an
inter-pixel auto-attention mechanism into the AREN encoder. This approach
allows us to efficiently capture and utilize contextual information across
latent vectors. Additionally, our models uses additional encoding levels to
further enhance the model's representational power. Our attention layer employs
a minimal parameter approach, ensuring that latent vectors are modified only
when pertinent information from other pixels is available. Experimental results
demonstrate that our proposed modifications lead to significant improvements in
data representation and generation, making VQVAEs even more suitable for a wide
range of applications.",http://arxiv.org/pdf/2309.11641v1
2309.11638v1,cs.LG,A survey on the semantics of sequential patterns with negation,2023-09-20 21:03:18+00:00,"A sequential pattern with negation, or negative sequential pattern, takes the
form of a sequential pattern for which the negation symbol may be used in front
of some of the pattern's itemsets. Intuitively, such a pattern occurs in a
sequence if negated itemsets are absent in the sequence. Recent work has shown
that different semantics can be attributed to these pattern forms, and that
state-of-the-art algorithms do not extract the same sets of patterns. This
raises the important question of the interpretability of sequential pattern
with negation. In this study, our focus is on exploring how potential users
perceive negation in sequential patterns. Our aim is to determine whether
specific semantics are more ""intuitive"" than others and whether these align
with the semantics employed by one or more state-of-the-art algorithms. To
achieve this, we designed a questionnaire to reveal the semantics' intuition of
each user. This article presents both the design of the questionnaire and an
in-depth analysis of the 124 responses obtained. The outcomes indicate that two
of the semantics are predominantly intuitive; however, neither of them aligns
with the semantics of the primary state-of-the-art algorithms. As a result, we
provide recommendations to account for this disparity in the conclusions drawn.",http://arxiv.org/pdf/2309.11638v1
2309.11619v1,cs.RO,Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots,2023-09-20 20:04:42+00:00,"Assigning repetitive and physically-demanding construction tasks to robots
can alleviate human workers's exposure to occupational injuries. Transferring
necessary dexterous and adaptive artisanal construction craft skills from
workers to robots is crucial for the successful delegation of construction
tasks and achieving high-quality robot-constructed work. Predefined motion
planning scripts tend to generate rigid and collision-prone robotic behaviors
in unstructured construction site environments. In contrast, Imitation Learning
(IL) offers a more robust and flexible skill transfer scheme. However, the
majority of IL algorithms rely on human workers to repeatedly demonstrate task
performance at full scale, which can be counterproductive and infeasible in the
case of construction work. To address this concern, this paper proposes an
immersive, cloud robotics-based virtual demonstration framework that serves two
primary purposes. First, it digitalizes the demonstration process, eliminating
the need for repetitive physical manipulation of heavy construction objects.
Second, it employs a federated collection of reusable demonstrations that are
transferable for similar tasks in the future and can thus reduce the
requirement for repetitive illustration of tasks by human agents. Additionally,
to enhance the trustworthiness, explainability, and ethical soundness of the
robot training, this framework utilizes a Hierarchical Imitation Learning (HIL)
model to decompose human manipulation skills into sequential and reactive
sub-skills. These two layers of skills are represented by deep generative
models, enabling adaptive control of robot actions. By delegating the physical
strains of construction work to human-trained robots, this framework promotes
the inclusion of workers with diverse physical capabilities and educational
backgrounds within the construction industry.",http://arxiv.org/pdf/2309.11619v1
2309.11610v1,cs.CV,Hand Gesture Recognition with Two Stage Approach Using Transfer Learning and Deep Ensemble Learning,2023-09-20 19:53:05+00:00,"Human-Computer Interaction (HCI) has been the subject of research for many
years, and recent studies have focused on improving its performance through
various techniques. In the past decade, deep learning studies have shown high
performance in various research areas, leading researchers to explore their
application to HCI. Convolutional neural networks can be used to recognize hand
gestures from images using deep architectures. In this study, we evaluated
pre-trained high-performance deep architectures on the HG14 dataset, which
consists of 14 different hand gesture classes. Among 22 different models,
versions of the VGGNet and MobileNet models attained the highest accuracy
rates. Specifically, the VGG16 and VGG19 models achieved accuracy rates of
94.64% and 94.36%, respectively, while the MobileNet and MobileNetV2 models
achieved accuracy rates of 96.79% and 94.43%, respectively. We performed hand
gesture recognition on the dataset using an ensemble learning technique, which
combined the four most successful models. By utilizing these models as base
learners and applying the Dirichlet ensemble technique, we achieved an accuracy
rate of 98.88%. These results demonstrate the effectiveness of the deep
ensemble learning technique for HCI and its potential applications in areas
such as augmented reality, virtual reality, and game technologies.",http://arxiv.org/pdf/2309.11610v1
2309.11608v1,cs.AI,Dataset Factory: A Toolchain For Generative Computer Vision Datasets,2023-09-20 19:43:37+00:00,"Generative AI workflows heavily rely on data-centric tasks - such as
filtering samples by annotation fields, vector distances, or scores produced by
custom classifiers. At the same time, computer vision datasets are quickly
approaching petabyte volumes, rendering data wrangling difficult. In addition,
the iterative nature of data preparation necessitates robust dataset sharing
and versioning mechanisms, both of which are hard to implement ad-hoc. To solve
these challenges, we propose a ""dataset factory"" approach that separates the
storage and processing of samples from metadata and enables data-centric
operations at scale for machine learning teams and individual researchers.",http://arxiv.org/pdf/2309.11608v1
2309.11587v1,cs.LG,CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches,2023-09-20 18:52:56+00:00,"The prevalence of ubiquitous location-aware devices and mobile Internet
enables us to collect massive individual-level trajectory dataset from users.
Such trajectory big data bring new opportunities to human mobility research but
also raise public concerns with regard to location privacy. In this work, we
present the Conditional Adversarial Trajectory Synthesis (CATS), a
deep-learning-based GeoAI methodological framework for privacy-preserving
trajectory data generation and publication. CATS applies K-anonymity to the
underlying spatiotemporal distributions of human movements, which provides a
distributional-level strong privacy guarantee. By leveraging conditional
adversarial training on K-anonymized human mobility matrices, trajectory global
context learning using the attention-based mechanism, and recurrent bipartite
graph matching of adjacent trajectory points, CATS is able to reconstruct
trajectory topology from conditionally sampled locations and generate
high-quality individual-level synthetic trajectory data, which can serve as
supplements or alternatives to raw data for privacy-preserving trajectory data
publication. The experiment results on over 90k GPS trajectories show that our
method has a better performance in privacy preservation, spatiotemporal
characteristic preservation, and downstream utility compared with baseline
methods, which brings new insights into privacy-preserving human mobility
research using generative AI techniques and explores data ethics issues in
GIScience.",http://arxiv.org/pdf/2309.11587v1
2309.11575v1,cs.CV,Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge,2023-09-20 18:25:44+00:00,"Text-conditioned image generation models have recently achieved astonishing
image quality and alignment results. Consequently, they are employed in a
fast-growing number of applications. Since they are highly data-driven, relying
on billion-sized datasets randomly scraped from the web, they also produce
unsafe content. As a contribution to the Adversarial Nibbler challenge, we
distill a large set of over 1,000 potential adversarial inputs from existing
safety benchmarks. Our analysis of the gathered prompts and corresponding
images demonstrates the fragility of input filters and provides further
insights into systematic safety issues in current generative image models.",http://arxiv.org/pdf/2309.11575v1
2309.11568v1,cs.AI,BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model,2023-09-20 18:12:56+00:00,"We introduce the Bittensor Language Model, called ""BTLM-3B-8K"", a new
state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was
trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and
8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models
by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B
parameter models. Additionally, BTLM-3B-8K provides excellent long context
performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192
context length. We trained the model on a cleaned and deduplicated SlimPajama
dataset; aggressively tuned the \textmu P hyperparameters and schedule; used
ALiBi position embeddings; and adopted the SwiGLU nonlinearity.
  On Hugging Face, the most popular models have 7B parameters, indicating that
users prefer the quality-size ratio of 7B models. Compacting the 7B parameter
model to one with 3B parameters, with little performance impact, is an
important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision
and takes 2.5x less inference compute than 7B models, helping to open up access
to a powerful language model on mobile and edge devices. BTLM-3B-8K is
available under an Apache 2.0 license on Hugging Face:
https://huggingface.co/cerebras/btlm-3b-8k-base.",http://arxiv.org/pdf/2309.11568v1
2309.11555v1,cs.NE,Limitations in odour recognition and generalisation in a neuromorphic olfactory circuit,2023-09-20 18:00:05+00:00,"Neuromorphic computing is one of the few current approaches that have the
potential to significantly reduce power consumption in Machine Learning and
Artificial Intelligence. Imam & Cleland presented an odour-learning algorithm
that runs on a neuromorphic architecture and is inspired by circuits described
in the mammalian olfactory bulb. They assess the algorithm's performance in
""rapid online learning and identification"" of gaseous odorants and odorless
gases (short ""gases"") using a set of gas sensor recordings of different odour
presentations and corrupting them by impulse noise. We replicated parts of the
study and discovered limitations that affect some of the conclusions drawn.
First, the dataset used suffers from sensor drift and a non-randomised
measurement protocol, rendering it of limited use for odour identification
benchmarks. Second, we found that the model is restricted in its ability to
generalise over repeated presentations of the same gas. We demonstrate that the
task the study refers to can be solved with a simple hash table approach,
matching or exceeding the reported results in accuracy and runtime. Therefore,
a validation of the model that goes beyond restoring a learned data sample
remains to be shown, in particular its suitability to odour identification
tasks.",http://arxiv.org/pdf/2309.11555v1
2309.11495v1,cs.CL,Chain-of-Verification Reduces Hallucination in Large Language Models,2023-09-20 17:50:55+00:00,"Generation of plausible yet incorrect factual information, termed
hallucination, is an unsolved issue in large language models. We study the
ability of language models to deliberate on the responses they give in order to
correct their mistakes. We develop the Chain-of-Verification (CoVe) method
whereby the model first (i) drafts an initial response; then (ii) plans
verification questions to fact-check its draft; (iii) answers those questions
independently so the answers are not biased by other responses; and (iv)
generates its final verified response. In experiments, we show CoVe decreases
hallucinations across a variety of tasks, from list-based questions from
Wikidata, closed book MultiSpanQA and longform text generation.",http://arxiv.org/pdf/2309.11495v1
2309.11489v2,cs.LG,Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning,2023-09-20 17:39:13+00:00,"Designing reward functions is a longstanding challenge in reinforcement
learning (RL); it requires specialized knowledge or domain data, leading to
high costs for development. To address this, we introduce Text2Reward, a
data-free framework that automates the generation of dense reward functions
based on large language models (LLMs). Given a goal described in natural
language, Text2Reward generates dense reward functions as an executable program
grounded in a compact representation of the environment. Unlike inverse RL and
recent work that uses LLMs to write sparse reward codes, Text2Reward produces
interpretable, free-form dense reward codes that cover a wide range of tasks,
utilize existing packages, and allow iterative refinement with human feedback.
We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,
MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17
manipulation tasks, policies trained with generated reward codes achieve
similar or better task success rates and convergence speed than expert-written
reward codes. For locomotion tasks, our method learns six novel locomotion
behaviors with a success rate exceeding 94%. Furthermore, we show that the
policies trained in the simulator with our method can be deployed in the real
world. Finally, Text2Reward further improves the policies by refining their
reward functions with human feedback. Video results are available at
https://text-to-reward.github.io",http://arxiv.org/pdf/2309.11489v2
2309.11485v1,eess.SP,Decision-Directed Hybrid RIS Channel Estimation with Minimal Pilot Overhead,2023-09-20 17:29:30+00:00,"To reap the benefits of reconfigurable intelligent surfaces (RIS), channel
state information (CSI) is generally required. However, CSI acquisition in RIS
systems is challenging and often results in very large pilot overhead,
especially in unstructured channel environments. Consequently, the RIS channel
estimation problem has attracted a lot of interest and also been a subject of
intense study in recent years. In this paper, we propose a decision-directed
RIS channel estimation framework for general unstructured channel models. The
employed RIS contains some hybrid elements that can simultaneously reflect and
sense the incoming signal. We show that with the help of the hybrid RIS
elements, it is possible to accurately recover the CSI with a pilot overhead
proportional to the number of users. Therefore, the proposed framework
substantially improves the system spectral efficiency compared to systems with
passive RIS arrays since the pilot overhead in passive RIS systems is
proportional to the number of RIS elements times the number of users. We also
perform a detailed spectral efficiency analysis for both the pilot-directed and
decision-directed frameworks. Our analysis takes into account both the channel
estimation and data detection errors at both the RIS and the BS. Finally, we
present numerous simulation results to verify the accuracy of the analysis as
well as to show the benefits of the proposed decision-directed framework.",http://arxiv.org/pdf/2309.11485v1
2309.11478v1,cs.AI,"Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs",2023-09-20 17:23:05+00:00,"We address the integration of storytelling and Large Language Models (LLMs)
to develop engaging and believable Social Chatbots (SCs) in community settings.
Motivated by the potential of fictional characters to enhance social
interactions, we introduce Storytelling Social Chatbots (SSCs) and the concept
of story engineering to transform fictional game characters into ""live"" social
entities within player communities. Our story engineering process includes
three steps: (1) Character and story creation, defining the SC's personality
and worldview, (2) Presenting Live Stories to the Community, allowing the SC to
recount challenges and seek suggestions, and (3) Communication with community
members, enabling interaction between the SC and users. We employed the LLM
GPT-3 to drive our SSC prototypes, ""David"" and ""Catherine,"" and evaluated their
performance in an online gaming community, ""DE (Alias),"" on Discord. Our
mixed-method analysis, based on questionnaires (N=15) and interviews (N=8) with
community members, reveals that storytelling significantly enhances the
engagement and believability of SCs in community settings.",http://arxiv.org/pdf/2309.11478v1
2309.11473v1,cs.AI,Multi-view Fuzzy Representation Learning with Rules based Model,2023-09-20 17:13:15+00:00,"Unsupervised multi-view representation learning has been extensively studied
for mining multi-view data. However, some critical challenges remain. On the
one hand, the existing methods cannot explore multi-view data comprehensively
since they usually learn a common representation between views, given that
multi-view data contains both the common information between views and the
specific information within each view. On the other hand, to mine the nonlinear
relationship between data, kernel or neural network methods are commonly used
for multi-view representation learning. However, these methods are lacking in
interpretability. To this end, this paper proposes a new multi-view fuzzy
representation learning method based on the interpretable Takagi-Sugeno-Kang
(TSK) fuzzy system (MVRL_FS). The method realizes multi-view representation
learning from two aspects. First, multi-view data are transformed into a
high-dimensional fuzzy feature space, while the common information between
views and specific information of each view are explored simultaneously.
Second, a new regularization method based on L_(2,1)-norm regression is
proposed to mine the consistency information between views, while the geometric
structure of the data is preserved through the Laplacian graph. Finally,
extensive experiments on many benchmark multi-view datasets are conducted to
validate the superiority of the proposed method.",http://arxiv.org/pdf/2309.11473v1
2309.11469v1,cs.AI,Multi-Label Takagi-Sugeno-Kang Fuzzy System,2023-09-20 17:09:09+00:00,"Multi-label classification can effectively identify the relevant labels of an
instance from a given set of labels. However,the modeling of the relationship
between the features and the labels is critical to the classification
performance. To this end, we propose a new multi-label classification method,
called Multi-Label Takagi-Sugeno-Kang Fuzzy System (ML-TSK FS), to improve the
classification performance. The structure of ML-TSK FS is designed using fuzzy
rules to model the relationship between features and labels. The fuzzy system
is trained by integrating fuzzy inference based multi-label correlation
learning with multi-label regression loss. The proposed ML-TSK FS is evaluated
experimentally on 12 benchmark multi-label datasets. 1 The results show that
the performance of ML-TSK FS is competitive with existing methods in terms of
various evaluation metrics, indicating that it is able to model the
feature-label relationship effectively using fuzzy inference rules and enhances
the classification performance.",http://arxiv.org/pdf/2309.11469v1
2309.11462v1,cs.CR,"AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition",2023-09-20 16:59:22+00:00,"Automatic Speech Recognition systems have been shown to be vulnerable to
adversarial attacks that manipulate the command executed on the device. Recent
research has focused on exploring methods to create such attacks, however, some
issues relating to Over-The-Air (OTA) attacks have not been properly addressed.
In our work, we examine the needed properties of robust attacks compatible with
the OTA model, and we design a method of generating attacks with arbitrary such
desired properties, namely the invariance to synchronization, and the
robustness to filtering: this allows a Denial-of-Service (DoS) attack against
ASR systems. We achieve these characteristics by constructing attacks in a
modified frequency domain through an inverse Fourier transform. We evaluate our
method on standard keyword classification tasks and analyze it in OTA, and we
analyze the properties of the cross-domain attacks to explain the efficiency of
the approach.",http://arxiv.org/pdf/2309.11462v1
2309.11456v1,cs.AI,Generative Agent-Based Modeling: Unveiling Social System Dynamics through Coupling Mechanistic Models with Generative Artificial Intelligence,2023-09-20 16:43:05+00:00,"We discuss the emerging new opportunity for building feedback-rich
computational models of social systems using generative artificial
intelligence. Referred to as Generative Agent-Based Models (GABMs), such
individual-level models utilize large language models such as ChatGPT to
represent human decision-making in social settings. We provide a GABM case in
which human behavior can be incorporated in simulation models by coupling a
mechanistic model of human interactions with a pre-trained large language
model. This is achieved by introducing a simple GABM of social norm diffusion
in an organization. For educational purposes, the model is intentionally kept
simple. We examine a wide range of scenarios and the sensitivity of the results
to several changes in the prompt. We hope the article and the model serve as a
guide for building useful diffusion models that include realistic human
reasoning and decision-making.",http://arxiv.org/pdf/2309.11456v1
2309.11452v1,cs.AI,Using deep learning to construct stochastic local search SAT solvers with performance bounds,2023-09-20 16:27:52+00:00,"The Boolean Satisfiability problem (SAT) is the most prototypical NP-complete
problem and of great practical relevance. One important class of solvers for
this problem are stochastic local search (SLS) algorithms that iteratively and
randomly update a candidate assignment. Recent breakthrough results in
theoretical computer science have established sufficient conditions under which
SLS solvers are guaranteed to efficiently solve a SAT instance, provided they
have access to suitable ""oracles"" that provide samples from an
instance-specific distribution, exploiting an instance's local structure.
Motivated by these results and the well established ability of neural networks
to learn common structure in large datasets, in this work, we train oracles
using Graph Neural Networks and evaluate them on two SLS solvers on random SAT
instances of varying difficulty. We find that access to GNN-based oracles
significantly boosts the performance of both solvers, allowing them, on
average, to solve 17% more difficult instances (as measured by the ratio
between clauses and variables), and to do so in 35% fewer steps, with
improvements in the median number of steps of up to a factor of 8. As such,
this work bridges formal results from theoretical computer science and
practically motivated research on deep learning for constraint satisfaction
problems and establishes the promise of purpose-trained SAT solvers with
performance guarantees.",http://arxiv.org/pdf/2309.11452v1
2309.11438v1,cond-mat.soft,Brain-inspired computing with fluidic iontronic nanochannels,2023-09-20 16:13:18+00:00,"The unparalleled energy efficiency of the brain is driving researchers to
seek out new brain-inspired (neuromorphic) computing paradigms. Artificial
aqueous ion channels are emerging as an exciting new platform for neuromorphic
computing, representing a departure from conventional solid-state devices by
directly mimicking the fluidic ion transport found in the brain. However,
despite recent interest, a tangible demonstration of neuromorphic computing
remains a challenge. Here we successfully perform neuromorphic reservoir
computing using easy to fabricate tapered microchannels that embed a conducting
network of fluidic nanochannels between colloids, which we show to be a novel
memristor (memory resistor). Remarkably, a wide range of typical conductance
memory timescales can easily be achieved by constructing channels of different
length, a unique and highly desirable feature. This work is inspired and
supported by a new theoretical model, which stems directly from traditional
diffusion-conduction equations and shows excellent agreement with the
experiments, predicting the features and relevant parameters presented here.
Our results represent a fundamental step in realising the promise of ion
channels as a new platform to emulate the rich aqueous dynamics of the brain.",http://arxiv.org/pdf/2309.11438v1
2309.11436v2,cs.CL,You Only Look at Screens: Multimodal Chain-of-Action Agents,2023-09-20 16:12:32+00:00,"Autonomous user interface (UI) agents aim to facilitate task automation by
interacting with the user interface without manual intervention. Recent studies
have investigated eliciting the capabilities of large language models (LLMs)
for effective engagement in diverse environments. To align with the
input-output requirement of LLMs, existing approaches are developed under a
sandbox setting where they rely on external tools and application-specific APIs
to parse the environment into textual elements and interpret the predicted
actions. Consequently, those approaches often grapple with inference
inefficiency and error propagation risks. To mitigate the challenges, we
introduce Auto-UI, a multimodal solution that directly interacts with the
interface, bypassing the need for environment parsing or reliance on
application-dependent APIs. Moreover, we propose a chain-of-action technique --
leveraging a series of intermediate previous action histories and future action
plans -- to help the agent decide what action to execute. We evaluate our
approach on a new device-control benchmark AITW with 30K unique instructions,
spanning multi-step tasks such as application operation, web searching, and web
shopping. Experimental results show that Auto-UI achieves state-of-the-art
performance with an action type prediction accuracy of 90% and an overall
action success rate of 74%. Code is publicly available at
https://github.com/cooelf/Auto-UI.",http://arxiv.org/pdf/2309.11436v2
2309.11433v1,cs.CV,A Systematic Review of Few-Shot Learning in Medical Imaging,2023-09-20 16:10:53+00:00,"The lack of annotated medical images limits the performance of deep learning
models, which usually need large-scale labelled datasets. Few-shot learning
techniques can reduce data scarcity issues and enhance medical image analysis,
especially with meta-learning. This systematic review gives a comprehensive
overview of few-shot learning in medical imaging. We searched the literature
systematically and selected 80 relevant articles published from 2018 to 2023.
We clustered the articles based on medical outcomes, such as tumour
segmentation, disease classification, and image registration; anatomical
structure investigated (i.e. heart, lung, etc.); and the meta-learning method
used. For each cluster, we examined the papers' distributions and the results
provided by the state-of-the-art. In addition, we identified a generic pipeline
shared among all the studies. The review shows that few-shot learning can
overcome data scarcity in most outcomes and that meta-learning is a popular
choice to perform few-shot learning because it can adapt to new tasks with few
labelled samples. In addition, following meta-learning, supervised learning and
semi-supervised learning stand out as the predominant techniques employed to
tackle few-shot learning challenges in medical imaging and also best
performing. Lastly, we observed that the primary application areas
predominantly encompass cardiac, pulmonary, and abdominal domains. This
systematic review aims to inspire further research to improve medical image
analysis and patient care.",http://arxiv.org/pdf/2309.11433v1
2309.11427v1,cs.LG,Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing,2023-09-20 16:01:45+00:00,"This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.",http://arxiv.org/pdf/2309.11427v1
2309.11414v1,cs.RO,EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning,2023-09-20 15:40:32+00:00,"Classical motion planning for robotic manipulation includes a set of general
algorithms that aim to minimize a scene-specific cost of executing a given
plan. This approach offers remarkable adaptability, as they can be directly
used off-the-shelf for any new scene without needing specific training
datasets. However, without a prior understanding of what diverse valid
trajectories are and without specially designed cost functions for a given
scene, the overall solutions tend to have low success rates. While
deep-learning-based algorithms tremendously improve success rates, they are
much harder to adopt without specialized training datasets. We propose EDMP, an
Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the
strengths of classical and deep-learning-based motion planning. Our
diffusion-based network is trained on a set of diverse kinematically valid
trajectories. Like classical planning, for any new scene at the time of
inference, we compute scene-specific costs such as ""collision cost"" and guide
the diffusion to generate valid trajectories that satisfy the scene-specific
constraints. Further, instead of a single cost function that may be
insufficient in capturing diversity across scenes, we use an ensemble of costs
to guide the diffusion process, significantly improving the success rate
compared to classical planners. EDMP performs comparably with SOTA
deep-learning-based methods while retaining the generalization capabilities
primarily associated with classical planners.",http://arxiv.org/pdf/2309.11414v1
2309.11400v1,q-fin.TR,Transformers versus LSTMs for electronic trading,2023-09-20 15:25:43+00:00,"With the rapid development of artificial intelligence, long short term memory
(LSTM), one kind of recurrent neural network (RNN), has been widely applied in
time series prediction.
  Like RNN, Transformer is designed to handle the sequential data. As
Transformer achieved great success in Natural Language Processing (NLP),
researchers got interested in Transformer's performance on time series
prediction, and plenty of Transformer-based solutions on long time series
forecasting have come out recently. However, when it comes to financial time
series prediction, LSTM is still a dominant architecture. Therefore, the
question this study wants to answer is: whether the Transformer-based model can
be applied in financial time series prediction and beat LSTM.
  To answer this question, various LSTM-based and Transformer-based models are
compared on multiple financial prediction tasks based on high-frequency limit
order book data. A new LSTM-based model called DLSTM is built and new
architecture for the Transformer-based model is designed to adapt for financial
prediction. The experiment result reflects that the Transformer-based model
only has the limited advantage in absolute price sequence prediction. The
LSTM-based models show better and more robust performance on difference
sequence prediction, such as price difference and price movement.",http://arxiv.org/pdf/2309.11400v1
2309.11384v1,cs.CL,Long-Form End-to-End Speech Translation via Latent Alignment Segmentation,2023-09-20 15:10:12+00:00,"Current simultaneous speech translation models can process audio only up to a
few seconds long. Contemporary datasets provide an oracle segmentation into
sentences based on human-annotated transcripts and translations. However, the
segmentation into sentences is not available in the real world. Current speech
segmentation approaches either offer poor segmentation quality or have to trade
latency for quality. In this paper, we propose a novel segmentation approach
for a low-latency end-to-end speech translation. We leverage the existing
speech translation encoder-decoder architecture with ST CTC and show that it
can perform the segmentation task without supervision or additional parameters.
To the best of our knowledge, our method is the first that allows an actual
end-to-end simultaneous speech translation, as the same model is used for
translation and segmentation at the same time. On a diverse set of language
pairs and in- and out-of-domain data, we show that the proposed approach
achieves state-of-the-art quality at no additional computational cost.",http://arxiv.org/pdf/2309.11384v1
2309.11382v1,cs.RO,Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions,2023-09-20 15:04:49+00:00,"Visual language navigation (VLN) is an embodied task demanding a wide range
of skills encompassing understanding, perception, and planning. For such a
multifaceted challenge, previous VLN methods totally rely on one model's own
thinking to make predictions within one round. However, existing models, even
the most advanced large language model GPT4, still struggle with dealing with
multiple tasks by single-round self-thinking. In this work, drawing inspiration
from the expert consultation meeting, we introduce a novel zero-shot VLN
framework. Within this framework, large models possessing distinct abilities
are served as domain experts. Our proposed navigation agent, namely DiscussNav,
can actively discuss with these experts to collect essential information before
moving at every step. These discussions cover critical navigation subtasks like
instruction understanding, environment perception, and completion estimation.
Through comprehensive experiments, we demonstrate that discussions with domain
experts can effectively facilitate navigation by perceiving
instruction-relevant information, correcting inadvertent errors, and sifting
through in-consistent movement decisions. The performances on the
representative VLN task R2R show that our method surpasses the leading
zero-shot VLN model by a large margin on all metrics. Additionally, real-robot
experiments display the obvious advantages of our method over single-round
self-thinking.",http://arxiv.org/pdf/2309.11382v1
2309.11379v1,cs.CL,Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff,2023-09-20 14:59:06+00:00,"Blockwise self-attentional encoder models have recently emerged as one
promising end-to-end approach to simultaneous speech translation. These models
employ a blockwise beam search with hypothesis reliability scoring to determine
when to wait for more input speech before translating further. However, this
method maintains multiple hypotheses until the entire speech input is consumed
-- this scheme cannot directly show a single \textit{incremental} translation
to users. Further, this method lacks mechanisms for \textit{controlling} the
quality vs. latency tradeoff. We propose a modified incremental blockwise beam
search incorporating local agreement or hold-$n$ policies for quality-latency
control. We apply our framework to models trained for online or offline
translation and demonstrate that both types can be effectively used in online
mode.
  Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing
latency or 0.8-1.4 s latency improvement without changing quality.",http://arxiv.org/pdf/2309.11379v1
2309.11378v1,cs.LG,Preconditioned Federated Learning,2023-09-20 14:58:47+00:00,"Federated Learning (FL) is a distributed machine learning approach that
enables model training in communication efficient and privacy-preserving
manner. The standard optimization method in FL is Federated Averaging (FedAvg),
which performs multiple local SGD steps between communication rounds. FedAvg
has been considered to lack algorithm adaptivity compared to modern first-order
adaptive optimizations. In this paper, we propose new communication-efficient
FL algortithms based on two adaptive frameworks: local adaptivity (PreFed) and
server-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using a
novel covariance matrix preconditioner. Theoretically, we provide convergence
guarantees for our algorithms. The empirical experiments show our methods
achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.",http://arxiv.org/pdf/2309.11378v1
2309.11368v1,cs.RO,Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition,2023-09-20 14:51:09+00:00,"Human-robot collaboration has benefited users with higher efficiency towards
interactive tasks. Nevertheless, most collaborative schemes rely on complicated
human-machine interfaces, which might lack the requisite intuitiveness compared
with natural limb control. We also expect to understand human intent with low
training data requirements. In response to these challenges, this paper
introduces an innovative human-robot collaborative framework that seamlessly
integrates hand gesture and dynamic movement recognition, voice recognition,
and a switchable control adaptation strategy. These modules provide a
user-friendly approach that enables the robot to deliver the tools as per user
need, especially when the user is working with both hands. Therefore, users can
focus on their task execution without additional training in the use of
human-machine interfaces, while the robot interprets their intuitive gestures.
The proposed multimodal interaction framework is executed in the UR5e robot
platform equipped with a RealSense D435i camera, and the effectiveness is
assessed through a soldering circuit board task. The experiment results have
demonstrated superior performance in hand gesture recognition, where the static
hand gesture recognition module achieves an accuracy of 94.3\%, while the
dynamic motion recognition module reaches 97.6\% accuracy. Compared with human
solo manipulation, the proposed approach facilitates higher efficiency tool
delivery, without significantly distracting from human intents.",http://arxiv.org/pdf/2309.11368v1
2309.11361v1,cs.AI,Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG),2023-09-20 14:43:43+00:00,"We present a comprehensive benchmark dataset for Knowledge Graph Question
Answering in Materials Science (KGQA4MAT), with a focus on metal-organic
frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has
been constructed by integrating structured databases and knowledge extracted
from the literature. To enhance MOF-KG accessibility for domain experts, we aim
to develop a natural language interface for querying the knowledge graph. We
have developed a benchmark comprised of 161 complex questions involving
comparison, aggregation, and complicated graph structures. Each question is
rephrased in three additional variations, resulting in 644 questions and 161 KG
queries. To evaluate the benchmark, we have developed a systematic approach for
utilizing ChatGPT to translate natural language questions into formal KG
queries. We also apply the approach to the well-known QALD-9 dataset,
demonstrating ChatGPT's potential in addressing KGQA issues for different
platforms and query languages. The benchmark and the proposed approach aim to
stimulate further research and development of user-friendly and efficient
interfaces for querying domain-specific materials science knowledge graphs,
thereby accelerating the discovery of novel materials.",http://arxiv.org/pdf/2309.11361v1
2309.11357v1,cs.CV,3D Face Reconstruction: the Road to Forensics,2023-09-20 14:39:03+00:00,"3D face reconstruction algorithms from images and videos are applied to many
fields, from plastic surgery to the entertainment sector, thanks to their
advantageous features. However, when looking at forensic applications, 3D face
reconstruction must observe strict requirements that still make its possible
role in bringing evidence to a lawsuit unclear. An extensive investigation of
the constraints, potential, and limits of its application in forensics is still
missing. Shedding some light on this matter is the goal of the present survey,
which starts by clarifying the relation between forensic applications and
biometrics, with a focus on face recognition. Therefore, it provides an
analysis of the achievements of 3D face reconstruction algorithms from
surveillance videos and mugshot images and discusses the current obstacles that
separate 3D face reconstruction from an active role in forensic applications.
Finally, it examines the underlying data sets, with their advantages and
limitations, while proposing alternatives that could substitute or complement
them.",http://arxiv.org/pdf/2309.11357v1
2309.11356v1,cs.AI,A Comprehensive Survey on Rare Event Prediction,2023-09-20 14:36:57+00:00,"Rare event prediction involves identifying and forecasting events with a low
probability using machine learning and data analysis. Due to the imbalanced
data distributions, where the frequency of common events vastly outweighs that
of rare events, it requires using specialized methods within each step of the
machine learning pipeline, i.e., from data processing to algorithms to
evaluation protocols. Predicting the occurrences of rare events is important
for real-world applications, such as Industry 4.0, and is an active research
area in statistical and machine learning. This paper comprehensively reviews
the current approaches for rare event prediction along four dimensions: rare
event data, data processing, algorithmic approaches, and evaluation approaches.
Specifically, we consider 73 datasets from different modalities (i.e.,
numerical, image, text, and audio), four major categories of data processing,
five major algorithmic groupings, and two broader evaluation approaches. This
paper aims to identify gaps in the current literature and highlight the
challenges of predicting rare events. It also suggests potential research
directions, which can help guide practitioners and researchers.",http://arxiv.org/pdf/2309.11356v1
2309.11351v1,cs.GR,C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters,2023-09-20 14:34:45+00:00,"We present C$\cdot$ASE, an efficient and effective framework that learns
conditional Adversarial Skill Embeddings for physics-based characters. Our
physically simulated character can learn a diverse repertoire of skills while
providing controllability in the form of direct manipulation of the skills to
be performed. C$\cdot$ASE divides the heterogeneous skill motions into distinct
subsets containing homogeneous samples for training a low-level conditional
model to learn conditional behavior distribution. The skill-conditioned
imitation learning naturally offers explicit control over the character's
skills after training. The training course incorporates the focal skill
sampling, skeletal residual forces, and element-wise feature masking to balance
diverse skills of varying complexities, mitigate dynamics mismatch to master
agile motions and capture more general behavior characteristics, respectively.
Once trained, the conditional model can produce highly diverse and realistic
skills, outperforming state-of-the-art models, and can be repurposed in various
downstream tasks. In particular, the explicit skill control handle allows a
high-level policy or user to direct the character with desired skill
specifications, which we demonstrate is advantageous for interactive character
animation.",http://arxiv.org/pdf/2309.11351v1
2309.11338v1,cs.CL,TRAVID: An End-to-End Video Translation Framework,2023-09-20 14:13:05+00:00,"In today's globalized world, effective communication with people from diverse
linguistic backgrounds has become increasingly crucial. While traditional
methods of language translation, such as written text or voice-only
translations, can accomplish the task, they often fail to capture the complete
context and nuanced information conveyed through nonverbal cues like facial
expressions and lip movements. In this paper, we present an end-to-end video
translation system that not only translates spoken language but also
synchronizes the translated speech with the lip movements of the speaker. Our
system focuses on translating educational lectures in various Indian languages,
and it is designed to be effective even in low-resource system settings. By
incorporating lip movements that align with the target language and matching
them with the speaker's voice using voice cloning techniques, our application
offers an enhanced experience for students and users. This additional feature
creates a more immersive and realistic learning environment, ultimately making
the learning process more effective and engaging.",http://arxiv.org/pdf/2309.11338v1
2309.11331v2,cs.CV,Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism,2023-09-20 14:03:47+00:00,"In the past years, YOLO-series models have emerged as the leading approaches
in the area of real-time object detection. Many studies pushed up the baseline
to a higher level by modifying the architecture, augmenting data and designing
new losses. However, we find previous models still suffer from information
fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation
Network (PANet) have alleviated this. Therefore, this study provides an
advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with
convolution and self-attention operations. This new designed model named as
Gold-YOLO, which boosts the multi-scale feature fusion capabilities and
achieves an ideal balance between latency and accuracy across all model scales.
Additionally, we implement MAE-style pretraining in the YOLO-series for the
first time, allowing YOLOseries models could be to benefit from unsupervised
pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017
datasets and 1030 FPS on a T4 GPU, which outperforms the previous SOTA model
YOLOv6-3.0-N with similar FPS by +2.4%. The PyTorch code is available at
https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO,
and the MindSpore code is available at
https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO.",http://arxiv.org/pdf/2309.11331v2
2309.11325v1,cs.CL,DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services,2023-09-20 13:50:26+00:00,"We propose DISC-LawLLM, an intelligent legal system utilizing large language
models (LLMs) to provide a wide range of legal services. We adopt legal
syllogism prompting strategies to construct supervised fine-tuning datasets in
the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability.
We augment LLMs with a retrieval module to enhance models' ability to access
and utilize external legal knowledge. A comprehensive legal benchmark,
DISC-Law-Eval, is presented to evaluate intelligent legal systems from both
objective and subjective dimensions. Quantitative and qualitative results on
DISC-Law-Eval demonstrate the effectiveness of our system in serving various
users across diverse legal scenarios. The detailed resources are available at
https://github.com/FudanDISC/DISC-LawLLM.",http://arxiv.org/pdf/2309.11325v1
2309.11316v1,cs.GT,Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory,2023-09-20 13:41:45+00:00,"The competitive nature of Cloud marketplaces as new concerns in delivery of
services makes the pricing policies a crucial task for firms. so that, pricing
strategies has recently attracted many researchers. Since game theory can
handle such competing well this concern is addressed by designing a normal form
game between providers in current research. A committee is considered in which
providers register for improving their competition based pricing policies. The
functionality of game theory is applied to design dynamic pricing policies. The
usage of the committee makes the game a complete information one, in which each
player is aware of every others payoff functions. The players enhance their
pricing policies to maximize their profits. The contribution of this paper is
the quantitative modeling of Cloud marketplaces in form of a game to provide
novel dynamic pricing strategies; the model is validated by proving the
existence and the uniqueness of Nash equilibrium of the game.",http://arxiv.org/pdf/2309.11316v1
2309.11312v1,cs.GT,A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques,2023-09-20 13:38:43+00:00,"Cloud computing as a fairly new commercial paradigm, widely investigated by
different researchers, already has a great range of challenges. Pricing is a
major problem in Cloud computing marketplace; as providers are competing to
attract more customers without knowing the pricing policies of each other. To
overcome this lack of knowledge, we model their competition by an
incomplete-information game. Considering the issue, this work proposes a
pricing policy related to the regret minimization algorithm and applies it to
the considered incomplete-information game. Based on the competition based
marketplace of the Cloud, providers update the distribution of their strategies
using the experienced regret. The idea of iteratively applying the algorithm
for updating probabilities of strategies causes the regret get minimized
faster. The experimental results show much more increase in profits of the
providers in comparison with other pricing policies. Besides, the efficiency of
a variety of regret minimization techniques in a simulated marketplace of Cloud
are discussed which have not been observed in the studied literature. Moreover,
return on investment of providers in considered organizations is studied and
promising results appeared.",http://arxiv.org/pdf/2309.11312v1
2309.11307v1,cs.CL,Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features,2023-09-20 13:34:03+00:00,"Predicting the success of Conversational Task Assistants (CTA) can be
critical to understand user behavior and act accordingly. In this paper, we
propose TB-Rater, a Transformer model which combines conversational-flow
features with user behavior features for predicting user ratings in a CTA
scenario. In particular, we use real human-agent conversations and ratings
collected in the Alexa TaskBot challenge, a novel multimodal and multi-turn
conversational context. Our results show the advantages of modeling both the
conversational-flow and behavioral aspects of the conversation in a single
model for offline rating prediction. Additionally, an analysis of the
CTA-specific behavioral features brings insights into this setting and can be
used to bootstrap future systems.",http://arxiv.org/pdf/2309.11307v1
2309.11306v1,cs.CV,FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion,2023-09-20 13:33:00+00:00,"Speech-driven 3D facial animation synthesis has been a challenging task both
in industry and research. Recent methods mostly focus on deterministic deep
learning methods meaning that given a speech input, the output is always the
same. However, in reality, the non-verbal facial cues that reside throughout
the face are non-deterministic in nature. In addition, majority of the
approaches focus on 3D vertex based datasets and methods that are compatible
with existing facial animation pipelines with rigged characters is scarce. To
eliminate these issues, we present FaceDiffuser, a non-deterministic deep
learning model to generate speech-driven facial animations that is trained with
both 3D vertex and blendshape based datasets. Our method is based on the
diffusion technique and uses the pre-trained large speech representation model
HuBERT to encode the audio input. To the best of our knowledge, we are the
first to employ the diffusion method for the task of speech-driven 3D facial
animation synthesis. We have run extensive objective and subjective analyses
and show that our approach achieves better or comparable results in comparison
to the state-of-the-art methods. We also introduce a new in-house dataset that
is based on a blendshape based rigged character. We recommend watching the
accompanying supplementary video. The code and the dataset will be publicly
available.",http://arxiv.org/pdf/2309.11306v1
2309.11299v1,cs.DC,A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing,2023-09-20 13:27:30+00:00,"Due to the recent wide use of computational resources in cloud computing, new
resource provisioning challenges have been emerged. Resource provisioning
techniques must keep total costs to a minimum while meeting the requirements of
the requests. According to widely usage of cloud services, it seems more
challenging to develop effective schemes for provisioning services
cost-effectively; we have proposed a novel learning based resource provisioning
approach that achieves cost-reduction guarantees of demands. The contributions
of our optimized resource provisioning (ORP) approach are as follows. Firstly,
it is designed to provide a cost-effective method to efficiently handle the
provisioning of requested applications; while most of the existing models allow
only workflows in general which cares about the dependencies of the tasks, ORP
performs based on services of which applications comprised and cares about
their efficient provisioning totally. Secondly, it is a learning automata-based
approach which selects the most proper resources for hosting each service of
the demanded application; our approach considers both cost and service
requirements together for deploying applications. Thirdly, a comprehensive
evaluation is performed for three typical workloads: data-intensive,
process-intensive and normal applications. The experimental results show that
our method adapts most of the requirements efficiently, and furthermore the
resulting performance meets our design goals.",http://arxiv.org/pdf/2309.11299v1
2309.11295v1,cs.CL,CPLLM: Clinical Prediction with Large Language Models,2023-09-20 13:24:12+00:00,"We present Clinical Prediction with Large Language Models (CPLLM), a method
that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical
disease prediction. We utilized quantization and fine-tuned the LLM using
prompts, with the task of predicting whether patients will be diagnosed with a
target disease during their next visit or in the subsequent diagnosis,
leveraging their historical diagnosis records. We compared our results versus
various baselines, including Logistic Regression, RETAIN, and Med-BERT, which
is the current state-of-the-art model for disease prediction using structured
EHR data. Our experiments have shown that CPLLM surpasses all the tested models
in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements
compared to the baseline models.",http://arxiv.org/pdf/2309.11295v1
2309.11285v1,cs.CL,Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains,2023-09-20 13:10:06+00:00,"This paper presents the overview of the AuTexTification shared task as part
of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the
framework of the SEPLN 2023 conference. AuTexTification consists of two
subtasks: for Subtask 1, participants had to determine whether a text is
human-authored or has been generated by a large language model. For Subtask 2,
participants had to attribute a machine-generated text to one of six different
text generation models. Our AuTexTification 2023 dataset contains more than
160.000 texts across two languages (English and Spanish) and five domains
(tweets, reviews, news, legal, and how-to articles). A total of 114 teams
signed up to participate, of which 36 sent 175 runs, and 20 of them sent their
working notes. In this overview, we present the AuTexTification dataset and
task, the submitted participating systems, and the results.",http://arxiv.org/pdf/2309.11285v1
2309.11284v1,cs.AI,Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting,2023-09-20 13:08:34+00:00,"With the acceleration of urbanization, traffic forecasting has become an
essential role in smart city construction. In the context of spatio-temporal
prediction, the key lies in how to model the dependencies of sensors. However,
existing works basically only consider the micro relationships between sensors,
where the sensors are treated equally, and their macroscopic dependencies are
neglected. In this paper, we argue to rethink the sensor's dependency modeling
from two hierarchies: regional and global perspectives. Particularly, we merge
original sensors with high intra-region correlation as a region node to
preserve the inter-region dependency. Then, we generate representative and
common spatio-temporal patterns as global nodes to reflect a global dependency
between sensors and provide auxiliary information for spatio-temporal
dependency learning. In pursuit of the generality and reality of node
representations, we incorporate a Meta GCN to calibrate the regional and global
nodes in the physical data space. Furthermore, we devise the cross-hierarchy
graph convolution to propagate information from different hierarchies. In a
nutshell, we propose a Hierarchical Information Enhanced Spatio-Temporal
prediction method, HIEST, to create and utilize the regional dependency and
common spatio-temporal patterns. Extensive experiments have verified the
leading performance of our HIEST against state-of-the-art baselines. We
publicize the code to ease reproducibility.",http://arxiv.org/pdf/2309.11284v1
2309.11275v1,cs.RO,Open-endedness induced through a predator-prey scenario using modular robots,2023-09-20 12:58:51+00:00,"This work investigates how a predator-prey scenario can induce the emergence
of Open-Ended Evolution (OEE). We utilize modular robots of fixed morphologies
whose controllers are subject to evolution. In both species, robots can send
and receive signals and perceive the relative positions of other robots in the
environment. Specifically, we introduce a feature we call a tagging system: it
modifies how individuals can perceive each other and is expected to increase
behavioral complexity. Our results show the emergence of adaptive strategies,
demonstrating the viability of inducing OEE through predator-prey dynamics
using modular robots. Such emergence, nevertheless, seemed to depend on
conditioning reproduction to an explicit behavioral criterion.",http://arxiv.org/pdf/2309.11275v1
2309.11274v1,cs.AI,Machine Learning Data Suitability and Performance Testing Using Fault Injection Testing Framework,2023-09-20 12:58:35+00:00,"Creating resilient machine learning (ML) systems has become necessary to
ensure production-ready ML systems that acquire user confidence seamlessly. The
quality of the input data and the model highly influence the successful
end-to-end testing in data-sensitive systems. However, the testing approaches
of input data are not as systematic and are few compared to model testing. To
address this gap, this paper presents the Fault Injection for Undesirable
Learning in input Data (FIUL-Data) testing framework that tests the resilience
of ML models to multiple intentionally-triggered data faults. Data mutators
explore vulnerabilities of ML systems against the effects of different fault
injections. The proposed framework is designed based on three main ideas: The
mutators are not random; one data mutator is applied at an instance of time,
and the selected ML models are optimized beforehand. This paper evaluates the
FIUL-Data framework using data from analytical chemistry, comprising retention
time measurements of anti-sense oligonucleotide. Empirical evaluation is
carried out in a two-step process in which the responses of selected ML models
to data mutation are analyzed individually and then compared with each other.
The results show that the FIUL-Data framework allows the evaluation of the
resilience of ML models. In most experiments cases, ML models show higher
resilience at larger training datasets, where gradient boost performed better
than support vector regression in smaller training sets. Overall, the mean
squared error metric is useful in evaluating the resilience of models due to
its higher sensitivity to data mutation.",http://arxiv.org/pdf/2309.11274v1
2309.11271v1,cs.CL,Grounded Complex Task Segmentation for Conversational Assistants,2023-09-20 12:55:46+00:00,"Following complex instructions in conversational assistants can be quite
daunting due to the shorter attention and memory spans when compared to reading
the same instructions. Hence, when conversational assistants walk users through
the steps of complex tasks, there is a need to structure the task into
manageable pieces of information of the right length and complexity. In this
paper, we tackle the recipes domain and convert reading structured instructions
into conversational structured ones. We annotated the structure of instructions
according to a conversational scenario, which provided insights into what is
expected in this setting. To computationally model the conversational step's
characteristics, we tested various Transformer-based architectures, showing
that a token-based approach delivers the best results. A further user study
showed that users tend to favor steps of manageable complexity and length, and
that the proposed methodology can improve the original web-based instructional
text. Specifically, 86% of the evaluated tasks were improved from a
conversational suitability point of view.",http://arxiv.org/pdf/2309.11271v1
2309.11267v1,cs.CV,From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring,2023-09-20 12:50:52+00:00,"Monitoring surface cracks in infrastructure is crucial for structural health
monitoring. Automatic visual inspection offers an effective solution,
especially in hard-to-reach areas. Machine learning approaches have proven
their effectiveness but typically require large annotated datasets for
supervised training. Once a crack is detected, monitoring its severity often
demands precise segmentation of the damage. However, pixel-level annotation of
images for segmentation is labor-intensive. To mitigate this cost, one can
leverage explainable artificial intelligence (XAI) to derive segmentations from
the explanations of a classifier, requiring only weak image-level supervision.
This paper proposes applying this methodology to segment and monitor surface
cracks. We evaluate the performance of various XAI methods and examine how this
approach facilitates severity quantification and growth monitoring. Results
reveal that while the resulting segmentation masks may exhibit lower quality
than those produced by supervised methods, they remain meaningful and enable
severity monitoring, thus reducing substantial labeling costs.",http://arxiv.org/pdf/2309.11267v1
2309.11263v1,eess.SP,Active Inference for Sum Rate Maximization in UAV-Assisted Cognitive NOMA Networks,2023-09-20 12:42:50+00:00,"Given the surge in wireless data traffic driven by the emerging Internet of
Things (IoT), unmanned aerial vehicles (UAVs), cognitive radio (CR), and
non-orthogonal multiple access (NOMA) have been recognized as promising
techniques to overcome massive connectivity issues. As a result, there is an
increasing need to intelligently improve the channel capacity of future
wireless networks. Motivated by active inference from cognitive neuroscience,
this paper investigates joint subchannel and power allocation for an uplink
UAV-assisted cognitive NOMA network. Maximizing the sum rate is often a highly
challenging optimization problem due to dynamic network conditions and power
constraints. To address this challenge, we propose an active inference-based
algorithm. We transform the sum rate maximization problem into abnormality
minimization by utilizing a generalized state-space model to characterize the
time-changing network environment. The problem is then solved using an Active
Generalized Dynamic Bayesian Network (Active-GDBN). The proposed framework
consists of an offline perception stage, in which a UAV employs a hierarchical
GDBN structure to learn an optimal generative model of discrete subchannels and
continuous power allocation. In the online active inference stage, the UAV
dynamically selects discrete subchannels and continuous power to maximize the
sum rate of secondary users. By leveraging the errors in each episode, the UAV
can adapt its resource allocation policies and belief updating to improve its
performance over time. Simulation results demonstrate the effectiveness of our
proposed algorithm in terms of cumulative sum rate compared to benchmark
schemes.",http://arxiv.org/pdf/2309.11263v1
2309.11259v1,cs.CL,Sequence-to-Sequence Spanish Pre-trained Language Models,2023-09-20 12:35:19+00:00,"In recent years, substantial advancements in pre-trained language models have
paved the way for the development of numerous non-English language versions,
with a particular focus on encoder-only and decoder-only architectures. While
Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited
prowess in natural language understanding and generation, there remains a
scarcity of encoder-decoder models designed for sequence-to-sequence tasks
involving input-output pairs. This paper breaks new ground by introducing the
implementation and evaluation of renowned encoder-decoder architectures,
exclusively pre-trained on Spanish corpora. Specifically, we present Spanish
versions of BART, T5, and BERT2BERT-style models and subject them to a
comprehensive assessment across a diverse range of sequence-to-sequence tasks,
spanning summarization, rephrasing, and generative question answering. Our
findings underscore the competitive performance of all models, with BART and T5
emerging as top performers across all evaluated tasks. As an additional
contribution, we have made all models publicly available to the research
community, fostering future exploration and development in Spanish language
processing.",http://arxiv.org/pdf/2309.11259v1
2309.11247v1,cs.LG,Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering,2023-09-20 12:16:00+00:00,"The application of artificial intelligence to simulate air-to-air combat
scenarios is attracting increasing attention. To date the high-dimensional
state and action spaces, the high complexity of situation information (such as
imperfect and filtered information, stochasticity, incomplete knowledge about
mission targets) and the nonlinear flight dynamics pose significant challenges
for accurate air combat decision-making. These challenges are exacerbated when
multiple heterogeneous agents are involved. We propose a hierarchical
multi-agent reinforcement learning framework for air-to-air combat with
multiple heterogeneous agents. In our framework, the decision-making process is
divided into two stages of abstraction, where heterogeneous low-level policies
control the action of individual units, and a high-level commander policy
issues macro commands given the overall mission targets. Low-level policies are
trained for accurate unit combat control. Their training is organized in a
learning curriculum with increasingly complex training scenarios and
league-based self-play. The commander policy is trained on mission targets
given pre-trained low-level policies. The empirical validation advocates the
advantages of our design choices.",http://arxiv.org/pdf/2309.11247v1
2309.11243v1,eess.AS,Joint Minimum Processing Beamforming and Near-end Listening Enhancement,2023-09-20 12:11:06+00:00,"We consider speech enhancement for signals picked up in one noisy environment
that must be rendered to a listener in another noisy environment. For both
far-end noise reduction and near-end listening enhancement, it has been shown
that excessive focus on noise suppression or intelligibility maximization may
lead to excessive speech distortions and quality degradations in favorable
noise conditions, where intelligibility is already at ceiling level. Recently
[1,2] propose to remedy this with a minimum processing framework that either
reduces noise or enhances listening a minimum amount given that a certain
intelligibility criterion is still satisfied. Additionally, it has been shown
that joint consideration of both environments improves speech enhancement
performance. In this paper, we formulate a joint far- and near-end minimum
processing framework, that improves intelligibility while limiting speech
distortions in favorable noise conditions. We provide closed-form solutions to
specific boundary scenarios and investigate performance for the general case
using numerical optimization. We also show that concatenating existing minimum
processing far- and near-end enhancement methods preserves the effects of the
initial methods. Results show that the joint optimization can further improve
performance compared to the concatenated approach.",http://arxiv.org/pdf/2309.11243v1
2309.11236v1,cs.AI,Colour Passing Revisited: Lifted Model Construction with Commutative Factors,2023-09-20 11:57:19+00:00,"Lifted probabilistic inference exploits symmetries in a probabilistic model
to allow for tractable probabilistic inference with respect to domain sizes. To
apply lifted inference, a lifted representation has to be obtained, and to do
so, the so-called colour passing algorithm is the state of the art. The colour
passing algorithm, however, is bound to a specific inference algorithm and we
found that it ignores commutativity of factors while constructing a lifted
representation. We contribute a modified version of the colour passing
algorithm that uses logical variables to construct a lifted representation
independent of a specific inference algorithm while at the same time exploiting
commutativity of factors during an offline-step. Our proposed algorithm
efficiently detects more symmetries than the state of the art and thereby
drastically increases compression, yielding significantly faster online query
times for probabilistic inference when the resulting model is applied.",http://arxiv.org/pdf/2309.11236v1
2309.11231v1,cs.AI,ChatGPT-4 as a Tool for Reviewing Academic Books in Spanish,2023-09-20 11:44:45+00:00,"This study evaluates the potential of ChatGPT-4, an artificial intelligence
language model developed by OpenAI, as an editing tool for Spanish literary and
academic books. The need for efficient and accessible reviewing and editing
processes in the publishing industry has driven the search for automated
solutions. ChatGPT-4, being one of the most advanced language models, offers
notable capabilities in text comprehension and generation. In this study, the
features and capabilities of ChatGPT-4 are analyzed in terms of grammatical
correction, stylistic coherence, and linguistic enrichment of texts in Spanish.
Tests were conducted with 100 literary and academic texts, where the edits made
by ChatGPT-4 were compared to those made by expert human reviewers and editors.
The results show that while ChatGPT-4 is capable of making grammatical and
orthographic corrections with high accuracy and in a very short time, it still
faces challenges in areas such as context sensitivity, bibliometric analysis,
deep contextual understanding, and interaction with visual content like graphs
and tables. However, it is observed that collaboration between ChatGPT-4 and
human reviewers and editors can be a promising strategy for improving
efficiency without compromising quality. Furthermore, the authors consider that
ChatGPT-4 represents a valuable tool in the editing process, but its use should
be complementary to the work of human editors to ensure high-caliber editing in
Spanish literary and academic books.",http://arxiv.org/pdf/2309.11231v1
2309.11224v1,cs.AI,Leveraging Diversity in Online Interactions,2023-09-20 11:28:39+00:00,"This paper addresses the issue of connecting people online to help them find
support with their day-to-day problems. We make use of declarative norms for
mediating online interactions, and we specifically focus on the issue of
leveraging diversity when connecting people. We run pilots at different
university sites, and the results show relative success in the diversity of the
selected profiles, backed by high user satisfaction.",http://arxiv.org/pdf/2309.11224v1
2309.11206v2,cs.CL,Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering,2023-09-20 10:42:08+00:00,"Despite their competitive performance on knowledge-intensive tasks, large
language models (LLMs) still have limitations in memorizing all world knowledge
especially long tail knowledge. In this paper, we study the KG-augmented
language model approach for solving the knowledge graph question answering
(KGQA) task that requires rich world knowledge. Existing work has shown that
retrieving KG knowledge to enhance LLMs prompting can significantly improve
LLMs performance in KGQA. However, their approaches lack a well-formed
verbalization of KG knowledge, i.e., they ignore the gap between KG
representations and textual representations. To this end, we propose an
answer-sensitive KG-to-Text approach that can transform KG knowledge into
well-textualized statements most informative for KGQA. Based on this approach,
we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.
Experiments on several KGQA benchmarks show that the proposed KG-to-Text
augmented LLMs approach outperforms previous KG-augmented LLMs approaches
regarding answer accuracy and usefulness of knowledge statements.",http://arxiv.org/pdf/2309.11206v2
2309.11202v1,cs.AI,Using Artificial Intelligence for the Automation of Knitting Patterns,2023-09-20 10:38:08+00:00,"Knitting patterns are a crucial component in the creation and design of
knitted materials. Traditionally, these patterns were taught informally, but
thanks to advancements in technology, anyone interested in knitting can use the
patterns as a guide to start knitting. Perhaps because knitting is mostly a
hobby, with the exception of industrial manufacturing utilising specialised
knitting machines, the use of Al in knitting is less widespread than its
application in other fields. However, it is important to determine whether
knitted pattern classification using an automated system is viable. In order to
recognise and classify knitting patterns. Using data augmentation and a
transfer learning technique, this study proposes a deep learning model. The
Inception ResNet-V2 is the main feature extraction and classification algorithm
used in the model. Metrics like accuracy, logarithmic loss, F1-score,
precision, and recall score were used to evaluate the model. The model
evaluation's findings demonstrate high model accuracy, precision, recall, and
F1 score. In addition, the AUC score for majority of the classes was in the
range (0.7-0.9). A comparative analysis was done using other pretrained models
and a ResNet-50 model with transfer learning and the proposed model evaluation
results surpassed all others. The major limitation for this project is time, as
with more time, there might have been better accuracy over a larger number of
epochs.",http://arxiv.org/pdf/2309.11202v1
2309.11196v1,cs.LG,When to Trust AI: Advances and Challenges for Certification of Neural Networks,2023-09-20 10:31:09+00:00,"Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.",http://arxiv.org/pdf/2309.11196v1
2309.11193v1,cs.LG,RHALE: Robust and Heterogeneity-aware Accumulated Local Effects,2023-09-20 10:27:41+00:00,"Accumulated Local Effects (ALE) is a widely-used explainability method for
isolating the average effect of a feature on the output, because it handles
cases with correlated features well. However, it has two limitations. First, it
does not quantify the deviation of instance-level (local) effects from the
average (global) effect, known as heterogeneity. Second, for estimating the
average effect, it partitions the feature domain into user-defined, fixed-sized
bins, where different bin sizes may lead to inconsistent ALE estimations. To
address these limitations, we propose Robust and Heterogeneity-aware ALE
(RHALE). RHALE quantifies the heterogeneity by considering the standard
deviation of the local effects and automatically determines an optimal
variable-size bin-splitting. In this paper, we prove that to achieve an
unbiased approximation of the standard deviation of local effects within each
bin, bin splitting must follow a set of sufficient conditions. Based on these
conditions, we propose an algorithm that automatically determines the optimal
partitioning, balancing the estimation bias and variance. Through evaluations
on synthetic and real datasets, we demonstrate the superiority of RHALE
compared to other methods, including the advantages of automatic bin splitting,
especially in cases with correlated features.",http://arxiv.org/pdf/2309.11193v1
2309.11177v1,cs.IR,Long-tail Augmented Graph Contrastive Learning for Recommendation,2023-09-20 09:57:20+00:00,"Graph Convolutional Networks (GCNs) has demonstrated promising results for
recommender systems, as they can effectively leverage high-order relationship.
However, these methods usually encounter data sparsity issue in real-world
scenarios. To address this issue, GCN-based recommendation methods employ
contrastive learning to introduce self-supervised signals. Despite their
effectiveness, these methods lack consideration of the significant degree
disparity between head and tail nodes. This can lead to non-uniform
representation distribution, which is a crucial factor for the performance of
contrastive learning methods. To tackle the above issue, we propose a novel
Long-tail Augmented Graph Contrastive Learning (LAGCL) method for
recommendation. Specifically, we introduce a learnable long-tail augmentation
approach to enhance tail nodes by supplementing predicted neighbor information,
and generate contrastive views based on the resulting augmented graph. To make
the data augmentation schema learnable, we design an auto drop module to
generate pseudo-tail nodes from head nodes and a knowledge transfer module to
reconstruct the head nodes from pseudo-tail nodes. Additionally, we employ
generative adversarial networks to ensure that the distribution of the
generated tail/head nodes matches that of the original tail/head nodes.
Extensive experiments conducted on three benchmark datasets demonstrate the
significant improvement in performance of our model over the state-of-the-arts.
Further analyses demonstrate the uniformity of learned representations and the
superiority of LAGCL on long-tail performance. Code is publicly available at
https://github.com/im0qianqian/LAGCL",http://arxiv.org/pdf/2309.11177v1
2309.11166v1,cs.CL,Are Large Language Models Really Robust to Word-Level Perturbations?,2023-09-20 09:23:46+00:00,"The swift advancement in the scale and capabilities of Large Language Models
(LLMs) positions them as promising tools for a variety of downstream tasks. In
addition to the pursuit of better performance and the avoidance of violent
feedback on a certain prompt, to ensure the responsibility of the LLM, much
attention is drawn to the robustness of LLMs. However, existing evaluation
methods mostly rely on traditional question answering datasets with predefined
supervised labels, which do not align with the superior generation capabilities
of contemporary LLMs. To address this issue, we propose a novel rational
evaluation approach that leverages pre-trained reward models as diagnostic
tools to evaluate the robustness of LLMs, which we refer to as the Reward Model
for Reasonable Robustness Evaluation (TREvaL). Our extensive empirical
experiments have demonstrated that TREval provides an accurate method for
evaluating the robustness of an LLM, especially when faced with more
challenging open questions. Furthermore, our results demonstrate that LLMs
frequently exhibit vulnerability to word-level perturbations, which are
commonplace in daily language usage. Notably, we were surprised to discover
that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted.
The code of TREval is available in https://github.com/Harry-mic/TREval.",http://arxiv.org/pdf/2309.11166v1
2309.11161v2,cs.IT,Beamforming Design for RIS-Aided THz Wideband Communication Systems,2023-09-20 09:18:26+00:00,"Benefiting from tens of GHz of bandwidth, terahertz (THz) communications has
become a promising technology for future 6G networks. However, the conventional
hybrid beamforming architecture based on frequency-independent phase-shifters
is not able to cope with the beam split effect (BSE) in THz massive
multiple-input multiple-output (MIMO) systems. Despite some work introducing
the frequency-dependent phase shifts via the time delay network to mitigate the
beam splitting in THz wideband communications, the corresponding issue in
reconfigurable intelligent surface (RIS)-aided communications has not been well
investigated. In this paper, the BSE in THz massive MIMO is quantified by
analyzing the array gain loss. A new beamforming architecture has been proposed
to mitigate this effect under RIS-aided communications scenarios. Simulations
are performed to evaluate the effectiveness of the proposed system architecture
in combating the array gain loss.",http://arxiv.org/pdf/2309.11161v2
2309.11155v1,cs.AI,ProtoExplorer: Interpretable Forensic Analysis of Deepfake Videos using Prototype Exploration and Refinement,2023-09-20 09:03:56+00:00,"In high-stakes settings, Machine Learning models that can provide predictions
that are interpretable for humans are crucial. This is even more true with the
advent of complex deep learning based models with a huge number of tunable
parameters. Recently, prototype-based methods have emerged as a promising
approach to make deep learning interpretable. We particularly focus on the
analysis of deepfake videos in a forensics context. Although prototype-based
methods have been introduced for the detection of deepfake videos, their use in
real-world scenarios still presents major challenges, in that prototypes tend
to be overly similar and interpretability varies between prototypes. This paper
proposes a Visual Analytics process model for prototype learning, and, based on
this, presents ProtoExplorer, a Visual Analytics system for the exploration and
refinement of prototype-based deepfake detection models. ProtoExplorer offers
tools for visualizing and temporally filtering prototype-based predictions when
working with video data. It disentangles the complexity of working with
spatio-temporal prototypes, facilitating their visualization. It further
enables the refinement of models by interactively deleting and replacing
prototypes with the aim to achieve more interpretable and less biased
predictions while preserving detection accuracy. The system was designed with
forensic experts and evaluated in a number of rounds based on both open-ended
think aloud evaluation and interviews. These sessions have confirmed the
strength of our prototype based exploration of deepfake videos while they
provided the feedback needed to continuously improve the system.",http://arxiv.org/pdf/2309.11155v1
2309.11143v1,cs.CL,CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought,2023-09-20 08:42:06+00:00,"Unsupervised sentence representation learning aims to transform input
sentences into fixed-length vectors enriched with intricate semantic
information while obviating the reliance on labeled data. Recent progress
within this field, propelled by contrastive learning and prompt engineering,
has significantly bridged the gap between unsupervised and supervised
strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains
largely untapped within this trajectory. To unlock latent capabilities within
pre-trained models, such as BERT, we propose a two-stage approach for sentence
representation: comprehension and summarization. Subsequently, the output of
the latter phase is harnessed as the vectorized representation of the input
sentence. For further performance enhancement, we meticulously refine both the
contrastive learning loss function and the template denoising technique for
prompt engineering. Rigorous experimentation substantiates our method,
CoT-BERT, transcending a suite of robust baselines without necessitating other
text representation models or external databases.",http://arxiv.org/pdf/2309.11143v1
2309.11142v1,cs.CL,Prototype of a robotic system to assist the learning process of English language with text-generation through DNN,2023-09-20 08:39:51+00:00,"In the last ongoing years, there has been a significant ascending on the
field of Natural Language Processing (NLP) for performing multiple tasks
including English Language Teaching (ELT). An effective strategy to favor the
learning process uses interactive devices to engage learners in their
self-learning process. In this work, we present a working prototype of a
humanoid robotic system to assist English language self-learners through text
generation using Long Short Term Memory (LSTM) Neural Networks. The learners
interact with the system using a Graphic User Interface that generates text
according to the English level of the user. The experimentation was conducted
using English learners and the results were measured accordingly to
International English Language Testing System (IELTS) rubric. Preliminary
results show an increment in the Grammatical Range of learners who interacted
with the system.",http://arxiv.org/pdf/2309.11142v1
2309.11132v1,cs.CV,Contrastive Pseudo Learning for Open-World DeepFake Attribution,2023-09-20 08:29:22+00:00,"The challenge in sourcing attribution for forgery faces has gained widespread
attention due to the rapid development of generative techniques. While many
recent works have taken essential steps on GAN-generated faces, more
threatening attacks related to identity swapping or expression transferring are
still overlooked. And the forgery traces hidden in unknown attacks from the
open-world unlabeled faces still remain under-explored. To push the related
frontier research, we introduce a new benchmark called Open-World DeepFake
Attribution (OW-DFA), which aims to evaluate attribution performance against
various types of fake faces under open-world scenarios. Meanwhile, we propose a
novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task
through 1) introducing a Global-Local Voting module to guide the feature
alignment of forged faces with different manipulated regions, 2) designing a
Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused
by similar methods in unlabeled set. In addition, we extend the CPL framework
with a multi-stage paradigm that leverages pre-train technique and iterative
learning to further enhance traceability performance. Extensive experiments
verify the superiority of our proposed method on the OW-DFA and also
demonstrate the interpretability of deepfake attribution task and its impact on
improving the security of deepfake detection area.",http://arxiv.org/pdf/2309.11132v1
2309.11127v1,eess.SP,Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation,2023-09-20 08:19:05+00:00,"By integrating recent advances in large language models (LLMs) and generative
models into the emerging semantic communication (SC) paradigm, in this article
we put forward to a novel framework of language-oriented semantic communication
(LSC). In LSC, machines communicate using human language messages that can be
interpreted and manipulated via natural language processing (NLP) techniques
for SC efficiency. To demonstrate LSC's potential, we introduce three
innovative algorithms: 1) semantic source coding (SSC) which compresses a text
prompt into its key head words capturing the prompt's syntactic essence while
maintaining their appearance order to keep the prompt's context; 2) semantic
channel coding (SCC) that improves robustness against errors by substituting
head words with their lenghthier synonyms; and 3) semantic knowledge
distillation (SKD) that produces listener-customized prompts via in-context
learning the listener's language style. In a communication task for progressive
text-to-image generation, the proposed methods achieve higher perceptual
similarities with fewer transmissions while enhancing robustness in noisy
communication channels.",http://arxiv.org/pdf/2309.11127v1
2309.11528v1,cs.AI,Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction,2023-09-20 08:11:58+00:00,"Inductive link prediction -- where entities during training and inference
stages can be different -- has shown great potential for completing evolving
knowledge graphs in an entity-independent manner. Many popular methods mainly
focus on modeling graph-level features, while the edge-level interactions --
especially the semantic correlations between relations -- have been less
explored. However, we notice a desirable property of semantic correlations
between relations is that they are inherently edge-level and
entity-independent. This implies the great potential of the semantic
correlations for the entity-independent inductive link prediction task.
Inspired by this observation, we propose a novel subgraph-based method, namely
TACO, to model Topology-Aware COrrelations between relations that are highly
correlated to their topological structures within subgraphs. Specifically, we
prove that semantic correlations between any two relations can be categorized
into seven topological patterns, and then proposes Relational Correlation
Network (RCN) to learn the importance of each pattern. To further exploit the
potential of RCN, we propose Complete Common Neighbor induced subgraph that can
effectively preserve complete topological patterns within the subgraph.
Extensive experiments demonstrate that TACO effectively unifies the graph-level
information and edge-level interactions to jointly perform reasoning, leading
to a superior performance over existing state-of-the-art methods for the
inductive link prediction task.",http://arxiv.org/pdf/2309.11528v1
2309.11527v1,cs.IR,TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback,2023-09-20 07:21:50+00:00,"This work describes the TrueLearn Python library, which contains a family of
online learning Bayesian models for building educational (or more generally,
informational) recommendation systems. This family of models was designed
following the ""open learner"" concept, using humanly-intuitive user
representations. For the sake of interpretability and putting the user in
control, the TrueLearn library also contains different representations to help
end-users visualise the learner models, which may in the future facilitate user
interaction with their own models. Together with the library, we include a
previously publicly released implicit feedback educational dataset with
evaluation metrics to measure the performance of the models. The extensive
documentation and coding examples make the library highly accessible to both
machine learning developers and educational data mining and learning analytic
practitioners. The library and the support documentation with examples are
available at https://truelearn.readthedocs.io/en/latest.",http://arxiv.org/pdf/2309.11527v1
2309.11104v1,cs.CL,AttentionMix: Data augmentation method that relies on BERT attention mechanism,2023-09-20 07:18:53+00:00,"The Mixup method has proven to be a powerful data augmentation technique in
Computer Vision, with many successors that perform image mixing in a guided
manner. One of the interesting research directions is transferring the
underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP).
Even though there already exist several methods that apply Mixup to textual
data, there is still room for new, improved approaches. In this work, we
introduce AttentionMix, a novel mixing method that relies on attention-based
information. While the paper focuses on the BERT attention mechanism, the
proposed approach can be applied to generally any attention-based model.
AttentionMix is evaluated on 3 standard sentiment classification datasets and
in all three cases outperforms two benchmark approaches that utilize Mixup
mechanism, as well as the vanilla BERT method. The results confirm that the
attention-based information can be effectively used for data augmentation in
the NLP domain.",http://arxiv.org/pdf/2309.11104v1
2309.11101v1,cs.LG,A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making,2023-09-20 07:15:48+00:00,"In healthcare applications, understanding how machine/deep learning models
make decisions is crucial. In this study, we introduce a neural network
framework, $\textit{Truth Table rules}$ (TT-rules), that combines the global
and exact interpretability properties of rule-based models with the high
performance of deep neural networks. TT-rules is built upon $\textit{Truth
Table nets}$ (TTnet), a family of deep neural networks initially developed for
formal verification. By extracting the necessary and sufficient rules
$\mathcal{R}$ from the trained TTnet model (global interpretability) to yield
the same output as the TTnet (exact interpretability), TT-rules effectively
transforms the neural network into a rule-based model. This rule-based model
supports binary classification, multi-label classification, and regression
tasks for small to large tabular datasets. After outlining the framework, we
evaluate TT-rules' performance on healthcare applications and compare it to
state-of-the-art rule-based methods. Our results demonstrate that TT-rules
achieves equal or higher performance compared to other interpretable methods.
Notably, TT-rules presents the first accurate rule-based model capable of
fitting large tabular datasets, including two real-life DNA datasets with over
20K features.",http://arxiv.org/pdf/2309.11101v1
2309.11096v1,cs.LG,Delays in Reinforcement Learning,2023-09-20 07:04:46+00:00,"Delays are inherent to most dynamical systems. Besides shifting the process
in time, they can significantly affect their performance. For this reason, it
is usually valuable to study the delay and account for it. Because they are
dynamical systems, it is of no surprise that sequential decision-making
problems such as Markov decision processes (MDP) can also be affected by
delays. These processes are the foundational framework of reinforcement
learning (RL), a paradigm whose goal is to create artificial agents capable of
learning to maximise their utility by interacting with their environment.
  RL has achieved strong, sometimes astonishing, empirical results, but delays
are seldom explicitly accounted for. The understanding of the impact of delay
on the MDP is limited. In this dissertation, we propose to study the delay in
the agent's observation of the state of the environment or in the execution of
the agent's actions. We will repeatedly change our point of view on the problem
to reveal some of its structure and peculiarities. A wide spectrum of delays
will be considered, and potential solutions will be presented. This
dissertation also aims to draw links between celebrated frameworks of the RL
literature and the one of delays.",http://arxiv.org/pdf/2309.11096v1
2309.11526v1,cs.LG,Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems,2023-09-20 06:55:39+00:00,"An important task in the field of sensor technology is the efficient
implementation of adaptation procedures of measurements from one sensor to
another sensor of identical design. One idea is to use the estimation of an
affine transformation between different systems, which can be improved by the
knowledge of experts. This paper presents an improved solution from Glacier
Research that was published back in 1973. It is shown that this solution can be
adapted for software calibration of sensors, implementation of expert-based
adaptation, and federated learning methods. We evaluate our research with
simulations and also with real measured data of a multi-sensor board with 8
identical sensors. The results show an improvement for both the simulation and
the experiments with real data.",http://arxiv.org/pdf/2309.11526v1
2309.11089v1,eess.SY,Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling,2023-09-20 06:39:19+00:00,"This paper addresses the prediction stability, prediction accuracy and
control capability of the current probabilistic model-based reinforcement
learning (MBRL) built on neural networks. A novel approach dropout-based
probabilistic ensembles with trajectory sampling (DPETS) is proposed where the
system uncertainty is stably predicted by combining the Monte-Carlo dropout and
trajectory sampling in one framework. Its loss function is designed to correct
the fitting error of neural networks for more accurate prediction of
probabilistic models. The state propagation in its policy is extended to filter
the aleatoric uncertainty for superior control capability. Evaluated by several
Mujoco benchmark control tasks under additional disturbances and one practical
robot arm manipulation task, DPETS outperforms related MBRL approaches in both
average return and convergence velocity while achieving superior performance
than well-known model-free baselines with significant sample efficiency. The
open source code of DPETS is available at https://github.com/mrjun123/DPETS.",http://arxiv.org/pdf/2309.11089v1
2309.11087v1,q-bio.GN,Embed-Search-Align: DNA Sequence Alignment using Transformer Models,2023-09-20 06:30:39+00:00,"DNA sequence alignment involves assigning short DNA reads to the most
probable locations on an extensive reference genome. This process is crucial
for various genomic analyses, including variant calling, transcriptomics, and
epigenomics. Conventional methods, refined over decades, tackle this challenge
in two steps: genome indexing followed by efficient search to locate likely
positions for given reads. Building on the success of Large Language Models
(LLM) in encoding text into embeddings, where the distance metric captures
semantic similarity, recent efforts have explored whether the same Transformer
architecture can produce numerical representations for DNA sequences. Such
models have shown early promise in tasks involving classification of short DNA
sequences, such as the detection of coding vs non-coding regions, as well as
the identification of enhancer and promoter sequences. Performance at sequence
classification tasks does not, however, translate to sequence alignment, where
it is necessary to conduct a genome-wide search to successfully align every
read. We address this open problem by framing it as an Embed-Search-Align task.
In this framework, a novel encoder model DNA-ESA generates representations of
reads and fragments of the reference, which are projected into a shared vector
space where the read-fragment distance is used as surrogate for alignment. In
particular, DNA-ESA introduces: (1) Contrastive loss for self-supervised
training of DNA sequence representations, facilitating rich sequence-level
embeddings, and (2) a DNA vector store to enable search across fragments on a
global scale. DNA-ESA is >97% accurate when aligning 250-length reads onto a
human reference genome of 3 gigabases (single-haploid), far exceeds the
performance of 6 recent DNA-Transformer model baselines and shows task transfer
across chromosomes and species.",http://arxiv.org/pdf/2309.11087v1
2309.11077v1,cs.CV,Weak Supervision for Label Efficient Visual Bug Detection,2023-09-20 06:00:02+00:00,"As video games evolve into expansive, detailed worlds, visual quality becomes
essential, yet increasingly challenging. Traditional testing methods, limited
by resources, face difficulties in addressing the plethora of potential bugs.
Machine learning offers scalable solutions; however, heavy reliance on large
labeled datasets remains a constraint. Addressing this challenge, we propose a
novel method, utilizing unlabeled gameplay and domain-specific augmentations to
generate datasets & self-supervised objectives used during pre-training or
multi-task settings for downstream visual bug detection. Our methodology uses
weak-supervision to scale datasets for the crafted objectives and facilitates
both autonomous and interactive weak-supervision, incorporating unsupervised
clustering and/or an interactive approach based on text and geometric prompts.
We demonstrate on first-person player clipping/collision bugs (FPPC) within the
expansive Giantmap game world, that our approach is very effective, improving
over a strong supervised baseline in a practical, very low-prevalence, low data
regime (0.336 $\rightarrow$ 0.550 F1 score). With just 5 labeled ""good""
exemplars (i.e., 0 bugs), our self-supervised objective alone captures enough
signal to outperform the low-labeled supervised settings. Building on
large-pretrained vision models, our approach is adaptable across various visual
bugs. Our results suggest applicability in curating datasets for broader image
and video tasks within video games beyond visual bugs.",http://arxiv.org/pdf/2309.11077v1
2309.11069v1,cs.CV,"Dynamic Tiling: A Model-Agnostic, Adaptive, Scalable, and Inference-Data-Centric Approach for Efficient and Accurate Small Object Detection",2023-09-20 05:25:12+00:00,"We introduce Dynamic Tiling, a model-agnostic, adaptive, and scalable
approach for small object detection, anchored in our inference-data-centric
philosophy. Dynamic Tiling starts with non-overlapping tiles for initial
detections and utilizes dynamic overlapping rates along with a tile minimizer.
This dual approach effectively resolves fragmented objects, improves detection
accuracy, and minimizes computational overhead by reducing the number of
forward passes through the object detection model. Adaptable to a variety of
operational environments, our method negates the need for laborious
recalibration. Additionally, our large-small filtering mechanism boosts the
detection quality across a range of object sizes. Overall, Dynamic Tiling
outperforms existing model-agnostic uniform cropping methods, setting new
benchmarks for efficiency and accuracy.",http://arxiv.org/pdf/2309.11069v1
2309.11064v1,cs.AI,"Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness",2023-09-20 05:04:16+00:00,"As Large Language Models (LLMs) have advanced, they have brought forth new
challenges, with one of the prominent issues being LLM hallucination. While
various mitigation techniques are emerging to address hallucination, it is
equally crucial to delve into its underlying causes. Consequently, in this
preliminary exploratory investigation, we examine how linguistic factors in
prompts, specifically readability, formality, and concreteness, influence the
occurrence of hallucinations. Our experimental results suggest that prompts
characterized by greater formality and concreteness tend to result in reduced
hallucination. However, the outcomes pertaining to readability are somewhat
inconclusive, showing a mixed pattern.",http://arxiv.org/pdf/2309.11064v1
2309.11054v1,cs.CL,Design of Chain-of-Thought in Math Problem Solving,2023-09-20 04:17:28+00:00,"Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem
solving. We conduct a comprehensive examination of methods for designing CoT,
comparing conventional natural language CoT with various program CoTs,
including the self-describing program, the comment-describing program, and the
non-describing program. Furthermore, we investigate the impact of programming
language on program CoTs, comparing Python and Wolfram Language. Through
extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs
often have superior effectiveness in math problem solving. Notably, the best
performing combination with 30B parameters beats GPT-3.5-turbo by a significant
margin. The results show that self-describing program offers greater diversity
and thus can generally achieve higher performance. We also find that Python is
a better choice of language than Wolfram for program CoTs. The experimental
results provide a valuable guideline for future CoT designs that take into
account both programming language and coding style for further advancements.
Our datasets and code are publicly available.",http://arxiv.org/pdf/2309.11054v1
2309.11053v1,cs.CR,Fed-LSAE: Thwarting Poisoning Attacks against Federated Cyber Threat Detection System via Autoencoder-based Latent Space Inspection,2023-09-20 04:14:48+00:00,"The significant rise of security concerns in conventional centralized
learning has promoted federated learning (FL) adoption in building intelligent
applications without privacy breaches. In cybersecurity, the sensitive data
along with the contextual information and high-quality labeling in each
enterprise organization play an essential role in constructing high-performance
machine learning (ML) models for detecting cyber threats. Nonetheless, the
risks coming from poisoning internal adversaries against FL systems have raised
discussions about designing robust anti-poisoning frameworks. Whereas defensive
mechanisms in the past were based on outlier detection, recent approaches tend
to be more concerned with latent space representation. In this paper, we
investigate a novel robust aggregation method for FL, namely Fed-LSAE, which
takes advantage of latent space representation via the penultimate layer and
Autoencoder to exclude malicious clients from the training process. The
experimental results on the CIC-ToN-IoT and N-BaIoT datasets confirm the
feasibility of our defensive mechanism against cutting-edge poisoning attacks
for developing a robust FL-based threat detector in the context of IoT. More
specifically, the FL evaluation witnesses an upward trend of approximately 98%
across all metrics when integrating with our Fed-LSAE defense.",http://arxiv.org/pdf/2309.11053v1
2309.11044v1,cs.LG,Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion,2023-09-20 03:47:53+00:00,"Federated Learning (FL) is currently one of the most popular technologies in
the field of Artificial Intelligence (AI) due to its collaborative learning and
ability to preserve client privacy. However, it faces challenges such as
non-identically and non-independently distributed (non-IID) and data with
imbalanced labels among local clients. To address these limitations, the
research community has explored various approaches such as using local model
parameters, federated generative adversarial learning, and federated
representation learning. In our study, we propose a novel Clustered FedStack
framework based on the previously published Stacked Federated Learning
(FedStack) framework. The local clients send their model predictions and output
layer weights to a server, which then builds a robust global model. This global
model clusters the local clients based on their output layer weights using a
clustering mechanism. We adopt three clustering mechanisms, namely K-Means,
Agglomerative, and Gaussian Mixture Models, into the framework and evaluate
their performance. We use Bayesian Information Criterion (BIC) with the maximum
likelihood function to determine the number of clusters. The Clustered FedStack
models outperform baseline models with clustering mechanisms. To estimate the
convergence of our proposed framework, we use Cyclical learning rates.",http://arxiv.org/pdf/2309.11044v1
2309.11042v1,cs.CL,Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters,2023-09-20 03:39:56+00:00,"Recently, Large Language Models (LLMs) have achieved amazing zero-shot
learning performance over a variety of Natural Language Processing (NLP) tasks,
especially for text generative tasks. Yet, the large size of LLMs often leads
to the high computational cost of model training and online deployment. In our
work, we present ALTER, a system that effectively builds the multi-tAsk
Learners with mixTure-of-task-adaptERs upon small language models (with <1B
parameters) to address multiple NLP tasks simultaneously, capturing the
commonalities and differences between tasks, in order to support
domain-specific applications. Specifically, in ALTER, we propose the
Mixture-of-Task-Adapters (MTA) module as an extension to the transformer
architecture for the underlying model to capture the intra-task and inter-task
knowledge. A two-stage training method is further proposed to optimize the
collaboration between adapters at a small computational cost. Experimental
results over a mixture of NLP tasks show that our proposed MTA architecture and
the two-stage training method achieve good performance. Based on ALTER, we have
also produced MTA-equipped language models for various domains.",http://arxiv.org/pdf/2309.11042v1
2309.11039v1,cs.LG,Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems,2023-09-20 03:39:30+00:00,"Intelligent transportation systems (ITSs) have been fueled by the rapid
development of communication technologies, sensor technologies, and the
Internet of Things (IoT). Nonetheless, due to the dynamic characteristics of
the vehicle networks, it is rather challenging to make timely and accurate
decisions of vehicle behaviors. Moreover, in the presence of mobile wireless
communications, the privacy and security of vehicle information are at constant
risk. In this context, a new paradigm is urgently needed for various
applications in dynamic vehicle environments. As a distributed machine learning
technology, federated learning (FL) has received extensive attention due to its
outstanding privacy protection properties and easy scalability. We conduct a
comprehensive survey of the latest developments in FL for ITS. Specifically, we
initially research the prevalent challenges in ITS and elucidate the
motivations for applying FL from various perspectives. Subsequently, we review
existing deployments of FL in ITS across various scenarios, and discuss
specific potential issues in object recognition, traffic management, and
service providing scenarios. Furthermore, we conduct a further analysis of the
new challenges introduced by FL deployment and the inherent limitations that FL
alone cannot fully address, including uneven data distribution, limited storage
and computing power, and potential privacy and security concerns. We then
examine the existing collaborative technologies that can help mitigate these
challenges. Lastly, we discuss the open challenges that remain to be addressed
in applying FL in ITS and propose several future research directions.",http://arxiv.org/pdf/2309.11039v1
2309.11022v1,cs.LG,Information Leakage from Data Updates in Machine Learning Models,2023-09-20 02:55:03+00:00,"In this paper we consider the setting where machine learning models are
retrained on updated datasets in order to incorporate the most up-to-date
information or reflect distribution shifts. We investigate whether one can
infer information about these updates in the training data (e.g., changes to
attribute values of records). Here, the adversary has access to snapshots of
the machine learning model before and after the change in the dataset occurs.
Contrary to the existing literature, we assume that an attribute of a single or
multiple training data points are changed rather than entire data records are
removed or added. We propose attacks based on the difference in the prediction
confidence of the original model and the updated model. We evaluate our attack
methods on two public datasets along with multi-layer perceptron and logistic
regression models. We validate that two snapshots of the model can result in
higher information leakage in comparison to having access to only the updated
model. Moreover, we observe that data records with rare values are more
vulnerable to attacks, which points to the disparate vulnerability of privacy
attacks in the update setting. When multiple records with the same original
attribute value are updated to the same new value (i.e., repeated changes), the
attacker is more likely to correctly guess the updated values since repeated
changes leave a larger footprint on the trained model. These observations point
to vulnerability of machine learning models to attribute inference attacks in
the update setting.",http://arxiv.org/pdf/2309.11022v1
2309.11013v1,cs.LG,ModelGiF: Gradient Fields for Model Functional Distance,2023-09-20 02:27:40+00:00,"The last decade has witnessed the success of deep learning and the surge of
publicly released trained models, which necessitates the quantification of the
model functional distance for various purposes. However, quantifying the model
functional distance is always challenging due to the opacity in inner workings
and the heterogeneity in architectures or tasks. Inspired by the concept of
""field"" in physics, in this work we introduce Model Gradient Field (abbr.
ModelGiF) to extract homogeneous representations from the heterogeneous
pre-trained models. Our main assumption underlying ModelGiF is that each
pre-trained deep model uniquely determines a ModelGiF over the input space. The
distance between models can thus be measured by the similarity between their
ModelGiFs. We validate the effectiveness of the proposed ModelGiF with a suite
of testbeds, including task relatedness estimation, intellectual property
protection, and model unlearning verification. Experimental results demonstrate
the versatility of the proposed ModelGiF on these tasks, with significantly
superiority performance to state-of-the-art competitors. Codes are available at
https://github.com/zju-vipa/modelgif.",http://arxiv.org/pdf/2309.11013v1
2309.10987v1,cs.NE,Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World,2023-09-20 01:04:57+00:00,"Spiking neuron networks (SNNs) have been thriving on numerous tasks to
leverage their promising energy efficiency and exploit their potentialities as
biologically plausible intelligence. Meanwhile, the Neural Radiance Fields
(NeRF) render high-quality 3D scenes with massive energy consumption, and few
works delve into the energy-saving solution with a bio-inspired approach. In
this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance
ray with the temporal dimension of SNN, to naturally accommodate the SNN to the
reconstruction of Radiance Fields. Thus, the computation turns into a
spike-based, multiplication-free manner, reducing the energy consumption. In
SpikingNeRF, each sampled point on the ray is matched onto a particular time
step, and represented in a hybrid manner where the voxel grids are maintained
as well. Based on the voxel grids, sampled points are determined whether to be
masked for better training and inference. However, this operation also incurs
irregular temporal length. We propose the temporal condensing-and-padding (TCP)
strategy to tackle the masked samples to maintain regular temporal length,
i.e., regular tensors, for hardware-friendly computation. Extensive experiments
on a variety of datasets demonstrate that our method reduces the $76.74\%$
energy consumption on average and obtains comparable synthesis quality with the
ANN baseline.",http://arxiv.org/pdf/2309.10987v1
2309.10982v1,cs.AI,Is GPT4 a Good Trader?,2023-09-20 00:47:52+00:00,"Recently, large language models (LLMs), particularly GPT-4, have demonstrated
significant capabilities in various planning and reasoning tasks
\cite{cheng2023gpt4,bubeck2023sparks}. Motivated by these advancements, there
has been a surge of interest among researchers to harness the capabilities of
GPT-4 for the automated design of quantitative factors that do not overlap with
existing factor libraries, with an aspiration to achieve alpha returns
\cite{webpagequant}. In contrast to these work, this study aims to examine the
fidelity of GPT-4's comprehension of classic trading theories and its
proficiency in applying its code interpreter abilities to real-world trading
data analysis. Such an exploration is instrumental in discerning whether the
underlying logic GPT-4 employs for trading is intrinsically reliable.
Furthermore, given the acknowledged interpretative latitude inherent in most
trading theories, we seek to distill more precise methodologies of deploying
these theories from GPT-4's analytical process, potentially offering invaluable
insights to human traders.
  To achieve this objective, we selected daily candlestick (K-line) data from
specific periods for certain assets, such as the Shanghai Stock Index. Through
meticulous prompt engineering, we guided GPT-4 to analyze the technical
structures embedded within this data, based on specific theories like the
Elliott Wave Theory. We then subjected its analytical output to manual
evaluation, assessing its interpretative depth and accuracy vis-\`a-vis these
trading theories from multiple dimensions. The results and findings from this
study could pave the way for a synergistic amalgamation of human expertise and
AI-driven insights in the realm of trading.",http://arxiv.org/pdf/2309.10982v1
2309.10980v1,cs.LG,AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning,2023-09-20 00:42:08+00:00,"Effective patient monitoring is vital for timely interventions and improved
healthcare outcomes. Traditional monitoring systems often struggle to handle
complex, dynamic environments with fluctuating vital signs, leading to delays
in identifying critical conditions. To address this challenge, we propose a
novel AI-driven patient monitoring framework using multi-agent deep
reinforcement learning (DRL). Our approach deploys multiple learning agents,
each dedicated to monitoring a specific physiological feature, such as heart
rate, respiration, and temperature. These agents interact with a generic
healthcare monitoring environment, learn the patients' behavior patterns, and
make informed decisions to alert the corresponding Medical Emergency Teams
(METs) based on the level of emergency estimated. In this study, we evaluate
the performance of the proposed multi-agent DRL framework using real-world
physiological and motion data from two datasets: PPG-DaLiA and WESAD. We
compare the results with several baseline models, including Q-Learning, PPO,
Actor-Critic, Double DQN, and DDPG, as well as monitoring frameworks like
WISEML and CA-MAQL. Our experiments demonstrate that the proposed DRL approach
outperforms all other baseline models, achieving more accurate monitoring of
patient's vital signs. Furthermore, we conduct hyperparameter optimization to
fine-tune the learning process of each agent. By optimizing hyperparameters, we
enhance the learning rate and discount factor, thereby improving the agents'
overall performance in monitoring patient health status. Our AI-driven patient
monitoring system offers several advantages over traditional methods, including
the ability to handle complex and uncertain environments, adapt to varying
patient conditions, and make real-time decisions without external supervision.",http://arxiv.org/pdf/2309.10980v1
2309.12314v1,cs.CV,TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance,2023-09-21 17:59:53+00:00,"In this paper, we propose a novel cross-modal distillation method, called
TinyCLIP, for large-scale language-image pre-trained models. The method
introduces two core techniques: affinity mimicking and weight inheritance.
Affinity mimicking explores the interaction between modalities during
distillation, enabling student models to mimic teachers' behavior of learning
cross-modal feature alignment in a visual-linguistic affinity space. Weight
inheritance transmits the pre-trained weights from the teacher models to their
student counterparts to improve distillation efficiency. Moreover, we extend
the method into a multi-stage progressive distillation to mitigate the loss of
informative weights during extreme compression. Comprehensive experiments
demonstrate the efficacy of TinyCLIP, showing that it can reduce the size of
the pre-trained CLIP ViT-B/32 by 50%, while maintaining comparable zero-shot
performance. While aiming for comparable performance, distillation with weight
inheritance can speed up the training by 1.4 - 7.8 $\times$ compared to
training from scratch. Moreover, our TinyCLIP ViT-8M/16, trained on YFCC-15M,
achieves an impressive zero-shot top-1 accuracy of 41.1% on ImageNet,
surpassing the original CLIP ViT-B/16 by 3.5% while utilizing only 8.9%
parameters. Finally, we demonstrate the good transferability of TinyCLIP in
various downstream tasks. Code and models will be open-sourced at
https://aka.ms/tinyclip.",http://arxiv.org/pdf/2309.12314v1
2309.12306v1,cs.CV,TalkNCE: Improving Active Speaker Detection with Talk-Aware Contrastive Learning,2023-09-21 17:59:11+00:00,"The goal of this work is Active Speaker Detection (ASD), a task to determine
whether a person is speaking or not in a series of video frames. Previous works
have dealt with the task by exploring network architectures while learning
effective representations has been less explored. In this work, we propose
TalkNCE, a novel talk-aware contrastive loss. The loss is only applied to part
of the full segments where a person on the screen is actually speaking. This
encourages the model to learn effective representations through the natural
correspondence of speech and facial movements. Our loss can be jointly
optimized with the existing objectives for training ASD models without the need
for additional supervision or training data. The experiments demonstrate that
our loss can be easily integrated into the existing ASD frameworks, improving
their performance. Our method achieves state-of-the-art performances on
AVA-ActiveSpeaker and ASW datasets.",http://arxiv.org/pdf/2309.12306v1
2309.12304v1,cs.CV,SlowFast Network for Continuous Sign Language Recognition,2023-09-21 17:59:04+00:00,"The objective of this work is the effective extraction of spatial and dynamic
features for Continuous Sign Language Recognition (CSLR). To accomplish this,
we utilise a two-pathway SlowFast network, where each pathway operates at
distinct temporal resolutions to separately capture spatial (hand shapes,
facial expressions) and dynamic (movements) information. In addition, we
introduce two distinct feature fusion methods, carefully designed for the
characteristics of CSLR: (1) Bi-directional Feature Fusion (BFF), which
facilitates the transfer of dynamic semantics into spatial semantics and vice
versa; and (2) Pathway Feature Enhancement (PFE), which enriches dynamic and
spatial representations through auxiliary subnetworks, while avoiding the need
for extra inference time. As a result, our model further strengthens spatial
and dynamic representations in parallel. We demonstrate that the proposed
framework outperforms the current state-of-the-art performance on popular CSLR
datasets, including PHOENIX14, PHOENIX14-T, and CSL-Daily.",http://arxiv.org/pdf/2309.12304v1
2309.12302v1,cs.CV,Text-Guided Vector Graphics Customization,2023-09-21 17:59:01+00:00,"Vector graphics are widely used in digital art and valued by designers for
their scalability and layer-wise topological properties. However, the creation
and editing of vector graphics necessitate creativity and design expertise,
leading to a time-consuming process. In this paper, we propose a novel pipeline
that generates high-quality customized vector graphics based on textual prompts
while preserving the properties and layer-wise information of a given exemplar
SVG. Our method harnesses the capabilities of large pre-trained text-to-image
models. By fine-tuning the cross-attention layers of the model, we generate
customized raster images guided by textual prompts. To initialize the SVG, we
introduce a semantic-based path alignment method that preserves and transforms
crucial paths from the exemplar SVG. Additionally, we optimize path parameters
using both image-level and vector-level losses, ensuring smooth shape
deformation while aligning with the customized raster image. We extensively
evaluate our method using multiple metrics from vector-level, image-level, and
text-level perspectives. The evaluation results demonstrate the effectiveness
of our pipeline in generating diverse customizations of vector graphics with
exceptional quality. The project page is
https://intchous.github.io/SVGCustomization.",http://arxiv.org/pdf/2309.12302v1
2309.12296v1,math-ph,A rigorous model reduction for the anisotropic-scattering transport process,2023-09-21 17:56:29+00:00,"In this letter, we propose a reduced-order model to bridge the particle
transport mechanics and the macroscopic fluid dynamics in the highly scattered
regime. A rigorous mathematical derivation and a concise physical
interpretation are presented for an anisotropic-scattering transport process
with arbitrary order of scattering kernel. The prediction of the theoretical
model perfectly agrees with the numerical experiments. A clear picture of the
diffusion physics is revealed for the neutral particle transport in the
asymptotic optically thick regime.",http://arxiv.org/pdf/2309.12296v1
2309.12294v1,cs.CL,Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models,2023-09-21 17:54:58+00:00,"Large language models (LLMs) have demonstrated impressive capabilities in
natural language generation. However, their output quality can be inconsistent,
posing challenges for generating natural language from logical forms (LFs).
This task requires the generated outputs to embody the exact semantics of LFs,
without missing any LF semantics or creating any hallucinations. In this work,
we tackle this issue by proposing a novel generate-and-rerank approach. Our
approach involves initially generating a set of candidate outputs by prompting
an LLM and subsequently reranking them using a task-specific reranker model. In
addition, we curate a manually collected dataset to evaluate the alignment
between different ranking metrics and human judgements. The chosen ranking
metrics are utilized to enhance the training and evaluation of the reranker
model. By conducting extensive experiments on three diverse datasets, we
demonstrate that the candidates selected by our reranker outperform those
selected by baseline methods in terms of semantic consistency and fluency, as
measured by three comprehensive metrics. Our findings provide strong evidence
for the effectiveness of our approach in improving the quality of generated
outputs.",http://arxiv.org/pdf/2309.12294v1
2309.12283v1,cs.SD,Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis,2023-09-21 17:44:57+00:00,"Generating multi-instrument music from symbolic music representations is an
important task in Music Information Retrieval (MIR). A central but still
largely unsolved problem in this context is musically and acoustically informed
control in the generation process. As the main contribution of this work, we
propose enhancing control of multi-instrument synthesis by conditioning a
generative model on a specific performance and recording environment, thus
allowing for better guidance of timbre and style. Building on state-of-the-art
diffusion-based music generative models, we introduce performance conditioning
- a simple tool indicating the generative model to synthesize music with style
and timbre of specific instruments taken from specific performances. Our
prototype is evaluated using uncurated performances with diverse
instrumentation and achieves state-of-the-art FAD realism scores while allowing
novel timbre and style control. Our project page, including samples and
demonstrations, is available at benadar293.github.io/midipm",http://arxiv.org/pdf/2309.12283v1
2309.12278v1,cs.CL,Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition,2023-09-21 17:39:53+00:00,"Large language models (LLMs) have demonstrated dominating performance in many
NLP tasks, especially on generative tasks. However, they often fall short in
some information extraction tasks, particularly those requiring domain-specific
knowledge, such as Biomedical Named Entity Recognition (NER). In this paper,
inspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER
step-by-step: break down the NER task into entity span extraction and entity
type determination. Additionally, for entity type determination, we inject
entity knowledge to address the problem that LLM's lack of domain knowledge
when predicting entity category. Experimental results show a significant
improvement in our two-step BioNER approach compared to previous few-shot LLM
baseline. Additionally, the incorporation of external knowledge significantly
enhances entity category determination performance.",http://arxiv.org/pdf/2309.12278v1
2309.12273v1,cs.CL,Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports,2023-09-21 17:29:37+00:00,"Rapid and accurate identification of Venous thromboembolism (VTE), a severe
cardiovascular condition including deep vein thrombosis (DVT) and pulmonary
embolism (PE), is important for effective treatment. Leveraging Natural
Language Processing (NLP) on radiology reports, automated methods have shown
promising advancements in identifying VTE events from retrospective data
cohorts or aiding clinical experts in identifying VTE events from radiology
reports. However, effectively training Deep Learning (DL) and the NLP models is
challenging due to limited labeled medical text data, the complexity and
heterogeneity of radiology reports, and data imbalance. This study proposes
novel method combinations of DL methods, along with data augmentation, adaptive
pre-trained NLP model selection, and a clinical expert NLP rule-based
classifier, to improve the accuracy of VTE identification in unstructured
(free-text) radiology reports. Our experimental results demonstrate the model's
efficacy, achieving an impressive 97\% accuracy and 97\% F1 score in predicting
DVT, and an outstanding 98.3\% accuracy and 98.4\% F1 score in predicting PE.
These findings emphasize the model's robustness and its potential to
significantly contribute to VTE research.",http://arxiv.org/pdf/2309.12273v1
2309.12269v1,cs.CL,The Cambridge Law Corpus: A Corpus for Legal AI Research,2023-09-21 17:24:40+00:00,"We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research.
It consists of over 250 000 court cases from the UK. Most cases are from the
21st century, but the corpus includes cases as old as the 16th century. This
paper presents the first release of the corpus, containing the raw text and
meta-data. Together with the corpus, we provide annotations on case outcomes
for 638 cases, done by legal experts. Using our annotated data, we have trained
and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to
provide benchmarks. We include an extensive legal and ethical discussion to
address the potentially sensitive nature of this material. As a consequence,
the corpus will only be released for research purposes under certain
restrictions.",http://arxiv.org/pdf/2309.12269v1
2309.12263v1,cs.CL,On the Relationship between Skill Neurons and Robustness in Prompt Tuning,2023-09-21 17:13:21+00:00,"Prompt Tuning is a popular parameter-efficient finetuning method for
pre-trained large language models (PLMs). Recently, based on experiments with
RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in
the transformer's feed-forward networks, that are highly predictive and
selective for the given task. In this paper, we study the robustness of Prompt
Tuning in relation to these ""skill neurons"", using RoBERTa and T5. We show that
prompts tuned for a specific task are transferable to tasks of the same type
but are not very robust to adversarial data, with higher robustness for T5 than
RoBERTa. At the same time, we replicate the existence of skill neurons in
RoBERTa and further show that skill neurons also seem to exist in T5.
Interestingly, the skill neurons of T5 determined on non-adversarial data are
also among the most predictive neurons on the adversarial data, which is not
the case for RoBERTa. We conclude that higher adversarial robustness may be
related to a model's ability to activate the relevant skill neurons on
adversarial data.",http://arxiv.org/pdf/2309.12263v1
2309.12260v1,math.MG,The dual Orlicz curvature measures for log-concave functions and their related Minkowski problems,2023-09-21 17:07:51+00:00,"The variation of a class of Orlicz moments with respect to the Asplund sum
within the class of log-concave functions is demonstrated. Such a variational
formula naturally leads to a family of dual Orlicz curvature measures for
log-concave functions. They are functional analogs of dual (Orlicz) curvature
measures for convex bodies. Partial existence results for the functional dual
Orlicz Minkowski problem are shown.",http://arxiv.org/pdf/2309.12260v1
2309.12259v1,cs.LG,Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance,2023-09-21 17:07:31+00:00,"Stochastic Gradient Descent (SGD), a widely used optimization algorithm in
deep learning, is often limited to converging to local optima due to the
non-convex nature of the problem. Leveraging these local optima to improve
model performance remains a challenging task. Given the inherent complexity of
neural networks, the simple arithmetic averaging of the obtained local optima
models in undesirable results. This paper proposes a {\em soft merging} method
that facilitates rapid merging of multiple models, simplifies the merging of
specific parts of neural networks, and enhances robustness against malicious
models with extreme values. This is achieved by learning gate parameters
through a surrogate of the $l_0$ norm using hard concrete distribution without
modifying the model weights of the given local optima models. This merging
process not only enhances the model performance by converging to a better local
optimum, but also minimizes computational costs, offering an efficient and
explicit learning process integrated with stochastic gradient descent. Thorough
experiments underscore the effectiveness and superior performance of the merged
neural networks.",http://arxiv.org/pdf/2309.12259v1
2309.12254v1,cs.ET,Variational Quantum Harmonizer: Generating Chord Progressions and Other Sonification Methods with the VQE Algorithm,2023-09-21 16:58:35+00:00,"This work investigates a case study of using physical-based sonification of
Quadratic Unconstrained Binary Optimization (QUBO) problems, optimized by the
Variational Quantum Eigensolver (VQE) algorithm. The VQE approximates the
solution of the problem by using an iterative loop between the quantum computer
and a classical optimization routine. This work explores the intermediary
statevectors found in each VQE iteration as the means of sonifying the
optimization process itself. The implementation was realised in the form of a
musical interface prototype named Variational Quantum Harmonizer (VQH),
providing potential design strategies for musical applications, focusing on
chords, chord progressions, and arpeggios. The VQH can be used both to enhance
data visualization or to create artistic pieces. The methodology is also
relevant in terms of how an artist would gain intuition towards achieving a
desired musical sound by carefully designing QUBO cost functions. Flexible
mapping strategies could supply a broad portfolio of sounds for QUBO and
quantum-inspired musical compositions, as demonstrated in a case study
composition, ""Dependent Origination"" by Peter Thomas and Paulo Itaborai.",http://arxiv.org/pdf/2309.12254v1
2309.12252v1,cs.LG,Parallelizing non-linear sequential models over the sequence length,2023-09-21 16:52:34+00:00,"Sequential models, such as Recurrent Neural Networks and Neural Ordinary
Differential Equations, have long suffered from slow training due to their
inherent sequential nature. For many years this bottleneck has persisted, as
many thought sequential models could not be parallelized. We challenge this
long-held belief with our parallel algorithm that accelerates GPU evaluation of
sequential models by up to 3 orders of magnitude faster without compromising
output accuracy. The algorithm does not need any special structure in the
sequential models' architecture, making it applicable to a wide range of
architectures. Using our method, training sequential models can be more than 10
times faster than the common sequential method without any meaningful
difference in the training results. Leveraging this accelerated training, we
discovered the efficacy of the Gated Recurrent Unit in a long time series
classification problem with 17k time samples. By overcoming the training
bottleneck, our work serves as the first step to unlock the potential of
non-linear sequential models for long sequence problems.",http://arxiv.org/pdf/2309.12252v1
2309.12250v1,cs.CL,SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References,2023-09-21 16:51:30+00:00,"Evaluation of QA systems is very challenging and expensive, with the most
reliable approach being human annotations of correctness of answers for
questions. Recent works (AVA, BEM) have shown that transformer LM encoder based
similarity metrics transfer well for QA evaluation, but they are limited by the
usage of a single correct reference answer. We propose a new evaluation metric:
SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference
answers (combining multiple correct and incorrect references) for sentence-form
QA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and
generative (GenQA) QA systems, across multiple academic and industrial
datasets, and show that it outperforms previous baselines and obtains the
highest correlation with human annotations.",http://arxiv.org/pdf/2309.12250v1
2309.12245v1,eess.IV,Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images,2023-09-21 16:43:29+00:00,"Biomedical image datasets can be imbalanced due to the rarity of targeted
diseases. Generative Adversarial Networks play a key role in addressing this
imbalance by enabling the generation of synthetic images to augment datasets.
It is important to generate synthetic images that incorporate a diverse range
of features to accurately represent the distribution of features present in the
training imagery. Furthermore, the absence of diverse features in synthetic
images can degrade the performance of machine learning classifiers. The mode
collapse problem impacts Generative Adversarial Networks' capacity to generate
diversified images. Mode collapse comes in two varieties: intra-class and
inter-class. In this paper, both varieties of the mode collapse problem are
investigated, and their subsequent impact on the diversity of synthetic X-ray
images is evaluated. This work contributes an empirical demonstration of the
benefits of integrating the adaptive input-image normalization with the Deep
Convolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapse
problems. Synthetically generated images are utilized for data augmentation and
training a Vision Transformer model. The classification performance of the
model is evaluated using accuracy, recall, and precision scores. Results
demonstrate that the DCGAN and the ACGAN with adaptive input-image
normalization outperform the DCGAN and ACGAN with un-normalized X-ray images as
evidenced by the superior diversity scores and classification scores.",http://arxiv.org/pdf/2309.12245v1
2309.12242v1,cs.SD,Weakly-supervised Automated Audio Captioning via text only training,2023-09-21 16:40:46+00:00,"In recent years, datasets of paired audio and captions have enabled
remarkable success in automatically generating descriptions for audio clips,
namely Automated Audio Captioning (AAC). However, it is labor-intensive and
time-consuming to collect a sufficient number of paired audio and captions.
Motivated by the recent advances in Contrastive Language-Audio Pretraining
(CLAP), we propose a weakly-supervised approach to train an AAC model assuming
only text data and a pre-trained CLAP model, alleviating the need for paired
target data. Our approach leverages the similarity between audio and text
embeddings in CLAP. During training, we learn to reconstruct the text from the
CLAP text embedding, and during inference, we decode using the audio
embeddings. To mitigate the modality gap between the audio and text embeddings
we employ strategies to bridge the gap during training and inference stages. We
evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its
ability to achieve a relative performance of up to ~$83\%$ compared to fully
supervised approaches trained with paired target data.",http://arxiv.org/pdf/2309.12242v1
2309.12240v1,physics.chem-ph,Unitary Coupled-Cluster for Quantum Computation of Molecular Properties in a Strong Magnetic Field,2023-09-21 16:34:58+00:00,"In truncated coupled-cluster (CC) theories, non-variational and/or generally
complex ground-state energies can occur. This is due to the non-Hermitian
nature of the similarity transformed Hamiltonian matrix in combination with CC
truncation. For chemical problems that deal with real-valued Hamiltonian
matrices, complex CC energies rarely occur. However, for complex-valued
Hamiltonian matrices, such as those that arise in the presence of strong
magnetic fields, complex CC energies can be regularly observed unless certain
symmetry conditions are fulfilled. Therefore, in the presence of magnetic
fields, it is desirable to pursue CC methods that are guaranteed to give
upper-bound, real-valued energies. In this work, we present the first
application of unitary CC (UCC) to chemical systems in a strong magnetic field.
This is achieved utilizing the variational quantum eigensolver (VQE) algorithm
applied to the unitary coupled-cluster singles and doubles (UCCSD) method. We
benchmark the method on the H$_2$ molecule in a strong magnetic field, and then
calculate UCCSD energies for the H$_4$ molecule as a function of both geometry
and field angle. We show that while standard CCSD can yield generally complex
energies that are not an upper-bound to the true energy, UCCSD always results
in variational and real-valued energies. We also show that the imaginary
components of the CCSD energy are largest in the strongly correlated region.
Lastly, the UCCSD calculations capture a large percentage of the correlation
energy.",http://arxiv.org/pdf/2309.12240v1
2309.12239v1,cs.DB,ContTune: Continuous Tuning by Conservative Bayesian Optimization for Distributed Stream Data Processing Systems,2023-09-21 16:34:18+00:00,"The past decade has seen rapid growth of distributed stream data processing
systems. Under these systems, a stream application is realized as a Directed
Acyclic Graph (DAG) of operators, where the level of parallelism of each
operator has a substantial impact on its overall performance. However, finding
optimal levels of parallelism remains challenging. Most existing methods are
heavily coupled with the topological graph of operators, unable to efficiently
tune under-provisioned jobs. They either insufficiently use previous tuning
experience by treating successively tuning independently, or explore the
configuration space aggressively, violating the Service Level Agreements (SLA).
  To address the above problems, we propose ContTune, a continuous tuning
system for stream applications. It is equipped with a novel Big-small
algorithm, in which the Big phase decouples the tuning from the topological
graph by decomposing the job tuning problem into sub-problems that can be
solved concurrently. We propose a conservative Bayesian Optimization (CBO)
technique in the Small phase to speed up the tuning process by utilizing the
previous observations. It leverages the state-of-the-art (SOTA) tuning method
as conservative exploration to avoid SLA violations. Experimental results show
that ContTune reduces up to 60.75% number of reconfigurations under synthetic
workloads and up to 57.5% number of reconfigurations under real workloads,
compared to the SOTA method DS2.",http://arxiv.org/pdf/2309.12239v1
2309.12234v1,cs.CL,Bridging the Gaps of Both Modality and Language: Synchronous Bilingual CTC for Speech Translation and Speech Recognition,2023-09-21 16:28:42+00:00,"In this study, we present synchronous bilingual Connectionist Temporal
Classification (CTC), an innovative framework that leverages dual CTC to bridge
the gaps of both modality and language in the speech translation (ST) task.
Utilizing transcript and translation as concurrent objectives for CTC, our
model bridges the gap between audio and text as well as between source and
target languages. Building upon the recent advances in CTC application, we
develop an enhanced variant, BiL-CTC+, that establishes new state-of-the-art
performances on the MuST-C ST benchmarks under resource-constrained scenarios.
Intriguingly, our method also yields significant improvements in speech
recognition performance, revealing the effect of cross-lingual learning on
transcription and demonstrating its broad applicability. The source code is
available at https://github.com/xuchennlp/S2T.",http://arxiv.org/pdf/2309.12234v1
2309.12225v1,cond-mat.soft,"Conformations, correlations, and instabilities of a flexible fiber in an active fluid",2023-09-21 16:21:34+00:00,"Fluid-structure interactions between active and passive components are
important for many biological systems to function. A particular example is
chromatin in the cell nucleus, where ATP-powered processes drive coherent
motions of the chromatin fiber over micron lengths. Motivated by this system,
we develop a multiscale model of a long flexible polymer immersed in a
suspension of active force dipoles as an analog to a chromatin fiber in an
active fluid -- the nucleoplasm. Linear analysis identifies an orientational
instability driven by hydrodynamic and alignment interactions between the fiber
and the suspension, and numerical simulations show activity can drive coherent
motions and structured conformations. These results demonstrate how active and
passive components, connected through fluid-structure interactions, can
generate coherent structures and self-organize on large scales.",http://arxiv.org/pdf/2309.12225v1
2309.12224v1,cs.CL,Towards Answering Health-related Questions from Medical Videos: Datasets and Approaches,2023-09-21 16:21:28+00:00,"The increase in the availability of online videos has transformed the way we
access information and knowledge. A growing number of individuals now prefer
instructional videos as they offer a series of step-by-step procedures to
accomplish particular tasks. The instructional videos from the medical domain
may provide the best possible visual answers to first aid, medical emergency,
and medical education questions. Toward this, this paper is focused on
answering health-related questions asked by the public by providing visual
answers from medical videos. The scarcity of large-scale datasets in the
medical domain is a key challenge that hinders the development of applications
that can help the public with their health-related questions. To address this
issue, we first proposed a pipelined approach to create two large-scale
datasets: HealthVidQA-CRF and HealthVidQA-Prompt. Later, we proposed monomodal
and multimodal approaches that can effectively provide visual answers from
medical videos to natural language questions. We conducted a comprehensive
analysis of the results, focusing on the impact of the created datasets on
model training and the significance of visual features in enhancing the
performance of the monomodal and multi-modal approaches. Our findings suggest
that these datasets have the potential to enhance the performance of medical
visual answer localization tasks and provide a promising future direction to
further enhance the performance by using pre-trained language-vision models.",http://arxiv.org/pdf/2309.12224v1
2309.12220v1,cs.CR,De-authentication using Ambient Light Sensor,2023-09-21 16:18:51+00:00,"While user authentication happens before initiating or resuming a login
session, de-authentication detects the absence of a previously-authenticated
user to revoke her currently active login session. The absence of proper
de-authentication can lead to well-known lunchtime attacks, where a nearby
adversary takes over a carelessly departed user's running login session. The
existing solutions for automatic de-authentication have distinct practical
limitations, e.g., extraordinary deployment requirements or high initial cost
of external equipment.
  In this paper, we propose ""DE-authentication using Ambient Light sensor""
(DEAL), a novel, inexpensive, fast, and user-friendly de-authentication
approach. DEAL utilizes the built-in ambient light sensor of a modern computer
to determine if the user is leaving her work-desk. DEAL, by design, is
resilient to natural shifts in lighting conditions and can be configured to
handle abrupt changes in ambient illumination (e.g., due to toggling of room
lights). We collected data samples from 4800 sessions with 120 volunteers in 4
typical workplace settings and conducted a series of experiments to evaluate
the quality of our proposed approach thoroughly. Our results show that DEAL can
de-authenticate a departing user within 4 seconds with a hit rate of 89.15% and
a fall-out of 7.35%. Finally, bypassing DEAL to launch a lunchtime attack is
practically infeasible as it requires the attacker to either take the user's
position within a few seconds or manipulate the sensor readings sophisticatedly
in real-time.",http://arxiv.org/pdf/2309.12220v1
2309.12216v1,quant-ph,Coherent anharmonicity transfer from matter to light in the THz regime,2023-09-21 16:16:40+00:00,"Optical nonlinearities are fundamental in several types of optical
information processing protocols. However, the high laser intensities needed
for implementing phase nonlinearities using conventional optical materials
represent a challenge for nonlinear optics in the few-photon regime. We
introduce an infrared cavity quantum electrodynamics (QED) approach for
imprinting nonlinear phase shifts on individual THz pulses in reflection
setups, conditional on the input power. Power-dependent phase shifts on the
order of $ 0.1\, \pi$ can be achieved with femtosecond pulses of only a few
$\mu$W input power. The proposed scheme involves a small number of intersubband
quantum well transition dipoles evanescently coupled to the near field of an
infrared resonator. The field evolution is nonlinear due to the dynamical
transfer of spectral anharmonicity from material dipoles to the infrared
vacuum, through an effective dipolar chirping mechanism that transiently
detunes the quantum well transitions from the vacuum field, leading to photon
blockade. We develop analytical theory that describes the dependence of the
imprinted nonlinear phase shift on relevant physical parameters. For a pair of
quantum well dipoles, the phase control scheme is shown to be robust with
respect to inhomogeneities in the dipole transition frequencies and relaxation
rates. Numerical results based on the Lindblad quantum master equation validate
the theory in the regime where the material dipoles are populated up to the
second excitation manifold. In contrast with conventional QED schemes for phase
control that require strong light-matter interaction, the proposed phase
nonlinearity works best in weak coupling, increasing the prospects for its
experimental realization using current nanophotonic technology.",http://arxiv.org/pdf/2309.12216v1
2309.12195v1,q-bio.NC,Mental imagery timing affects hybrid BCI control of robotic arms,2023-09-21 16:02:00+00:00,"Mental imagery-based brain-computer interfaces (BCIs) allow to interact with
the external environment by naturally bypassing the musculoskeletal system.
Making BCIs efficient and accurate is paramount to improve the reliability of
real-life and clinical applications, from open-loop device control to
closed-loop neurorehabilitation. By promoting sense of agency and embodiment,
realistic setups including multimodal channels of communication, such as
eye-gaze, and robotic prostheses aim to improve BCI performance. However, how
the mental imagery command should be integrated in those hybrid systems so as
to ensure the best interaction is still poorly understood.To address this
question, we performed a hybrid EEG-based BCI experiment involving healthy
volunteers enrolled in a reach-and-grasp action operated by a robotic arm. Main
results showed that the mental imagery timing significantly affects the BCI
accuracy as well as the spatiotemporal brain dynamics. Better performance was
obtained when motor imagery is performed just after the reaching phase, as
compared to before or during the robot action. This condition favored
intentional binding -- a key embodiment component -- and was characterized by
stronger motor-related brain activity and functional connectivity in the
sensorimotor areas.Taken together, these findings provided fresh evidence about
the effects of motor imagery timing on human behavior and cortical network
dynamics that can be exploited to design a new generation of efficient
brain-machine interfaces.",http://arxiv.org/pdf/2309.12195v1
2309.12194v1,cs.HC,Empowering People with Intellectual and Developmental Disabilities through Cognitively Accessible Visualizations,2023-09-21 16:01:32+00:00,"Data has transformative potential to empower people with Intellectual and
Developmental Disabilities (IDD). However, conventional data visualizations
often rely on complex cognitive processes, and existing approaches for
day-to-day analysis scenarios fail to consider neurodivergent capabilities,
creating barriers for people with IDD to access data and leading to even
further marginalization. We argue that visualizations could be an equalizer for
people with IDD to participate in data-driven conversations. Drawing on
preliminary research findings and our experiences working with people with IDD
and their data, we introduce and expand on the concept of cognitively
accessible visualizations, unpack its meaning and roles in increasing IDD
individuals' access to data, and discuss two immediate research objectives.
Specifically, we argue that cognitively accessible visualizations should
support people with IDD in personal data storytelling for effective
self-advocacy and self-expression, and balance novelty and familiarity in data
design to accommodate cognitive diversity and promote inclusivity.",http://arxiv.org/pdf/2309.12194v1
2309.12193v1,eess.IV,Brain Tumor Detection Using Deep Learning Approaches,2023-09-21 15:59:06+00:00,"Brain tumors are collections of abnormal cells that can develop into masses
or clusters. Because they have the potential to infiltrate other tissues, they
pose a risk to the patient. The main imaging technique used, MRI, may be able
to identify a brain tumor with accuracy. The fast development of Deep Learning
methods for use in computer vision applications has been facilitated by a vast
amount of training data and improvements in model construction that offer
better approximations in a supervised setting. The need for these approaches
has been the main driver of this expansion. Deep learning methods have shown
promise in improving the precision of brain tumor detection and classification
using magnetic resonance imaging (MRI). The study on the use of deep learning
techniques, especially ResNet50, for brain tumor identification is presented in
this abstract. As a result, this study investigates the possibility of
automating the detection procedure using deep learning techniques. In this
study, I utilized five transfer learning models which are VGG16, VGG19,
DenseNet121, ResNet50 and YOLO V4 where ResNet50 provide the best or highest
accuracy 99.54%. The goal of the study is to guide researchers and medical
professionals toward powerful brain tumor detecting systems by employing deep
learning approaches by way of this evaluation and analysis.",http://arxiv.org/pdf/2309.12193v1
2309.12186v1,physics.ins-det,Integrable Magnetic Fluid Hyperthermia Systems for 3D Magnetic Particle Imaging,2023-09-21 15:51:52+00:00,"Background: Combining magnetic particle imaging (MPI) and magnetic fluid
hyperthermia (MFH) offers the ability to perform localized hyperthermia and
magnetic particle imaging-assisted ther-mometry of hyperthermia treatment. This
allows precise regional selective heating inside the body without invasive
interventions. In current MPI-MFH platforms, separate systems are used, which
require object transfer from one system to another. Here, we present the
design, development and evaluation process for integrable MFH platforms, which
extends a commercial MPI scanner with the functionality of MFH. Methods: The
biggest issue of integrating magnetic fluid hyperthermia platforms into a
magnetic par-ticle imaging system is the magnetic coupling of the devices,
which induces high voltage in the imaging system, and is harming its
components. In this paper we use a self-compensation approach derived from
heuristic algorithms to protect the magnetic particle imaging scanner. The
integrable platforms are evaluated regarding electrical and magnetic
characteristics, cooling capability, field strength, the magnetic coupling to a
replica of the magnetic particle imaging system's main solenoid and particle
heating. Results: The MFH platforms generate suitable magnetic fields for
magnetic heating of particles and are compatible with a commercial magnetic
particle imaging scanner. In combination with the imaging system, selective
heating with a gradient field and steerable heating positioning using the MPI
focus fields are possible. Conclusion: The proposed MFH platforms serve as a
therapeutic tool to unlock MFH functionality of a commercial magnetic particle
imaging scanner, enabling its use in future preclinical trials of MPI-guided,
spatially selective magnetic hyperthermia therapy.",http://arxiv.org/pdf/2309.12186v1
2309.12182v1,quant-ph,Hungarian Qubit Assignment for Optimized Mapping of Quantum Circuits on Multi-Core Architectures,2023-09-21 15:48:45+00:00,"Modular quantum computing architectures offer a promising alternative to
monolithic designs for overcoming the scaling limitations of current quantum
computers. To achieve scalability beyond small prototypes, quantum
architectures are expected to adopt a modular approach, featuring clusters of
tightly connected quantum bits with sparser connections between these clusters.
Efficiently distributing qubits across multiple processing cores is critical
for improving quantum computing systems' performance and scalability. To
address this challenge, we propose the Hungarian Qubit Assignment (HQA)
algorithm, which leverages the Hungarian algorithm to improve qubit-to-core
assignment. The HQA algorithm considers the interactions between qubits over
the entire circuit, enabling fine-grained partitioning and enhanced qubit
utilization. We compare the HQA algorithm with state-of-the-art alternatives
through comprehensive experiments using both real-world quantum algorithms and
random quantum circuits. The results demonstrate the superiority of our
proposed approach, outperforming existing methods, with an average improvement
of 1.28$\times$.",http://arxiv.org/pdf/2309.12182v1
2309.12179v1,cs.CV,Autoregressive Sign Language Production: A Gloss-Free Approach with Discrete Representations,2023-09-21 15:46:01+00:00,"Gloss-free Sign Language Production (SLP) offers a direct translation of
spoken language sentences into sign language, bypassing the need for gloss
intermediaries. This paper presents the Sign language Vector Quantization
Network, a novel approach to SLP that leverages Vector Quantization to derive
discrete representations from sign pose sequences. Our method, rooted in both
manual and non-manual elements of signing, supports advanced decoding methods
and integrates latent-level alignment for enhanced linguistic coherence.
Through comprehensive evaluations, we demonstrate superior performance of our
method over prior SLP methods and highlight the reliability of Back-Translation
and Fr\'echet Gesture Distance as evaluation metrics.",http://arxiv.org/pdf/2309.12179v1
2309.12175v1,gr-qc,Dynamics of redshift/blueshift during free fall under the Schwarzschild horizon,2023-09-21 15:33:18+00:00,"We consider a free-falling observer who crosses the event horizon in the
Schwarzschild background. In the course of this fall, he/she can receive
signals from an object (like a star surface) that emits radiation. We study how
the frequency received by an observer changes depending on the proper time on
his/her trajectory. The scenarios are classified depending on whether the
frequency is infinite, finite or zero near the singularity and the horizon.
This depends crucially on the angular momenta of an observer and a photon. In
this work we consider also emission process, and, as we show, conditions of
emission strongly influence parameters of a photon, and thus received
frequency. As one of our main results, we present numerical calculations
showing evolution of the received frequency during the process of diving into a
black hole, depending on parameters of an observer and emitter. We also analyze
how a falling observer will see a night sky as he/she approaches the
singularity. We show that there appear several blind zones, which were not
analyzed previously.",http://arxiv.org/pdf/2309.12175v1
2309.12172v1,cs.CV,"SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset",2023-09-21 15:28:04+00:00,"We introduce SANPO, a large-scale egocentric video dataset focused on dense
prediction in outdoor environments. It contains stereo video sessions collected
across diverse outdoor environments, as well as rendered synthetic video
sessions. (Synthetic data was provided by Parallel Domain.) All sessions have
(dense) depth and odometry labels. All synthetic sessions and a subset of real
sessions have temporally consistent dense panoptic segmentation labels. To our
knowledge, this is the first human egocentric video dataset with both large
scale dense panoptic segmentation and depth annotations. In addition to the
dataset we also provide zero-shot baselines and SANPO benchmarks for future
research. We hope that the challenging nature of SANPO will help advance the
state-of-the-art in video segmentation, depth estimation, multi-task visual
modeling, and synthetic-to-real domain adaptation, while enabling human
navigation systems.
  SANPO is available here:
https://google-research-datasets.github.io/sanpo_dataset/",http://arxiv.org/pdf/2309.12172v1
2309.12168v1,cs.HC,This is the Table I Want! Interactive Data Transformation on Desktop and in Virtual Reality,2023-09-21 15:25:46+00:00,"Data transformation is an essential step in data science. While experts
primarily use programming to transform their data, there is an increasing need
to support non-programmers with user interface-based tools. With the rapid
development in interaction techniques and computing environments, we report our
empirical findings about the effects of interaction techniques and environments
on performing data transformation tasks. Specifically, we studied the potential
benefits of direct interaction and virtual reality (VR) for data
transformation. We compared gesture interaction versus a standard WIMP user
interface, each on the desktop and in VR. With the tested data and tasks, we
found time performance was similar between desktop and VR. Meanwhile, VR
demonstrates preliminary evidence to better support provenance and sense-making
throughout the data transformation process. Our exploration of performing data
transformation in VR also provides initial affirmation for enabling an
iterative and fully immersive data science workflow.",http://arxiv.org/pdf/2309.12168v1
2309.12164v1,cs.PL,Stratified Type Theory,2023-09-21 15:22:04+00:00,"To exploit the expressivity of being able to refer to the type of types, such
as for large elimination, dependent type systems will either employ a universe
hierarchy or else contend with an inconsistent type-in-type rule. However,
these are not be the only possible options. Taking inspiration from Stratified
System F, we introduce Stratified Type Theory (StraTT), where rather than
stratifying universes by levels, we stratify typing judgements and restrict the
domain of dependent function types to some fixed level strictly lower than that
of the overall type. Even in the presence of type-in-type, this restriction
suffices to enforce consistency of the system.
  We explore the expressivity of several extensions atop this design. First,
the subsystem subStraTT employs McBride's crude-but-effective stratification
(also known as displacement) as a simple form of level polymorphism where
top-level definitions can be displaced uniformly to any higher level as needed,
which is valid due to level cumulativity and plays well with stratified
judgements. Second, to recover some expressivity lost due to the restriction on
dependent function domains, the full StraTT system includes a separate
nondependent function type with floating domains, whose level instead matches
that of the overall type. Finally, we have implemented a prototype type checker
for StraTT extended with datatypes along with a small type checked core
library.
  While it's possible to show that the subsystem is consistent, showing
consistency for the full system with floating nondependent functions remains
open. Nevertheless, we believe that the full system is also consistent and have
mechanized a syntactic proof of subject reduction. Furthermore, we use our
implementation to investigate various well-known type-theoretic type-in-type
paradoxes. These examples all fail to type check in expected ways as evidence
towards consistency.",http://arxiv.org/pdf/2309.12164v1
2309.12160v1,eess.SY,Flow separation control design with experimental validation,2023-09-21 15:14:36+00:00,"Flow control aims at modifying a natural flow state to reach an other flow
state considered as advantageous. In this paper, active feedback flow
separation control is investigated with two different closed-loop control
strategies, involving a reference signal tracking architecture. Firstly, a
data-driven control law, leading to a linear (integral) controller is employed.
Secondly, a phenomenological/model-driven approach, leading to a non-linear
positive (integral) control strategy is investigated. While the former benefits
of a tuning simplicity, the latter prevents undesirable effects and formally
guarantees closed-loop stability. Both control approaches were validated
through wind tunnel experiments of flow separation over a movable NACA 4412
plain flap. These control laws were designed with respect to hot film
measurements, performed over the flap for different deflection angles. Both
control approaches proved efficient in avoiding flow separation. The main
contribution of this work is to provide practitioners simple but yet efficient
ways to design a flow separation controller. In addition, a complete validation
campaign data-set is provided.",http://arxiv.org/pdf/2309.12160v1
2309.12159v1,cs.CR,Information Forensics and Security: A quarter-century-long journey,2023-09-21 15:13:35+00:00,"Information Forensics and Security (IFS) is an active R&D area whose goal is
to ensure that people use devices, data, and intellectual properties for
authorized purposes and to facilitate the gathering of solid evidence to hold
perpetrators accountable. For over a quarter century since the 1990s, the IFS
research area has grown tremendously to address the societal needs of the
digital information era. The IEEE Signal Processing Society (SPS) has emerged
as an important hub and leader in this area, and the article below celebrates
some landmark technical contributions. In particular, we highlight the major
technological advances on some selected focus areas in the field developed in
the last 25 years from the research community and present future trends.",http://arxiv.org/pdf/2309.12159v1
2309.12158v1,cs.SD,Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval,2023-09-21 15:11:16+00:00,"A range of applications of multi-modal music information retrieval is centred
around the problem of connecting large collections of sheet music (images) to
corresponding audio recordings, that is, identifying pairs of audio and score
excerpts that refer to the same musical content. One of the typical and most
recent approaches to this task employs cross-modal deep learning architectures
to learn joint embedding spaces that link the two distinct modalities - audio
and sheet music images. While there has been steady improvement on this front
over the past years, a number of open problems still prevent large-scale
employment of this methodology. In this article we attempt to provide an
insightful examination of the current developments on audio-sheet music
retrieval via deep learning methods. We first identify a set of main challenges
on the road towards robust and large-scale cross-modal music retrieval in real
scenarios. We then highlight the steps we have taken so far to address some of
these challenges, documenting step-by-step improvement along several
dimensions. We conclude by analysing the remaining challenges and present ideas
for solving these, in order to pave the way to a unified and robust methodology
for cross-modal music retrieval.",http://arxiv.org/pdf/2309.12158v1
2309.12157v1,astro-ph.HE,Variability studies of active galactic nuclei from the long-term monitoring program with the Cherenkov Telescope Array,2023-09-21 15:11:15+00:00,"Blazars are active galactic nuclei (AGN) with a relativistic jet oriented
toward the observer. This jet is composed of accelerated particles which can
display emission over the entire electromagnetic spectrum. Spectral variability
has been observed on short- and long-time scales in AGN, with a power spectral
density (PSD) that can show a break at frequencies below the well-known
red-noise process. This break frequency in the PSD has been observed in X-rays
to scale with the accretion regime and the mass of the central black hole. It
is expected that a break could also be seen in the very-high-energy gamma rays,
but constraining the shape of the PSD in these wavelengths has not been
possible with the current instruments. The Cherenkov Telescope Array (CTA) will
be more sensitive by a factor of five to ten depending on energy than the
current generation of imaging atmospheric Cherenkov telescopes, therefore it
will be possible with CTA to reconstruct the PSD with a high accuracy, bringing
new information about AGN variability. In this work, we focus on the AGN
long-term monitoring program planned with CTA. The program is proposed to begin
with early-start observing campaigns with CTA precursors. This would allow us
to probe longer time scales on the AGN PSD.",http://arxiv.org/pdf/2309.12157v1
2309.12154v1,math.PR,Aldous-type spectral gap results for the complete monomial group,2023-09-21 15:08:22+00:00,"Let us consider the continuous-time random walk on $G\wr S_n$, the complete
monomial group of degree $n$ over a finite group $G$, as follows: An element in
$G\wr S_n$ can be multiplied (left or right) by an element of the form
\begin{itemize}
  \item $(u,v)_G:=(\mathbf{e},\dots,\mathbf{e};(u,v))$ with rate $x_{u,v}(\geq
0)$, or
  \item
$(g)^{(w)}:=(\dots,\mathbf{e},\hspace*{-0.65cm}\underset{\substack{\uparrow\\w\text{th
position}}}{g}\hspace*{-0.65cm},\mathbf{e},\dots;\mathbf{id})$ with rate
$y_w\alpha_g\; (y_w> 0,\;\alpha_g=\alpha_{g^{-1}}\geq 0)$, \end{itemize} such
that $\{(u,v)_G,\;(g)^{(w)}:x_{u,v}>0,\;y_w\alpha_g>0,\;1\leq u<v\leq n,\;g\in
G,\;1\leq w\leq n\}$ generates $G\wr S_n$. We also consider the continuous-time
random walk on $G\times\{1,\dots,n\}$ generated by one natural action of the
elements $(u,v)_G,1\leq u<v\leq n$ and $(g)^{(w)},\;g\in G,1\leq w\leq n$ on
$G\times\{1,\dots,n\}$ with the aforementioned rates. We show that the spectral
gaps of the two random walks are the same. This is an analogue of the Aldous'
spectral gap conjecture for the complete monomial group of degree $n$ over a
finite group $G$.",http://arxiv.org/pdf/2309.12154v1
2309.12151v1,cs.LO,Semantics for a Turing-complete Reversible Programming Language with Inductive Types,2023-09-21 15:05:52+00:00,"This paper is concerned with the expressivity and denotational semantics of a
functional higher-order reversible programming language based on Theseus. In
this language, pattern-matching is used to ensure the reversibility of
functions. We show how one can encode any Reversible Turing Machine in said
language. We then build a sound and adequate categorical semantics based on
join inverse categories, with additional structures to capture
pattern-matching. We then derive a full completeness result, stating that any
computable, partial injective function is the image of a term in the language.",http://arxiv.org/pdf/2309.12151v1
2309.12142v1,q-bio.OT,Characterizing Pulmonary Fibrosis Patterns in Post-COVID-19 Patients through Machine Learning Algorithms,2023-09-21 15:03:01+00:00,"The COVID-19 pandemic has left a lasting impact on global healthcare systems,
with increasing evidence of pulmonary fibrosis emerging as a post-infection
complication. This study presents a comprehensive analysis of pulmonary
fibrosis patterns in post-COVID-19 patients from South and Central Iraq,
employing advanced machine learning algorithms. Data were collected from 390
patients, and their medical records were systematically analyzed. Our findings
reveal distinct patterns of pulmonary fibrosis in this cohort, shedding light
on the heterogeneous nature of post-COVID-19 lung complications. Machine
learning models demonstrated robust predictive capabilities, offering valuable
insights into the characterization of fibrotic changes. The identification of
specific patterns contributes to early diagnosis and personalized treatment
strategies for affected individuals. This research underscores the importance
of data-driven approaches in understanding post-COVID-19 complications,
particularly in regions with unique demographic and healthcare characteristics.
It emphasizes the potential for machine learning to enhance clinical
decision-making and improve patient care in the aftermath of the pandemic.
Further investigations are warranted to validate these findings and explore
additional factors influencing pulmonary fibrosis in post-COVID-19 patients.",http://arxiv.org/pdf/2309.12142v1
2309.12141v1,physics.soc-ph,Guided rewiring of social networks reduces polarization and accelerates collective action,2023-09-21 15:02:24+00:00,"Global challenges like climate change may be considered as collective action
problems that require sufficient cooperation with pro-mitigation norms, soon
enough to be effective. Socio-political polarization is a barrier to collective
action. Prior agent-based models of behavioural change on structured networks
in a shared socio-political environment have shown that polarization emerges
naturally in such systems and that the speed of consensus formation is limited
by the rate at which polarized clusters can be dissolved. Here we study how
guided social link rewiring affects the speed of network depolarization. We
investigate rewiring algorithms representing random meetings, introduction by
mutual acquaintances, and bridging between socially distant communities. We
find that building lasting links between polarized individuals and communities
can accelerate consensus formation when the sociopolitical environment is
favourable. This strengthens the evidence that promoting connection between
polarized communities could accelerate collective action on urgent global
challenges.",http://arxiv.org/pdf/2309.12141v1
2309.12135v1,cond-mat.mtrl-sci,A fast approximate method for variable-width broadening of spectra,2023-09-21 14:55:02+00:00,"Spectral data is routinely broadened in order to improve appearance,
approximate a higher sampling level or model experimental measurement effects.
While there has been extensive work in the signal processing field to develop
efficient methods for the application of fixed-width broadening functions,
these are not suitable for all scientific applications -- for example, the
instrumental resolution of inelastic neutron scattering measurements varies
along the energy-transfer axis. Na\""ive application of a kernel to every point
has $O(N \times M)$ complexity and scales poorly for a high-resolution spectrum
over many data points. Here we present an approximate method with complexity
$O(N + W\times M \log M)$, where $W$ scales with the range of required
broadening widths; in practice the number and cost of mathematical operations
is drastically reduced to $N$ polynomial evaluations and a modest number of
discrete Fourier transforms. Applications are demonstrated for Gaussian
interpolation of density-of-states data and to instrumental resolution
functions. We anticipate that these performance improvements will assist
application of resolution functions inside fitting procedures and interactive
tools.",http://arxiv.org/pdf/2309.12135v1
2309.12134v1,cs.SD,Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems,2023-09-21 14:54:48+00:00,"Linking sheet music images to audio recordings remains a key problem for the
development of efficient cross-modal music retrieval systems. One of the
fundamental approaches toward this task is to learn a cross-modal embedding
space via deep neural networks that is able to connect short snippets of audio
and sheet music. However, the scarcity of annotated data from real musical
content affects the capability of such methods to generalize to real retrieval
scenarios. In this work, we investigate whether we can mitigate this limitation
with self-supervised contrastive learning, by exposing a network to a large
amount of real music data as a pre-training step, by contrasting randomly
augmented views of snippets of both modalities, namely audio and sheet images.
Through a number of experiments on synthetic and real piano data, we show that
pre-trained models are able to retrieve snippets with better precision in all
scenarios and pre-training configurations. Encouraged by these results, we
employ the snippet embeddings in the higher-level task of cross-modal piece
identification and conduct more experiments on several retrieval
configurations. In this task, we observe that the retrieval quality improves
from 30% up to 100% when real music data is present. We then conclude by
arguing for the potential of self-supervised contrastive learning for
alleviating the annotated data scarcity in multi-modal music retrieval models.",http://arxiv.org/pdf/2309.12134v1
2309.12129v1,quant-ph,Leveraging Analog Quantum Computing with Neutral Atoms for Solvent Configuration Prediction in Drug Discovery,2023-09-21 14:50:45+00:00,"We introduce quantum algorithms able to sample equilibrium water solvent
molecules configurations within proteins thanks to analog quantum computing. To
do so, we combine a quantum placement strategy to the 3D Reference Interaction
Site Model (3D-RISM), an approach capable of predicting continuous solvent
distributions. The intrinsic quantum nature of such coupling guarantees
molecules not to be placed too close to each other, a constraint usually
imposed by hand in classical approaches. We present first a full quantum
adiabatic evolution model that uses a local Rydberg Hamiltonian to cast the
general problem into an anti-ferromagnetic Ising model. Its solution, an
NP-hard problem in classical computing, is embodied into a Rydberg atom array
Quantum Processing Unit (QPU). Following a classical emulator implementation, a
QPU portage allows to experimentally validate the algorithm performances on an
actual quantum computer. As a perspective of use on next generation devices, we
emulate a second hybrid quantum-classical version of the algorithm. Such a
variational quantum approach (VQA) uses a classical Bayesian minimization
routine to find the optimal laser parameters. Overall, these Quantum-3D-RISM
(Q-3D-RISM) algorithms open a new route towards the application of analog
quantum computing in molecular modelling and drug design.",http://arxiv.org/pdf/2309.12129v1
2309.12121v1,eess.AS,A Multiscale Autoencoder (MSAE) Framework for End-to-End Neural Network Speech Enhancement,2023-09-21 14:41:54+00:00,"Neural network approaches to single-channel speech enhancement have received
much recent attention. In particular, mask-based architectures have achieved
significant performance improvements over conventional methods. This paper
proposes a multiscale autoencoder (MSAE) for mask-based end-to-end neural
network speech enhancement. The MSAE performs spectral decomposition of an
input waveform within separate band-limited branches, each operating with a
different rate and scale, to extract a sequence of multiscale embeddings. The
proposed framework features intuitive parameterization of the autoencoder,
including a flexible spectral band design based on the Constant-Q transform.
Additionally, the MSAE is constructed entirely of differentiable operators,
allowing it to be implemented within an end-to-end neural network, and be
discriminatively trained. The MSAE draws motivation both from recent multiscale
network topologies and from traditional multiresolution transforms in speech
processing. Experimental results show the MSAE to provide clear performance
benefits relative to conventional single-branch autoencoders. Additionally, the
proposed framework is shown to outperform a variety of state-of-the-art
enhancement systems, both in terms of objective speech quality metrics and
automatic speech recognition accuracy.",http://arxiv.org/pdf/2309.12121v1
2309.12119v1,stat.ME,Pseudo-Bayesian unit level modeling for small area estimation under informative sampling,2023-09-21 14:39:20+00:00,"When mapping subnational health and demographic indicators, direct weighted
estimators of small area means based on household survey data can be unreliable
when data are limited. If survey microdata are available, unit level models can
relate individual survey responses to unit level auxiliary covariates and
explicitly account for spatial dependence and between area variation using
random effects. These models can produce estimators with improved precision,
but often neglect to account for the design of the surveys used to collect
data. Pseudo-Bayesian approaches incorporate sampling weights to address
informative sampling when using such models to conduct population inference but
credible sets based on the resulting pseudo-posterior distributions can be
poorly calibrated without adjustment. We outline a pseudo-Bayesian strategy for
small area estimation that addresses informative sampling and incorporates a
post-processing rescaling step that produces credible sets with close to
nominal empirical frequentist coverage rates. We compare our approach with
existing design-based and model-based estimators using real and simulated data.",http://arxiv.org/pdf/2309.12119v1
2309.12117v1,cs.CL,How-to Guides for Specific Audiences: A Corpus and Initial Findings,2023-09-21 14:35:42+00:00,"Instructional texts for specific target groups should ideally take into
account the prior knowledge and needs of the readers in order to guide them
efficiently to their desired goals. However, targeting specific groups also
carries the risk of reflecting disparate social norms and subtle stereotypes.
In this paper, we investigate the extent to which how-to guides from one
particular platform, wikiHow, differ in practice depending on the intended
audience. We conduct two case studies in which we examine qualitative features
of texts written for specific audiences. In a generalization study, we
investigate which differences can also be systematically demonstrated using
computational methods. The results of our studies show that guides from
wikiHow, like other text genres, are subject to subtle biases. We aim to raise
awareness of these inequalities as a first step to addressing them in future
work.",http://arxiv.org/pdf/2309.12117v1
2309.12115v1,cs.HC,MeetScript: Designing Transcript-based Interactions to Support Active Participation in Group Video Meetings,2023-09-21 14:35:34+00:00,"While videoconferencing is prevalent, concurrent participation channels are
limited. People experience challenges keeping up with the discussion, and
misunderstanding frequently occurs. Through a formative study, we probed into
the design space of providing real-time transcripts as an extra communication
space for video meeting attendees. We then present MeetScript, a system that
provides parallel participation channels through real-time interactive
transcripts. MeetScript visualizes the discussion through a chat-alike
interface and allows meeting attendees to make real-time collaborative
annotations. Over time, MeetScript gradually hides extraneous content to retain
the most essential information on the transcript, with the goal of reducing the
cognitive load required on users to process the information in real time. In an
experiment with 80 users in 22 teams, we compared MeetScript with two baseline
conditions where participants used Zoom alone (business-as-usual), or Zoom with
an adds-on transcription service (Otter.ai). We found that MeetScript
significantly enhanced people's non-verbal participation and recollection of
their teams' decision-making processes compared to the baselines. Users liked
that MeetScript allowed them to easily navigate the transcript and
contextualize feedback and new ideas with existing ones.",http://arxiv.org/pdf/2309.12115v1
2309.12114v1,eess.IV,AutoPET Challenge 2023: Sliding Window-based Optimization of U-Net,2023-09-21 14:34:17+00:00,"Tumor segmentation in medical imaging is crucial and relies on precise
delineation. Fluorodeoxyglucose Positron-Emission Tomography (FDG-PET) is
widely used in clinical practice to detect metabolically active tumors.
However, FDG-PET scans may misinterpret irregular glucose consumption in
healthy or benign tissues as cancer. Combining PET with Computed Tomography
(CT) can enhance tumor segmentation by integrating metabolic and anatomic
information. FDG-PET/CT scans are pivotal for cancer staging and reassessment,
utilizing radiolabeled fluorodeoxyglucose to highlight metabolically active
regions. Accurately distinguishing tumor-specific uptake from physiological
uptake in normal tissues is a challenging aspect of precise tumor segmentation.
The AutoPET challenge addresses this by providing a dataset of 1014 FDG-PET/CT
studies, encouraging advancements in accurate tumor segmentation and analysis
within the FDG-PET/CT domain. Code:
https://github.com/matt3o/AutoPET2-Submission/",http://arxiv.org/pdf/2309.12114v1
2309.12111v1,cs.SD,Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval,2023-09-21 14:30:02+00:00,"Many applications of cross-modal music retrieval are related to connecting
sheet music images to audio recordings. A typical and recent approach to this
is to learn, via deep neural networks, a joint embedding space that correlates
short fixed-size snippets of audio and sheet music by means of an appropriate
similarity structure. However, two challenges that arise out of this strategy
are the requirement of strongly aligned data to train the networks, and the
inherent discrepancies of musical content between audio and sheet music
snippets caused by local and global tempo differences. In this paper, we
address these two shortcomings by designing a cross-modal recurrent network
that learns joint embeddings that can summarize longer passages of
corresponding audio and sheet music. The benefits of our method are that it
only requires weakly aligned audio-sheet music pairs, as well as that the
recurrent network handles the non-linearities caused by tempo variations
between audio and sheet music. We conduct a number of experiments on synthetic
and real piano data and scores, showing that our proposed recurrent method
leads to more accurate retrieval in all possible configurations.",http://arxiv.org/pdf/2309.12111v1
2309.12107v1,cs.CL,A Computational Analysis of Vagueness in Revisions of Instructional Texts,2023-09-21 14:26:04+00:00,"WikiHow is an open-domain repository of instructional articles for a variety
of tasks, which can be revised by users. In this paper, we extract pairwise
versions of an instruction before and after a revision was made. Starting from
a noisy dataset of revision histories, we specifically extract and analyze
edits that involve cases of vagueness in instructions. We further investigate
the ability of a neural model to distinguish between two versions of an
instruction in our data by adopting a pairwise ranking task from previous work
and showing improvements over existing baselines.",http://arxiv.org/pdf/2309.12107v1
2309.12106v1,cs.CV,FourierLoss: Shape-Aware Loss Function with Fourier Descriptors,2023-09-21 14:23:10+00:00,"Encoder-decoder networks become a popular choice for various medical image
segmentation tasks. When they are trained with a standard loss function, these
networks are not explicitly enforced to preserve the shape integrity of an
object in an image. However, this ability of the network is important to obtain
more accurate results, especially when there is a low-contrast difference
between the object and its surroundings. In response to this issue, this work
introduces a new shape-aware loss function, which we name FourierLoss. This
loss function relies on quantifying the shape dissimilarity between the ground
truth and the predicted segmentation maps through the Fourier descriptors
calculated on their objects, and penalizing this dissimilarity in network
training. Different than the previous studies, FourierLoss offers an adaptive
loss function with trainable hyperparameters that control the importance of the
level of the shape details that the network is enforced to learn in the
training process. This control is achieved by the proposed adaptive loss update
mechanism, which end-to-end learns the hyperparameters simultaneously with the
network weights by backpropagation. As a result of using this mechanism, the
network can dynamically change its attention from learning the general outline
of an object to learning the details of its contour points, or vice versa, in
different training epochs. Working on 2879 computed tomography images of 93
subjects, our experiments revealed that the proposed adaptive shape-aware loss
function led to statistically significantly better results for liver
segmentation, compared to its counterparts.",http://arxiv.org/pdf/2309.12106v1
2309.12104v1,math-ph,Cohomological Lagrangian field theory,2023-09-21 14:21:29+00:00,"This paper introduces a geometric framework for classical cohomological field
theories based on $G^{\star}$-algebras and gauge natural field theories. A
BV-BFV extension of the framework is provided, which incorporates the cotangent
lift of the Donaldson-Witten theory as an illustrative example.",http://arxiv.org/pdf/2309.12104v1
2309.12102v1,cs.CL,SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts,2023-09-21 14:19:04+00:00,"We describe SemEval-2022 Task 7, a shared task on rating the plausibility of
clarifications in instructional texts. The dataset for this task consists of
manually clarified how-to guides for which we generated alternative
clarifications and collected human plausibility judgements. The task of
participating systems was to automatically determine the plausibility of a
clarification in the respective context. In total, 21 participants took part in
this task, with the best system achieving an accuracy of 68.9%. This report
summarizes the results and findings from 8 teams and their system descriptions.
Finally, we show in an additional evaluation that predictions by the top
participating team make it possible to identify contexts with multiple
plausible clarifications with an accuracy of 75.2%.",http://arxiv.org/pdf/2309.12102v1
2309.12094v1,eess.SP,RadYOLOLet: Radar Detection and Parameter Estimation Using YOLO and WaveLet,2023-09-21 14:09:23+00:00,"Detection of radar signals without assistance from the radar transmitter is a
crucial requirement for emerging and future shared-spectrum wireless networks
like Citizens Broadband Radio Service (CBRS). In this paper, we propose a
supervised deep learning-based spectrum sensing approach called RadYOLOLet that
can detect low-power radar signals in the presence of interference and estimate
the radar signal parameters. The core of RadYOLOLet is two different
convolutional neural networks (CNN), RadYOLO and Wavelet-CNN, that are trained
independently. RadYOLO operates on spectrograms and provides most of the
capabilities of RadYOLOLet. However, it suffers from low radar detection
accuracy in the low signal-to-noise ratio (SNR) regime. We develop Wavelet-CNN
specifically to deal with this limitation of RadYOLO. Wavelet-CNN operates on
continuous Wavelet transform of the captured signals, and we use it only when
RadYOLO fails to detect any radar signal. We thoroughly evaluate RadYOLOLet
using different experiments corresponding to different types of interference
signals. Based on our evaluations, we find that RadYOLOLet can achieve 100%
radar detection accuracy for our considered radar types up to 16 dB SNR, which
cannot be guaranteed by other comparable methods. RadYOLOLet can also function
accurately under interference up to 16 dB SINR.",http://arxiv.org/pdf/2309.12094v1
2309.12093v1,math.CT,Module Monoidal Categories as Categorification of Associative Algebras,2023-09-21 14:07:06+00:00,"In [arXiv:1509.02937], the notion of a module tensor category was introduced
as a braided monoidal central functor $F\colon \mathcal{V}\longrightarrow
\mathcal{T}$ from a braided monoidal category $\mathcal{V}$ to a monoidal
category $\mathcal{T}$, which is a monoidal functor $F\colon
\mathcal{V}\longrightarrow\mathcal{T}$ together with a braided monoidal lift
$F^Z\colon \mathcal{V}\longrightarrow Z(\mathcal{T})$ to the Drinfeld center of
$\mathcal{T}$. This is a categorification of a unital associative algebra $A$
over a commutative ring $R$ via a ring homomorphism $f\colon R\longrightarrow
Z(A)$ into the center of $A$. In this paper, we want to categorify the
characterization of an associative algebra as a (not necessarily unital) ring
$A$ together with an $R$-module structure over a commutative ring $R$, such
that multiplication in $A$ and action of $R$ on $A$ are compatible. In doing
so, we introduce the more general notion of non-unital module monoidal
categories and obtain 2-categories of non-unital and unital module monoidal
categories, their functors and natural transformations. We will show that in
the unital case the latter definition is equivalent to the definition in
[arXiv:1509.02937] by explicitly writing down an equivalence of 2-categories.",http://arxiv.org/pdf/2309.12093v1
2309.12085v1,econ.GN,Techno-Economic Analysis of Synthetic Fuel Production from Existing Nuclear Power Plants across the United States,2023-09-21 13:56:42+00:00,"Low carbon synfuel can displace transport fossil fuels such as diesel and jet
fuel and help achieve the decarbonization of the transportation sector at a
global scale, but large-scale cost-effective production facilities are needed.
Meanwhile, nuclear power plants are closing due to economic difficulties:
electricity prices are too low and variable to cover their operational costs.
Using existing nuclear power plants to produce synfuels might prevent loss of
these low-carbon assets while producing synfuels at scale, but no
technoeconomic analysis of this Integrated Energy System exist. We quantify the
technoeconomic potential of coupling a synthetic fuel production process with
five example nuclear power plants across the U.S. to explore the influence of
different electricity markets, access to carbon dioxide sources, and fuel
markets. Coupling synfuel production increases nuclear plant profitability by
up to 792 million USD(2020) in addition to a 10 percent rate of return on
investment over a 20 year period. Our analysis identifies drivers for the
economic profitability of the synfuel IES. The hydrogen production tax credit
from the 2022 Inflation Reduction Act is essential to its overall profitability
representing on average three quarters of its revenues. The carbon feedstock
transportation is the highest cost - more than a third on average - closely
followed by the synfuel production process capital costs. Those results show
the key role of incentive policies for the decarbonization of the
transportation sector and the economic importance of the geographic location of
Integrated Energy Systems.",http://arxiv.org/pdf/2309.12085v1
2309.12084v1,cond-mat.mes-hall,Persistent current-carrying state of charge quasuparticles in $np$-ribbon featuring single Dirac cone,2023-09-21 13:56:34+00:00,"The formation of persistent charge currents in mesoscopic systems remains an
interesting and actual topic of condensed matter research. Here, we analyze the
formation of spontaneous arising persistent currents of charged fermions in
2-dimensional electron-hole ribbons on the top and bottom of a 3-dimensional
topological insulator. In such a device the two-dimensional Dirac fermions with
opposite chiralities are spatially separated that allows these currents to flow
in the opposite directions without compensating each other. The nature of this
phenomenon is based on the interference of the quasiparticle quantum waves
which are scattered with asymmetric scattering phases at the lateral n-p chiral
junction and then reflected back by the external boundaries of the ribbon. As a
result quasiparticles in the ribbon are shown to be in unified electron-hole
quantum states carrying the persistent current.",http://arxiv.org/pdf/2309.12084v1
2309.12083v1,astro-ph.CO,Varying fundamental constants meet Hubble,2023-09-21 13:54:48+00:00,"Fundamental physical constants need not be constant, neither spatially nor
temporally. -- This seeming simple statement has profound implications for a
wide range of physical processes and interactions, and can be probed through a
number of observations. In this chapter, we highlight how CMB measurements can
constrain variations of the fine-structure constant and the electron rest mass
during the cosmological recombination era. The sensitivity of the CMB
anisotropies to these constants arises because they directly affect the cosmic
ionization history and Thomson scattering rate, with a number of subtle atomic
physics effects coming together. Recent studies have revealed that variations
of the electron rest mass can indeed alleviate the Hubble tension, as we
explain here. Future opportunities through measurements of the cosmological
recombination radiation are briefly mentioned, highlighting how these could
provide an exciting avenue towards uncovering the physical origin of the Hubble
tension experimentally.",http://arxiv.org/pdf/2309.12083v1
2309.12074v1,physics.ed-ph,How understanding large language models can inform their use in physics education,2023-09-21 13:42:57+00:00,"The paper aims to fulfil three main functions: (1) to serve as an
introduction for the physics education community to the functioning of Large
Language Models (LLMs), (2) to present a series of illustrative examples
demonstrating how prompt-engineering techniques can impact LLMs performance on
conceptual physics tasks and (3) to discuss potential implications of the
understanding of LLMs and prompt engineering for physics teaching and learning.
We first summarise existing research on the performance of a popular LLM-based
chatbot (ChatGPT) on physics tasks. We then give a basic account of how LLMs
work, illustrate essential features of their functioning, and discuss their
strengths and limitations. Equipped with this knowledge, we discuss some
challenges with generating useful output with ChatGPT-4 in the context of
introductory physics, paying special attention to conceptual questions and
problems. We then provide a condensed overview of relevant literature on prompt
engineering and demonstrate through illustrative examples how selected
prompt-engineering techniques can be employed to improve ChatGPT-4's output on
conceptual introductory physics problems. Qualitatively studying these examples
provides additional insights into ChatGPT's functioning and its utility in
physics problem solving. Finally, we consider how insights from the paper can
inform the use of LMMs in the teaching and learning of physics.",http://arxiv.org/pdf/2309.12074v1
2309.12065v1,eess.AS,Is the Ideal Ratio Mask Really the Best? -- Exploring the Best Extraction Performance and Optimal Mask of Mask-based Beamformers,2023-09-21 13:35:20+00:00,"This study investigates mask-based beamformers (BFs), which estimate filters
to extract target speech using time-frequency masks. Although several BF
methods have been proposed, the following aspects are yet to be comprehensively
investigated. 1) Which BF can provide the best extraction performance in terms
of the closeness of the BF output to the target speech? 2) Is the optimal mask
for the best performance common for all BFs? 3) Is the ideal ratio mask (IRM)
identical to the optimal mask? Accordingly, we investigate these issues
considering four mask-based BFs: the maximum signal-to-noise ratio BF, two
variants of this, and the multichannel Wiener filter (MWF) BF. To obtain the
optimal mask corresponding to the peak performance for each BF, we employ an
approach that minimizes the mean square error between the BF output and target
speech for each utterance. Via the experiments with the CHiME-3 dataset, we
verify that the four BFs have the same peak performance as the upper bound
provided by the ideal MWF BF, whereas the optimal mask depends on the adopted
BF and differs from the IRM. These observations differ from the conventional
idea that the optimal mask is common for all BFs and that peak performance
differs for each BF. Hence, this study contributes to the design of mask-based
BFs.",http://arxiv.org/pdf/2309.12065v1
2309.12061v1,cs.AR,"A BEOL Compatible, 2-Terminals, Ferroelectric Analog Non-Volatile Memory",2023-09-21 13:30:06+00:00,"A Ferroelectric Analog Non-Volatile Memory based on a WOx electrode and
ferroelectric HfZrO$_4$ layer is fabricated at a low thermal budget
(~375$^\circ$C), enabling BEOL processes and CMOS integration. The devices show
suitable properties for integration in crossbar arrays and neural network
inference: analog potentiation/depression with constant field or constant pulse
width schemes, cycle to cycle and device to device variation <10%, ON/OFF ratio
up to 10 and good linearity. The physical mechanisms behind the resistive
switching and conduction mechanisms are discussed.",http://arxiv.org/pdf/2309.12061v1
2309.12055v1,math.PR,Genetic Composition of Supercritical Branching Populations under Power Law Mutation Rates,2023-09-21 13:22:55+00:00,"We aim at understanding the evolution of the genetic composition of cancer
cell populations. To this aim, we consider a branching individual based model
representing a cell population where cells divide, die and mutate along the
edges of a finite directed graph $(V,E)$. The process starts with only one cell
of trait $0$. Following typical parameter values in cancer cell populations we
study the model under large population and power law mutation rates limit, in
the sense that the mutation probabilities are parameterized by negative powers
of $n$ and the typical sizes of the population of our interest are positive
powers of $n$. Under non-increasing growth rate condition (namely the growth
rate of any sub-population is smaller than the growth rate of trait $0$), we
describe the time evolution of the first-order asymptotics of the size of each
sub-population on the $log(n)$ time scale, as well as in the random time scale
at which the initial population, resp. the total population, reaches the size
$n^{t}$. In particular, such results allow to characterize whose mutational
paths along the edges of the graph are actually contributing to the size order
of the sub-populations. Without any condition on the growth rate, we describe
the time evolution of the orders of magnitude of each sub-population. Adapting
techniques from Durrett and Mayberry 2011, we show that these converges to
positive deterministic non-decreasing piecewise linear continuous functions,
whose slopes are given by an algorithm.",http://arxiv.org/pdf/2309.12055v1
2309.12053v1,cs.CL,"AceGPT, Localizing Large Language Models in Arabic",2023-09-21 13:20:13+00:00,"This paper explores the imperative need and methodology for developing a
localized Large Language Model (LLM) tailored for Arabic, a language with
unique cultural characteristics that are not adequately addressed by current
mainstream models like ChatGPT. Key concerns additionally arise when
considering cultural sensitivity and local values. To this end, the paper
outlines a packaged solution, including further pre-training with Arabic texts,
supervised fine-tuning (SFT) using native Arabic instructions and GPT-4
responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using
a reward model that is sensitive to local culture and values. The objective is
to train culturally aware and value-aligned Arabic LLMs that can serve the
diverse application-specific needs of Arabic-speaking communities.
  Extensive evaluations demonstrated that the resulting LLM called
`\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including
instruction-following benchmark (i.e., Arabic Vicuna-80 and Arabic AlpacaEval),
knowledge benchmark (i.e., Arabic MMLU and EXAMs), as well as the
newly-proposed Arabic cultural \& value alignment benchmark. Notably, AceGPT
outperforms ChatGPT in the popular Vicuna-80 benchmark when evaluated with
GPT-4, despite the benchmark's limited scale. % Natural Language Understanding
(NLU) benchmark (i.e., ALUE)
  Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.",http://arxiv.org/pdf/2309.12053v1
2309.12051v1,cs.AR,"A Back-End-Of-Line Compatible, Ferroelectric Analog Non-Volatile Memory",2023-09-21 13:17:44+00:00,"A Ferroelectric Analog Non-Volatile Memory based on a WOx electrode and
ferroelectric HfZrO4 layer is fabricated at a low thermal budget (~375C),
enabling BEOL processes and CMOS integration. The devices show suitable
properties for integration in crossbar arrays and neural network inference:
analog potentiation/depression with constant field or constant pulse width
schemes, cycle to cycle and device to device variation <10%, ON/OFF ratio up to
10 and good linearity. The physical mechanisms behind the resistive switching
and conduction mechanisms are discussed.",http://arxiv.org/pdf/2309.12051v1
2309.12048v1,astro-ph.CO,Cosmology with multiple galaxies,2023-09-21 13:15:57+00:00,"Recent works have discovered a relatively tight correlation between
$\Omega_{\rm m}$ and properties of individual simulated galaxies. Because of
this, it has been shown that constraints on $\Omega_{\rm m}$ can be placed
using the properties of individual galaxies while accounting for uncertainties
on astrophysical processes such as feedback from supernova and active galactic
nuclei. In this work, we quantify whether using the properties of multiple
galaxies simultaneously can tighten those constraints. For this, we train
neural networks to perform likelihood-free inference on the value of two
cosmological parameters ($\Omega_{\rm m}$ and $\sigma_8$) and four
astrophysical parameters using the properties of several galaxies from
thousands of hydrodynamic simulations of the CAMELS project. We find that using
properties of more than one galaxy increases the precision of the $\Omega_{\rm
m}$ inference. Furthermore, using multiple galaxies enables the inference of
other parameters that were poorly constrained with one single galaxy. We show
that the same subset of galaxy properties are responsible for the constraints
on $\Omega_{\rm m}$ from one and multiple galaxies. Finally, we quantify the
robustness of the model and find that without identifying the model range of
validity, the model does not perform well when tested on galaxies from other
galaxy formation models.",http://arxiv.org/pdf/2309.12048v1
2309.12047v1,cs.CV,"Self-Calibrating, Fully Differentiable NLOS Inverse Rendering",2023-09-21 13:15:54+00:00,"Existing time-resolved non-line-of-sight (NLOS) imaging methods reconstruct
hidden scenes by inverting the optical paths of indirect illumination measured
at visible relay surfaces. These methods are prone to reconstruction artifacts
due to inversion ambiguities and capture noise, which are typically mitigated
through the manual selection of filtering functions and parameters. We
introduce a fully-differentiable end-to-end NLOS inverse rendering pipeline
that self-calibrates the imaging parameters during the reconstruction of hidden
scenes, using as input only the measured illumination while working both in the
time and frequency domains. Our pipeline extracts a geometric representation of
the hidden scene from NLOS volumetric intensities and estimates the
time-resolved illumination at the relay wall produced by such geometric
information using differentiable transient rendering. We then use gradient
descent to optimize imaging parameters by minimizing the error between our
simulated time-resolved illumination and the measured illumination. Our
end-to-end differentiable pipeline couples diffraction-based volumetric NLOS
reconstruction with path-space light transport and a simple ray marching
technique to extract detailed, dense sets of surface points and normals of
hidden scenes. We demonstrate the robustness of our method to consistently
reconstruct geometry and albedo, even under significant noise levels.",http://arxiv.org/pdf/2309.12047v1
2309.12041v1,cs.CR,S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees,2023-09-21 13:09:10+00:00,"Privacy-preserving learning of gradient boosting decision trees (GBDT) has
the potential for strong utility-privacy tradeoffs for tabular data, such as
census data or medical meta data: classical GBDT learners can extract
non-linear patterns from small sized datasets. The state-of-the-art notion for
provable privacy-properties is differential privacy, which requires that the
impact of single data points is limited and deniable. We introduce a novel
differentially private GBDT learner and utilize four main techniques to improve
the utility-privacy tradeoff. (1) We use an improved noise scaling approach
with tighter accounting of privacy leakage of a decision tree leaf compared to
prior work, resulting in noise that in expectation scales with $O(1/n)$, for
$n$ data points. (2) We integrate individual R\'enyi filters to our method to
learn from data points that have been underutilized during an iterative
training process, which -- potentially of independent interest -- results in a
natural yet effective insight to learning streams of non-i.i.d. data. (3) We
incorporate the concept of random decision tree splits to concentrate privacy
budget on learning leaves. (4) We deploy subsampling for privacy amplification.
Our evaluation shows for the Abalone dataset ($<4k$ training data points) a
$R^2$-score of $0.39$ for $\varepsilon=0.15$, which the closest prior work only
achieved for $\varepsilon=10.0$. On the Adult dataset ($50k$ training data
points) we achieve test error of $18.7\,\%$ for $\varepsilon=0.07$ which the
closest prior work only achieved for $\varepsilon=1.0$. For the Abalone dataset
for $\varepsilon=0.54$ we achieve $R^2$-score of $0.47$ which is very close to
the $R^2$-score of $0.54$ for the nonprivate version of GBDT. For the Adult
dataset for $\varepsilon=0.54$ we achieve test error $17.1\,\%$ which is very
close to the test error $13.7\,\%$ of the nonprivate version of GBDT.",http://arxiv.org/pdf/2309.12041v1
2309.12040v1,physics.flu-dyn,Optimization-based Level-Set Re-initialization: A Robust Interface Preserving Approach in Multiphase Problems,2023-09-21 13:07:39+00:00,"In spite of its overall efficiency and robustness for capturing the interface
in multiphase fluid dynamics simulations, the well-known shortcoming of the
level-set method is associated with the lack of a systematic approach for
preserving the regularity of the distance function. This is mainly due to the
stretching (or compressing) effect of the strain rate especially in the
vicinity of the liquid-gas interface. Level-set re-initialization is an
effective treatment for this issue. However, the traditional approach based on
the hyperbolic Hamilton-Jacobi equation is a computationally expensive
procedure. Crucially, due to the hyperbolic nature of the formulation, the
accuracy of the results hinges significantly on the specialized handling of
blind spots near the liquid-gas interface intersecting the substrate. The
present work proposes a two-step elliptic level-set re-initialization approach
that strictly preserves the location of zero level-set via incorporation of an
element splitting process. The primary initialization step helps remove any
non-smoothness in the to-be regularized level-set function dramatically
improving the efficiency of the secondary optimization step. Geometric
representation of the boundary conditions is utilized in the initialization
step, while the optimization step minimizes the reliance of the results on the
treatment of the blind spots. The performance of the proposed method is
examined for free and sessile three-dimensional droplets.",http://arxiv.org/pdf/2309.12040v1
2309.12032v1,cs.LG,Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets,2023-09-21 12:53:45+00:00,"Structure learning is the crux of causal inference. Notably, causal discovery
(CD) algorithms are brittle when data is scarce, possibly inferring imprecise
causal relations that contradict expert knowledge -- especially when
considering latent confounders. To aggravate the issue, most CD methods do not
provide uncertainty estimates, making it hard for users to interpret results
and improve the inference process. Surprisingly, while CD is a human-centered
affair, no works have focused on building methods that both 1) output
uncertainty estimates that can be verified by experts and 2) interact with
those experts to iteratively refine CD. To solve these issues, we start by
proposing to sample (causal) ancestral graphs proportionally to a belief
distribution based on a score function, such as the Bayesian information
criterion (BIC), using generative flow networks. Then, we leverage the
diversity in candidate graphs and introduce an optimal experimental design to
iteratively probe the expert about the relations among variables, effectively
reducing the uncertainty of our belief over ancestral graphs. Finally, we
update our samples to incorporate human feedback via importance sampling.
Importantly, our method does not require causal sufficiency (i.e., unobserved
confounders may exist). Experiments with synthetic observational data show that
our method can accurately sample from distributions over ancestral graphs and
that we can greatly improve inference quality with human aid.",http://arxiv.org/pdf/2309.12032v1
2309.12031v1,cs.DS,Simple Approximation Algorithms for Minimizing the Total Weighted Completion Time of Precedence-Constrained Jobs,2023-09-21 12:52:05+00:00,"We consider the precedence-constrained scheduling problem to minimize the
total weighted completion time. For a single machine several $2$-approximation
algorithms are known, which are based on linear programming and network flows.
We show that the same ratio is achieved by a simple weighted round-robin rule.
Moreover, for preemptive scheduling on identical parallel machines, we give a
strongly polynomial $3$-approximation, which computes processing rates by
solving a sequence of parametric flow problems. This matches the best known
constant performance guarantee, previously attained only by a weakly polynomial
LP-based algorithm. Our algorithms are both also applicable in non-clairvoyant
scheduling, where processing times are initially unknown. In this setting, our
performance guarantees improve upon the best competitive ratio of $8$ known so
far.",http://arxiv.org/pdf/2309.12031v1
2309.12030v1,cs.CL,CAMERA: A Multimodal Dataset and Benchmark for Ad Text Generation,2023-09-21 12:51:24+00:00,"In response to the limitations of manual online ad production, significant
research has been conducted in the field of automatic ad text generation (ATG).
However, comparing different methods has been challenging because of the lack
of benchmarks encompassing the entire field and the absence of well-defined
problem sets with clear model inputs and outputs. To address these challenges,
this paper aims to advance the field of ATG by introducing a redesigned task
and constructing a benchmark. Specifically, we defined ATG as a
cross-application task encompassing various aspects of the Internet
advertising. As part of our contribution, we propose a first benchmark dataset,
CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA), carefully designed
for ATG to be able to leverage multi-modal information and conduct an
industry-wise evaluation. Furthermore, we demonstrate the usefulness of our
proposed benchmark through evaluation experiments using multiple baseline
models, which vary in terms of the type of pre-trained language model used and
the incorporation of multi-modal information. We also discuss the current state
of the task and the future challenges.",http://arxiv.org/pdf/2309.12030v1
2309.12029v1,cs.CV,Unveiling the Hidden Realm: Self-supervised Skeleton-based Action Recognition in Occluded Environments,2023-09-21 12:51:11+00:00,"To integrate action recognition methods into autonomous robotic systems, it
is crucial to consider adverse situations involving target occlusions. Such a
scenario, despite its practical relevance, is rarely addressed in existing
self-supervised skeleton-based action recognition methods. To empower robots
with the capacity to address occlusion, we propose a simple and effective
method. We first pre-train using occluded skeleton sequences, then use k-means
clustering (KMeans) on sequence embeddings to group semantically similar
samples. Next, we employ K-nearest-neighbor (KNN) to fill in missing skeleton
data based on the closest sample neighbors. Imputing incomplete skeleton
sequences to create relatively complete sequences as input provides significant
benefits to existing skeleton-based self-supervised models. Meanwhile, building
on the state-of-the-art Partial Spatio-Temporal Learning (PSTL), we introduce
an Occluded Partial Spatio-Temporal Learning (OPSTL) framework. This
enhancement utilizes Adaptive Spatial Masking (ASM) for better use of
high-quality, intact skeletons. The effectiveness of our imputation methods is
verified on the challenging occluded versions of the NTURGB+D 60 and NTURGB+D
120. The source code will be made publicly available at
https://github.com/cyfml/OPSTL.",http://arxiv.org/pdf/2309.12029v1
2309.12027v1,cs.CV,Precision in Building Extraction: Comparing Shallow and Deep Models using LiDAR Data,2023-09-21 12:43:11+00:00,"Building segmentation is essential in infrastructure development, population
management, and geological observations. This article targets shallow models
due to their interpretable nature to assess the presence of LiDAR data for
supervised segmentation. The benchmark data used in this article are published
in NORA MapAI competition for deep learning model. Shallow models are compared
with deep learning models based on Intersection over Union (IoU) and Boundary
Intersection over Union (BIoU). In the proposed work, boundary masks from the
original mask are generated to improve the BIoU score, which relates to
building shapes' borderline. The influence of LiDAR data is tested by training
the model with only aerial images in task 1 and a combination of aerial and
LiDAR data in task 2 and then compared. shallow models outperform deep learning
models in IoU by 8% using aerial images (task 1) only and 2% in combined aerial
images and LiDAR data (task 2). In contrast, deep learning models show better
performance on BIoU scores. Boundary masks improve BIoU scores by 4% in both
tasks. Light Gradient-Boosting Machine (LightGBM) performs better than RF and
Extreme Gradient Boosting (XGBoost).",http://arxiv.org/pdf/2309.12027v1
2309.12012v1,math.NA,"An elastic properties-based topology optimization algorithm for linear orthotropic, functionally graded materials",2023-09-21 12:31:33+00:00,"Topology optimization (TO) has experienced a dramatic development over the
last decades aided by the arising of metamaterials and additive manufacturing
(AM) techniques, and it is intended to achieve the current and future
challenges. In this paper we propose an extension for linear orthotropic
materials of a three-dimensional TO algorithm which directly operates on the
six elastic properties -- three longitudinal and shear moduli, having fixed
three Poisson ratios -- of the finite element (FE) discretization of certain
analysis domain. By performing a gradient-descent-alike optimization on these
properties, the standard deviation of a strain-energy measurement is minimized,
thus coming up with optimized, strain-homogenized structures with variable
longitudinal and shear stiffness in their different material directions. To
this end, an orthotropic formulation with two approaches -- direct or
strain-based and complementary or stress-based -- has been developed for this
optimization problem, being the stress-based more efficient as previous works
on this topic have shown.
  The key advantages that we propose are: (1) the use of orthotropic ahead of
isotropic materials, which enables a more versatile optimization process since
the design space is increased by six times, and (2) no constraint needs to be
imposed (such as maximum volume) in contrast to other methods widely used in
this field such as Solid Isotropic Material with Penalization (SIMP), all of
this by setting one unique hyper-parameter. Results of four designed load cases
show that this orthotropic-TO algorithm outperforms the isotropic case, both
for the similar algorithm from which this is an extension and for a SIMP run in
a FE commercial software, presenting a comparable computational cost. We remark
that it works particularly effectively on pure shear or shear-governed problems
such as torsion loading.",http://arxiv.org/pdf/2309.12012v1
2309.12010v1,eess.IV,Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection,2023-09-21 12:28:23+00:00,"Synthetic aperture radar (SAR) image change detection is a critical task and
has received increasing attentions in the remote sensing community. However,
existing SAR change detection methods are mainly based on convolutional neural
networks (CNNs), with limited consideration of global attention mechanism. In
this letter, we explore Transformer-like architecture for SAR change detection
to incorporate global attention. To this end, we propose a convolution and
attention mixer (CAMixer). First, to compensate the inductive bias for
Transformer, we combine self-attention with shift convolution in a parallel
way. The parallel design effectively captures the global semantic information
via the self-attention and performs local feature extraction through shift
convolution simultaneously. Second, we adopt a gating mechanism in the
feed-forward network to enhance the non-linear feature transformation. The
gating mechanism is formulated as the element-wise multiplication of two
parallel linear layers. Important features can be highlighted, leading to
high-quality representations against speckle noise. Extensive experiments
conducted on three SAR datasets verify the superior performance of the proposed
CAMixer. The source codes will be publicly available at
https://github.com/summitgao/CAMixer .",http://arxiv.org/pdf/2309.12010v1
2309.12009v1,cs.CV,Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision,2023-09-21 12:27:43+00:00,"Self-supervised representation learning for human action recognition has
developed rapidly in recent years. Most of the existing works are based on
skeleton data while using a multi-modality setup. These works overlooked the
differences in performance among modalities, which led to the propagation of
erroneous knowledge between modalities while only three fundamental modalities,
i.e., joints, bones, and motions are used, hence no additional modalities are
explored.
  In this work, we first propose an Implicit Knowledge Exchange Module (IKEM)
which alleviates the propagation of erroneous knowledge between low-performance
modalities. Then, we further propose three new modalities to enrich the
complementary information between modalities. Finally, to maintain efficiency
when introducing new modalities, we propose a novel teacher-student framework
to distill the knowledge from the secondary modalities into the mandatory
modalities considering the relationship constrained by anchors, positives, and
negatives, named relational cross-modality knowledge distillation. The
experimental results demonstrate the effectiveness of our approach, unlocking
the efficient use of skeleton-based multi-modality data. Source code will be
made publicly available at https://github.com/desehuileng0o0/IKEM.",http://arxiv.org/pdf/2309.12009v1
2309.12008v1,cs.RO,NanoSLAM: Enabling Fully Onboard SLAM for Tiny Robots,2023-09-21 12:27:18+00:00,"Perceiving and mapping the surroundings are essential for enabling autonomous
navigation in any robotic platform. The algorithm class that enables accurate
mapping while correcting the odometry errors present in most robotics systems
is Simultaneous Localization and Mapping (SLAM). Today, fully onboard mapping
is only achievable on robotic platforms that can host high-wattage processors,
mainly due to the significant computational load and memory demands required
for executing SLAM algorithms. For this reason, pocket-size
hardware-constrained robots offload the execution of SLAM to external
infrastructures. To address the challenge of enabling SLAM algorithms on
resource-constrained processors, this paper proposes NanoSLAM, a lightweight
and optimized end-to-end SLAM approach specifically designed to operate on
centimeter-size robots at a power budget of only 87.9 mW. We demonstrate the
mapping capabilities in real-world scenarios and deploy NanoSLAM on a
nano-drone weighing 44 g and equipped with a novel commercial RISC-V low-power
parallel processor called GAP9. The algorithm is designed to leverage the
parallel capabilities of the RISC-V processing cores and enables mapping of a
general environment with an accuracy of 4.5 cm and an end-to-end execution time
of less than 250 ms.",http://arxiv.org/pdf/2309.12008v1
2309.11995v1,eess.IV,Identification of pneumonia on chest x-ray images through machine learning,2023-09-21 12:10:22+00:00,"Pneumonia is the leading infectious cause of infant death in the world. When
identified early, it is possible to alter the prognosis of the patient, one
could use imaging exams to help in the diagnostic confirmation. Performing and
interpreting the exams as soon as possible is vital for a good treatment, with
the most common exam for this pathology being chest X-ray. The objective of
this study was to develop a software that identify the presence or absence of
pneumonia in chest radiographs. The software was developed as a computational
model based on machine learning using transfer learning technique. For the
training process, images were collected from a database available online with
children's chest X-rays images taken at a hospital in China. After training,
the model was then exposed to new images, achieving relevant results on
identifying such pathology, reaching 98% sensitivity and 97.3% specificity for
the sample used for testing. It can be concluded that it is possible to develop
a software that identifies pneumonia in chest X-ray images.",http://arxiv.org/pdf/2309.11995v1
2309.11992v1,eess.SP,UAV Swarm Deployment and Trajectory for 3D Area Coverage via Reinforcement Learning,2023-09-21 12:04:11+00:00,"Unmanned aerial vehicles (UAVs) are recognized as promising technologies for
area coverage due to the flexibility and adaptability. However, the ability of
a single UAV is limited, and as for the large-scale three-dimensional (3D)
scenario, UAV swarms can establish seamless wireless communication services.
Hence, in this work, we consider a scenario of UAV swarm deployment and
trajectory to satisfy 3D coverage considering the effects of obstacles. In
detail, we propose a hierarchical swarm framework to efficiently serve the
large-area users. Then, the problem is formulated to minimize the total
trajectory loss of the UAV swarm. However, the problem is intractable due to
the non-convex property, and we decompose it into smaller issues of users
clustering, UAV swarm hovering points selection, and swarm trajectory
determination. Moreover, we design a Q-learning based algorithm to accelerate
the solution efficiency. Finally, we conduct extensive simulations to verify
the proposed mechanisms, and the designed algorithm outperforms other referred
methods.",http://arxiv.org/pdf/2309.11992v1
2309.11989v1,cs.RO,Crop Row Switching for Vision-Based Navigation: A Comprehensive Approach for Efficient Crop Field Navigation,2023-09-21 12:01:59+00:00,"Vision-based mobile robot navigation systems in arable fields are mostly
limited to in-row navigation. The process of switching from one crop row to the
next in such systems is often aided by GNSS sensors or multiple camera setups.
This paper presents a novel vision-based crop row-switching algorithm that
enables a mobile robot to navigate an entire field of arable crops using a
single front-mounted camera. The proposed row-switching manoeuvre uses deep
learning-based RGB image segmentation and depth data to detect the end of the
crop row, and re-entry point to the next crop row which would be used in a
multi-state row switching pipeline. Each state of this pipeline use visual
feedback or wheel odometry of the robot to successfully navigate towards the
next crop row. The proposed crop row navigation pipeline was tested in a real
sugar beet field containing crop rows with discontinuities, varying light
levels, shadows and irregular headland surfaces. The robot could successfully
exit from one crop row and re-enter the next crop row using the proposed
pipeline with absolute median errors averaging at 19.25 cm and 6.77{\deg} for
linear and rotational steps of the proposed manoeuvre.",http://arxiv.org/pdf/2309.11989v1
2309.11979v1,q-fin.CP,Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT,2023-09-21 11:26:36+00:00,"With the rapid development of big data and computing devices, low-latency
automatic trading platforms based on real-time information acquisition have
become the main components of the stock trading market, so the topic of
quantitative trading has received widespread attention. And for non-strongly
efficient trading markets, human emotions and expectations always dominate
market trends and trading decisions. Therefore, this paper starts from the
theory of emotion, taking East Money as an example, crawling user comment
titles data from its corresponding stock bar and performing data cleaning.
Subsequently, a natural language processing model BERT was constructed, and the
BERT model was fine-tuned using existing annotated data sets. The experimental
results show that the fine-tuned model has different degrees of performance
improvement compared to the original model and the baseline model.
Subsequently, based on the above model, the user comment data crawled is
labeled with emotional polarity, and the obtained label information is combined
with the Alpha191 model to participate in regression, and significant
regression results are obtained. Subsequently, the regression model is used to
predict the average price change for the next five days, and use it as a signal
to guide automatic trading. The experimental results show that the
incorporation of emotional factors increased the return rate by 73.8\% compared
to the baseline during the trading period, and by 32.41\% compared to the
original alpha191 model. Finally, we discuss the advantages and disadvantages
of incorporating emotional factors into quantitative trading, and give possible
directions for further research in the future.",http://arxiv.org/pdf/2309.11979v1
2309.11978v1,physics.soc-ph,"Kuramoto model, phase synchronisation and the network's core",2023-09-21 11:24:17+00:00,"In this note we show that for the Kuramoto model defined in a simple
undirected graph it is possible to decide which nodes form the core of the
network. The set of core-nodes is defined by its relevance to the phase
synchronisation process.",http://arxiv.org/pdf/2309.11978v1
2309.11977v1,cs.SD,Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts,2023-09-21 11:22:22+00:00,"Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's
voice without adaptation parameters. By quantizing speech waveform into
discrete acoustic tokens and modeling these tokens with the language model,
recent language model-based TTS models show zero-shot speaker adaptation
capabilities with only a 3-second acoustic prompt of an unseen speaker.
However, they are limited by the length of the acoustic prompt, which makes it
difficult to clone personal speaking style. In this paper, we propose a novel
zero-shot TTS model with the multi-scale acoustic prompts based on a neural
codec language model VALL-E. A speaker-aware text encoder is proposed to learn
the personal speaking style at the phoneme-level from the style prompt
consisting of multiple sentences. Following that, a VALL-E based acoustic
decoder is utilized to model the timbre from the timbre prompt at the
frame-level and generate speech. The experimental results show that our
proposed method outperforms baselines in terms of naturalness and speaker
similarity, and can achieve better performance by scaling out to a longer style
prompt.",http://arxiv.org/pdf/2309.11977v1
2309.11976v1,eess.AS,Multi-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation Using Simulated Data and a Teacher Model,2023-09-21 11:21:52+00:00,"Previous methods for predicting room acoustic parameters and speech quality
metrics have focused on the single-channel case, where room acoustics and Mean
Opinion Score (MOS) are predicted for a single recording device. However,
quality-based device selection for rooms with multiple recording devices may
benefit from a multi-channel approach where the descriptive metrics are
predicted for multiple devices in parallel. Following our hypothesis that a
model may benefit from multi-channel training, we develop a multi-channel model
for joint MOS and room acoustics prediction (MOSRA) for five channels in
parallel. The lack of multi-channel audio data with ground truth labels
necessitated the creation of simulated data using an acoustic simulator with
room acoustic labels extracted from the generated impulse responses and labels
for MOS generated in a student-teacher setup using a wav2vec2-based MOS
prediction model. Our experiments show that the multi-channel model improves
the prediction of the direct-to-reverberation ratio, clarity, and speech
transmission index over the single-channel model with roughly 5$\times$ less
computation while suffering minimal losses in the performance of the other
metrics.",http://arxiv.org/pdf/2309.11976v1
2309.11965v1,eess.SY,Coordination Control of Discrete Event Systems under Cyber Attacks,2023-09-21 10:45:54+00:00,"This paper investigates the coordination control of discrete event systems in
the presence of combined sensor and actuator attacks. Discrete event systems
are modeled as automata, and sensor attacks are defined using specific attack
languages. The approach involves employing multiple local supervisors to
control the system. The primary objective is to devise these local supervisors
to ensure the system's safety, even when facing sensor and actuator attacks.
The paper establishes the necessary and sufficient conditions for the existence
of such supervisors in terms of conditional decomposability,
CA-controllability, and CA-observability. Furthermore, a methodology is
developed to compute local state estimates when sensor attacks occur. Based on
the local state estimates, local supervisors are designed to ensure the safety
of a system even under cyber attacks.",http://arxiv.org/pdf/2309.11965v1
2309.11961v1,math.CO,A fresh look to a randomized massively parallel graph coloring algorithm,2023-09-21 10:32:37+00:00,"Petford and Welsh introduced a sequential heuristic algorithm for
(approximately) solving the NP-hard graph coloring problem. The algorithm is
based on the antivoter model and mimics the behaviour of a physical process
based on a multi-particle system of statistical mechanics. It was later shown
that the algorithm can be implemented in massively parallel model of
computation. The increase of processing power in recent years allow us to
perform an extensive analysis of the algorithms on a larger scale, leading to
possibility of a more comprehensive understanding of the behaviour of the
algorithm including the phase transition phenomena.",http://arxiv.org/pdf/2309.11961v1
2309.11955v1,cs.CV,A Study of Forward-Forward Algorithm for Self-Supervised Learning,2023-09-21 10:14:53+00:00,"Self-supervised representation learning has seen remarkable progress in the
last few years, with some of the recent methods being able to learn useful
image representations without labels. These methods are trained using
backpropagation, the de facto standard. Recently, Geoffrey Hinton proposed the
forward-forward algorithm as an alternative training method. It utilizes two
forward passes and a separate loss function for each layer to train the network
without backpropagation.
  In this study, for the first time, we study the performance of
forward-forward vs. backpropagation for self-supervised representation learning
and provide insights into the learned representation spaces. Our benchmark
employs four standard datasets, namely MNIST, F-MNIST, SVHN and CIFAR-10, and
three commonly used self-supervised representation learning techniques, namely
rotation, flip and jigsaw.
  Our main finding is that while the forward-forward algorithm performs
comparably to backpropagation during (self-)supervised training, the transfer
performance is significantly lagging behind in all the studied settings. This
may be caused by a combination of factors, including having a loss function for
each layer and the way the supervised training is realized in the
forward-forward paradigm. In comparison to backpropagation, the forward-forward
algorithm focuses more on the boundaries and drops part of the information
unnecessary for making decisions which harms the representation learning goal.
Further investigation and research are necessary to stabilize the
forward-forward strategy for self-supervised learning, to work beyond the
datasets and configurations demonstrated by Geoffrey Hinton.",http://arxiv.org/pdf/2309.11955v1
2309.11950v1,cs.IT,State-aware Real-time Tracking and Remote Reconstruction of a Markov Source,2023-09-21 10:03:36+00:00,"The problem of real-time remote tracking and reconstruction of a two-state
Markov process is considered here. A transmitter sends samples from an observed
information source to a remote monitor over an unreliable wireless channel. The
receiver, in turn, performs an action according to the state of the
reconstructed source. We propose a state-aware randomized stationary sampling
and transmission policy which accounts for the importance of different states
of the information source, and their impact on the goal of the communication
process. We then analyze the performance of the proposed policy, and compare it
with existing goal-oriented joint sampling and transmission policies, with
respect to a set of performance metrics. Specifically, we study the real-time
reconstruction error, the cost of actuation error, the consecutive error, and a
new metric, coined importance-aware consecutive error. In addition, we
formulate and solve a constrained optimization problem that aims to obtain the
optimal sampling probabilities that minimize the average cost of actuation
error. Our results show that in the scenario of constrained sampling
generation, the optimal state-aware randomized stationary policy outperforms
all other sampling policies for fast evolving sources, and, under certain
conditions, for slowly varying sources. Otherwise, a semantics-aware policy
performs better only when the source is slowly varying.",http://arxiv.org/pdf/2309.11950v1
2309.11949v1,quant-ph,Quantum State Reconstruction in a Noisy Environment via Deep Learning,2023-09-21 10:03:30+00:00,"Quantum noise is currently limiting efficient quantum information processing
and computation. In this work, we consider the tasks of reconstructing and
classifying quantum states corrupted by the action of an unknown noisy channel
using classical feedforward neural networks. By framing reconstruction as a
regression problem, we show how such an approach can be used to recover with
fidelities exceeding 99% the noiseless density matrices of quantum states of up
to three qubits undergoing noisy evolution, and we test its performance with
both single-qubit (bit-flip, phase-flip, depolarising, and amplitude damping)
and two-qubit quantum channels (correlated amplitude damping). Moreover, we
also consider the task of distinguishing between different quantum noisy
channels, and show how a neural network-based classifier is able to solve such
a classification problem with perfect accuracy.",http://arxiv.org/pdf/2309.11949v1
2309.11946v1,astro-ph.EP,Thermal tides in neutrally stratified atmospheres: Revisiting the Earth's Precambrian rotational equilibrium,2023-09-21 10:00:34+00:00,"Rotational dynamics of the Earth, over geological timescales, have profoundly
affected local and global climatic evolution, probably contributing to the
evolution of life. To better retrieve the Earth's rotational history, and
motivated by the published hypothesis of a stabilized length of day during the
Precambrian, we examine the effect of thermal tides on the evolution of
planetary rotational motion. The hypothesized scenario is contingent upon
encountering a resonance in atmospheric Lamb waves, whereby an amplified
thermotidal torque cancels the opposing torque of the oceans and solid
interior, driving the Earth into a rotational equilibrium. With this scenario
in mind, we construct an ab initio model of thermal tides on rocky planets
describing a neutrally stratified atmosphere. The model takes into account
dissipative processes with Newtonian cooling and diffusive processes in the
planetary boundary layer. We retrieve from this model a closed-form solution
for the frequency-dependent tidal torque which captures the main spectral
features previously computed using 3D general circulation models. In
particular, under longwave heating, diffusive processes near the surface and
the delayed thermal response of the ground prove to be responsible for
attenuating, and possibly annihilating, the accelerating effect of the
thermotidal torque at the resonance. When applied to the Earth, our model
prediction suggests the occurrence of the Lamb resonance in the Phanerozoic,
but with an amplitude that is insufficient for the rotational equilibrium.
Interestingly, though our study was motivated by the Earth's history, the
generic tidal solution can be straightforwardly and efficiently applied in
exoplanetary settings.",http://arxiv.org/pdf/2309.11946v1
2309.11941v1,cs.MA,"A Digital Marketplace Combining WS-Agreement, Service Negotiation Protocols and Heterogeneous Services",2023-09-21 09:56:49+00:00,"With the ever increasing importance of web services and the Cloud as a
reliable commodity to provide business value as well as consolidate IT
infrastructure, electronic contracts have become very important. WS-Agreement
has itself established as a well accepted container format for describing such
contracts. However, the semantic interpretation of the terms contained in these
contracts, as well as the process of agreeing to contracts when multiple
options have to be considered (negotiation), are still pretty much dealt with
on a case by case basis. In this paper we address the issues of diverging
contracts and varying contract negotiation protocols by introducing the concept
of a contract aware marketplace, which abstracts from the heterogeneous offers
of different services providers. This allows for the automated consumption of
services solely based on preferences, instead of additional restrictions such
as understanding of contract terms and/or negotiation protocols. We also
contribute an evaluation of several existing negotiation concepts/protocols. We
think that reducing the complexity for automated contract negotiation and thus
service consumption is a key for the success of future service and Cloud
infrastructures.",http://arxiv.org/pdf/2309.11941v1
2309.11934v1,eess.SP,"Alteration of skeletal muscle energy metabolism assessed by 31P MRS in clinical routine, part 2: Clinical application",2023-09-21 09:47:49+00:00,"Background: In this second part of a two-part paper, we intend to demonstrate
the impact of the previously proposed advanced quality control pipeline. To
understand its benefit and challenge the proposed methodology in a real
scenario, we chose to compare the outcome when applying it to the analysis of
two patient populations with a significant but highly different types of
fatigue: COVID19 and multiple sclerosis (MS). Experimental: 31P-MRS was
performed on a 3T clinical MRI, in 19 COVID19 patients, 38 MS patients, and 40
matched healthy controls. Dynamic acquisitions using an MR-compatible ergometer
ran over a rest(40s), exercise(2min), and a recovery phase(6min). Long and
short TR acquisitions were also made at rest for T1 correction. The advanced
data quality control pipeline presented in part 1 is applied to the selected
patient cohorts to investigate its impact on clinical outcomes. We first used
power and sample size analysis to estimate objectively the impact of adding
QCS. Then, comparisons between patients and healthy control groups using
validated QCS were performed using unpaired T-tests or Mann-Whitney tests
(p<0.05).Results: The application of the QCS resulted in increased statistical
power, changed the values of several outcome measures, and reduced variability
(SD). A significant difference was found between the T1PCr and T1Pi of MS
patients and healthy controls. Furthermore, the use of a fixed correction
factor led to systematically higher estimated concentrations of PCr and Pi than
when using individually corrected factors. We observed significant differences
between the two patient populations and healthy controls for resting [PCr] --
MS only, [Pi], [ADP], [H2PO4-] and pH -- COVID19 only, and post-exercise
[PCr],[Pi] and [H2PO4-] - MS only. The dynamic indicators $\tau$PCr, $\tau$Pi,
ViPCr and Vmax were reduced for COVID19 and MS patients compared to controls.
Conclusion: Our results show that QCS in dynamic 31P-MRS studies results in
smaller data variability and therefore impacts study sample size and power.
Although QCS resulted in discarded data and therefore reduced the acceptable
data and subject numbers, this rigorous and unbiased approach allowed for
proper assessment of muscle metabolites and metabolism in patient populations.
The outcomes include an increased metabolite T1, which directly affect the T1
correction factor applied to the amplitudes of the metabolite, and a prolonged
$\tau$PCr indicating reduced muscle oxidative capacity for patients with MS and
COVID19.",http://arxiv.org/pdf/2309.11934v1
2309.11933v1,cs.CV,Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation,2023-09-21 09:47:47+00:00,"Referring Video Object Segmentation (RVOS) requires segmenting the object in
video referred by a natural language query. Existing methods mainly rely on
sophisticated pipelines to tackle such cross-modal task, and do not explicitly
model the object-level spatial context which plays an important role in
locating the referred object. Therefore, we propose an end-to-end RVOS
framework completely built upon transformers, termed \textit{Fully
Transformer-Equipped Architecture} (FTEA), which treats the RVOS task as a mask
sequence learning problem and regards all the objects in video as candidate
objects. Given a video clip with a text query, the visual-textual features are
yielded by encoder, while the corresponding pixel-level and word-level features
are aligned in terms of semantic similarity. To capture the object-level
spatial context, we have developed the Stacked Transformer, which individually
characterizes the visual appearance of each candidate object, whose feature map
is decoded to the binary mask sequence in order directly. Finally, the model
finds the best matching between mask sequence and text query. In addition, to
diversify the generated masks for candidate objects, we impose a diversity loss
on the model for capturing more accurate mask of the referred object. Empirical
studies have shown the superiority of the proposed method on three benchmarks,
e.g., FETA achieves 45.1% and 38.7% in terms of mAP on A2D Sentences (3782
videos) and J-HMDB Sentences (928 videos), respectively; it achieves 56.6% in
terms of $\mathcal{J\&F}$ on Ref-YouTube-VOS (3975 videos and 7451 objects).
Particularly, compared to the best candidate method, it has a gain of 2.1% and
3.2% in terms of P$@$0.5 on the former two, respectively, while it has a gain
of 2.9% in terms of $\mathcal{J}$ on the latter one.",http://arxiv.org/pdf/2309.11933v1
2309.11929v1,eess.SP,Index Modulation-based Information Harvesting for Far-Field RF Power Transfer,2023-09-21 09:43:42+00:00,"While wireless information transmission (WIT) is evolving into its sixth
generation (6G), maintaining terminal operations that rely on limited battery
capacities has become one of the most paramount challenges for
Internet-of-Things (IoT) platforms. In this respect, there exists a growing
interest in energy harvesting technology from ambient resources, and wireless
power transfer (WPT) can be the key solution towards enabling battery-less
infrastructures referred to as zero-power communication technology. Indeed,
eclectic integration approaches between WPT and WIT mechanisms are becoming a
vital necessity to limit the need for replacing batteries. Beyond the
conventional separation between data and power components of the emitted
waveforms, as in simultaneous wireless information and power transfer (SWIPT)
mechanisms, a novel protocol referred to as information harvesting (IH) has
recently emerged. IH leverages existing WPT mechanisms for data communication
by incorporating index modulation (IM) techniques on top of the existing
far-field power transfer mechanism. In this paper, a unified framework for the
IM-based IH mechanisms has been presented where the feasibility of various IM
techniques are evaluated based on different performance metrics. The presented
results demonstrate the substantial potential to enable data communication
within existing far-field WPT systems, particularly in the context of
next-generation IoT wireless networks.",http://arxiv.org/pdf/2309.11929v1
2309.11925v1,cs.CL,Scaling up COMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task,2023-09-21 09:38:56+00:00,"We present the joint contribution of Unbabel and Instituto Superior T\'ecnico
to the WMT 2023 Shared Task on Quality Estimation (QE). Our team participated
on all tasks: sentence- and word-level quality prediction (task 1) and
fine-grained error span detection (task 2). For all tasks, we build on the
COMETKIWI-22 model (Rei et al., 2022b). Our multilingual approaches are ranked
first for all tasks, reaching state-of-the-art performance for quality
estimation at word-, span- and sentence-level granularity. Compared to the
previous state-of-the-art COMETKIWI-22, we show large improvements in
correlation with human judgements (up to 10 Spearman points). Moreover, we
surpass the second-best multilingual submission to the shared-task with up to
3.8 absolute points.",http://arxiv.org/pdf/2309.11925v1
2309.11924v1,cs.CR,Generic Selfish Mining MDP for DAG Protocols,2023-09-21 09:34:37+00:00,"Selfish Mining is strategic rule-breaking to maximize rewards in
proof-of-work protocols [3] and Markov Decision Processes (MDPs) are the
preferred tool for finding optimal strategies in Bitcoin [4, 10] and similar
linear chain protocols [12]. Protocols increasingly adopt non-sequential chain
structures [11], for which MDP analysis is more involved [2]. To date,
researchers have tailored specific attack spaces for each protocol [2, 4, 5, 7,
10, 12]. Assumptions differ, and validating and comparing results is difficult.
To overcome this, we propose a generic attack space that supports the wide
class of DAG protocols that provide a total ordering of blocks [11], e. g.,
Ethereum, Fruitchains, and Parallel Proof-of-Work. Our approach is modular: we
specify each protocol as one program, and then derive the Selfish Mining MDPs
automatically.",http://arxiv.org/pdf/2309.11924v1
2309.11923v1,cs.CV,TextCLIP: Text-Guided Face Image Generation And Manipulation Without Adversarial Training,2023-09-21 09:34:20+00:00,"Text-guided image generation aimed to generate desired images conditioned on
given texts, while text-guided image manipulation refers to semantically edit
parts of a given image based on specified texts. For these two similar tasks,
the key point is to ensure image fidelity as well as semantic consistency. Many
previous approaches require complex multi-stage generation and adversarial
training, while struggling to provide a unified framework for both tasks. In
this work, we propose TextCLIP, a unified framework for text-guided image
generation and manipulation without adversarial training. The proposed method
accepts input from images or random noise corresponding to these two different
tasks, and under the condition of the specific texts, a carefully designed
mapping network that exploits the powerful generative capabilities of StyleGAN
and the text image representation capabilities of Contrastive Language-Image
Pre-training (CLIP) generates images of up to $1024\times1024$ resolution that
can currently be generated. Extensive experiments on the Multi-modal CelebA-HQ
dataset have demonstrated that our proposed method outperforms existing
state-of-the-art methods, both on text-guided generation tasks and manipulation
tasks.",http://arxiv.org/pdf/2309.11923v1
2309.11922v1,eess.AS,Cluster-based pruning techniques for audio data,2023-09-21 09:33:41+00:00,"Deep learning models have become widely adopted in various domains, but their
performance heavily relies on a vast amount of data. Datasets often contain a
large number of irrelevant or redundant samples, which can lead to
computational inefficiencies during the training. In this work, we introduce,
for the first time in the context of the audio domain, the k-means clustering
as a method for efficient data pruning. K-means clustering provides a way to
group similar samples together, allowing the reduction of the size of the
dataset while preserving its representative characteristics. As an example, we
perform clustering analysis on the keyword spotting (KWS) dataset. We discuss
how k-means clustering can significantly reduce the size of audio datasets
while maintaining the classification performance across neural networks (NNs)
with different architectures. We further comment on the role of scaling
analysis in identifying the optimal pruning strategies for a large number of
samples. Our studies serve as a proof-of-principle, demonstrating the potential
of data selection with distance-based clustering algorithms for the audio
domain and highlighting promising research avenues.",http://arxiv.org/pdf/2309.11922v1
2309.11913v1,eess.IV,Spatial-Temporal Transformer based Video Compression Framework,2023-09-21 09:23:13+00:00,"Learned video compression (LVC) has witnessed remarkable advancements in
recent years. Similar as the traditional video coding, LVC inherits motion
estimation/compensation, residual coding and other modules, all of which are
implemented with neural networks (NNs). However, within the framework of NNs
and its training mechanism using gradient backpropagation, most existing works
often struggle to consistently generate stable motion information, which is in
the form of geometric features, from the input color features. Moreover, the
modules such as the inter-prediction and residual coding are independent from
each other, making it inefficient to fully reduce the spatial-temporal
redundancy. To address the above problems, in this paper, we propose a novel
Spatial-Temporal Transformer based Video Compression (STT-VC) framework. It
contains a Relaxed Deformable Transformer (RDT) with Uformer based offsets
estimation for motion estimation and compensation, a Multi-Granularity
Prediction (MGP) module based on multi-reference frames for prediction
refinement, and a Spatial Feature Distribution prior based Transformer (SFD-T)
for efficient temporal-spatial joint residual compression. Specifically, RDT is
developed to stably estimate the motion information between frames by
thoroughly investigating the relationship between the similarity based
geometric motion feature extraction and self-attention. MGP is designed to fuse
the multi-reference frame information by effectively exploring the
coarse-grained prediction feature generated with the coded motion information.
SFD-T is to compress the residual information by jointly exploring the spatial
feature distributions in both residual and temporal prediction to further
reduce the spatial-temporal redundancy. Experimental results demonstrate that
our method achieves the best result with 13.5% BD-Rate saving over VTM.",http://arxiv.org/pdf/2309.11913v1
2309.11911v1,cs.CL,InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework,2023-09-21 09:22:07+00:00,"The development of emotion recognition in dialogue (ERC) has been
consistently hindered by the complexity of pipeline designs, leading to ERC
models that often overfit to specific datasets and dialogue patterns. In this
study, we propose a novel approach, namely
  InstructERC, to reformulates the ERC task from a discriminative framework to
a generative framework based on Large Language Models (LLMs) . InstructERC has
two significant contributions: Firstly, InstructERC introduces a simple yet
effective retrieval template module, which helps the model explicitly integrate
multi-granularity dialogue supervision information by concatenating the
historical dialog content, label statement, and emotional domain demonstrations
with high semantic similarity. Furthermore, we introduce two additional emotion
alignment tasks, namely speaker identification and emotion prediction tasks, to
implicitly model the dialogue role relationships and future emotional
tendencies in conversations. Our LLM-based plug-and-play plugin framework
significantly outperforms all previous models and achieves comprehensive SOTA
on three commonly used ERC datasets. Extensive analysis of parameter-efficient
and data-scaling experiments provide empirical guidance for applying
InstructERC in practical scenarios. Our code will be released after blind
review.",http://arxiv.org/pdf/2309.11911v1
2309.11908v1,cs.CC,Recognizing unit multiple intervals is hard,2023-09-21 09:18:00+00:00,"Multiple interval graphs are a well-known generalization of interval graphs
introduced in the 1970s to deal with situations arising naturally in scheduling
and allocation. A $d$-interval is the union of $d$ intervals on the real line,
and a graph is a $d$-interval graph if it is the intersection graph of
$d$-intervals. In particular, it is a unit $d$-interval graph if it admits a
$d$-interval representation where every interval has unit length.
  Whereas it has been known for a long time that recognizing 2-interval graphs
and other related classes such as 2-track interval graphs is NP-complete, the
complexity of recognizing unit 2-interval graphs remains open. Here, we settle
this question by proving that the recognition of unit 2-interval graphs is also
NP-complete. Our proof technique uses a completely different approach from the
other hardness results of recognizing related classes. Furthermore, we extend
the result for unit $d$-interval graphs for any $d\geq 2$, which does not
follow directly in graph recognition problems --as an example, it took almost
20 years to close the gap between $d=2$ and $d> 2$ for the recognition of
$d$-track interval graphs. Our result has several implications, including that
recognizing $(x, \dots, x)$ $d$-interval graphs and depth $r$ unit 2-interval
graphs is NP-complete for every $x\geq 11$ and every $r\geq 4$.",http://arxiv.org/pdf/2309.11908v1
2309.11901v1,physics.soc-ph,Urban Segregation on multilayered transport networks: a random walk approach,2023-09-21 09:12:54+00:00,"We present a novel method for analysing socio-spatial segregation in cities
by considering constraints imposed by transportation networks. Using a
multilayered network approach, we model the interaction probabilities of
socio-economic groups with random walks and L\'evy flights. This method allows
for evaluation of new transport infrastructure's impact on segregation while
quantifying each network's contribution to interaction opportunities. The
proposed random walk segregation index measures the probability of individuals
encountering diverse social groups based on their available means of transit
via random walks. The index incorporates temporal constraints in urban mobility
with a parameter, $\alpha \in [0,1)$, of the probability of the random walk
continuing at each time step. By applying this to a toy model and conducting a
sensitivity analysis, we explore how the index changes dependent on this
temporal constraint. When the parameter equals zero, the measure simplifies to
an isolation index. When the parameter approaches one it represents the city's
overall socio-economic distribution by mirroring the steady-state of the random
walk process. Using Cuenca, Ecuador as a case study, we illustrate the method's
applicability in transportation planning as a valuable tool for policymakers,
addressing spatial distribution of socio-economic groups and the connectivity
of existing transport networks, thus promoting equitable interactions
throughout the city.",http://arxiv.org/pdf/2309.11901v1
2309.11898v1,cs.NI,REM-U-net: Deep Learning Based Agile REM Prediction with Energy-Efficient Cell-Free Use Case,2023-09-21 09:06:09+00:00,"Radio environment maps (REMs) hold a central role in optimizing wireless
network deployment, enhancing network performance, and ensuring effective
spectrum management. Conventional REM prediction methods are either excessively
time-consuming, e.g., ray tracing, or inaccurate, e.g., statistical models,
limiting their adoption in modern inherently dynamic wireless networks.
Deep-learning-based REM prediction has recently attracted considerable
attention as an appealing, accurate, and time-efficient alternative. However,
existing works on REM prediction using deep learning are either confined to 2D
maps or use a limited dataset. In this paper, we introduce a runtime-efficient
REM prediction framework based on u-nets, trained on a large-scale 3D maps
dataset. In addition, data preprocessing steps are investigated to further
refine the REM prediction accuracy. The proposed u-net framework, along with
preprocessing steps, are evaluated in the context of the 2023 IEEE ICASSP
Signal Processing Grand Challenge, namely, the First Pathloss Radio Map
Prediction Challenge. The evaluation results demonstrate that the proposed
method achieves an average normalized root-mean-square error (RMSE) of 0.045
with an average of 14 milliseconds (ms) runtime. Finally, we position our
achieved REM prediction accuracy in the context of a relevant cell-free massive
multiple-input multiple-output (CF-mMIMO) use case. We demonstrate that one can
obviate consuming energy on large-scale fading measurements and rely on
predicted REM instead to decide on which sleep access points (APs) to switch on
in a CF-mMIMO network that adopts a minimum propagation loss AP switch ON/OFF
strategy.",http://arxiv.org/pdf/2309.11898v1
2309.11896v1,cs.CL,Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection,2023-09-21 08:59:24+00:00,"Although pre-trained large language models (PLMs) have achieved
state-of-the-art on many NLP tasks, they lack understanding of subtle
expressions of implicit hate speech. Such nuanced and implicit hate is often
misclassified as non-hate. Various attempts have been made to enhance the
detection of (implicit) hate content by augmenting external context or
enforcing label separation via distance-based metrics. We combine these two
approaches and introduce FiADD, a novel Focused Inferential Adaptive Density
Discrimination framework. FiADD enhances the PLM finetuning pipeline by
bringing the surface form of an implicit hate speech closer to its implied form
while increasing the inter-cluster distance among various class labels. We test
FiADD on three implicit hate datasets and observe significant improvement in
the two-way and three-way hate classification tasks. We further experiment on
the generalizability of FiADD on three other tasks, namely detecting sarcasm,
irony, and stance, in which surface and implied forms differ, and observe
similar performance improvement. We analyze the generated latent space to
understand its evolution under FiADD, which corroborates the advantage of
employing FiADD for implicit hate speech detection.",http://arxiv.org/pdf/2309.11896v1
2309.11891v1,eess.IV,Heart Rate Detection Using an Event Camera,2023-09-21 08:51:30+00:00,"Event cameras, also known as neuromorphic cameras, are an emerging technology
that offer advantages over traditional shutter and frame-based cameras,
including high temporal resolution, low power consumption, and selective data
acquisition. In this study, we propose to harnesses the capabilities of
event-based cameras to capture subtle changes in the surface of the skin caused
by the pulsatile flow of blood in the wrist region. We investigate whether an
event camera could be used for continuous noninvasive monitoring of heart rate
(HR). Event camera video data from 25 participants, comprising varying age
groups and skin colours, was collected and analysed. Ground-truth HR
measurements obtained using conventional methods were used to evaluate of the
accuracy of automatic detection of HR from event camera data. Our experimental
results and comparison to the performance of other non-contact HR measurement
methods demonstrate the feasibility of using event cameras for pulse detection.
We also acknowledge the challenges and limitations of our method, such as
light-induced flickering and the sub-conscious but naturally-occurring tremors
of an individual during data capture.",http://arxiv.org/pdf/2309.11891v1
2309.11888v1,cs.CL,Is It Really Useful to Jointly Parse Constituency and Dependency Trees? A Revisit,2023-09-21 08:45:41+00:00,"This work visits the topic of jointly parsing constituency and dependency
trees, i.e., to produce compatible constituency and dependency trees
simultaneously for input sentences, which is attractive considering that the
two types of trees are complementary in representing syntax. Compared with
previous works, we make progress in four aspects: (1) adopting a much more
efficient decoding algorithm, (2) exploring joint modeling at the training
phase, instead of only at the inference phase, (3) proposing high-order scoring
components for constituent-dependency interaction, (4) gaining more insights
via in-depth experiments and analysis.",http://arxiv.org/pdf/2309.11888v1
2309.11874v1,cond-mat.supr-con,Cuprate universal electronic spin response and the pseudogap from NMR,2023-09-21 08:20:32+00:00,"It is shown that three independently measured NMR shifts in the cuprates in
their whole temperature dependence are linearly related to each other with a
doping and family dependent, but temperature independent constant. It is the Cu
shift anisotropy that changes in proportion to the planar O shift for all
materials found in the literature, independent of sample origin or details of
the measurements. Such a relation involving three shifts rules out a single
spin component description of the cuprates. It is argued that the relation is
so robust since it depends for Cu on ($A_\perp-A_\parallel$), the hyperfine
coefficient $A_\alpha$ for the Cu $3d(x^2-y^2)$ hole, and not on the isotropic
Cu term $B$ from transferred spin. The Cu $3d(x^2-y^2)$ spin together with a
second spin that determines the planar O shift can explain the data. For
overdoped metallic samples, both become temperature dependent only at the
critical temperature of superconductivity, $T_\mathrm{c}$, where both begin to
decrease. However, the Cu spin component turns increasingly negative until the
second spin has disappeared. In the presence of a small pseudogap the onset
temperature of this process coincides with the onset of the temperature
dependence of the shifts, which is now above $T_\mathrm{c}$. As the pseudogap
increases further, the behavior does not change even as $T_\mathrm{c}$ begins
to decrease again. The temperature independent constant in the linear
relationship describing the negative spin is related to the size of the
uncoupled spin components and depends on the planar oxygen hole content that is
known to correlate with the maximum $T_\mathrm{c}$. The Cu spin component does
not appear to carry significant entropy as the nuclear relaxation ceases in the
condensed state.",http://arxiv.org/pdf/2309.11874v1
2309.11872v1,cs.IT,Near-Field Beam Training: Joint Angle and Range Estimation with DFT Codebook,2023-09-21 08:19:01+00:00,"Prior works on near-field beam training have mostly assumed dedicated
polar-domain codebook and on-grid range estimation, which, however, may suffer
long training overhead and degraded estimation accuracy. To address these
issues, we propose in this paper new and efficient beam training schemes with
off-grid range estimation by using conventional discrete Fourier transform
(DFT) codebook. Specifically, we first analyze the received beam pattern at the
user when far-field beamforming vectors are used for beam scanning, and show an
interesting result that this beam pattern contains useful user angle and range
information. Then, we propose two efficient schemes to jointly estimate the
user angle and range with the DFT codebook. The first scheme estimates the user
angle based on a defined angular support and resolves the user range by
leveraging an approximated angular support width, while the second scheme
estimates the user range by minimizing a power ratio mean square error (MSE) to
improve the range estimation accuracy. Finally, numerical simulations show that
our proposed schemes greatly reduce the near-field beam training overhead and
improve the range estimation accuracy as compared to various benchmark schemes.",http://arxiv.org/pdf/2309.11872v1
2309.11871v1,cs.HC,A critical review of mobile device-to-device communication,2023-09-21 08:17:42+00:00,"Since the advent of mobile devices, both end-users and the IT industry have
been longing for direct device-to-device (D2D) communication capabilities,
expecting new kinds of interactive, personalized, and collaborative services.
Fifteen years later, many D2D solutions have been implemented and deployed, but
their availability and functionality are underwhelming. Arguably, the most
widely-adopted D2D use case covers the pairing of accessories with smartphones;
however, many other use cases-such as mobile media sharing-did not progress.
Pervasive computing and cyber-physical convergence need local communication
paradigms to scale. For inherently local use cases, they are even more
appealing than ever: eschewing third-parties simultaneously fosters
environmental sustainability, privacy and network resiliency. This paper
proposes a survey on D2D communication, investigates its deployment and
adoption, with the objective of easing the creation and adoption of modern D2D
frameworks. We present the results of an online poll that estimates end-users'
utilisation of D2D processes, and review enabling technologies and security
models.",http://arxiv.org/pdf/2309.11871v1
2309.11870v1,cs.DC,Automated Probe Life-Cycle Management for Monitoring-as-a-Service,2023-09-21 08:15:33+00:00,"Cloud services must be continuously monitored to guarantee that misbehaviors
can be timely revealed, compensated, and fixed. While simple applications can
be easily monitored and controlled, monitoring non-trivial cloud systems with
dynamic behavior requires the operators to be able to rapidly adapt the set of
collected indicators. Although the currently available monitoring frameworks
are equipped with a rich set of probes to virtually collect any indicator, they
do not provide the automation capabilities required to quickly and easily
change (i.e., deploy and undeploy) the probes used to monitor a target system.
Indeed, changing the collected indicators beyond standard platform-level
indicators can be an error-prone and expensive process, which often requires
manual intervention. This paper presents a Monitoring-as-a-Service framework
that provides the capability to automatically deploy and undeploy arbitrary
probes based on a user-provided set of indicators to be collected. The
life-cycle of the probes is fully governed by the framework, including the
detection and resolution of the erroneous states at deployment time. The
framework can be used jointly with existing monitoring technologies, without
requiring the adoption of a specific probing technology. We experimented our
framework with cloud systems based on containers and virtual machines,
obtaining evidence of the efficiency and effectiveness of the proposed
solution.",http://arxiv.org/pdf/2309.11870v1
2309.11869v1,cs.CL,Syntactic Variation Across the Grammar: Modelling a Complex Adaptive System,2023-09-21 08:14:34+00:00,"While language is a complex adaptive system, most work on syntactic variation
observes a few individual constructions in isolation from the rest of the
grammar. This means that the grammar, a network which connects thousands of
structures at different levels of abstraction, is reduced to a few disconnected
variables. This paper quantifies the impact of such reductions by
systematically modelling dialectal variation across 49 local populations of
English speakers in 16 countries. We perform dialect classification with both
an entire grammar as well as with isolated nodes within the grammar in order to
characterize the syntactic differences between these dialects. The results
show, first, that many individual nodes within the grammar are subject to
variation but, in isolation, none perform as well as the grammar as a whole.
This indicates that an important part of syntactic variation consists of
interactions between different parts of the grammar. Second, the results show
that the similarity between dialects depends heavily on the sub-set of the
grammar being observed: for example, New Zealand English could be more similar
to Australian English in phrasal verbs but at the same time more similar to UK
English in dative phrases.",http://arxiv.org/pdf/2309.11869v1
2309.11867v1,cond-mat.soft,Tuning brittleness in multi-component metallic glasses through chemical disorder aging,2023-09-21 08:10:54+00:00,"Shear localization in slowly-driven bulk metallic glasses (BMGs) is typically
accompanied by a sharp drop in the bulk stress response as a signature of the
plastic yielding transition. It is also observed that the sharpness of this
elastic-plastic dynamical transition depends on the extent of local chemical
and microstructural orders, as well as the glass preparation protocol ( ie.
thermal annealing). Here, we investigate sheared multi-element BMGs in
molecular dynamics (MD) simulations, and demonstrate that glass aging,
implemented through a hybrid Monte-Carlo(MC)-MD process, sharpens the
elastic-plastic transition through a distinct crossover, seen in strain
patterns that gradually shift from diffuse features in as-quenched samples to
localized (yet system-spanning) patterns in well-annealed glasses. This effect
of glass aging on the elastic-plastic transition is found to be correlated to
the inherent interplay between aging-induced icosahedra ordering and
co-operative formation of shear transformation zones. The observed crossover is
quantified through a measure of the age-dependent susceptibility to plastic
rearrangements, exhibiting strong (anti-)correlations to local ordering
features, and the corresponding spatial correlation length grows with the aging
timescale.",http://arxiv.org/pdf/2309.11867v1
2309.11865v1,math.PR,Colored Interacting Particle Systems on the Ring: Stationary Measures from Yang-Baxter Equation,2023-09-21 08:08:13+00:00,"Recently, there has been much progress in understanding stationary measures
for colored (also called multi-species or multi-type) interacting particle
systems, motivated by asymptotic phenomena and rich underlying algebraic and
combinatorial structures (such as nonsymmetric Macdonald polynomials).
  In this paper, we present a unified approach to constructing stationary
measures for most of the known colored particle systems on the ring and the
line, including (1) the Asymmetric Simple Exclusion Process (multispecies ASEP,
or mASEP); (2) the q-deformed Totally Asymmetric Zero Range Process (TAZRP)
also known as the q-Boson particle system; (3) the q-deformed Pushing Totally
Asymmetric Simple Exclusion Process (q-PushTASEP). Our method is based on
integrable stochastic vertex models and the Yang-Baxter equation. We express
the stationary measures as partition functions of new ""queue vertex models"" on
the cylinder. The stationarity property is a direct consequence of the
Yang-Baxter equation.
  For the mASEP on the ring, a particular case of our vertex model is
equivalent to the multiline queues of Martin (arXiv:1810.10650). For the
colored q-Boson process and the q-PushTASEP on the ring, we recover and
generalize known stationary measures constructed using multiline queues or
other methods by Ayyer-Mandelshtam-Martin (arXiv:2011.06117, arXiv:2209.09859),
and Bukh-Cox (arXiv:1912.03510). Our proofs of stationarity use the Yang-Baxter
equation and bypass the Matrix Product Ansatz used for the mASEP by
Prolhac-Evans-Mallick (arXiv:0812.3293).
  On the line and in a quadrant, we use the Yang-Baxter equation to establish a
general colored Burke's theorem, which implies that suitable specializations of
our queue vertex models produce stationary measures for particle systems on the
line. We also compute the colored particle currents in stationarity.",http://arxiv.org/pdf/2309.11865v1
2309.11860v1,cs.DB,QUEST: An Efficient Query Evaluation Scheme Towards Scan-Intensive Cross-Model Analysis,2023-09-21 08:04:55+00:00,"Modern data-driven applications require that databases support fast
cross-model analytical queries. Achieving fast analytical queries in a database
system is challenging since they are usually scan-intensive (i.e., they need to
intensively scan over a large number of records) which results in huge I/O and
memory costs. And it becomes tougher when the analytical queries are
cross-model. It is hard to accelerate cross-model analytical queries in
existing databases due to the lack of appropriate storage layout and efficient
query processing techniques. In this paper, we present QUEST (QUery Evaluation
Scheme Towards scan-intensive cross-model analysis) to push scan-intensive
queries down to unified columnar storage layout and seamlessly deliver payloads
across different data models. QUEST employs a columnar data layout to unify the
representation of multi-model data. Then, a novel index structure named
Skip-Tree is developed for QUEST to enable the query evaluation more efficient.
With the help of two pairwise bitset-based operations coupled with Skip-Tree,
the scan of most irrelevant instances can be pruned so as to avoid the giant
intermediate result, thus reducing query response latency and saving the
computational resources significantly when evaluating scan-intensive
cross-model analysis. The proposed methods are implemented on an open-source
platform. Through comprehensive theoretical analysis and extensive experiments,
we demonstrate that QUEST improves the performance by 3.7x - 178.2x compared to
state-of-the-art multi-model databases when evaluating scan-intensive
cross-model analytical queries.",http://arxiv.org/pdf/2309.11860v1
2309.11852v1,cs.CL,Knowledge Sanitization of Large Language Models,2023-09-21 07:49:55+00:00,"We explore a knowledge sanitization approach to mitigate the privacy concerns
associated with large language models (LLMs). LLMs trained on a large corpus of
Web data can memorize and potentially reveal sensitive or confidential
information, raising critical security concerns. Our technique fine-tunes these
models, prompting them to generate harmless responses such as ``I don't know''
when queried about specific information. Experimental results in a closed-book
question-answering task show that our straightforward method not only minimizes
particular knowledge leakage but also preserves the overall performance of LLM.
These two advantages strengthen the defense against extraction attacks and
reduces the emission of harmful content such as hallucinations.",http://arxiv.org/pdf/2309.11852v1
2309.11851v1,cs.CV,DEYOv3: DETR with YOLO for Real-time Object Detection,2023-09-21 07:49:07+00:00,"Recently, end-to-end object detectors have gained significant attention from
the research community due to their outstanding performance. However, DETR
typically relies on supervised pretraining of the backbone on ImageNet, which
limits the practical application of DETR and the design of the backbone,
affecting the model's potential generalization ability. In this paper, we
propose a new training method called step-by-step training. Specifically, in
the first stage, the one-to-many pre-trained YOLO detector is used to
initialize the end-to-end detector. In the second stage, the backbone and
encoder are consistent with the DETR-like model, but only the detector needs to
be trained from scratch. Due to this training method, the object detector does
not need the additional dataset (ImageNet) to train the backbone, which makes
the design of the backbone more flexible and dramatically reduces the training
cost of the detector, which is helpful for the practical application of the
object detector. At the same time, compared with the DETR-like model, the
step-by-step training method can achieve higher accuracy than the traditional
training method of the DETR-like model. With the aid of this novel training
method, we propose a brand-new end-to-end real-time object detection model
called DEYOv3. DEYOv3-N achieves 41.1% on COCO val2017 and 270 FPS on T4 GPU,
while DEYOv3-L achieves 51.3% AP and 102 FPS. Without the use of additional
training data, DEYOv3 surpasses all existing real-time object detectors in
terms of both speed and accuracy. It is worth noting that for models of N, S,
and M scales, the training on the COCO dataset can be completed using a single
24GB RTX3090 GPU.",http://arxiv.org/pdf/2309.11851v1
2309.11849v1,cs.SD,A Discourse-level Multi-scale Prosodic Model for Fine-grained Emotion Analysis,2023-09-21 07:45:44+00:00,"This paper explores predicting suitable prosodic features for fine-grained
emotion analysis from the discourse-level text. To obtain fine-grained
emotional prosodic features as predictive values for our model, we extract a
phoneme-level Local Prosody Embedding sequence (LPEs) and a Global Style
Embedding as prosodic speech features from the speech with the help of a style
transfer model. We propose a Discourse-level Multi-scale text Prosodic Model
(D-MPM) that exploits multi-scale text to predict these two prosodic features.
The proposed model can be used to analyze better emotional prosodic features
and thus guide the speech synthesis model to synthesize more expressive speech.
To quantitatively evaluate the proposed model, we contribute a new and
large-scale Discourse-level Chinese Audiobook (DCA) dataset with more than
13,000 utterances annotated sequences to evaluate the proposed model.
Experimental results on the DCA dataset show that the multi-scale text
information effectively helps to predict prosodic features, and the
discourse-level text improves both the overall coherence and the user
experience. More interestingly, although we aim at the synthesis effect of the
style transfer model, the synthesized speech by the proposed text prosodic
analysis model is even better than the style transfer from the original speech
in some user evaluation indicators.",http://arxiv.org/pdf/2309.11849v1
2309.11848v1,cs.RO,TeachingBot: Robot Teacher for Human Handwriting,2023-09-21 07:45:25+00:00,"Teaching physical skills to humans requires one-on-one interaction between
the teacher and the learner. With a shortage of human teachers, such a teaching
mode faces the challenge of scaling up. Robots, with their replicable nature
and physical capabilities, offer a solution. In this work, we present
TeachingBot, a robotic system designed for teaching handwriting to human
learners. We tackle two primary challenges in this teaching task: the
adaptation to each learner's unique style and the creation of an engaging
learning experience. TeachingBot captures the learner's style using a
probabilistic learning approach based on the learner's handwriting. Then, based
on the learned style, it provides physical guidance to human learners with
variable impedance to make the learning experience engaging. Results from
human-subject experiments based on 15 human subjects support the effectiveness
of TeachingBot, demonstrating improved human learning outcomes compared to
baseline methods. Additionally, we illustrate how TeachingBot customizes its
teaching approach for individual learners, leading to enhanced overall
engagement and effectiveness.",http://arxiv.org/pdf/2309.11848v1
2309.11845v1,cs.SD,TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification,2023-09-21 07:39:08+00:00,"Audiovisual data is everywhere in this digital age, which raises higher
requirements for the deep learning models developed on them. To well handle the
information of the multi-modal data is the key to a better audiovisual modal.
We observe that these audiovisual data naturally have temporal attributes, such
as the time information for each frame in the video. More concretely, such data
is inherently multi-modal according to both audio and visual cues, which
proceed in a strict chronological order. It indicates that temporal information
is important in multi-modal acoustic event modeling for both intra- and
inter-modal. However, existing methods deal with each modal feature
independently and simply fuse them together, which neglects the mining of
temporal relation and thus leads to sub-optimal performance. With this
motivation, we propose a Temporal Multi-modal graph learning method for
Acoustic event Classification, called TMac, by modeling such temporal
information via graph learning techniques. In particular, we construct a
temporal graph for each acoustic event, dividing its audio data and video data
into multiple segments. Each segment can be considered as a node, and the
temporal relationships between nodes can be considered as timestamps on their
edges. In this case, we can smoothly capture the dynamic information in
intra-modal and inter-modal. Several experiments are conducted to demonstrate
TMac outperforms other SOTA models in performance. Our code is available at
https://github.com/MGitHubL/TMac.",http://arxiv.org/pdf/2309.11845v1
2309.11841v1,eess.SP,Semi-Supervised Variational Inference over Nonlinear Channels,2023-09-21 07:31:44+00:00,"Deep learning methods for communications over unknown nonlinear channels have
attracted considerable interest recently. In this paper, we consider
semi-supervised learning methods, which are based on variational inference, for
decoding unknown nonlinear channels. These methods, which include Monte Carlo
expectation maximization and a variational autoencoder, make efficient use of
few pilot symbols and the payload data. The best semi-supervised learning
results are achieved with a variational autoencoder. For sufficiently many
payload symbols, the variational autoencoder also has lower error rate compared
to meta learning that uses the pilot data of the present as well as previous
transmission blocks.",http://arxiv.org/pdf/2309.11841v1
2309.11836v1,cs.IT,Pre-configured Error Pattern Ordered Statistics Decoding for CRC-Polar Codes,2023-09-21 07:22:01+00:00,"In this paper, we propose a pre-configured error pattern ordered statistics
decoding (PEPOSD) algorithm and discuss its application to short cyclic
redundancy check (CRC)-polar codes. Unlike the traditional OSD that changes the
most reliable independent symbols, we regard the decoding process as testing
the error patterns, like guessing random additive noise decoding (GRAND). Also,
the pre-configurator referred from ordered reliability bits (ORB) GRAND can
better control the range and testing order of EPs. Offline-online structure can
accelerate the decoding process. Additionally, we also introduce two orders to
optimize the search order for testing EPs. Compared with CRC-aided OSD and list
decoding, PEPOSD can achieve a better trade-off between accuracy and
complexity.",http://arxiv.org/pdf/2309.11836v1
2309.11835v1,quant-ph,On the Spin Dependence of Detection Times and the Nonmeasurability of Arrival Times,2023-09-21 07:18:24+00:00,"According to a well-known principle of quantum physics, the statistics of the
outcomes of any quantum experiment are governed by a Positive Operator-Valued
Measure (POVM). In particular, for experiments designed to measure a specific
physical quantity, like the time of a particle's first arrival at a surface,
this principle establishes that if the probability distribution of that
quantity does not arise from a POVM, no such experiment exists. Such is the
case with the arrival time distributions proposed by Das and D\""urr
[arXiv:1802.07141], due to the nature of their spin dependence.",http://arxiv.org/pdf/2309.11835v1
2309.11832v1,cs.PL,"Design of Reversible Computing Systems; Large Logic, Languages, and Circuits",2023-09-21 07:13:09+00:00,"This PhD dissertation investigates garbage-free reversible computing systems
from abstract design to physical gate-level implementation. Designed in
reversible logic, we propose a ripple-block carry adder and work towards a
reversible circuit for general multiplication. At a higher-level, abstract
designs are proposed for reversible systems, such as a small von Neumann
architecture that can execute programs written in a simple reversible
two-address instruction set, a novel reversible arithmetic logic unit, and a
linear cosine transform. To aid the design of reversible logic circuits we have
designed two reversible functional hardware description languages: a
linear-typed higher-level language and a gate-level point-free combinator
language. We suggest a garbage-free design flow, where circuits are described
in the higher-level language and then translated to the combinator language,
from which methods to place-and-route of CMOS gates can be applied. We have
also made standard cell layouts of the reversible gates in complementary
pass-gate CMOS logic and used these to fabricate the ALU design. In total, this
dissertation has shown that it is possible to design non-trivial reversible
computing systems without garbage and that support from languages (computer
aided design) can make this process easier.",http://arxiv.org/pdf/2309.11832v1
2309.11831v1,astro-ph.GA,Deep Photometry of Suspected Gravitational Lensing Events: Potential Detection of a Cosmic String,2023-09-21 07:10:19+00:00,"Cosmic strings (CS) are one-dimensional cosmological-size objects predicted
in realistic models of the early Universe. Analysis of the cosmic microwave
background (CMB) anisotropy data from the Wilkinson Microwave Anisotropy Probe
(WMAP) and Planck surveys revealed several CS candidates. One of the
candidates, CSc-1, was found to be most reliable because of the statistically
significant chains of gravitational lensing (GL) candidates in its field. We
observed the brightest of the objects in the CSc-1 field, a galaxy pair
SDSSJ110429.61+233150.3. The significant correlation between the spectra of the
two components indicates the possible GL nature of the pair. Our simulations of
observational data in the CSc-1 field shows that a large number of pairs can be
explained by the complex geometry of the CS. Simulations of the SDSSJ110429
galaxy pair has shown that the observed angle between the components of the
pair can be explained if the CS is strongly inclined and, possibly, bent in the
image plane. In our preliminary data, we also detected the sign of the sharp
isophotal edge in one image, which along with CMB and spectral data strongly
suggests the possibility of a CS detection.",http://arxiv.org/pdf/2309.11831v1
2309.11830v1,cs.CL,A Chinese Prompt Attack Dataset for LLMs with Evil Content,2023-09-21 07:07:49+00:00,"Large Language Models (LLMs) present significant priority in text
understanding and generation. However, LLMs suffer from the risk of generating
harmful contents especially while being employed to applications. There are
several black-box attack methods, such as Prompt Attack, which can change the
behaviour of LLMs and induce LLMs to generate unexpected answers with harmful
contents. Researchers are interested in Prompt Attack and Defense with LLMs,
while there is no publicly available dataset to evaluate the abilities of
defending prompt attack. In this paper, we introduce a Chinese Prompt Attack
Dataset for LLMs, called CPAD. Our prompts aim to induce LLMs to generate
unexpected outputs with several carefully designed prompt attack approaches and
widely concerned attacking contents. Different from previous datasets involving
safety estimation, We construct the prompts considering three dimensions:
contents, attacking methods and goals, thus the responses can be easily
evaluated and analysed. We run several well-known Chinese LLMs on our dataset,
and the results show that our prompts are significantly harmful to LLMs, with
around 70% attack success rate. We will release CPAD to encourage further
studies on prompt attack and defense.",http://arxiv.org/pdf/2309.11830v1
2309.11828v1,math.GT,Heegaard splittings and the tight Giroux Correspondence,2023-09-21 07:00:34+00:00,"This paper presents a new proof of the Giroux Correspondence for tight
contact $3$-manifolds using techniques from Heegaard splittings and convex
surface theory. We introduce tight Heegaard splittings, which generalise the
Heegaard splittings naturally induced by an open book decomposition of a
contact manifold. Via a process called refinement, any tight Heegaard splitting
determines an open book, up to positive open book stabilisation. This allows us
to translate moves relating distinct tight Heegaard splittings into moves
relating their associated open books. We use these tools to show that every
Heegaard splitting of a contact 3-manifold may be stabilised to a splitting
associated to a supporting open book decomposition. Finally, we prove the tight
Giroux Correspondence, showing that any pair of open book decompositions
compatible with isotopic contact structures become isotopic after a sequence of
positive open book stabilisations.",http://arxiv.org/pdf/2309.11828v1
2309.11827v1,eess.AS,The Impact of Silence on Speech Anti-Spoofing,2023-09-21 06:59:22+00:00,"The current speech anti-spoofing countermeasures (CMs) show excellent
performance on specific datasets. However, removing the silence of test speech
through Voice Activity Detection (VAD) can severely degrade performance. In
this paper, the impact of silence on speech anti-spoofing is analyzed. First,
the reasons for the impact are explored, including the proportion of silence
duration and the content of silence. The proportion of silence duration in
spoof speech generated by text-to-speech (TTS) algorithms is lower than that in
bonafide speech. And the content of silence generated by different waveform
generators varies compared to bonafide speech. Then the impact of silence on
model prediction is explored. Even after retraining, the spoof speech generated
by neural network based end-to-end TTS algorithms suffers a significant rise in
error rates when the silence is removed. To demonstrate the reasons for the
impact of silence on CMs, the attention distribution of a CM is visualized
through class activation mapping (CAM). Furthermore, the implementation and
analysis of the experiments masking silence or non-silence demonstrates the
significance of the proportion of silence duration for detecting TTS and the
importance of silence content for detecting voice conversion (VC). Based on the
experimental results, improving the robustness of CMs against unknown spoofing
attacks by masking silence is also proposed. Finally, the attacks on
anti-spoofing CMs through concatenating silence, and the mitigation of VAD and
silence attack through low-pass filtering are introduced.",http://arxiv.org/pdf/2309.11827v1
2309.11826v1,cs.PL,Maximal Simplification of Polyhedral Reductions,2023-09-21 06:57:46+00:00,"Reductions combine multiple input values with an associative operator to
produce a single (or multiple) result(s). When the same input value contributes
to multiple outputs, there is an opportunity to reuse partial results, enabling
reduction simplification. Simplification produces a program with lower
asymptotic complexity. It is well known that reductions in polyhedral programs
may be simplified automatically but previous methods are incapable of
exploiting all available reuse. This paper resolves this long standing open
problem, thereby attaining minimal asymptotic complexity in the simplified
program.",http://arxiv.org/pdf/2309.11826v1
2309.11824v1,cs.CL,Word Embedding with Neural Probabilistic Prior,2023-09-21 06:54:32+00:00,"To improve word representation learning, we propose a probabilistic prior
which can be seamlessly integrated with word embedding models. Different from
previous methods, word embedding is taken as a probabilistic generative model,
and it enables us to impose a prior regularizing word representation learning.
The proposed prior not only enhances the representation of embedding vectors
but also improves the model's robustness and stability. The structure of the
proposed prior is simple and effective, and it can be easily implemented and
flexibly plugged in most existing word embedding models. Extensive experiments
show the proposed method improves word representation on various tasks.",http://arxiv.org/pdf/2309.11824v1
2309.11822v1,cond-mat.stat-mech,Quantum Annealing in Sherrington-Kirkpatrick Spin Glass in Presence of Time-Dependent Longitudinal Field,2023-09-21 06:48:03+00:00,"Motivated by the recent development of quantum technology using quantum
annealing technique and the recent works on the static properties of the
Sherrington-Kirkpatrick (SK) spin glass model, we study quantum annealing of
the spin glass model by tuning both transverse and longitudinal fields. We
solve the time-dependent Schr\""odinger equation of the total Hamiltonian when
both the fields are made time-dependent and eventually vanish at the same time.
We have computed the time-evolution of the probability of finding the system in
one of two degenerate ground states of the classical spin glass. At the end of
annealing, using the configuration averaged probability, we have shown a clear
advantage while the longitudinal field is annealed rather than keeping it
constant throughout the process of quantum annealing. We further investigate
the order parameter distribution of a quantum SK spin glass in presence of a
small longitudinal field and indicate the speeding up of the ergodicity as
compared to the zero longitudinal field case. Our speculation is that this
emergent ergodicity is responsible for the advantage in quantum annealing with
annealed longitudinal field.",http://arxiv.org/pdf/2309.11822v1
2309.11820v1,eess.IV,Automatic Endoscopic Ultrasound Station Recognition with Limited Data,2023-09-21 06:40:05+00:00,"Pancreatic cancer is a lethal form of cancer that significantly contributes
to cancer-related deaths worldwide. Early detection is essential to improve
patient prognosis and survival rates. Despite advances in medical imaging
techniques, pancreatic cancer remains a challenging disease to detect.
Endoscopic ultrasound (EUS) is the most effective diagnostic tool for detecting
pancreatic cancer. However, it requires expert interpretation of complex
ultrasound images to complete a reliable patient scan. To obtain complete
imaging of the pancreas, practitioners must learn to guide the endoscope into
multiple ""EUS stations"" (anatomical locations), which provide different views
of the pancreas. This is a difficult skill to learn, involving over 225
proctored procedures with the support of an experienced doctor. We build an
AI-assisted tool that utilizes deep learning techniques to identify these
stations of the stomach in real time during EUS procedures. This
computer-assisted diagnostic (CAD) will help train doctors more efficiently.
Historically, the challenge faced in developing such a tool has been the amount
of retrospective labeling required by trained clinicians. To solve this, we
developed an open-source user-friendly labeling web app that streamlines the
process of annotating stations during the EUS procedure with minimal effort
from the clinicians. Our research shows that employing only 43 procedures with
no hyperparameter fine-tuning obtained a balanced accuracy of 90%, comparable
to the current state of the art. In addition, we employ Grad-CAM, a
visualization technology that provides clinicians with interpretable and
explainable visualizations.",http://arxiv.org/pdf/2309.11820v1
2309.11819v1,cs.LO,The Undecidability of Third Order Pattern Matching in Calculi with Dependent Types or Type Constructors,2023-09-21 06:39:19+00:00,"We prove the undecidability of the third order pattern matching problem in
typed lambda-calculi with dependent types and in those with type constructors
by reducing the second order unification problem to them.",http://arxiv.org/pdf/2309.11819v1
2309.11815v1,quant-ph,Quantifying nonclassicality and entanglement of Gaussian states,2023-09-21 06:37:52+00:00,"Quantification of nonclassicality and entanglement in a quantum state is
crucial for quantum advantage in information processing and computation.
Robustness is one of the tractable measures for quantifying quantum resources.
Gaussian states are important in continuous variable quantum information for
their theoretically simple and experimentally easily accessible. We provide the
method of how to calculate the robustness of nonclassicality and enatnglement
for Gaussian states. The robustness of nonclassicality or entanglement is
demonstrated analytically for one-mode, two-mode Gaussain states and multimode
symmetric Gaussian states, the result shows a clear physical meaning for the
origin of nonclassicality and entanglement. For squeezed thermal states, the
nonclassicality is equal to the entanglement for the two-mode case, while they
are far apart for multimode cases.",http://arxiv.org/pdf/2309.11815v1
2309.12315v1,cs.CV,Active Stereo Without Pattern Projector,2023-09-21 17:59:56+00:00,"This paper proposes a novel framework integrating the principles of active
stereo in standard passive camera systems without a physical pattern projector.
We virtually project a pattern over the left and right images according to the
sparse measurements obtained from a depth sensor. Any such devices can be
seamlessly plugged into our framework, allowing for the deployment of a virtual
active stereo setup in any possible environment, overcoming the limitation of
pattern projectors, such as limited working range or environmental conditions.
Experiments on indoor/outdoor datasets, featuring both long and close-range,
support the seamless effectiveness of our approach, boosting the accuracy of
both stereo algorithms and deep networks.",http://arxiv.org/pdf/2309.12315v1
2309.12303v1,cs.CV,PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation,2023-09-21 17:59:02+00:00,"Panoramic videos contain richer spatial information and have attracted
tremendous amounts of attention due to their exceptional experience in some
fields such as autonomous driving and virtual reality. However, existing
datasets for video segmentation only focus on conventional planar images. To
address the challenge, in this paper, we present a panoramic video dataset,
PanoVOS. The dataset provides 150 videos with high video resolutions and
diverse motions. To quantify the domain gap between 2D planar videos and
panoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS)
models on PanoVOS. Through error analysis, we found that all of them fail to
tackle pixel-level content discontinues of panoramic videos. Thus, we present a
Panoramic Space Consistency Transformer (PSCFormer), which can effectively
utilize the semantic boundary information of the previous frame for pixel-level
matching with the current frame. Extensive experiments demonstrate that
compared with the previous SOTA models, our PSCFormer network exhibits a great
advantage in terms of segmentation results under the panoramic setting. Our
dataset poses new challenges in panoramic VOS and we hope that our PanoVOS can
advance the development of panoramic segmentation/tracking.",http://arxiv.org/pdf/2309.12303v1
2309.12297v1,math.GT,Satellite knots and immersed Heegaard Floer homology,2023-09-21 17:57:12+00:00,"We describe a new method for computing the $UV = 0$ knot Floer complex of a
satellite knot given the $UV = 0$ knot Floer complex for the companion and a
doubly pointed bordered Heegaard diagram for the pattern, showing that the
complex for the satellite can be computed from an immersed doubly pointed
Heegaard diagram obtained from the Heegaard diagram for the pattern by
overlaying the immersed curve representing the complex for the companion. This
method streamlines the usual bordered Floer method of tensoring with a bimodule
associated to the pattern by giving an immersed curve interpretation of that
pairing, and computing the module from the immersed diagram is often easier
than computing the relevant bordered bimodule. In particular, for (1,1)
patterns the resulting immersed diagram is genus one, and thus the computation
is combinatorial. For (1,1) patterns this generalizes previous work of the
first author which showed that such immersed Heegaard diagram computes the
$V=0$ knot Floer complex of the satellite. As a key technical step, which is of
independent interest, we extend the construction of a bigraded complex from a
doubly pointed Heegaard diagram and of an extended type D structure from a
torus-boundary bordered Heegaard diagram to allow Heegaard diagrams containing
an immersed alpha curve.",http://arxiv.org/pdf/2309.12297v1
2309.12291v1,cs.IT,Linearity of Gray Codes via Schur Product,2023-09-21 17:53:18+00:00,"We propose an original approach to investigate the linearity of Gray codes
obtained from $\mathbb{Z}_{2^L}$-additive codes by introducing two related
binary codes: the associated and concatenated. Once they are defined, one could
perform a straightforward analysis of the Schur product between their codewords
and determine the linearity of the respective Gray code. This work expands on
earlier contributions from the literature, where the linearity was established
with respect to the kernel of a code and/or operations on $\mathbb{Z}_{2^L}$.
The $\mathbb{Z}_{2^L}$-additive codes we apply the Gray map and check the
linearity are the well-known Hadamard, simplex, MacDonald, Kerdock, and
Preparata codes. We also present a family of Reed-Muller codes that yield to
linear Gray codes and perform a computational verification of our proposed
method applied to other $\mathbb{Z}_{2^L}$-additive codes.",http://arxiv.org/pdf/2309.12291v1
2309.12289v1,cs.RO,Real-Time Capable Decision Making for Autonomous Driving Using Reachable Sets,2023-09-21 17:52:20+00:00,"Despite large advances in recent years, real-time capable motion planning for
autonomous road vehicles remains a huge challenge. In this work, we present a
decision module that is based on set-based reachability analysis: First, we
identify all possible driving corridors by computing the reachable set for the
longitudinal position of the vehicle along the lanelets of the road network,
where lane changes are modeled as discrete events. Next, we select the best
driving corridor based on a cost function that penalizes lane changes and
deviations from a desired velocity profile. Finally, we generate a reference
trajectory inside the selected driving corridor, which can be used to guide or
warm start low-level trajectory planners. For the numerical evaluation we
combine our decision module with a motion-primitive-based and an
optimization-based planner and evaluate the performance on 2000 challenging
CommonRoad traffic scenarios as well in the realistic CARLA simulator. The
results demonstrate that our decision module is real-time capable and yields
significant speed-ups compared to executing a motion planner standalone without
a decision module.",http://arxiv.org/pdf/2309.12289v1
2309.12275v1,cs.AR,AIM: Accelerating Arbitrary-precision Integer Multiplication on Heterogeneous Reconfigurable Computing Platform Versal ACAP,2023-09-21 17:32:42+00:00,"Arbitrary-precision integer multiplication is the core kernel of many
applications in simulation, cryptography, etc. Existing acceleration of
arbitrary-precision integer multiplication includes CPUs, GPUs, FPGAs, and
ASICs. Among these accelerators, FPGAs are promised to provide both good energy
efficiency and flexibility. Surprisingly, in our implementations, FPGA has the
lowest energy efficiency, i.e., 0.29x of the CPU and 0.17x of the GPU with the
same generation fabrication. Therefore, key questions arise: Where do the
energy efficiency gains of CPUs and GPUs come from? Can reconfigurable
computing do better? If can, how to achieve that?
  We identify that the biggest energy efficiency gains of the CPUs and GPUs
come from the dedicated vector units. FPGA uses DSPs and lookup tables to
compose the needed computation, which incurs overhead when compared to using
vector units directly. New reconfigurable computing, e.g., 'FPGA+vector units'
is a novel and feasible solution to improve energy efficiency. In this paper,
we propose to map arbitrary-precision integer multiplication onto such a
heterogeneous platform, i.e., AMD/Xilinx Versal ACAP architecture. Designing on
Versal ACAP incurs several challenges and we propose AIM: Arbitrary-precision
Integer Multiplication on Versal ACAP to automate and optimize the design. AIM
framework includes design space exploration and AIM automatic code generation
to facilitate the system design and verification. We deploy the AIM framework
on three different applications, including large integer multiplication (LIM),
RSA, and Mandelbrot, on the AMD/Xilinx Versal ACAP VCK190 evaluation board. Our
experimental results show that AIM achieves up to 12.6x, and 2.1x energy
efficiency gains over the Intel Xeon Ice Lake 6346 CPU, and NVidia A5000 GPU
respectively, which brings reconfigurable computing the most energy-efficient
platform among CPUs and GPUs.",http://arxiv.org/pdf/2309.12275v1
2309.12268v1,math.DG,A new renormalized volume type invariant,2023-09-21 17:24:35+00:00,"In this paper, we define a new conformal invariant on complete non-compact
hyperbolic surfaces that can be conformally compactified to bounded domains in
$\mathbb{C}$. We study and compute this invariant up to one-connected surfaces.
Our results give a new geometric criterion for choosing canonical
representations of bounded domains in $\mathbb{C}$.",http://arxiv.org/pdf/2309.12268v1
2309.12265v1,math.CO,Cost-sharing in Parking Games,2023-09-21 17:17:00+00:00,"We introduce parking games, which are coalitional cost-sharing games in
characteristic function form derived from the total displacement of parking
functions. Motivated by the ``fair'' distribution of parking costs, our main
contribution is a polynomial-time algorithm to compute the Shapley value of
these games.",http://arxiv.org/pdf/2309.12265v1
2309.12261v1,cs.LO,Strong Call-by-Value and Multi Types,2023-09-21 17:08:12+00:00,"This paper provides foundations for strong (that is, possibly under
abstraction) call-by-value evaluation for the lambda-calculus. Recently,
Accattoli et al. proposed a form of call-by-value strong evaluation for the
lambda-calculus, the external strategy, and proved it reasonable for time.
Here, we study the external strategy using a semantical tool, namely Ehrhard's
call-by-value multi types, a variant of intersection types. We show that the
external strategy terminates exactly when a term is typable with so-called
shrinking multi types, mimicking similar results for strong call-by-name.
Additionally, the external strategy is normalizing in the untyped setting, that
is, it reaches the normal form whenever it exists.
  We also consider the call-by-extended-value approach to strong evaluation
shown reasonable for time by Biernacka et al. The two approaches turn out to
not be equivalent: terms may be externally divergent but terminating for
call-by-extended-value.",http://arxiv.org/pdf/2309.12261v1
2309.12248v1,math.CO,Enumerating combinatorial resultant decompositions of 2-connected rigidity circuits,2023-09-21 16:47:35+00:00,"A rigidity circuit (in 2D) is a minimal dependent set in the rigidity
matroid, i.e. a minimal graph supporting a non-trivial stress in any generic
placement of its vertices in $\mathbb R^2$. Any rigidity circuit on $n\geq 5$
vertices can be obtained from rigidity circuits on a fewer number of vertices
by applying the combinatorial resultant (CR) operation. The inverse operation
is called a combinatorial resultant decomposition (CR-decomp). Any rigidity
circuit on $n\geq 5$ vertices can be successively decomposed into smaller
circuits, until the complete graphs $K_4$ are reached. This sequence of
CR-decomps has the structure of a rooted binary tree called the combinatorial
resultant tree (CR-tree).
  A CR-tree encodes an elimination strategy for computing circuit polynomials
via Sylvester resultants. Different CR-trees lead to elimination strategies
that can vary greatly in time and memory consumption. It is an open problem to
establish criteria for optimal CR-trees, or at least to characterize those
CR-trees that lead to good elimination strategies. In [12] we presented an
algorithm for enumerating CR-trees where we give the algorithms for decomposing
3-connected rigidity circuits in polynomial time. In this paper we focus on
those circuits that are not 3-connected, which we simply call 2-connected.
  In order to enumerate CR-decomps of 2-connected circuits $G$, a brute force
exp-time search has to be performed among the subgraphs induced by the subsets
of $V(G)$. This exp-time bottleneck is not present in the 3-connected case. In
this paper we will argue that we do not have to account for all possible
CR-decomps of 2-connected rigidity circuits to find a good elimination
strategy; we only have to account for those CR-decomps that are a 2-split, all
of which can be enumerated in polynomial time. We present algorithms and
computational evidence in support of this heuristic.",http://arxiv.org/pdf/2309.12248v1
2309.12241v1,cs.IT,Algorithmic complexity and soficness of shifts in dimension two,2023-09-21 16:37:37+00:00,"In this manuscript we study properties of multidimensional shifts. More
precisely, we study the necessary and sufficient conditions for a shift to be
sofic, i.e. the boundary between sofic shifts and effective ones. To this end,
we use different versions of algorithmic complexity (a.k.a. Kolmogorov
complexity). In the first part of the work we suggest new necessary conditions
of soficness for multidimensional shift. These conditions are expressed in
terms of Kolmogorov complexity with bounded ressources. We discuss several
applications of this technique. In particular, we construct an example of a
two-dimensional effective and non sofic shift that has a very low combinatorial
complexity : the number of global admissible N x N patterns grows only
polynomially in N. We also show that the technique developed by S.Kass and
K.Madden is equivalent to a special case of our method. In the second part, we
discuss properties of subshifts defined in terms of density of letters. More
specifically, we study two-dimensional subshifts $S(\rho)$ in the binary
alphabet (white and black cells) where a configuration is admissible if every
pattern of size N x N contains at most $N^\rho$ black cells. We show that
$S(^\rho)$ is sofic for every $\rho<1$. Moreover, all effectif subshifts of
these shifts are also sofic. The proof of this result is principally based on
the construction of a self-simulating point-fixed tile set, with several new
ingredients: an ad hoc model of computation based on a non deterministic
cellular automaton (which allows to implement efficiently massively parallel
calculations) and some properties of flows in a specific type of planar graphs.",http://arxiv.org/pdf/2309.12241v1
2309.12235v1,math.OC,Distributed Conjugate Gradient Method via Conjugate Direction Tracking,2023-09-21 16:30:22+00:00,"We present a distributed conjugate gradient method for distributed
optimization problems, where each agent computes an optimal solution of the
problem locally without any central computation or coordination, while
communicating with its immediate, one-hop neighbors over a communication
network. Each agent updates its local problem variable using an estimate of the
average conjugate direction across the network, computed via a dynamic
consensus approach. Our algorithm enables the agents to use uncoordinated
step-sizes. We prove convergence of the local variable of each agent to the
optimal solution of the aggregate optimization problem, without requiring
decreasing step-sizes. In addition, we demonstrate the efficacy of our
algorithm in distributed state estimation problems, and its robust
counterparts, where we show its performance compared to existing distributed
first-order optimization methods.",http://arxiv.org/pdf/2309.12235v1
2309.12236v1,cs.LG,Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing,2023-09-21 16:30:22+00:00,"Calibration measures and reliability diagrams are two fundamental tools for
measuring and interpreting the calibration of probabilistic predictors.
Calibration measures quantify the degree of miscalibration, and reliability
diagrams visualize the structure of this miscalibration. However, the most
common constructions of reliability diagrams and calibration measures --
binning and ECE -- both suffer from well-known flaws (e.g. discontinuity). We
show that a simple modification fixes both constructions: first smooth the
observations using an RBF kernel, then compute the Expected Calibration Error
(ECE) of this smoothed function. We prove that with a careful choice of
bandwidth, this method yields a calibration measure that is well-behaved in the
sense of (B{\l}asiok, Gopalan, Hu, and Nakkiran 2023a) -- a consistent
calibration measure. We call this measure the SmoothECE. Moreover, the
reliability diagram obtained from this smoothed function visually encodes the
SmoothECE, just as binned reliability diagrams encode the BinnedECE.
  We also provide a Python package with simple, hyperparameter-free methods for
measuring and plotting calibration: `pip install relplot\`.",http://arxiv.org/pdf/2309.12236v1
2309.12226v1,cs.GT,Smooth Nash Equilibria: Algorithms and Complexity,2023-09-21 16:22:07+00:00,"A fundamental shortcoming of the concept of Nash equilibrium is its
computational intractability: approximating Nash equilibria in normal-form
games is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis,
we introduce a relaxed variant of Nash equilibrium called $\sigma$-smooth Nash
equilibrium, for a smoothness parameter $\sigma$. In a $\sigma$-smooth Nash
equilibrium, players only need to achieve utility at least as high as their
best deviation to a $\sigma$-smooth strategy, which is a distribution that does
not put too much mass (as parametrized by $\sigma$) on any fixed action. We
distinguish two variants of $\sigma$-smooth Nash equilibria: strong
$\sigma$-smooth Nash equilibria, in which players are required to play
$\sigma$-smooth strategies under equilibrium play, and weak $\sigma$-smooth
Nash equilibria, where there is no such requirement.
  We show that both weak and strong $\sigma$-smooth Nash equilibria have
superior computational properties to Nash equilibria: when $\sigma$ as well as
an approximation parameter $\epsilon$ and the number of players are all
constants, there is a constant-time randomized algorithm to find a weak
$\epsilon$-approximate $\sigma$-smooth Nash equilibrium in normal-form games.
In the same parameter regime, there is a polynomial-time deterministic
algorithm to find a strong $\epsilon$-approximate $\sigma$-smooth Nash
equilibrium in a normal-form game. These results stand in contrast to the
optimal algorithm for computing $\epsilon$-approximate Nash equilibria, which
cannot run in faster than quasipolynomial-time. We complement our upper bounds
by showing that when either $\sigma$ or $\epsilon$ is an inverse polynomial,
finding a weak $\epsilon$-approximate $\sigma$-smooth Nash equilibria becomes
computationally intractable.",http://arxiv.org/pdf/2309.12226v1
2309.12222v1,cond-mat.mtrl-sci,Composition-based phase stability model for multicomponent metal alloys,2023-09-21 16:20:43+00:00,"The vastness of the space of possible multicomponent metal alloys is hoped to
provide improved structural materials but also challenges traditional,
low-throughput materials design efforts. Computational screening could narrow
this search space if models for materials stability and desired properties
exist that are sufficiently inexpensive and accurate to efficiently guide
experiments. Towards this effort, here we develop a method to rapidly assess
the thermodynamic stability of a metal alloy composition of arbitrary number of
elements, stoichiometry, and temperature based on density functional theory
(DFT) data. In our model, the Gibbs free energy of the solid solution contains
binary enthalpy contributions and ideal configurational entropy, whereas only
enthalpy is considered for intermetallic competing phases. Compared to a past
model for predicting the formation of single-phase high-entropy alloys [Phys.
Rev. X 5, 011041 (2015)], our method is similarly inexpensive, since it
assesses enthalpies based on existing DFT data, but less heuristic, more
broadly applicable, and more accurate (70--75%) compared to experiment.",http://arxiv.org/pdf/2309.12222v1
2309.12214v1,cs.CV,Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?,2023-09-21 16:15:56+00:00,"Photovoltaic (PV) energy is crucial for the decarbonization of energy
systems. Due to the lack of centralized data, remote sensing of rooftop PV
installations is the best option to monitor the evolution of the rooftop PV
installed fleet at a regional scale. However, current techniques lack
reliability and are notably sensitive to shifts in the acquisition conditions.
To overcome this, we leverage the wavelet scale attribution method (WCAM),
which decomposes a model's prediction in the space-scale domain. The WCAM
enables us to assess on which scales the representation of a PV model rests and
provides insights to derive methods that improve the robustness to acquisition
conditions, thus increasing trust in deep learning systems to encourage their
use for the safe integration of clean energy in electric systems.",http://arxiv.org/pdf/2309.12214v1
2309.12212v1,cs.ET,SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices,2023-09-21 16:14:42+00:00,"Adiabatic Quantum-Flux-Parametron (AQFP) is a superconducting logic with
extremely high energy efficiency. By employing the distinct polarity of current
to denote logic `0' and `1', AQFP devices serve as excellent carriers for
binary neural network (BNN) computations. Although recent research has made
initial strides toward developing an AQFP-based BNN accelerator, several
critical challenges remain, preventing the design from being a comprehensive
solution. In this paper, we propose SupeRBNN, an AQFP-based randomized BNN
acceleration framework that leverages software-hardware co-optimization to
eventually make the AQFP devices a feasible solution for BNN acceleration.
Specifically, we investigate the randomized behavior of the AQFP devices and
analyze the impact of crossbar size on current attenuation, subsequently
formulating the current amplitude into the values suitable for use in BNN
computation. To tackle the accumulation problem and improve overall hardware
performance, we propose a stochastic computing-based accumulation module and a
clocking scheme adjustment-based circuit optimization method. We validate our
SupeRBNN framework across various datasets and network architectures, comparing
it with implementations based on different technologies, including CMOS, ReRAM,
and superconducting RSFQ/ERSFQ. Experimental results demonstrate that our
design achieves an energy efficiency of approximately 7.8x10^4 times higher
than that of the ReRAM-based BNN framework while maintaining a similar level of
model accuracy. Furthermore, when compared with superconductor-based
counterparts, our framework demonstrates at least two orders of magnitude
higher energy efficiency.",http://arxiv.org/pdf/2309.12212v1
2309.12211v1,cs.LG,Physics-informed State-space Neural Networks for Transport Phenomena,2023-09-21 16:14:36+00:00,"This work introduces Physics-informed State-space neural network Models
(PSMs), a novel solution to achieving real-time optimization, flexibility, and
fault tolerance in autonomous systems, particularly in transport-dominated
systems such as chemical, biomedical, and power plants. Traditional data-driven
methods fall short due to a lack of physical constraints like mass
conservation; PSMs address this issue by training deep neural networks with
sensor data and physics-informing using components' Partial Differential
Equations (PDEs), resulting in a physics-constrained, end-to-end differentiable
forward dynamics model. Through two in silico experiments - a heated channel
and a cooling system loop - we demonstrate that PSMs offer a more accurate
approach than purely data-driven models.
  Beyond accuracy, there are several compelling use cases for PSMs. In this
work, we showcase two: the creation of a nonlinear supervisory controller
through a sequentially updated state-space representation and the proposal of a
diagnostic algorithm using residuals from each of the PDEs. The former
demonstrates the ability of PSMs to handle both constant and time-dependent
constraints, while the latter illustrates their value in system diagnostics and
fault detection. We further posit that PSMs could serve as a foundation for
Digital Twins, constantly updated digital representations of physical systems.",http://arxiv.org/pdf/2309.12211v1
2309.12207v1,cs.LG,Boolformer: Symbolic Regression of Logic Functions with Transformers,2023-09-21 16:11:38+00:00,"In this work, we introduce Boolformer, the first Transformer architecture
trained to perform end-to-end symbolic regression of Boolean functions. First,
we show that it can predict compact formulas for complex functions which were
not seen during training, when provided a clean truth table. Then, we
demonstrate its ability to find approximate expressions when provided
incomplete and noisy observations. We evaluate the Boolformer on a broad set of
real-world binary classification datasets, demonstrating its potential as an
interpretable alternative to classic machine learning methods. Finally, we
apply it to the widespread task of modelling the dynamics of gene regulatory
networks. Using a recent benchmark, we show that Boolformer is competitive with
state-of-the art genetic algorithms with a speedup of several orders of
magnitude. Our code and models are available publicly.",http://arxiv.org/pdf/2309.12207v1
2309.12196v1,math.PR,A variational approach to free probability,2023-09-21 16:03:55+00:00,"Let $\mu$ and $\nu$ be compactly supported probability measures on the real
line with densities with respect to Lebesgue measure. We show that for all
large real $z$, if $\mu \boxplus \nu$ is their additive free convolution, we
have \begin{equation*} \int_{-\infty}^\infty \log(z - x) \mu \boxplus \nu
(\mathrm{d}x) = \sup_{\Pi} \left\{ \mathbb{E}_\Pi[\log(z - (X+Y)] -
\mathcal{E}[\Pi]+\mathcal{E}[\mu]+\mathcal{E}[\nu] \right\}, \end{equation*}
where the supremum is taken over all probability laws $\Pi$ on $\mathbb{R}^2$
for a pair of real-valued random variables $(X,Y)$ with respective marginal
laws $\mu$ and $\nu$, and given a probability law $P$ with density function $f$
on $\mathbb{R}^k$, $\mathcal{E}[P] := \int_{\mathbb{R}^k} f \log f$ is its
classical entropy. We prove similar formulas for the multiplicative free
convolution $\mu \boxtimes \nu$ and the free compression $[\mu]_\tau$ of
probability laws. The maximisers in our variational descriptions of these free
operations on measures can be computed explicitly, and from these we can then
deduce the standard $R$- and $S$-transform descriptions of additive and
multiplicative free convolution. We use our formulation to derive several new
inequalities relating free and classical convolutions of random variables, such
as \begin{equation*} \int_{-\infty}^\infty \log(z - x) \mu \boxplus \nu
(\mathrm{d}x) \geq \mathbb{E}[\log(z - (X+Y)], \end{equation*} valid for all
large $z$, where on the right-hand side $X,Y$ are independent classical random
variables with respective laws $\mu,\nu$. Our approach is based on applying a
large deviation principle on the symmetric group to the celebrated quadrature
formulas of Marcus, Spielman and Srivastava.",http://arxiv.org/pdf/2309.12196v1
2309.12188v1,cs.RO,SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on Scene Graphs,2023-09-21 15:54:33+00:00,"Object rearrangement is pivotal in robotic-environment interactions,
representing a significant capability in embodied AI. In this paper, we present
SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme
with a scene graph as the scene representation. Unlike previous methods that
rely on either known goal priors or zero-shot large models, SG-Bot exemplifies
lightweight, real-time, and user-controllable characteristics, seamlessly
blending the consideration of commonsense knowledge with automatic generation
capabilities. SG-Bot employs a three-fold procedure--observation, imagination,
and execution--to adeptly address the task. Initially, objects are discerned
and extracted from a cluttered scene during the observation. These objects are
first coarsely organized and depicted within a scene graph, guided by either
commonsense or user-defined criteria. Then, this scene graph subsequently
informs a generative model, which forms a fine-grained goal scene considering
the shape information from the initial scene and object semantics. Finally, for
execution, the initial and envisioned goal scenes are matched to formulate
robotic action policies. Experimental results demonstrate that SG-Bot
outperforms competitors by a large margin.",http://arxiv.org/pdf/2309.12188v1
2309.12184v1,physics.chem-ph,On the sensitivity of computed partial charges toward basis set and (exchange-)correlation treatment,2023-09-21 15:51:10+00:00,"Partial charges are a central concept in general chemistry and chemical
biology, yet dozens of different computational definitions exist. In prior work
[M. Cho et al., \textit{ChemPhysChem} {\bf 21}, 688-696 (2020)], we showed that
these can be reduced to at most three `principal components of ionicity'. The
present study addressed the dependance on computed partial charges $q$ on
1-particle basis set and (for WFT methods) $n$-particle correlation treatment
or (for DFT methods) exchange-correlation functional, for several
representative partial charge definitions such as QTAIM, Hirshfeld,
Hirshfeld-I, HLY (electrostatic), NPA, and APT. Our findings show that
semi-empirical double hybrids can closely approach the CCSD(T) `gold standard'
for this property. In fact, owing to an error compensation in MP2, CCSD partial
charges are further away from CCSD(T) than is MP2. The non-local correlation is
important, especially when there is a substantial amount of non-local exchange.
Employing range separation provides no clear advantage, while global hybrids
with 20-30\% Hartree-Fock exchange exhibit the best performance across all
charge types. Basis set convergence analysis shows that an augmented
triple-zeta haVTZ+d basis set is sufficient for Hirshfeld, Hirshfeld-I, HLY,
and APT methods. In contrast, QTAIM and NPA display slower basis set
convergence. It is noteworthy that for both NPA and QTAIM, HF exhibits the
slowest basis set convergence when contrasted with the correlation components
of MP2 and CCSD. Triples corrections in CCSD(T), denoted as CCSD(T)-CCSD,
exhibit even faster basis set convergence.",http://arxiv.org/pdf/2309.12184v1
2309.12183v1,cs.CV,ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average Texture and Mesh Encoding,2023-09-21 15:50:04+00:00,"In 3D human shape and pose estimation from a monocular video, models trained
with limited labeled data cannot generalize well to videos with occlusion,
which is common in the wild videos. The recent human neural rendering
approaches focusing on novel view synthesis initialized by the off-the-shelf
human shape and pose methods have the potential to correct the initial human
shape. However, the existing methods have some drawbacks such as, erroneous in
handling occlusion, sensitive to inaccurate human segmentation, and ineffective
loss computation due to the non-regularized opacity field. To address these
problems, we introduce ORTexME, an occlusion-robust temporal method that
utilizes temporal information from the input video to better regularize the
occluded body parts. While our ORTexME is based on NeRF, to determine the
reliable regions for the NeRF ray sampling, we utilize our novel average
texture learning approach to learn the average appearance of a person, and to
infer a mask based on the average texture. In addition, to guide the
opacity-field updates in NeRF to suppress blur and noise, we propose the use of
human body mesh. The quantitative evaluation demonstrates that our method
achieves significant improvement on the challenging multi-person 3DPW dataset,
where our method achieves 1.8 P-MPJPE error reduction. The SOTA rendering-based
methods fail and enlarge the error up to 5.6 on the same dataset.",http://arxiv.org/pdf/2309.12183v1
2309.12173v1,math.OC,Interpolation Constraints for Computing Worst-Case Bounds in Performance Estimation Problems,2023-09-21 15:28:28+00:00,"The Performance Estimation Problem (PEP) approach consists in computing
worst-case performance bounds on optimization algorithms by solving an
optimization problem: one maximizes an error criterion over all initial
conditions allowed and all functions in a given class of interest. The maximal
value is then a worst-case bound, and the maximizer provides an example
reaching that worst case. This approach was introduced for optimization
algorithms but could in principle be applied to many other contexts involving
worst-case bounds. The key challenge is the representation of
infinite-dimensional objects involved in these optimization problems such as
functions, and complex or non-convex objects as linear operators and their
powers, networks in decentralized optimization etc. This challenge can be
resolved by interpolation constraints, which allow representing the effect of
these objects on vectors of interest, rather than the whole object, leading to
tractable finite dimensional problems. We review several recent interpolation
results and their implications in obtaining of worst-case bounds via PEP.",http://arxiv.org/pdf/2309.12173v1
2309.12170v1,cs.HC,A Click Ahead: Real-Time Forecasting of Keyboard and Mouse Actions using RNNs and Computer Vision,2023-09-21 15:26:27+00:00,"Computer input is more complex than a sequence of single mouse clicks and
keyboard presses. We introduce a novel method to identify and represent the
user interactions and build a system which predicts - in real-time - the action
a user is most likely going to take next. For this, a recurrent neural network
(RNN) is trained on a person's usage of the computer. We demonstrate that it is
enough to train the RNN on a user's activity over approximately a week to
achieve an accuracy of 34.63 % when predicting the next action from a set of
almost 500 possible actions. Specific examples for how these predictions may be
leveraged to build tools for improving and speeding up workflows of computer
users are discussed.",http://arxiv.org/pdf/2309.12170v1
2309.12165v1,quant-ph,Analysis of the Error-Correcting Radius of a Renormalisation Decoder for Kitaev's Toric Code,2023-09-21 15:23:41+00:00,"Kitaev's toric code is arguably the most studied quantum code and is expected
to be implemented in future generations of quantum computers. The
renormalisation decoders introduced by Duclos-Cianci and Poulin exhibit one of
the best trade-offs between efficiency and speed, but one question that was
left open is how they handle worst-case or adversarial errors, i.e. what is the
order of magnitude of the smallest weight of an error pattern that will be
wrongly decoded. We initiate such a study involving a simple hard-decision and
deterministic version of a renormalisation decoder. We exhibit an uncorrectable
error pattern whose weight scales like $d^{1/2}$ and prove that the decoder
corrects all error patterns of weight less than $\frac{5}{6}
d^{\log_{2}(6/5)}$, where $d$ is the minimum distance of the toric code.",http://arxiv.org/pdf/2309.12165v1
2309.12145v1,cond-mat.mtrl-sci,A new generation of effective core potentials: selected Lanthanides and heavy elements,2023-09-21 15:04:08+00:00,"We construct correlation-consistent effective core potentials (ccECPs) for a
selected set of heavy atoms and f-elements that are of significant current
interest in materials and chemical applications, including Y, Zr, Nb, Rh, Ta,
Re, Pt, Gd, and Tb. As customary, ccECPs consist of spin-orbit averaged
relativistic effective potential (AREP) and effective spin-orbit (SO) terms.
For the AREP part, our constructions are carried out within a relativistic
coupled-cluster framework while also taking into objective function
one-particle characteristics for improved convergence in optimizations. The
transferability is adjusted using binding curves of hydride and oxide
molecules. We address the difficulties encountered with f-elements, such as the
presence of large cores and multiple near-degeneracies of excited levels. For
these elements, we construct ccECPs with core-valence partitioning that
includes 4f-subshell in the valence space. The developed ccECPs achieve an
excellent balance between accuracy, size of the valence space, and
transferability and are also suitable to be used in plane wave codes with
reasonable energy cutoffs.",http://arxiv.org/pdf/2309.12145v1
2309.12120v1,cs.SE,Sustainability indicators in an open online community,2023-09-21 14:41:41+00:00,"Software is often abandoned or shut down, for one reason or another, and
whilst research on academic open source software is sparse, there seems little
reason to assume it is any different. While some reasons may be
straightforward, e.g. a sole maintainer has moved on, or grant funding has
ceased - some projects are able to withstand these barriers and may remain
active and maintained despite adversity. This study monitored open source
projects over the period of a year, measuring common performance indicators,
using both subjective and qualitative measures (participant surveys), as well
as using scripts to analyse indicators associated with these projects' online
source control codebases. We find that these health indicators can not be used
as cross project benchmarks, due to the significant variation in context for
each project. They can, however, often be useful in signifying changes in a
single project's health, providing they are not used to compare between
different unrelated projects.",http://arxiv.org/pdf/2309.12120v1
2309.12118v1,cs.CV,Vulnerability of 3D Face Recognition Systems to Morphing Attacks,2023-09-21 14:36:10+00:00,"In recent years face recognition systems have been brought to the mainstream
due to development in hardware and software. Consistent efforts are being made
to make them better and more secure. This has also brought developments in 3D
face recognition systems at a rapid pace. These 3DFR systems are expected to
overcome certain vulnerabilities of 2DFR systems. One such problem that the
domain of 2DFR systems face is face image morphing. A substantial amount of
research is being done for generation of high quality face morphs along with
detection of attacks from these morphs. Comparatively the understanding of
vulnerability of 3DFR systems against 3D face morphs is less. But at the same
time an expectation is set from 3DFR systems to be more robust against such
attacks. This paper attempts to research and gain more information on this
matter. The paper describes a couple of methods that can be used to generate 3D
face morphs. The face morphs that are generated using this method are then
compared to the contributing faces to obtain similarity scores. The highest
MMPMR is obtained around 40% with RMMR of 41.76% when 3DFRS are attacked with
look-a-like morphs.",http://arxiv.org/pdf/2309.12118v1
2309.12112v1,cs.LO,Confluence Criteria for Logically Constrained Rewrite Systems (Full Version),2023-09-21 14:30:14+00:00,"Numerous confluence criteria for plain term rewrite systems are known. For
logically constrained rewrite system, an attractive extension of term rewriting
in which rules are equipped with logical constraints, much less is known. In
this paper we extend the strongly-closed and (almost) parallel-closed critical
pair criteria of Huet and Toyama to the logically constrained setting. We
discuss the challenges for automation and present crest, a new tool for
logically constrained rewriting in which the confluence criteria are
implemented, together with experimental data.",http://arxiv.org/pdf/2309.12112v1
2309.12110v1,cs.CV,Exploiting CLIP-based Multi-modal Approach for Artwork Classification and Retrieval,2023-09-21 14:29:44+00:00,"Given the recent advances in multimodal image pretraining where visual models
trained with semantically dense textual supervision tend to have better
generalization capabilities than those trained using categorical attributes or
through unsupervised techniques, in this work we investigate how recent CLIP
model can be applied in several tasks in artwork domain. We perform exhaustive
experiments on the NoisyArt dataset which is a dataset of artwork images
crawled from public resources on the web. On such dataset CLIP achieves
impressive results on (zero-shot) classification and promising results in both
artwork-to-artwork and description-to-artwork domain.",http://arxiv.org/pdf/2309.12110v1
2309.12108v1,math.NA,Wavelet-based Edge Multiscale Finite Element Methods for Singularly Perturbed Convection-Diffusion Equations,2023-09-21 14:27:57+00:00,"We propose a novel efficient and robust Wavelet-based Edge Multiscale Finite
Element Method (WEMsFEM) motivated by \cite{MR3980476,GL18} to solve the
singularly perturbed convection-diffusion equations. The main idea is to first
establish a local splitting of the solution over a local region by a local
bubble part and local Harmonic extension part, and then derive a global
splitting by means of Partition of Unity. This facilitates a representation of
the solution as a summation of a global bubble part and a global Harmonic
extension part, where the first part can be computed locally in parallel. To
approximate the second part, we construct an edge multiscale ansatz space
locally with hierarchical bases as the local boundary data that has a
guaranteed approximation rate without higher regularity requirement on the
solution. The key innovation of this proposed WEMsFEM lies in a provable
convergence rate with little restriction on the mesh size or the regularity of
the solution. Its convergence rate with respect to the computational degree of
freedom is rigorously analyzed, which is verified by extensive 2-d and 3-d
numerical tests.",http://arxiv.org/pdf/2309.12108v1
2309.12095v1,stat.ML,Bayesian sparsification for deep neural networks with Bayesian model reduction,2023-09-21 14:10:47+00:00,"Deep learning's immense capabilities are often constrained by the complexity
of its models, leading to an increasing demand for effective sparsification
techniques. Bayesian sparsification for deep learning emerges as a crucial
approach, facilitating the design of models that are both computationally
efficient and competitive in terms of performance across various deep learning
applications. The state-of-the-art -- in Bayesian sparsification of deep neural
networks -- combines structural shrinkage priors on model weights with an
approximate inference scheme based on black-box stochastic variational
inference. However, model inversion of the full generative model is
exceptionally computationally demanding, especially when compared to standard
deep learning of point estimates. In this context, we advocate for the use of
Bayesian model reduction (BMR) as a more efficient alternative for pruning of
model weights. As a generalization of the Savage-Dickey ratio, BMR allows a
post-hoc elimination of redundant model weights based on the posterior
estimates under a straightforward (non-hierarchical) generative model. Our
comparative study highlights the computational efficiency and the pruning rate
of the BMR method relative to the established stochastic variational inference
(SVI) scheme, when applied to the full hierarchical generative model. We
illustrate the potential of BMR to prune model parameters across various deep
learning architectures, from classical networks like LeNet to modern frameworks
such as Vision Transformers and MLP-Mixers.",http://arxiv.org/pdf/2309.12095v1
2309.12090v1,cs.CV,Multi-Task Cooperative Learning via Searching for Flat Minima,2023-09-21 14:00:11+00:00,"Multi-task learning (MTL) has shown great potential in medical image
analysis, improving the generalizability of the learned features and the
performance in individual tasks. However, most of the work on MTL focuses on
either architecture design or gradient manipulation, while in both scenarios,
features are learned in a competitive manner. In this work, we propose to
formulate MTL as a multi/bi-level optimization problem, and therefore force
features to learn from each task in a cooperative approach. Specifically, we
update the sub-model for each task alternatively taking advantage of the
learned sub-models of the other tasks. To alleviate the negative transfer
problem during the optimization, we search for flat minima for the current
objective function with regard to features from other tasks. To demonstrate the
effectiveness of the proposed approach, we validate our method on three
publicly available datasets. The proposed method shows the advantage of
cooperative learning, and yields promising results when compared with the
state-of-the-art MTL approaches. The code will be available online.",http://arxiv.org/pdf/2309.12090v1
2309.12088v1,physics.chem-ph,A High-Throughput Steered Molecular Dynamics Study on the Free Energy Profile of Ion Permeation through Gramicidin A,2023-09-21 13:58:14+00:00,"Steered molecular dynamics (SMD) simulations for the calculation of free
energies are well suited for high-throughput molecular simulations on a
distributed infrastructure due to the simplicity of the setup and parallel
granularity of the runs. However, so far, the computational cost limited the
estimation of the free energy typically over just a few pullings, thus impeding
the evaluation of statistical uncertainties involved. In this work, we
performed two thousand pulls for the permeation of a potassium ion in the
gramicidin A pore by all-atom molecular dynamics in order to assess the
bidirectional SMD protocol with a proper amount of sampling. The estimated free
energy profile still shows a statistical error of several kcal/mol, while the
work distributions are estimated to be non-Gaussian at pulling speeds of 10
{\AA}/ns. We discuss the methodology and the confidence intervals in relation
to increasing amounts of computed trajectories and how different permeation
pathways for the potassium ion, knock-on and sideways, affect the sampling and
the free energy estimation.",http://arxiv.org/pdf/2309.12088v1
2309.12072v1,astro-ph.EP,On the origin of accretion bursts in FUORs,2023-09-21 13:40:31+00:00,"Accretion luminosity of young star FU Ori increased from undetectable levels
to hundreds of Solar luminosities in 1937 and remains nearly as high at the
present time. In a recent paper we showed how Extreme Evaporation (EE) of a
young gas giant planet that migrated to a 10 day orbit around the star may
power FU Ori. However, our model assumed a power-law mass-radius relation for
the evaporating planet. Here we employ a stellar evolution code to model mass
losing planets. We find that adiabatic planets expand rapidly, which results in
runaway FUOR bursts. Super-adiabatic planets contract while losing mass; their
outbursts are dimming with time. Long steadily declining bursts such as FU Ori
require relatively fine tuned internal planetary structure, which may be rare.
More commonly we find that super-adiabatic planets contract too rapidly and
their EE falters, leading to FUOR burst stutter. This stutter allows a single
planet to produce many short repeating bursts, which may be relevant to bursts
observed in V346 Nor, V899, V1647. We compute broad band spectra of our best
fitting scenario for FU Ori. Since the outburst is triggered behind the planet
location, the mid-IR emission rises many months before the optical, similar to
bursts in Gaia-17bpi and Gaia-18dvy. We show that in outbursts powered by the
classic thermal instability, mid-IR lags the optical, whereas the dead zone
activation models predict mid-IR light precede the optical burst by many years
to decades. We comment on the stellar flyby scenario for FU Ori.",http://arxiv.org/pdf/2309.12072v1
2309.12070v1,cs.AR,"High-Conductance, Ohmic-like HfZrO$_4$ Ferroelectric Memristor",2023-09-21 13:39:26+00:00,"The persistent and switchable polarization of ferroelectric materials based
on HfO$_2$-based ferroelectric compounds, compatible with large-scale
integration, are attractive synaptic elements for neuromorphic computing. To
achieve a record current density of 0.01 A/cm$^2$ (at a read voltage of 80 mV)
as well as ideal memristive behavior (linear current-voltage relation and
analog resistive switching), devices based on an ultra-thin (2.7 nm thick),
polycrystalline HfZrO$_4$ ferroelectric layer are fabricated by Atomic Layer
Deposition. The use of a semiconducting oxide interlayer (WO$_{x<3}$) at one of
the interfaces, induces an asymmetric energy profile upon ferroelectric
polarization reversal and thus the long-term potentiation / depression
(conductance increase / decrease) of interest. Moreover, it favors the stable
retention of both the low and the high resistive states. Thanks to the low
operating voltage (<3.5 V), programming requires less than 10${^-12}$ J for 20
ns long pulses. Remarkably, the memristors show no wake-up or fatigue effect.",http://arxiv.org/pdf/2309.12070v1
2309.12064v1,physics.optics,How many parameters do we need to obtain a BIC in symmetric and asymmetric structures?,2023-09-21 13:34:18+00:00,"Photonic bound states in the continuum (BICs) are nonradiating eigenmodes of
structures with open scattering channels. Most often, BICs are studied in
highly symmetric structures with one open scattering channel. In this simplest
case, the so-called symmetry-protected BICs can be found by tuning a single
parameter, which is the light frequency. Another kind of BICs -- accidental
BICs -- can be obtained by tuning two parameters. For more complex structures
lacking certain symmetries or having several open scattering channels, more
than two parameters might be required. In the present work, we propose an
algebraic approach for computing the number of parameters required to obtain a
BIC by expressing it through the dimension of the solution set of certain
algebraic equations. Computing this dimension allows us to relate the required
number of parameters with the number of open scattering channels without
solving Maxwell's equations. We show that different relations take place when
the scattering matrices describing the system are symmetric or asymmetric. The
obtained theoretical results are confirmed by the results of rigorous
electromagnetic simulations.",http://arxiv.org/pdf/2309.12064v1
2309.12042v1,cs.CV,Beyond Image Borders: Learning Feature Extrapolation for Unbounded Image Composition,2023-09-21 13:10:28+00:00,"For improving image composition and aesthetic quality, most existing methods
modulate the captured images by striking out redundant content near the image
borders. However, such image cropping methods are limited in the range of image
views. Some methods have been suggested to extrapolate the images and predict
cropping boxes from the extrapolated image. Nonetheless, the synthesized
extrapolated regions may be included in the cropped image, making the image
composition result not real and potentially with degraded image quality. In
this paper, we circumvent this issue by presenting a joint framework for both
unbounded recommendation of camera view and image composition (i.e., UNIC). In
this way, the cropped image is a sub-image of the image acquired by the
predicted camera view, and thus can be guaranteed to be real and consistent in
image quality. Specifically, our framework takes the current camera preview
frame as input and provides a recommendation for view adjustment, which
contains operations unlimited by the image borders, such as zooming in or out
and camera movement. To improve the prediction accuracy of view adjustment
prediction, we further extend the field of view by feature extrapolation. After
one or several times of view adjustments, our method converges and results in
both a camera view and a bounding box showing the image composition
recommendation. Extensive experiments are conducted on the datasets constructed
upon existing image cropping datasets, showing the effectiveness of our UNIC in
unbounded recommendation of camera view and image composition. The source code,
dataset, and pretrained models is available at
https://github.com/liuxiaoyu1104/UNIC.",http://arxiv.org/pdf/2309.12042v1
2309.12035v1,cs.CV,BASE: Probably a Better Approach to Multi-Object Tracking,2023-09-21 12:58:21+00:00,"The field of visual object tracking is dominated by methods that combine
simple tracking algorithms and ad hoc schemes. Probabilistic tracking
algorithms, which are leading in other fields, are surprisingly absent from the
leaderboards. We found that accounting for distance in target kinematics,
exploiting detector confidence and modelling non-uniform clutter
characteristics is critical for a probabilistic tracker to work in visual
tracking. Previous probabilistic methods fail to address most or all these
aspects, which we believe is why they fall so far behind current
state-of-the-art (SOTA) methods (there are no probabilistic trackers in the
MOT17 top 100). To rekindle progress among probabilistic approaches, we propose
a set of pragmatic models addressing these challenges, and demonstrate how they
can be incorporated into a probabilistic framework. We present BASE (Bayesian
Approximation Single-hypothesis Estimator), a simple, performant and easily
extendible visual tracker, achieving state-of-the-art (SOTA) on MOT17 and
MOT20, without using Re-Id. Code will be made available at
https://github.com/ffi-no",http://arxiv.org/pdf/2309.12035v1
2309.12033v1,cs.CV,Face Identity-Aware Disentanglement in StyleGAN,2023-09-21 12:54:09+00:00,"Conditional GANs are frequently used for manipulating the attributes of face
images, such as expression, hairstyle, pose, or age. Even though the
state-of-the-art models successfully modify the requested attributes, they
simultaneously modify other important characteristics of the image, such as a
person's identity. In this paper, we focus on solving this problem by
introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles
face attributes from a person's identity. Our key idea is to perform training
on images retrieved from movie frames, where a given person appears in various
poses and with different attributes. By applying a type of contrastive loss, we
encourage the model to group images of the same person in similar regions of
latent space. Our experiments demonstrate that the modifications of face
attributes performed by PluGeN4Faces are significantly less invasive on the
remaining characteristics of the image than in the existing state-of-the-art
models.",http://arxiv.org/pdf/2309.12033v1
2309.12025v1,cs.DS,Robust Approximation Algorithms for Non-monotone $k$-Submodular Maximization under a Knapsack Constraint,2023-09-21 12:42:52+00:00,"The problem of non-monotone $k$-submodular maximization under a knapsack
constraint ($\kSMK$) over the ground set size $n$ has been raised in many
applications in machine learning, such as data summarization, information
propagation, etc. However, existing algorithms for the problem are facing
questioning of how to overcome the non-monotone case and how to fast return a
good solution in case of the big size of data. This paper introduces two
deterministic approximation algorithms for the problem that competitively
improve the query complexity of existing algorithms.
  Our first algorithm, $\LAA$, returns an approximation ratio of $1/19$ within
$O(nk)$ query complexity. The second one, $\RLA$, improves the approximation
ratio to $1/5-\epsilon$ in $O(nk)$ queries, where $\epsilon$ is an input
parameter.
  Our algorithms are the first ones that provide constant approximation ratios
within only $O(nk)$ query complexity for the non-monotone objective. They,
therefore, need fewer the number of queries than state-of-the-the-art ones by a
factor of $\Omega(\log n)$.
  Besides the theoretical analysis, we have evaluated our proposed ones with
several experiments in some instances: Influence Maximization and Sensor
Placement for the problem. The results confirm that our algorithms ensure
theoretical quality as the cutting-edge techniques and significantly reduce the
number of queries.",http://arxiv.org/pdf/2309.12025v1
2309.12021v1,hep-th,More Fermionic Supersymmetric Wilson loops in Four Dimensions,2023-09-21 12:39:21+00:00,"We construct supersymmetric fermionic Wilson loops along general curves in
four-dimensional $\mathcal{N}=4$ super Yang-Mills theory and along general
planar curves in $\mathcal{N}=2$ superconformal $SU(N)\times SU(N)$ quiver
theory. These loops are generalizations of the Zarembo loops and are
cohomologically equivalent to them. In $\mathcal{N}=4$ super Yang-Mills theory,
we compute their expectation values and verify the cohomological equivalence
relation up to the order $g^4$ in perturbation theory.",http://arxiv.org/pdf/2309.12021v1
2309.12019v1,math.NA,Dissipative WENO stabilization of high-order discontinuous Galerkin methods for hyperbolic problems,2023-09-21 12:38:39+00:00,"We present a new approach to stabilizing high-order Runge-Kutta discontinuous
Galerkin (RKDG) schemes using weighted essentially non-oscillatory (WENO)
reconstructions in the context of hyperbolic conservation laws. In contrast to
RKDG schemes that overwrite finite element solutions with WENO reconstructions,
our approach employs the reconstruction-based smoothness sensor presented by
Kuzmin and Vedral (J. Comput. Phys. 487:112153, 2023) to control the amount of
added numerical dissipation. Incorporating a dissipation-based WENO
stabilization term into a discontinuous Galerkin (DG) discretization, the
proposed methodology achieves high-order accuracy while effectively capturing
discontinuities in the solution. As such, our approach offers an attractive
alternative to WENO-based slope limiters for DG schemes. The reconstruction
procedure that we use performs Hermite interpolation on stencils composed of a
mesh cell and its neighboring cells. The amount of numerical dissipation is
determined by the relative differences between the partial derivatives of
reconstructed candidate polynomials and those of the underlying finite element
approximation. The employed smoothness sensor takes all derivatives into
account to properly assess the local smoothness of a high-order DG solution.
Numerical experiments demonstrate the ability of our scheme to capture
discontinuities sharply. Optimal convergence rates are obtained for all
polynomial degrees.",http://arxiv.org/pdf/2309.12019v1
2309.12017v1,physics.atom-ph,Reconstructing Lattice Vibrations of Crystals with Electron Ptychography,2023-09-21 12:37:25+00:00,"While capable of imaging the atoms constituting thin slabs of material, the
achievable resolution of conventional electron imaging techniques in a
transmission electron microscope (TEM) is very sensitive to the partial spatial
coherence of the electron source, lens aberrations and mechanical instabilities
of the microscope. The desire to break free from the limitations of the
apparatus spurred the popularity of ptychography, a computational phase
retrieval technique that, to some extent, can compensate for the imperfections
of the equipment. Recently it was shown that ptychography is capable of
resolving specimen features as fine as the blurring due to the vibrations of
atoms, a limit defined not by the microscope, but by the investigated sample
itself. Here we report on the successful application of a mixed-object
formalism in the ptychographic reconstruction that enables the resolution of
fluctuations in atomic positions within real space. We show a reconstruction of
a symmetric {\Sigma}9 grain boundary in silicon from realistically (molecular
dynamics) simulated data. By reconstructing the object as an ensemble of 10
different states we were able to observe movements of atoms in the range of
0.1-0.2 \AA in agreement with the expectation. This is a significant step
forward in the field of electron ptychography, as it enables the study of
dynamic systems with unprecedented precision and overcomes the resolution limit
so far considered to be imposed by the thermal motion of the atoms.",http://arxiv.org/pdf/2309.12017v1
2309.11994v1,cs.NE,Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation Model for Expensive Optimization,2023-09-21 12:09:55+00:00,"Surrogate-assisted evolutionary algorithms (SAEAs) hold significant
importance in resolving expensive optimization problems~(EOPs). Extensive
efforts have been devoted to improving the efficacy of SAEAs through the
development of proficient model-assisted selection methods. However, generating
high-quality solutions is a prerequisite for selection. The fundamental
paradigm of evaluating a limited number of solutions in each generation within
SAEAs reduces the variance of adjacent populations, thus impacting the quality
of offspring solutions. This is a frequently encountered issue, yet it has not
gained widespread attention. This paper presents a framework using unevaluated
solutions to enhance the efficiency of SAEAs. The surrogate model is employed
to identify high-quality solutions for direct generation of new solutions
without evaluation. To ensure dependable selection, we have introduced two
tailored relation models for the selection of the optimal solution and the
unevaluated population. A comprehensive experimental analysis is performed on
two test suites, which showcases the superiority of the relation model over
regression and classification models in the selection phase. Furthermore, the
surrogate-selected unevaluated solutions with high potential have been shown to
significantly enhance the efficiency of the algorithm.",http://arxiv.org/pdf/2309.11994v1
2309.11993v1,cs.GR,Neural Stochastic Screened Poisson Reconstruction,2023-09-21 12:04:15+00:00,"Reconstructing a surface from a point cloud is an underdetermined problem. We
use a neural network to study and quantify this reconstruction uncertainty
under a Poisson smoothness prior. Our algorithm addresses the main limitations
of existing work and can be fully integrated into the 3D scanning pipeline,
from obtaining an initial reconstruction to deciding on the next best sensor
position and updating the reconstruction upon capturing more data.",http://arxiv.org/pdf/2309.11993v1
2309.11986v1,cs.CV,ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers,2023-09-21 11:53:01+00:00,"As robotic systems increasingly encounter complex and unconstrained
real-world scenarios, there is a demand to recognize diverse objects. The
state-of-the-art 6D object pose estimation methods rely on object-specific
training and therefore do not generalize to unseen objects. Recent novel object
pose estimation methods are solving this issue using task-specific fine-tuned
CNNs for deep template matching. This adaptation for pose estimation still
requires expensive data rendering and training procedures. MegaPose for example
is trained on a dataset consisting of two million images showing 20,000
different objects to reach such generalization capabilities. To overcome this
shortcoming we introduce ZS6D, for zero-shot novel object 6D pose estimation.
Visual descriptors, extracted using pre-trained Vision Transformers (ViT), are
used for matching rendered templates against query images of objects and for
establishing local correspondences. These local correspondences enable deriving
geometric correspondences and are used for estimating the object's 6D pose with
RANSAC-based PnP. This approach showcases that the image descriptors extracted
by pre-trained ViTs are well-suited to achieve a notable improvement over two
state-of-the-art novel object 6D pose estimation methods, without the need for
task-specific fine-tuning. Experiments are performed on LMO, YCBV, and TLESS.
In comparison to one of the two methods we improve the Average Recall on all
three datasets and compared to the second method we improve on two datasets.",http://arxiv.org/pdf/2309.11986v1
2309.11972v1,cs.DC,"Generalize Synchronization Mechanism: Specification, Properties, Limits",2023-09-21 11:08:02+00:00,"Shared resources synchronization is a well studied problem, in both shared
memory environment or distributed memory environment. Many synchronization
mechanisms are proposed, with their own way to reach certain consistency level.
This thesis further found that there is no perfect synchronization mechanism.
Each of them has its properties at different level. For example, to enforce
strong consistency, writers may loose writing freedom or it would take more
time to coordinate. This thesis proposes a framework to generalize all
synchronization mechanism in a formal way for better reasoning on properties,
from the perspective of multi-writer to single-writer convergence. Therefore,
limitations prevent a synchronization mechanism from achieving every property
at its optimal level. CAP and ROLL were proposed in previous works to explain
such. CAP theorem states that it can only achieve two of Consistency,
Availability and Partition tolerance properties. ROLL Theorem uses a framework
to model leaderless SMR protocol and states quorum size and fault tolerance are
trading off. The thesis covers five properties in a more understandable way and
improves existed mechanisms by analyzing with the framework.",http://arxiv.org/pdf/2309.11972v1
2309.11969v1,cs.NI,A survey of trends and motivations regarding Communication Service Providers' metro area network implementations,2023-09-21 10:51:50+00:00,"Relevance of research on telecommunications networks is predicated upon the
implementations which it explicitly claims or implicitly subsumes. This paper
supports researchers through a survey of Communications Service Providers
current implementations within the metro area, and trends that are expected to
shape the next-generation metro area network. The survey is composed of a
quantitative component, complemented by a qualitative component carried out
among field experts. Among the several findings, it has been found that service
providers with large subscriber base sizes, are less agile in their response to
technological change than those with smaller subscriber base sizes: thus,
copper media are still an important component in the set of access network
technologies. On the other hand, service providers with large subscriber base
sizes are strongly committed to deploying distributed access architectures,
notably using remote access nodes like remote OLT and remote MAC-PHY. This
study also shows that the extent of remote node deployment for multi-access
edge computing is about the same as remote node deployment for distributed
access architectures, indicating that these two aspects of metro area networks
are likely to be co-deployed.",http://arxiv.org/pdf/2309.11969v1
2309.11966v1,cs.CV,NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields,2023-09-21 10:47:29+00:00,"We present NeuralLabeling, a labeling approach and toolset for annotating a
scene using either bounding boxes or meshes and generating segmentation masks,
affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth
maps and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as
renderer, allowing labeling to be performed using 3D spatial tools while
incorporating geometric clues such as occlusions, relying only on images
captured from multiple viewpoints as input. To demonstrate the applicability of
NeuralLabeling to a practical problem in robotics, we added ground truth depth
maps to 30000 frames of transparent object RGB and noisy depth maps of glasses
placed in a dishwasher captured using an RGBD sensor, yielding the
Dishwasher30k dataset. We show that training a simple deep neural network with
supervision using the annotated depth maps yields a higher reconstruction
performance than training with the previously applied weakly supervised
approach.",http://arxiv.org/pdf/2309.11966v1
2309.11962v1,cs.CV,Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views,2023-09-21 10:34:35+00:00,"We present Ego3DPose, a highly accurate binocular egocentric 3D pose
reconstruction system. The binocular egocentric setup offers practicality and
usefulness in various applications, however, it remains largely under-explored.
It has been suffering from low pose estimation accuracy due to viewing
distortion, severe self-occlusion, and limited field-of-view of the joints in
egocentric 2D images. Here, we notice that two important 3D cues, stereo
correspondences, and perspective, contained in the egocentric binocular input
are neglected. Current methods heavily rely on 2D image features, implicitly
learning 3D information, which introduces biases towards commonly observed
motions and leads to low overall accuracy. We observe that they not only fail
in challenging occlusion cases but also in estimating visible joint positions.
To address these challenges, we propose two novel approaches. First, we design
a two-path network architecture with a path that estimates pose per limb
independently with its binocular heatmaps. Without full-body information
provided, it alleviates bias toward trained full-body distribution. Second, we
leverage the egocentric view of body limbs, which exhibits strong perspective
variance (e.g., a significantly large-size hand when it is close to the
camera). We propose a new perspective-aware representation using trigonometry,
enabling the network to estimate the 3D orientation of limbs. Finally, we
develop an end-to-end pose reconstruction network that synergizes both
techniques. Our comprehensive evaluations demonstrate that Ego3DPose
outperforms state-of-the-art models by a pose estimation error (i.e., MPJPE)
reduction of 23.1% in the UnrealEgo dataset. Our qualitative results highlight
the superiority of our approach across a range of scenarios and challenges.",http://arxiv.org/pdf/2309.11962v1
2309.11959v1,cs.SE,A Multi-faceted Analysis of the Performance Variability of Virtual Machines,2023-09-21 10:25:14+00:00,"Cloud computing and virtualization solutions allow one to rent the virtual
machines (VMs) needed to run applications on a pay-per-use basis, but rented
VMs do not offer any guarantee on their performance. Cloud platforms are known
to be affected by performance variability, but a better understanding is still
required. This paper moves in that direction and presents an in-depth,
multi-faceted study on the performance variability of VMs. Unlike previous
studies, our assessment covers a wide range of factors: 16 VM types from 4
well-known cloud providers, 10 benchmarks, and 28 different metrics. We present
four new contributions. First, we introduce a new benchmark suite (VMBS) that
let researchers and practitioners systematically collect a diverse set of
performance data. Second, we present a new indicator, called Variability
Indicator, that allows for measuring variability in the performance of VMs.
Third, we illustrate an analysis of the collected data across four different
dimensions: resources, isolation, time, and cost. Fourth, we present multiple
predictive models based on Machine Learning that aim to forecast future
performance and detect time patterns. Our experiments provide important
insights on the resource variability of VMs, highlighting differences and
similarities between various cloud providers. To the best of our knowledge,
this is the widest analysis ever conducted on the topic.",http://arxiv.org/pdf/2309.11959v1
2309.11947v1,quant-ph,Weak Schur sampling with logarithmic quantum memory,2023-09-21 10:02:46+00:00,"The quantum Schur transform maps the computational basis of a system of $n$
qudits onto a \textit{Schur basis}, which spans the minimal invariant subspaces
of the representations of the unitary and the symmetric groups acting on the
state space of $n$ $d$-level systems. We introduce a new algorithm for the task
of weak Schur sampling. Our algorithm efficiently determines both the Young
label which indexes the irreducible representations and the multiplicity label
of the symmetric group. There are two major advantages of our algorithm for
weak Schur sampling when compared to existing approaches which proceed via
quantum Schur transform algorithm or Generalized Phase Estimation algorithm.
First, our algorihtm is suitable for streaming applications and second it is
exponentially more efficient in its memory usage. We show that an instance of
our weak Schur sampling algorithm on $n$ qubits to accuracy $\epsilon$ requires
only $O(\log_2n)$ qubits of memory and $O(n^3\log_2(\frac{n}{\epsilon}))$ gates
from the Clifford+T set. Further, we show that our weak Schur sampling
algorithm on $n$ qudits decomposes into
$O\big(dn^{2d}\log_2^p\big(\frac{n^{2d}}{\epsilon}\big)\big)$ gates from an
arbitrary fault-tolerant qudit universal set, for $p\approx 4$, and requires a
memory of $O(\log_dn)$ qudits to implement.",http://arxiv.org/pdf/2309.11947v1
2309.11945v1,cond-mat.mtrl-sci,Magnetic order in the computational 2D materials database (C2DB) from high throughput spin spiral calculations,2023-09-21 10:00:12+00:00,"We report a detailed investigation of the magnetic order in 192 stable
magnetic two-dimensional materials from the Computational 2D Materials Database
having one magnetic atom in the unit cell. The calculations are based on a
systematic workflow that employs spin spiral calculations and yields the
magnetic order in terms of a two-dimensional ordering vector $\mathbf{Q}$. We
then include spin-orbit coupling to extract the easy and hard axes for
collinear structures and the orientation of spiral planes in non-collinear
structures. Finally, for all predicted ferromagnets we compute the
Dzyaloshinskii-Moriya interactions and determine whether or not these are
strong enough to overcome the magnetic anisotropy and stabilise a chiral spin
spiral ground state. These steps completely determines the ground state order
within the spiralling ansatz. We find 58 ferromagnets, 21 collinear
anti-ferromagnets, and 85 non-collinear ground states of which 15 are chiral
spin spirals driven by Dzyaloshinskii-Moriya interactions. The results show
that non-collinear order is in fact as common as collinear order in these
materials and emphasise the need for detailed investigation of the magnetic
ground state when reporting magnetic properties of new materials. Furthermore,
non-collinear order typically breaks symmetries inherent to the lattice and may
give rise to emergent properties such as multiferroicity, magnetoelectricity or
second order optical effects that would be predicted as absent based on a
collinear assumption.",http://arxiv.org/pdf/2309.11945v1
2309.11944v1,eess.SY,Reachability Analysis of ARMAX Models,2023-09-21 10:00:07+00:00,"Reachability analysis is a powerful tool for computing the set of states or
outputs reachable for a system. While previous work has focused on systems
described by state-space models, we present the first methods to compute
reachable sets of ARMAX models - one of the most common input-output models
originating from data-driven system identification. The first approach we
propose can only be used with dependency-preserving set representations such as
symbolic zonotopes, while the second one is valid for arbitrary set
representations but relies on a reformulation of the ARMAX model. By analyzing
the computational complexities, we show that both approaches scale
quadratically with respect to the time horizon of the reachability problem when
using symbolic zonotopes. To reduce the computational complexity, we propose a
third approach that scales linearly with respect to the time horizon when using
set representations that are closed under Minkowski addition and linear
transformation and that satisfy that the computational complexity of the
Minkowski sum is independent of the representation size of the operands. Our
numerical experiments demonstrate that the reachable sets of ARMAX models are
tighter than the reachable sets of equivalent state space models in case of
unknown initial states. Therefore, this methodology has the potential to
significantly reduce the conservatism of various verification techniques.",http://arxiv.org/pdf/2309.11944v1
2309.11930v1,cs.LG,Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning,2023-09-21 09:44:39+00:00,"In open-world semi-supervised learning, a machine learning model is tasked
with uncovering novel categories from unlabeled data while maintaining
performance on seen categories from labeled data. The central challenge is the
substantial learning gap between seen and novel categories, as the model learns
the former faster due to accurate supervisory information. To address this, we
introduce 1) an adaptive margin loss based on estimated class distribution,
which encourages a large negative margin for samples in seen classes, to
synchronize learning paces, and 2) pseudo-label contrastive clustering, which
pulls together samples which are likely from the same class in the output
space, to enhance novel class discovery. Our extensive evaluations on multiple
datasets demonstrate that existing models still hinder novel class learning,
whereas our approach strikingly balances both seen and novel classes, achieving
a remarkable 3% average accuracy increase on the ImageNet dataset compared to
the prior state-of-the-art. Additionally, we find that fine-tuning the
self-supervised pre-trained backbone significantly boosts performance over the
default in prior literature. After our paper is accepted, we will release the
code.",http://arxiv.org/pdf/2309.11930v1
2309.11926v1,cs.SE,Quantum Microservices Development and Deployment,2023-09-21 09:40:55+00:00,"Early advances in the field of quantum computing have provided new
opportunities to tackle intricate problems in areas as diverse as mathematics,
physics, or healthcare. However, the technology required to construct such
systems where different pieces of quantum and classical software collaborate is
currently lacking. For this reason, significant advancements in quantum
service-oriented computing are necessary to enable developers to create and
operate quantum services and microservices comparable to their classical
counterparts. Therefore, the core objective of this work is to establish the
necessary technological infrastructure that enables the application of the
benefits and lessons learned from service-oriented computing to the domain of
quantum software engineering. To this end, we propose a pipeline for the
continuous deployment of services. Additionally, we have validated the proposal
by making use of a modification of the OpenAPI specification, the GitHub
Actions, and AWS.",http://arxiv.org/pdf/2309.11926v1
2309.11912v1,cs.CR,The supersingular endomorphism ring problem given one endomorphism,2023-09-21 09:22:43+00:00,"Given a supersingular elliptic curve E and a non-scalar endomorphism $\alpha$
of E, we prove that the endomorphism ring of E can be computed in classical
time about disc(Z[$\alpha$])^1/4 , and in quantum subexponential time, assuming
the generalised Riemann hypothesis. Previous results either had higher
complexities, or relied on heuristic assumptions. Along the way, we prove that
the Primitivisation problem can be solved in polynomial time (a problem
previously believed to be hard), and we prove that the action of smooth ideals
on oriented elliptic curves can be computed in polynomial time (previous
results of this form required the ideal to be powersmooth, i.e., not divisible
by any large prime power). Following the attacks on SIDH, isogenies in high
dimension are a central ingredient of our results.",http://arxiv.org/pdf/2309.11912v1
2309.11903v1,cs.CR,Full mesh networking technology with peer to peer grid topology based on variable parameter full dimensional space,2023-09-21 09:14:31+00:00,"The continuous development of computer network technology has accelerated the
pace of informatization, and at the same time, network security issues are
becoming increasingly prominent. Networking technology with different network
topologies is one of the important means to solve network security problems.
The security of VPN is based on the division of geographical boundaries, but
the granularity is relatively coarse, which is difficult to cope with the
dynamic changes of the security situation. Zero trust network solves the VPN
problem through peer to peer authorization and continuous verification, but
most of the solutions use a central proxy device, resulting in the central node
becoming the bottleneck of the network. This paper put forward the hard-Nat
traversal formula based on the birthday paradox, which solves the long-standing
problem of hard NAT traversal. A full mesh networking mechanism with variable
parameter full-dimensional spatial peer-to-peer grid topology was proposed,
which covers all types of networking schemes and achieve peer-2-peer resource
interconnection on both methodological and engineering level.",http://arxiv.org/pdf/2309.11903v1
2309.11902v1,cs.NI,A Switch Architecture for Time-Triggered Transmission with Best-Effort Delivery,2023-09-21 09:14:03+00:00,"In Time-Triggered (TT) or time-sensitive networks, the transmission of a TT
frame is required to be scheduled at a precise time instant for industrial
distributed real-time control systems. Other (or {\em best-effort} (BE)) frames
are forwarded in a BE manner. Under this scheduling strategy, the transmission
of a TT frame must wait until its scheduled instant even if it could have been
transmitted sooner. On the other hand, BE frames are transmitted whenever
possible but may miss deadlines or may even be dropped due to congestion. As a
result, TT transmission and BE delivery are incompatible with each other.
  To remedy this incompatibility, we propose a synergistic switch architecture
(SWA) for TT transmission with BE delivery to dynamically improve the
end-to-end (e2e) latency of TT frames by opportunistically exploiting BE
delivery. Given a TT frame, the SWA generates and transmits a cloned copy with
BE delivery. The first frame arriving at the receiver device is delivered with
a configured jitter and the other copy ignored. So, the SWA achieves shorter
latency and controllable jitter, the best of both worlds. We have implemented
SWA using FPGAs in an industry-strength TT switches and used four test
scenarios to demonstrate SWA's improvements of e2e latency and controllable
jitter over the state-of-the-art TT transmission scheme.",http://arxiv.org/pdf/2309.11902v1
2309.11892v1,cs.NI,Latency-Aware Radio Resource Optimization in Learning-Based Cloud-Aided Small Cell Wireless Networks,2023-09-21 08:55:58+00:00,"Low latency communication is one of the fundamental requirements for 5G
wireless networks and beyond. In this paper, a novel approach for joint
caching, user scheduling and resource allocation is proposed for minimizing the
queuing latency in serving user's requests in cloud-aided wireless networks.
Due to the slow temporal variations in user requests, a time-scale separation
technique is used to decouple the joint caching problem from user scheduling
and radio resource allocation problems. To serve the spatio-temporal user
requests under storage limitations, a Reinforcement Learning (RL) approach is
used to optimize the caching strategy at the small cell base stations by
minimizing the content fetching cost. A spectral clustering algorithm is
proposed to speed-up the convergence of the RL algorithm for a large content
caching problem by clustering contents based on user requests. Meanwhile, a
dynamic mechanism is proposed to locally group coupled base stations based on
user requests to collaboratively optimize the caching strategies. To further
improve the latency in fetching and serving user requests, a dynamic matching
algorithm is proposed to schedule users and to allocate users to radio
resources based on user requests and queue lengths under probabilistic latency
constraints. Simulation results show the proposed approach significantly
reduces the average delay from 21% to 90% compared to random caching strategy,
random resource allocation and random scheduling baselines.",http://arxiv.org/pdf/2309.11892v1
2309.11890v1,cs.ET,Towards In-Cabin Monitoring: A Preliminary Study on Sensors Data Collection and Analysis,2023-09-21 08:49:59+00:00,"The last decade's market has been characterized by wearable devices, mainly
smartwatches, edge, and cloud computing. A possible application of these
technologies is to improve the safety of dangerous activities, especially
driving motor vehicles. Common enabling technologies, such as system-on-chip,
ultra-low-power computational platforms, and wide-band wireless connectivity,
push all these trends. On the one hand, wearable devices, thanks to the
continuous contact with the user's body, can measure physiological parameters.
On the other hand, edge computing and machine learning techniques, alongside
cameras, allow the implementation of contactless computer vision systems
capable of providing information about the user's current behavior. Another
trend is the usage of RADARs in automotive applications, both for collision
avoidance and monitoring driver behavior. These technologies can be combined to
develop systems designed to aid the driver. For the sake of this paper, we are
focusing on warning drivers, allowing them to know whenever they are drowsy and
hence risking a sleep onset or are not paying attention to the road. Developing
such systems poses many challenges, such as automatic classification of
physiological signal patterns, facial expression recognition, head movements
and eye gaze detection. These challenges have been individually addressed in
the literature. Anyway, we noticed a need for more description on implementing
data fusion. Two main reasons for adopting the fusion approach are to improve
the quality of the overall representation (increasing accuracy and specificity
against drowsy) and make a more reliable system due to redundancy.",http://arxiv.org/pdf/2309.11890v1
2309.11883v1,cs.CV,On-the-Fly SfM: What you capture is What you get,2023-09-21 08:34:01+00:00,"Over the last decades, ample achievements have been made on Structure from
motion (SfM). However, the vast majority of them basically work in an offline
manner, i.e., images are firstly captured and then fed together into a SfM
pipeline for obtaining poses and sparse point cloud. In this work, on the
contrary, we present an on-the-fly SfM: running online SfM while image
capturing, the newly taken On-the-Fly image is online estimated with the
corresponding pose and points, i.e., what you capture is what you get.
Specifically, our approach firstly employs a vocabulary tree that is
unsupervised trained using learning-based global features for fast image
retrieval of newly fly-in image. Then, a robust feature matching mechanism with
least squares (LSM) is presented to improve image registration performance.
Finally, via investigating the influence of newly fly-in image's connected
neighboring images, an efficient hierarchical weighted local bundle adjustment
(BA) is used for optimization. Extensive experimental results demonstrate that
on-the-fly SfM can meet the goal of robustly registering the images while
capturing in an online way.",http://arxiv.org/pdf/2309.11883v1
2309.11881v1,cs.CV,Using Saliency and Cropping to Improve Video Memorability,2023-09-21 08:30:46+00:00,"Video memorability is a measure of how likely a particular video is to be
remembered by a viewer when that viewer has no emotional connection with the
video content. It is an important characteristic as videos that are more
memorable are more likely to be shared, viewed, and discussed. This paper
presents results of a series of experiments where we improved the memorability
of a video by selectively cropping frames based on image saliency. We present
results of a basic fixed cropping as well as the results from dynamic cropping
where both the size of the crop and the position of the crop within the frame,
move as the video is played and saliency is tracked. Our results indicate that
especially for videos of low initial memorability, the memorability score can
be improved.",http://arxiv.org/pdf/2309.11881v1
2309.11866v1,cond-mat.mes-hall,Light-to-heat conversion of optically trapped hot Brownian particles,2023-09-21 08:10:46+00:00,"Anisotropic hybrid nanostructures stand out as promising therapeutic agents
in photothermal conversion-based treatments. Accordingly, understanding local
heat generation mediated by light-to-heat conversion of absorbing
multicomponent nanoparticles at the single particle level have both forthwith
become a subject of broad and current interest. Nonetheless, evaluating
reliable temperature profiles around a single trapped nanoparticle is
challenging from all experimental, computational, and fundamental viewpoints.
Committed to filling this gap, the heat generation of an anisotropic hybrid
nanostructure is explored by means of two different experimental approaches
from which the local temperature is measured in a direct or indirect way, all
in the context of the hot Brownian motion theory. The results were compared
with analytical results supported by the numerical computation of the
wavelength-dependent absorption efficiencies in the discrete dipole
approximation for scattering calculations, which has been here extended to
inhomogeneous nanostructures. Overall, we provide a consistent and
comprehensive view of the heat generation in optical traps of highly absorbing
particles under the viewpoint of the hot Brownian motion theory.",http://arxiv.org/pdf/2309.11866v1
2309.11864v1,math.NA,A Golub-Welsch version for simultaneous Gaussian quadrature,2023-09-21 08:06:50+00:00,"The zeros of type II multiple orthogonal polynomials can be used for
quadrature formulas that approximate $r$ integrals of the same function $f$
with respect to $r$ measures $\mu_1,\ldots,\mu_r$ in the spirit of Gaussian
quadrature. This was first suggested by Borges in 1994. We give a method to
compute the quadrature nodes and the quadrature weights which extends the
Golub-Welsch approach using the eigenvalues and left and right eigenvectors of
a banded Hessenberg matrix. This method was already described by Coussement and
Van Assche in 2005 but it seems to have gone unnoticed. We describe the result
in detail for $r=2$ and give some examples.",http://arxiv.org/pdf/2309.11864v1
2309.11861v1,cs.CE,Data-driven quantitative analysis of an integrated open digital ecosystems platform for user-centric energy retrofits: A case study in Northern Sweden,2023-09-21 08:05:10+00:00,"We present an open digital ecosystem based on web-framework with a functional
back-end server in user-centric energy retrofits. This data-driven web
framework is proposed for building energy renovation benchmarking as part of an
energy advisory service development for the V\""asterbotten region, Sweden. A
4-tiers architecture is developed and programmed to achieve users' interactive
design and visualization via a web browser. Six data-driven methods are
integrated into this framework as backend server functions. Based on those
functions the users can be supported by this decision-making system when they
want to know if it needs to be renovated or not. Meanwhile, influential factors
(input values) from databases that affect energy usage in buildings are to be
analyzed via quantitative analysis, i.e., sensitive analysis. The contributions
to this open ecosystem platform in energy renovation are: 1) A systematic
framework that can be applied to energy efficiency with data-driven approaches,
2) A user-friendly web-based platform that is easy and flexible to use, and 3)
integrated quantitative analysis into the framework to obtain the importance
among all the relevant factors. This computational framework is designed for
stakeholders who would like to get preliminary information in energy advisory.
The improved energy advisor service enabled by the developed platform can
significantly reduce the cost of decision-making, enabling decision-makers to
participate in such professional knowledge-required decisions in a deliberate
and efficient manner. This work is funded by the AURORAL project, which
integrates an open and interoperable digital platform, demonstrated through
regional large-scale pilots in different countries of Europe by
interdisciplinary applications.",http://arxiv.org/pdf/2309.11861v1
2309.11859v1,cs.DS,BalCon -- resource balancing algorithm for VM consolidation,2023-09-21 08:00:21+00:00,"Cloud providers handle substantial number of requests to create and delete
virtual machines (VMs) on a daily basis, where the unknown sequence of requests
eventually leads to resource fragmentation. To mitigate this issue, periodic
consolidation of VMs into fewer number of physical hosts is an important
cost-saving procedure, closely related to the vector bin-packing problem. In
this paper, we propose the BalCon algorithm for consolidation that steadily
reduces the number of active hosts and keeps migration costs low. BalCon
classifies the cluster's state and selects one of three heuristics to balance
resources for superior consolidation. To evaluate BalCon's performance with
respect to optimality, we introduce integer programming models. BalCon finds
99.7% of the optimal solutions for over 750 problem instances. This outstanding
result was achieved due to the Force Step of our algorithm, which is the key
improvement detail for common heuristics. We compare BalCon with a modified
Sercon heuristic using Huawei and synthetic datasets with two resources for
allocation.",http://arxiv.org/pdf/2309.11859v1
2309.11857v1,cs.CV,TCOVIS: Temporally Consistent Online Video Instance Segmentation,2023-09-21 07:59:15+00:00,"In recent years, significant progress has been made in video instance
segmentation (VIS), with many offline and online methods achieving
state-of-the-art performance. While offline methods have the advantage of
producing temporally consistent predictions, they are not suitable for
real-time scenarios. Conversely, online methods are more practical, but
maintaining temporal consistency remains a challenging task. In this paper, we
propose a novel online method for video instance segmentation, called TCOVIS,
which fully exploits the temporal information in a video clip. The core of our
method consists of a global instance assignment strategy and a spatio-temporal
enhancement module, which improve the temporal consistency of the features from
two aspects. Specifically, we perform global optimal matching between the
predictions and ground truth across the whole video clip, and supervise the
model with the global optimal objective. We also capture the spatial feature
and aggregate it with the semantic feature between frames, thus realizing the
spatio-temporal enhancement. We evaluate our method on four widely adopted VIS
benchmarks, namely YouTube-VIS 2019/2021/2022 and OVIS, and achieve
state-of-the-art performance on all benchmarks without bells-and-whistles. For
instance, on YouTube-VIS 2021, TCOVIS achieves 49.5 AP and 61.3 AP with
ResNet-50 and Swin-L backbones, respectively. Code is available at
https://github.com/jun-long-li/TCOVIS.",http://arxiv.org/pdf/2309.11857v1
2309.12279v1,cs.LG,"The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains",2023-09-21 17:40:44+00:00,"Initialization of neural network weights plays a pivotal role in determining
their performance. Feature Imitating Networks (FINs) offer a novel strategy by
initializing weights to approximate specific closed-form statistical features,
setting a promising foundation for deep learning architectures. While the
applicability of FINs has been chiefly tested in biomedical domains, this study
extends its exploration into other time series datasets. Three different
experiments are conducted in this study to test the applicability of imitating
Tsallis entropy for performance enhancement: Bitcoin price prediction, speech
emotion recognition, and chronic neck pain detection. For the Bitcoin price
prediction, models embedded with FINs reduced the root mean square error by
around 1000 compared to the baseline. In the speech emotion recognition task,
the FIN-augmented model increased classification accuracy by over 3 percent.
Lastly, in the CNP detection experiment, an improvement of about 7 percent was
observed compared to established classifiers. These findings validate the broad
utility and potency of FINs in diverse applications.",http://arxiv.org/pdf/2309.12279v1
2309.12238v1,math.ST,Model-based Clustering using Non-parametric Hidden Markov Models,2023-09-21 16:31:04+00:00,"Thanks to their dependency structure, non-parametric Hidden Markov Models
(HMMs) are able to handle model-based clustering without specifying group
distributions. The aim of this work is to study the Bayes risk of clustering
when using HMMs and to propose associated clustering procedures. We first give
a result linking the Bayes risk of classification and the Bayes risk of
clustering, which we use to identify the key quantity determining the
difficulty of the clustering task. We also give a proof of this result in the
i.i.d. framework, which might be of independent interest. Then we study the
excess risk of the plugin classifier. All these results are shown to remain
valid in the online setting where observations are clustered sequentially.
Simulations illustrate our findings.",http://arxiv.org/pdf/2309.12238v1
2309.12221v1,astro-ph.HE,Optimizing the Wasserstein GAN for TeV Gamma Ray Detection with VERITAS,2023-09-21 16:19:15+00:00,"The observation of very-high-energy (VHE, E>100 GeV) gamma rays is mediated
by the imaging atmospheric Cherenkov technique (IACTs). At these energies,
gamma rays interact with the atmosphere to create a cascade of electromagnetic
air showers that are visible to the IACT cameras on the ground with distinct
morphological and temporal features. However, hadrons with significantly higher
incidence rates are also imaged with similar features, and must be
distinguished with handpicked parameters extracted from the images. The advent
of sophisticated deep learning models has enabled an alternative image analysis
technique that has been shown to improve the detection of gamma rays, by
improving background rejection. In this study, we propose an unsupervised
Wasserstein Generative Adversarial Network (WGAN) framework trained on
normalized, uncleaned stereoscopic shower images of real events from the
VERITAS observatory to extract the landscape of their latent space and optimize
against the corresponding inferred latent space of simulated gamma-ray events.
We aim to develop a data driven approach to guide the understanding of the
extracted features of real gamma-ray images, and will optimize the WGAN to
calculate a probabilistic prediction of ""gamma-ness"" per event. In this poster,
we present results of ongoing work toward the optimization of the WGAN,
including the exploration of conditional parameters and multi-task learning.",http://arxiv.org/pdf/2309.12221v1
2309.12219v1,cs.RO,Generating robotic elliptical excisions with human-like tool-tissue interactions,2023-09-21 16:18:33+00:00,"In surgery, the application of appropriate force levels is critical for the
success and safety of a given procedure. While many studies are focused on
measuring in situ forces, little attention has been devoted to relating these
observed forces to surgical techniques. Answering questions like ""Can certain
changes to a surgical technique result in lower forces and increased safety
margins?"" could lead to improved surgical practice, and importantly, patient
outcomes. However, such studies would require a large number of trials and
professional surgeons, which is generally impractical to arrange. Instead, we
show how robots can learn several variations of a surgical technique from a
smaller number of surgical demonstrations and interpolate learnt behaviour via
a parameterised skill model. This enables a large number of trials to be
performed by a robotic system and the analysis of surgical techniques and their
downstream effects on tissue. Here, we introduce a parameterised model of the
elliptical excision skill and apply a Bayesian optimisation scheme to optimise
the excision behaviour with respect to expert ratings, as well as individual
characteristics of excision forces. Results show that the proposed framework
can successfully align the generated robot behaviour with subjects across
varying levels of proficiency in terms of excision forces.",http://arxiv.org/pdf/2309.12219v1
2309.12215v1,cs.LG,Regionally Additive Models: Explainable-by-design models minimizing feature interactions,2023-09-21 16:16:22+00:00,"Generalized Additive Models (GAMs) are widely used explainable-by-design
models in various applications. GAMs assume that the output can be represented
as a sum of univariate functions, referred to as components. However, this
assumption fails in ML problems where the output depends on multiple features
simultaneously. In these cases, GAMs fail to capture the interaction terms of
the underlying function, leading to subpar accuracy. To (partially) address
this issue, we propose Regionally Additive Models (RAMs), a novel class of
explainable-by-design models. RAMs identify subregions within the feature space
where interactions are minimized. Within these regions, it is more accurate to
express the output as a sum of univariate functions (components). Consequently,
RAMs fit one component per subregion of each feature instead of one component
per feature. This approach yields a more expressive model compared to GAMs
while retaining interpretability. The RAM framework consists of three steps.
Firstly, we train a black-box model. Secondly, using Regional Effect Plots, we
identify subregions where the black-box model exhibits near-local additivity.
Lastly, we fit a GAM component for each identified subregion. We validate the
effectiveness of RAMs through experiments on both synthetic and real-world
datasets. The results confirm that RAMs offer improved expressiveness compared
to GAMs while maintaining interpretability.",http://arxiv.org/pdf/2309.12215v1
2309.12162v1,stat.ME,Optimal Conditional Inference in Adaptive Experiments,2023-09-21 15:17:38+00:00,"We study batched bandit experiments and consider the problem of inference
conditional on the realized stopping time, assignment probabilities, and target
parameter, where all of these may be chosen adaptively using information up to
the last batch of the experiment. Absent further restrictions on the
experiment, we show that inference using only the results of the last batch is
optimal. When the adaptive aspects of the experiment are known to be
location-invariant, in the sense that they are unchanged when we shift all
batch-arm means by a constant, we show that there is additional information in
the data, captured by one additional linear function of the batch-arm means. In
the more restrictive case where the stopping time, assignment probabilities,
and target parameter are known to depend on the data only through a collection
of polyhedral events, we derive computationally tractable and optimal
conditional inference procedures.",http://arxiv.org/pdf/2309.12162v1
2309.12149v1,cs.NI,Performance Model for Similarity Caching,2023-09-21 15:04:50+00:00,"Similarity caching allows requests for an item to be served by a similar
item. Applications include recommendation systems, multimedia retrieval, and
machine learning. Recently, many similarity caching policies have been
proposed, like SIM-LRU and RND-LRU, but the performance analysis of their hit
rate is still wanting. In this paper, we show how to extend the popular
time-to-live approximation in classic caching to similarity caching. In
particular, we propose a method to estimate the hit rate of the similarity
caching policy RND-LRU. Our method, the RND-TTL approximation, introduces the
RND-TTL cache model and then tunes its parameters in such a way to mimic the
behavior of RND-LRU. The parameter tuning involves solving a fixed point system
of equations for which we provide an algorithm for numerical resolution and
sufficient conditions for its convergence. Our approach for approximating the
hit rate of RND-LRU is evaluated on both synthetic and real world traces.",http://arxiv.org/pdf/2309.12149v1
2309.12128v1,cs.LG,Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems,2023-09-21 14:48:02+00:00,"Neural networks have become a prominent approach to solve inverse problems in
recent years. While a plethora of such methods was developed to solve inverse
problems empirically, we are still lacking clear theoretical guarantees for
these methods. On the other hand, many works proved convergence to optimal
solutions of neural networks in a more general setting using
overparametrization as a way to control the Neural Tangent Kernel. In this work
we investigate how to bridge these two worlds and we provide deterministic
convergence and recovery guarantees for the class of unsupervised feedforward
multilayer neural networks trained to solve inverse problems. We also derive
overparametrization bounds under which a two-layers Deep Inverse Prior network
with smooth activation function will benefit from our guarantees.",http://arxiv.org/pdf/2309.12128v1
2309.12078v1,cs.LG,Clustering-based Domain-Incremental Learning,2023-09-21 13:49:05+00:00,"We consider the problem of learning multiple tasks in a continual learning
setting in which data from different tasks is presented to the learner in a
streaming fashion. A key challenge in this setting is the so-called
""catastrophic forgetting problem"", in which the performance of the learner in
an ""old task"" decreases when subsequently trained on a ""new task"". Existing
continual learning methods, such as Averaged Gradient Episodic Memory (A-GEM)
and Orthogonal Gradient Descent (OGD), address catastrophic forgetting by
minimizing the loss for the current task without increasing the loss for
previous tasks. However, these methods assume the learner knows when the task
changes, which is unrealistic in practice. In this paper, we alleviate the need
to provide the algorithm with information about task changes by using an online
clustering-based approach on a dynamically updated finite pool of samples or
gradients. We thereby successfully counteract catastrophic forgetting in one of
the hardest settings, namely: domain-incremental learning, a setting for which
the problem was previously unsolved. We showcase the benefits of our approach
by applying these ideas to projection-based methods, such as A-GEM and OGD,
which lead to task-agnostic versions of them. Experiments on real datasets
demonstrate the effectiveness of the proposed strategy and its promising
performance compared to state-of-the-art methods.",http://arxiv.org/pdf/2309.12078v1
2309.12036v1,cs.LG,Uplift vs. predictive modeling: a theoretical analysis,2023-09-21 12:59:17+00:00,"Despite the growing popularity of machine-learning techniques in
decision-making, the added value of causal-oriented strategies with respect to
pure machine-learning approaches has rarely been quantified in the literature.
These strategies are crucial for practitioners in various domains, such as
marketing, telecommunications, health care and finance. This paper presents a
comprehensive treatment of the subject, starting from firm theoretical
foundations and highlighting the parameters that influence the performance of
the uplift and predictive approaches. The focus of the paper is on a binary
outcome case and a binary action, and the paper presents a theoretical analysis
of uplift modeling, comparing it with the classical predictive approach. The
main research contributions of the paper include a new formulation of the
measure of profit, a formal proof of the convergence of the uplift curve to the
measure of profit ,and an illustration, through simulations, of the conditions
under which predictive approaches still outperform uplift modeling. We show
that the mutual information between the features and the outcome plays a
significant role, along with the variance of the estimators, the distribution
of the potential outcomes and the underlying costs and benefits of the
treatment and the outcome.",http://arxiv.org/pdf/2309.12036v1
2309.11983v1,cs.LG,Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling,2023-09-21 11:39:33+00:00,"Connectionist temporal classification (CTC) is commonly adopted for sequence
modeling tasks like speech recognition, where it is necessary to preserve order
between the input and target sequences. However, CTC is only applied to
deterministic sequence models, where the latent space is discontinuous and
sparse, which in turn makes them less capable of handling data variability when
compared to variational models. In this paper, we integrate CTC with a
variational model and derive loss functions that can be used to train more
generalizable sequence models that preserve order. Specifically, we derive two
versions of the novel variational CTC based on two reasonable assumptions, the
first being that the variational latent variables at each time step are
conditionally independent; and the second being that these latent variables are
Markovian. We show that both loss functions allow direct optimization of the
variational lower bound for the model log-likelihood, and present
computationally tractable forms for implementing them.",http://arxiv.org/pdf/2309.11983v1
2309.11963v1,cs.LG,Generating Hierarchical Structures for Improved Time Series Classification Using Stochastic Splitting Functions,2023-09-21 10:34:50+00:00,"This study introduces a novel hierarchical divisive clustering approach with
stochastic splitting functions (SSFs) to enhance classification performance in
multi-class datasets through hierarchical classification (HC). The method has
the unique capability of generating hierarchy without requiring explicit
information, making it suitable for datasets lacking prior knowledge of
hierarchy. By systematically dividing classes into two subsets based on their
discriminability according to the classifier, the proposed approach constructs
a binary tree representation of hierarchical classes. The approach is evaluated
on 46 multi-class time series datasets using popular classifiers (svm and
rocket) and SSFs (potr, srtr, and lsoo). The results reveal that the approach
significantly improves classification performance in approximately half and a
third of the datasets when using rocket and svm as the classifier,
respectively. The study also explores the relationship between dataset features
and HC performance. While the number of classes and flat classification (FC)
score show consistent significance, variations are observed with different
splitting functions. Overall, the proposed approach presents a promising
strategy for enhancing classification by generating hierarchical structure in
multi-class time series datasets. Future research directions involve exploring
different splitting functions, classifiers, and hierarchy structures, as well
as applying the approach to diverse domains beyond time series data. The source
code is made openly available to facilitate reproducibility and further
exploration of the method.",http://arxiv.org/pdf/2309.11963v1
2309.11943v1,physics.app-ph,Multi-contrast x-ray identification of inhomogeneous materials and their discrimination through deep learning approaches,2023-09-21 09:58:28+00:00,"Recent innovations in x-ray technology (namely phase-based and
energy-resolved imaging) offer unprecedented opportunities for material
discrimination, however they are often used in isolation or in limited
combinations. Here we show that the optimized combination of contrast channels
(attenuation at three x-ray energies, ultra-small angle scattering at two,
standard deviation of refraction) significantly enhances material
identification abilities compared to dual-energy x-ray imaging alone, and that
a combination of off-the-shelf machine learning approaches can effectively
discriminate e.g., threat materials in complex datasets. The methodology is
validated on a range of materials and image dataset that are both an order of
magnitude larger than those used in previous studies. Our results can provide
an effective methodology to discriminate, and in some cases identify, different
materials in complex imaging scenarios, with prospective applications across
the life and physical sciences. While the detection of threat materials is used
as a demonstrator here, the methodology could be equally applied to e.g., the
distinction between diseased and healthy tissues or degraded vs. pristine
materials.",http://arxiv.org/pdf/2309.11943v1
2309.11942v1,stat.ME,On the Probability of Immunity,2023-09-21 09:57:03+00:00,"This work is devoted to the study of the probability of immunity, i.e. the
effect occurs whether exposed or not. We derive necessary and sufficient
conditions for non-immunity and $\epsilon$-bounded immunity, i.e. the
probability of immunity is zero and $\epsilon$-bounded, respectively. The
former allows us to estimate the probability of benefit (i.e., the effect
occurs if and only if exposed) from a randomized controlled trial, and the
latter allows us to produce bounds of the probability of benefit that are
tighter than the existing ones. We also introduce the concept of indirect
immunity (i.e., through a mediator) and repeat our previous analysis for it.
Finally, we propose a method for sensitivity analysis of the probability of
immunity under unmeasured confounding.",http://arxiv.org/pdf/2309.11942v1
2309.11939v1,physics.flu-dyn,Applying latent data assimilation to a fluid dynamics problem,2023-09-21 09:55:11+00:00,"Shallow water equations are extensively considered in the domains of oceans,
atmospheric modelling, and engineering research (Franca et al., 2022), which
play significant roles in floods and tsunami governance. Nonetheless, the
accurate prediction of shallow water behaviours is regarded as an arduous
undertaking, particularly when confronted with multi-dimensional data and
potential errors within the model. To address these challenges and improve the
accuracy of forecasts, this study employs an integrated approach, involving
dimensionality reduction methods, deep learning architectures, and data
assimilation techniques. Indeed, Reduced-order modelling facilitates the
conversions of high-dimensional data, extracting important features and
attenuating the complexity of problems (Zhong et al., 2023). Subsequently,
three different predictive models are utilized to prognosticate shallow water
data in the reduced latent space, followed by comparisons of their prediction
performance. Moreover, Bach and Ghil (2023) propose that through the
amalgamation of model forecasts with observational metrics, the data
assimilation algorithm can rectify their discrepancies, thereby enhancing the
model's predictive prowess. Finally, the experimental results demonstrate that
prediction values are congruent with actual observations, which accentuates the
resilience and effectiveness of this comprehensive methodology. Its potential
to accurately forecast shallow water data holds the applicability and
referential significance in preventing storm swell and other meteorological
events.",http://arxiv.org/pdf/2309.11939v1
2309.11914v1,stat.ME,Survival causal rule ensemble method considering the main effect for estimating heterogeneous treatment effects,2023-09-21 09:23:33+00:00,"With an increasing focus on precision medicine in medical research, numerous
studies have been conducted in recent years to clarify the relationship between
treatment effects and patient characteristics. The treatment effects for
patients with different characteristics are always heterogeneous, and various
heterogeneous treatment effect machine learning estimation methods have been
proposed owing to their flexibility and high prediction accuracy. However, most
machine learning methods rely on black-box models, preventing direct
interpretation of the relationship between patient characteristics and
treatment effects. Moreover, most of these studies have focused on continuous
or binary outcomes, although survival outcomes are also important in medical
research. To address these challenges, we propose a heterogeneous treatment
effect estimation method for survival data based on RuleFit, an interpretable
machine learning method. Numerical simulation results confirmed that the
prediction performance of the proposed method was comparable to that of
existing methods. We also applied a dataset from an HIV study, the AIDS
Clinical Trials Group Protocol 175 dataset, to illustrate the interpretability
of the proposed method using real data. Consequently, the proposed method
established an interpretable model with sufficient prediction accuracy.",http://arxiv.org/pdf/2309.11914v1
2309.11894v1,cs.CR,DeepTheft: Stealing DNN Model Architectures through Power Side Channel,2023-09-21 08:58:14+00:00,"Deep Neural Network (DNN) models are often deployed in resource-sharing
clouds as Machine Learning as a Service (MLaaS) to provide inference
services.To steal model architectures that are of valuable intellectual
properties, a class of attacks has been proposed via different side-channel
leakage, posing a serious security challenge to MLaaS.
  Also targeting MLaaS, we propose a new end-to-end attack, DeepTheft, to
accurately recover complex DNN model architectures on general processors via
the RAPL-based power side channel. However, an attacker can acquire only a low
sampling rate (1 KHz) of the time-series energy traces from the RAPL interface,
rendering existing techniques ineffective in stealing large and deep DNN
models. To this end, we design a novel and generic learning-based framework
consisting of a set of meta-models, based on which DeepTheft is demonstrated to
have high accuracy in recovering a large number (thousands) of models
architectures from different model families including the deepest ResNet152.
Particularly, DeepTheft has achieved a Levenshtein Distance Accuracy of 99.75%
in recovering network structures, and a weighted average F1 score of 99.60% in
recovering diverse layer-wise hyperparameters. Besides, our proposed learning
framework is general to other time-series side-channel signals. To validate its
generalization, another existing side channel is exploited, i.e., CPU
frequency. Different from RAPL, CPU frequency is accessible to unprivileged
users in bare-metal OSes. By using our generic learning framework trained
against CPU frequency traces, DeepTheft has shown similarly high attack
performance in stealing model architectures.",http://arxiv.org/pdf/2309.11894v1
2309.11856v1,stat.ML,Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization,2023-09-21 07:59:08+00:00,"Efficient training of large-scale graph neural networks (GNNs) has been
studied with a specific focus on reducing their memory consumption. Work by Liu
et al. (2022) proposed extreme activation compression (EXACT) which
demonstrated drastic reduction in memory consumption by performing quantization
of the intermediate activation maps down to using INT2 precision. They showed
little to no reduction in performance while achieving large reductions in GPU
memory consumption. In this work, we present an improvement to the EXACT
strategy by using block-wise quantization of the intermediate activation maps.
We experimentally analyze different block sizes and show further reduction in
memory consumption (>15%), and runtime speedup per epoch (about 5%) even when
performing extreme extents of quantization with similar performance trade-offs
as with the original EXACT. Further, we present a correction to the assumptions
on the distribution of intermediate activation maps in EXACT (assumed to be
uniform) and show improved variance estimations of the quantization and
dequantization steps.",http://arxiv.org/pdf/2309.11856v1
2309.11847v1,cs.CV,MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion,2023-09-21 07:43:03+00:00,"In this paper, we introduce a new approach for high-quality multi-exposure
image fusion (MEF). We show that the fusion weights of an exposure can be
encoded into a 1D lookup table (LUT), which takes pixel intensity value as
input and produces fusion weight as output. We learn one 1D LUT for each
exposure, then all the pixels from different exposures can query 1D LUT of that
exposure independently for high-quality and efficient fusion. Specifically, to
learn these 1D LUTs, we involve attention mechanism in various dimensions
including frame, channel and spatial ones into the MEF task so as to bring us
significant quality improvement over the state-of-the-art (SOTA). In addition,
we collect a new MEF dataset consisting of 960 samples, 155 of which are
manually tuned by professionals as ground-truth for evaluation. Our network is
trained by this dataset in an unsupervised manner. Extensive experiments are
conducted to demonstrate the effectiveness of all the newly proposed
components, and results show that our approach outperforms the SOTA in our and
another representative dataset SICE, both qualitatively and quantitatively.
Moreover, our 1D LUT approach takes less than 4ms to run a 4K image on a PC
GPU. Given its high quality, efficiency and robustness, our method has been
shipped into millions of Android mobiles across multiple brands world-wide.
Code is available at: https://github.com/Hedlen/MEFLUT.",http://arxiv.org/pdf/2309.11847v1
2309.11804v1,cs.CV,FGFusion: Fine-Grained Lidar-Camera Fusion for 3D Object Detection,2023-09-21 06:24:59+00:00,"Lidars and cameras are critical sensors that provide complementary
information for 3D detection in autonomous driving. While most prevalent
methods progressively downscale the 3D point clouds and camera images and then
fuse the high-level features, the downscaled features inevitably lose low-level
detailed information. In this paper, we propose Fine-Grained Lidar-Camera
Fusion (FGFusion) that make full use of multi-scale features of image and point
cloud and fuse them in a fine-grained way. First, we design a dual pathway
hierarchy structure to extract both high-level semantic and low-level detailed
features of the image. Second, an auxiliary network is introduced to guide
point cloud features to better learn the fine-grained spatial information.
Finally, we propose multi-scale fusion (MSF) to fuse the last N feature maps of
image and point cloud. Extensive experiments on two popular autonomous driving
benchmarks, i.e. KITTI and Waymo, demonstrate the effectiveness of our method.",http://arxiv.org/pdf/2309.11804v1
2309.11803v1,eess.SP,A Comprehensive Study of PAPR Reduction Techniques for Deep Joint Source Channel Coding in OFDM Systems,2023-09-21 06:21:37+00:00,"Recently, deep joint source channel coding (DJSCC) techniques have been
extensively studied and have shown significant performance with limited
bandwidth and low signal to noise ratio. Most DJSCC work considers
discrete-time analog transmission, while combining it with orthogonal frequency
division multiplexing (OFDM) creates serious high peak-to-average power ratio
(PAPR) problem. This paper conducts a comprehensive analysis on the use of
various OFDM PAPR reduction techniques in the DJSCC system, including both
conventional techniques such as clipping, companding, SLM and PTS, and deep
learning-based PAPR reduction techniques such as PAPR loss and clipping with
retraining. Our investigation shows that although conventional PAPR reduction
techniques can be applied to DJSCC, their performance in DJSCC is different
from the conventional split source channel coding. Moreover, we observe that
for signal distortion PAPR reduction techniques, clipping with retraining
achieves the best performance in terms of both PAPR reduction and recovery
accuracy. It is also noticed that signal non-distortion PAPR reduction
techniques can successfully reduce the PAPR in DJSCC without compromise to
signal reconstruction.",http://arxiv.org/pdf/2309.11803v1
2309.11798v1,cs.SI,A Comprehensive Review of Community Detection in Graphs,2023-09-21 06:04:06+00:00,"The study of complex networks has significantly advanced our understanding of
community structures which serves as a crucial feature of real-world graphs.
Detecting communities in graphs is a challenging problem with applications in
sociology, biology, and computer science. Despite the efforts of an
interdisciplinary community of scientists, a satisfactory solution to this
problem has not yet been achieved. This review article delves into the topic of
community detection in graphs, which serves as a crucial role in understanding
the organization and functioning of complex systems. We begin by introducing
the concept of community structure, which refers to the arrangement of vertices
into clusters, with strong internal connections and weaker connections between
clusters. Then, we provide a thorough exposition of various community detection
methods, including a new method designed by us. Additionally, we explore
real-world applications of community detection in diverse networks. In
conclusion, this comprehensive review provides a deep understanding of
community detection in graphs. It serves as a valuable resource for researchers
and practitioners in multiple disciplines, offering insights into the
challenges, methodologies, and applications of community detection in complex
networks.",http://arxiv.org/pdf/2309.11798v1
2309.11783v1,cs.HC,Frame Pairwise Distance Loss for Weakly-supervised Sound Event Detection,2023-09-21 05:14:58+00:00,"Weakly-supervised learning has emerged as a promising approach to leverage
limited labeled data in various domains by bridging the gap between fully
supervised methods and unsupervised techniques. Acquisition of strong
annotations for detecting sound events is prohibitively expensive, making
weakly supervised learning a more cost-effective and broadly applicable
alternative. In order to enhance the recognition rate of the learning of
detection of weakly-supervised sound events, we introduce a Frame Pairwise
Distance (FPD) loss branch, complemented with a minimal amount of synthesized
data. The corresponding sampling and label processing strategies are also
proposed. Two distinct distance metrics are employed to evaluate the proposed
approach. Finally, the method is validated on the standard DCASE dataset. The
obtained experimental results corroborated the efficacy of this approach.",http://arxiv.org/pdf/2309.11783v1
2309.11773v1,cs.CV,"A Real-Time Multi-Task Learning System for Joint Detection of Face, Facial Landmark and Head Pose",2023-09-21 04:15:26+00:00,"Extreme head postures pose a common challenge across a spectrum of facial
analysis tasks, including face detection, facial landmark detection (FLD), and
head pose estimation (HPE). These tasks are interdependent, where accurate FLD
relies on robust face detection, and HPE is intricately associated with these
key points. This paper focuses on the integration of these tasks, particularly
when addressing the complexities posed by large-angle face poses. The primary
contribution of this study is the proposal of a real-time multi-task detection
system capable of simultaneously performing joint detection of faces, facial
landmarks, and head poses. This system builds upon the widely adopted YOLOv8
detection framework. It extends the original object detection head by
incorporating additional landmark regression head, enabling efficient
localization of crucial facial landmarks. Furthermore, we conduct optimizations
and enhancements on various modules within the original YOLOv8 framework. To
validate the effectiveness and real-time performance of our proposed model, we
conduct extensive experiments on 300W-LP and AFLW2000-3D datasets. The results
obtained verify the capability of our model to tackle large-angle face pose
challenges while delivering real-time performance across these interconnected
tasks.",http://arxiv.org/pdf/2309.11773v1
2309.11772v1,stat.ME,Active Learning for a Recursive Non-Additive Emulator for Multi-Fidelity Computer Experiments,2023-09-21 04:11:35+00:00,"Computer simulations have become essential for analyzing complex systems, but
high-fidelity simulations often come with significant computational costs. To
tackle this challenge, multi-fidelity computer experiments have emerged as a
promising approach that leverages both low-fidelity and high-fidelity
simulations, enhancing both the accuracy and efficiency of the analysis. In
this paper, we introduce a new and flexible statistical model, the Recursive
Non-Additive (RNA) emulator, that integrates the data from multi-fidelity
computer experiments. Unlike conventional multi-fidelity emulation approaches
that rely on an additive auto-regressive structure, the proposed RNA emulator
recursively captures the relationships between multi-fidelity data using
Gaussian process priors without making the additive assumption, allowing the
model to accommodate more complex data patterns. Importantly, we derive the
posterior predictive mean and variance of the emulator, which can be
efficiently computed in a closed-form manner, leading to significant
improvements in computational efficiency. Additionally, based on this emulator,
we introduce three active learning strategies that optimize the balance between
accuracy and simulation costs to guide the selection of the fidelity level and
input locations for the next simulation run. We demonstrate the effectiveness
of the proposed approach in a suite of synthetic examples and a real-world
problem. An R package for the proposed methodology is provided in an open
repository.",http://arxiv.org/pdf/2309.11772v1
2309.11766v1,cs.CR,Dictionary Attack on IMU-based Gait Authentication,2023-09-21 04:00:21+00:00,"We present a novel adversarial model for authentication systems that use gait
patterns recorded by the inertial measurement unit (IMU) built into
smartphones. The attack idea is inspired by and named after the concept of a
dictionary attack on knowledge (PIN or password) based authentication systems.
In particular, this work investigates whether it is possible to build a
dictionary of IMUGait patterns and use it to launch an attack or find an
imitator who can actively reproduce IMUGait patterns that match the target's
IMUGait pattern. Nine physically and demographically diverse individuals walked
at various levels of four predefined controllable and adaptable gait factors
(speed, step length, step width, and thigh-lift), producing 178 unique IMUGait
patterns. Each pattern attacked a wide variety of user authentication models.
The deeper analysis of error rates (before and after the attack) challenges the
belief that authentication systems based on IMUGait patterns are the most
difficult to spoof; further research is needed on adversarial models and
associated countermeasures.",http://arxiv.org/pdf/2309.11766v1
2309.11765v1,cs.LG,Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation,2023-09-21 03:59:00+00:00,"We study the problem of in-context learning (ICL) with large language models
(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak
or regurgitate the private examples demonstrated in the prompt. We propose a
novel algorithm that generates synthetic few-shot demonstrations from the
private dataset with formal differential privacy (DP) guarantees, and show
empirically that it can achieve effective ICL. We conduct extensive experiments
on standard benchmarks and compare our algorithm with non-private ICL and
zero-shot solutions. Our results demonstrate that our algorithm can achieve
competitive performance with strong privacy levels. These results open up new
possibilities for ICL with privacy protection for a broad range of
applications.",http://arxiv.org/pdf/2309.11765v1
2309.11763v1,eess.IV,Bloch Equation Enables Physics-informed Neural Network in Parametric Magnetic Resonance Imaging,2023-09-21 03:53:33+00:00,"Magnetic resonance imaging (MRI) is an important non-invasive imaging method
in clinical diagnosis. Beyond the common image structures, parametric imaging
can provide the intrinsic tissue property thus could be used in quantitative
evaluation. The emerging deep learning approach provides fast and accurate
parameter estimation but still encounters the lack of network interpretation
and enough training data. Even with a large amount of training data, the
mismatch between the training and target data may introduce errors. Here, we
propose one way that solely relies on the target scanned data and does not need
a pre-defined training database. We provide a proof-of-concept that embeds the
physical rule of MRI, the Bloch equation, into the loss of physics-informed
neural network (PINN). PINN enables learning the Bloch equation, estimating the
T2 parameter, and generating a series of physically synthetic data.
Experimental results are conducted on phantom and cardiac imaging to
demonstrate its potential in quantitative MRI.",http://arxiv.org/pdf/2309.11763v1
2309.11758v1,cs.CV,SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks,2023-09-21 03:41:08+00:00,"In the analysis of optical coherence tomography angiography (OCTA) images,
the operation of segmenting specific targets is necessary. Existing methods
typically train on supervised datasets with limited samples (approximately a
few hundred), which can lead to overfitting. To address this, the low-rank
adaptation technique is adopted for foundation model fine-tuning and proposed
corresponding prompt point generation strategies to process various
segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been
experimented on the publicly available OCTA-500 dataset. While achieving
state-of-the-art performance metrics, this method accomplishes local vessel
segmentation as well as effective artery-vein segmentation, which was not
well-solved in previous works. The code is available at:
https://github.com/ShellRedia/SAM-OCTA.",http://arxiv.org/pdf/2309.11758v1
2309.11756v1,eess.AS,Sparsely Shared LoRA on Whisper for Child Speech Recognition,2023-09-21 03:35:49+00:00,"Whisper is a powerful automatic speech recognition (ASR) model. Nevertheless,
its zero-shot performance on low-resource speech requires further improvement.
Child speech, as a representative type of low-resource speech, is leveraged for
adaptation. Recently, parameter-efficient fine-tuning (PEFT) in NLP was shown
to be comparable and even better than full fine-tuning, while only needing to
tune a small set of trainable parameters. However, current PEFT methods have
not been well examined for their effectiveness on Whisper. In this paper, only
parameter composition types of PEFT approaches such as LoRA and Bitfit are
investigated as they do not bring extra inference costs. Different popular PEFT
methods are examined. Particularly, we compare LoRA and AdaLoRA and figure out
the learnable rank coefficient is a good design. Inspired by the sparse rank
distribution allocated by AdaLoRA, a novel PEFT approach Sparsely Shared LoRA
(S2-LoRA) is proposed. The two low-rank decomposed matrices are globally
shared. Each weight matrix only has to maintain its specific rank coefficients
that are constrained to be sparse. Experiments on low-resource Chinese child
speech show that with much fewer trainable parameters, S2-LoRA can achieve
comparable in-domain adaptation performance to AdaLoRA and exhibit better
generalization ability on out-of-domain data. In addition, the rank
distribution automatically learned by S2-LoRA is found to have similar patterns
to AdaLoRA's allocation.",http://arxiv.org/pdf/2309.11756v1
2309.11745v1,eess.IV,PIE: Simulating Disease Progression via Progressive Image Editing,2023-09-21 02:46:32+00:00,"Disease progression simulation is a crucial area of research that has
significant implications for clinical diagnosis, prognosis, and treatment. One
major challenge in this field is the lack of continuous medical imaging
monitoring of individual patients over time. To address this issue, we develop
a novel framework termed Progressive Image Editing (PIE) that enables
controlled manipulation of disease-related image features, facilitating precise
and realistic disease progression simulation. Specifically, we leverage recent
advancements in text-to-image generative models to simulate disease progression
accurately and personalize it for each patient. We theoretically analyze the
iterative refining process in our framework as a gradient descent with an
exponentially decayed learning rate. To validate our framework, we conduct
experiments in three medical imaging domains. Our results demonstrate the
superiority of PIE over existing methods such as Stable Diffusion Walk and
Style-Based Manifold Extrapolation based on CLIP score (Realism) and Disease
Classification Confidence (Alignment). Our user study collected feedback from
35 veteran physicians to assess the generated progressions. Remarkably, 76.2%
of the feedback agrees with the fidelity of the generated progressions. To our
best knowledge, PIE is the first of its kind to generate disease progression
images meeting real-world standards. It is a promising tool for medical
research and clinical practice, potentially allowing healthcare providers to
model disease trajectories over time, predict future treatment responses, and
improve patient outcomes.",http://arxiv.org/pdf/2309.11745v1
2309.11741v1,cs.IR,Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph Pruning and Intent Graph for Effective Recommendations,2023-09-21 02:32:17+00:00,"The recommendation of appropriate development pathways, also known as
ecological civilization patterns for achieving Sustainable Development Goals
(namely, sustainable development patterns), are of utmost importance for
promoting ecological, economic, social, and resource sustainability in a
specific region. To achieve this, the recommendation process must carefully
consider the region's natural, environmental, resource, and economic
characteristics. However, current recommendation algorithms in the field of
computer science fall short in adequately addressing the spatial heterogeneity
related to environment and sparsity of regional historical interaction data,
which limits their effectiveness in recommending sustainable development
patterns. To overcome these challenges, this paper proposes a method called
User Graph after Pruning and Intent Graph (UGPIG). Firstly, we utilize the
high-density linking capability of the pruned User Graph to address the issue
of spatial heterogeneity neglect in recommendation algorithms. Secondly, we
construct an Intent Graph by incorporating the intent network, which captures
the preferences for attributes including environmental elements of target
regions. This approach effectively alleviates the problem of sparse historical
interaction data in the region. Through extensive experiments, we demonstrate
that UGPIG outperforms state-of-the-art recommendation algorithms like KGCN,
KGAT, and KGIN in sustainable development pattern recommendations, with a
maximum improvement of 9.61% in Top-3 recommendation performance.",http://arxiv.org/pdf/2309.11741v1
2309.11739v1,stat.OT,Classroom Community amid Covid-19: A Mixed-Methods Study of Undergraduate Students in Introductory Mathematics and Statistics,2023-09-21 02:27:25+00:00,"A strong sense of classroom community is associated with many positive
learning outcomes and is a critical contributor to undergraduate students'
persistence in STEM, particularly for women and students of color. This
manuscript describes a mixed-methods investigation into the relationship
between classroom community and course attributes in introductory undergraduate
mathematics and statistics courses, mediated by student demographics. The
primary quantitative instrument is the validated Classroom Community Scale -
Short Form survey. Data were collected from online courses in the 2020-21
academic year along with hybrid and in-person courses in Fall 2021 and analyzed
using structural equation modeling. These quantitative results are complemented
and contextualized by thematic and textual analyses of focus group data
gathered using a newly developed protocol piloted at the close of Fall 2021 All
data come from a highly selective private university in the United States.
While the study was conducted amidst the height of the Covid-19 pandemic,
potential ramifications extend more broadly. These preliminary practical
implications of the study include the value of synchronous participation in
fostering connectedness and the importance of attending to students' personal
identities in understanding their experiences of belonging.",http://arxiv.org/pdf/2309.11739v1
2309.11730v1,eess.AS,Leveraging In-the-Wild Data for Effective Self-Supervised Pretraining in Speaker Recognition,2023-09-21 02:03:26+00:00,"Current speaker recognition systems primarily rely on supervised approaches,
constrained by the scale of labeled datasets. To boost the system performance,
researchers leverage large pretrained models such as WavLM to transfer learned
high-level features to the downstream speaker recognition task. However, this
approach introduces extra parameters as the pretrained model remains in the
inference stage. Another group of researchers directly apply self-supervised
methods such as DINO to speaker embedding learning, yet they have not explored
its potential on large-scale in-the-wild datasets. In this paper, we present
the effectiveness of DINO training on the large-scale WenetSpeech dataset and
its transferability in enhancing the supervised system performance on the
CNCeleb dataset. Additionally, we introduce a confidence-based data filtering
algorithm to remove unreliable data from the pretraining dataset, leading to
better performance with less training data. The associated pretrained models,
confidence files, pretraining and finetuning scripts will be made available in
the Wespeaker toolkit.",http://arxiv.org/pdf/2309.11730v1
2309.11727v1,cs.RO,Person Re-Identification for Robot Person Following with Online Continual Learning,2023-09-21 02:01:38+00:00,"Robot person following (RPF) is a crucial capability in human-robot
interaction (HRI) applications, allowing a robot to persistently follow a
designated person. In practical RPF scenarios, the person often be occluded by
other objects or people. Consequently, it is necessary to re-identify the
person when he/she re-appears within the robot's field of view. Previous person
re-identification (ReID) approaches to person following rely on offline-trained
features and short-term experiences. Such an approach i) has a limited capacity
to generalize across scenarios; and ii) often fails to re-identify the person
when his re-appearance is out of the learned domain represented by the
short-term experiences. Based on this observation, in this work, we propose a
ReID framework for RPF that leverages long-term experiences. The experiences
are maintained by a loss-guided keyframe selection strategy, to enable online
continual learning of the appearance model. Our experiments demonstrate that
even in the presence of severe appearance changes and distractions from
visually similar people, the proposed method can still re-identify the person
more accurately than the state-of-the-art methods.",http://arxiv.org/pdf/2309.11727v1
2309.11726v1,cs.PL,Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs,2023-09-21 01:59:20+00:00,"Programmers and researchers are increasingly developing surrogates of
programs, models of a subset of the observable behavior of a given program, to
solve a variety of software development challenges. Programmers train
surrogates from measurements of the behavior of a program on a dataset of input
examples. A key challenge of surrogate construction is determining what
training data to use to train a surrogate of a given program.
  We present a methodology for sampling datasets to train neural-network-based
surrogates of programs. We first characterize the proportion of data to sample
from each region of a program's input space (corresponding to different
execution paths of the program) based on the complexity of learning a surrogate
of the corresponding execution path. We next provide a program analysis to
determine the complexity of different paths in a program. We evaluate these
results on a range of real-world programs, demonstrating that complexity-guided
sampling results in empirical improvements in accuracy.",http://arxiv.org/pdf/2309.11726v1
2309.11722v1,cs.GT,Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning,2023-09-21 01:47:39+00:00,"Federated learning is a distributed machine learning system that uses
participants' data to train an improved global model. In federated learning,
participants cooperatively train a global model, and they will receive the
global model and payments. Rational participants try to maximize their
individual utility, and they will not input their high-quality data truthfully
unless they are provided with satisfactory payments based on their data
quality. Furthermore, federated learning benefits from the cooperative
contributions of participants. Accordingly, how to establish an incentive
mechanism that both incentivizes inputting data truthfully and promotes stable
cooperation has become an important issue to consider. In this paper, we
introduce a data sharing game model for federated learning and employ
game-theoretic approaches to design a core-selecting incentive mechanism by
utilizing a popular concept in cooperative games, the core. In federated
learning, the core can be empty, resulting in the core-selecting mechanism
becoming infeasible. To address this, our core-selecting mechanism employs a
relaxation method and simultaneously minimizes the benefits of inputting false
data for all participants. However, this mechanism is computationally expensive
because it requires aggregating exponential models for all possible coalitions,
which is infeasible in federated learning. To address this, we propose an
efficient core-selecting mechanism based on sampling approximation that only
aggregates models on sampled coalitions to approximate the exact result.
Extensive experiments verify that the efficient core-selecting mechanism can
incentivize inputting high-quality data and stable cooperation, while it
reduces computational overhead compared to the core-selecting mechanism.",http://arxiv.org/pdf/2309.11722v1
2309.11717v1,eess.SP,A class-weighted supervised contrastive learning long-tailed bearing fault diagnosis approach using quadratic neural network,2023-09-21 01:36:46+00:00,"Deep learning has achieved remarkable success in bearing fault diagnosis.
However, its performance oftentimes deteriorates when dealing with highly
imbalanced or long-tailed data, while such cases are prevalent in industrial
settings because fault is a rare event that occurs with an extremely low
probability. Conventional data augmentation methods face fundamental
limitations due to the scarcity of samples pertaining to the minority class. In
this paper, we propose a supervised contrastive learning approach with a
class-aware loss function to enhance the feature extraction capability of
neural networks for fault diagnosis. The developed class-weighted contrastive
learning quadratic network (CCQNet) consists of a quadratic convolutional
residual network backbone, a contrastive learning branch utilizing a
class-weighted contrastive loss, and a classifier branch employing
logit-adjusted cross-entropy loss. By utilizing class-weighted contrastive loss
and logit-adjusted cross-entropy loss, our approach encourages equidistant
representation of class features, thereby inducing equal attention on all the
classes. We further analyze the superior feature extraction ability of
quadratic network by establishing the connection between quadratic neurons and
autocorrelation in signal processing. Experimental results on public and
proprietary datasets are used to validate the effectiveness of CCQNet, and
computational results reveal that CCQNet outperforms SOTA methods in handling
extremely imbalanced data substantially.",http://arxiv.org/pdf/2309.11717v1
2309.11713v1,stat.ML,Quasi-Monte Carlo for 3D Sliced Wasserstein,2023-09-21 01:32:42+00:00,"Monte Carlo (MC) approximation has been used as the standard computation
approach for the Sliced Wasserstein (SW) distance, which has an intractable
expectation in its analytical form. However, the MC method is not optimal in
terms of minimizing the absolute approximation error. To provide a better class
of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that
rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of
QMC for SW, we focus on the 3D setting, specifically computing the SW between
probability measures in three dimensions. In greater detail, we empirically
verify various ways of constructing QMC points sets on the 3D unit-hypersphere,
including Gaussian-based mapping, equal area mapping, generalized spiral
points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased
estimation for stochastic optimization, we extend QSW into Randomized
Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed
low-discrepancy sequences. For theoretical properties, we prove the asymptotic
convergence of QSW and the unbiasedness of RQSW. Finally, we conduct
experiments on various 3D tasks, such as point-cloud comparison, point-cloud
interpolation, image style transfer, and training deep point-cloud
autoencoders, to demonstrate the favorable performance of the proposed QSW and
RQSW variants.",http://arxiv.org/pdf/2309.11713v1
2309.11711v1,cs.CV,MoDA: Leveraging Motion Priors from Videos for Advancing Unsupervised Domain Adaptation in Semantic Segmentation,2023-09-21 01:31:54+00:00,"Unsupervised domain adaptation (UDA) is an effective approach to handle the
lack of annotations in the target domain for the semantic segmentation task. In
this work, we consider a more practical UDA setting where the target domain
contains sequential frames of the unlabeled videos which are easy to collect in
practice. A recent study suggests self-supervised learning of the object motion
from unlabeled videos with geometric constraints. We design a motion-guided
domain adaptive semantic segmentation framework (MoDA), that utilizes
self-supervised object motion to learn effective representations in the target
domain. MoDA differs from previous methods that use temporal consistency
regularization for the target domain frames. Instead, MoDA deals separately
with the domain alignment on the foreground and background categories using
different strategies. Specifically, MoDA contains foreground object discovery
and foreground semantic mining to align the foreground domain gaps by taking
the instance-level guidance from the object motion. Additionally, MoDA includes
background adversarial training which contains a background category-specific
discriminator to handle the background domain gaps. Experimental results on
multiple benchmarks highlight the effectiveness of MoDA against existing
approaches in the domain adaptive image segmentation and domain adaptive video
segmentation. Moreover, MoDA is versatile and can be used in conjunction with
existing state-of-the-art approaches to further improve performance.",http://arxiv.org/pdf/2309.11711v1
2309.11705v1,cs.LG,Meta OOD Learning for Continuously Adaptive OOD Detection,2023-09-21 01:05:45+00:00,"Out-of-distribution (OOD) detection is crucial to modern deep learning
applications by identifying and alerting about the OOD samples that should not
be tested or used for making predictions. Current OOD detection methods have
made significant progress when in-distribution (ID) and OOD samples are drawn
from static distributions. However, this can be unrealistic when applied to
real-world systems which often undergo continuous variations and shifts in ID
and OOD distributions over time. Therefore, for an effective application in
real-world systems, the development of OOD detection methods that can adapt to
these dynamic and evolving distributions is essential. In this paper, we
propose a novel and more realistic setting called continuously adaptive
out-of-distribution (CAOOD) detection which targets on developing an OOD
detection model that enables dynamic and quick adaptation to a new arriving
distribution, with insufficient ID samples during deployment time. To address
CAOOD, we develop meta OOD learning (MOL) by designing a learning-to-adapt
diagram such that a good initialized OOD detection model is learned during the
training process. In the testing process, MOL ensures OOD detection performance
over shifting distributions by quickly adapting to new distributions with a few
adaptations. Extensive experiments on several OOD benchmarks endorse the
effectiveness of our method in preserving both ID classification accuracy and
OOD detection performance on continuously shifting distributions.",http://arxiv.org/pdf/2309.11705v1
2309.11702v1,cs.LG,Incentivized Communication for Federated Bandits,2023-09-21 00:59:20+00:00,"Most existing works on federated bandits take it for granted that all clients
are altruistic about sharing their data with the server for the collective good
whenever needed. Despite their compelling theoretical guarantee on performance
and communication efficiency, this assumption is overly idealistic and
oftentimes violated in practice, especially when the algorithm is operated over
self-interested clients, who are reluctant to share data without explicit
benefits. Negligence of such self-interested behaviors can significantly affect
the learning efficiency and even the practical operability of federated bandit
learning. In light of this, we aim to spark new insights into this
under-explored research area by formally introducing an incentivized
communication problem for federated bandits, where the server shall motivate
clients to share data by providing incentives. Without loss of generality, we
instantiate this bandit problem with the contextual linear setting and propose
the first incentivized communication protocol, namely, Inc-FedUCB, that
achieves near-optimal regret with provable communication and incentive cost
guarantees. Extensive empirical experiments on both synthetic and real-world
datasets further validate the effectiveness of the proposed method across
various environments.",http://arxiv.org/pdf/2309.11702v1
2309.11692v1,cs.CL,Semi-supervised News Discourse Profiling with Contrastive Learning,2023-09-20 23:51:34+00:00,"News Discourse Profiling seeks to scrutinize the event-related role of each
sentence in a news article and has been proven useful across various downstream
applications. Specifically, within the context of a given news discourse, each
sentence is assigned to a pre-defined category contingent upon its depiction of
the news event structure. However, existing approaches suffer from an
inadequacy of available human-annotated data, due to the laborious and
time-intensive nature of generating discourse-level annotations. In this paper,
we present a novel approach, denoted as Intra-document Contrastive Learning
with Distillation (ICLD), for addressing the news discourse profiling task,
capitalizing on its unique structural characteristics. Notably, we are the
first to apply a semi-supervised methodology within this task paradigm, and
evaluation demonstrates the effectiveness of the presented approach.",http://arxiv.org/pdf/2309.11692v1
2309.11687v1,cs.LG,Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening,2023-09-20 23:43:42+00:00,"Virtual screening of large compound libraries to identify potential hit
candidates is one of the earliest steps in drug discovery. As the size of
commercially available compound collections grows exponentially to the scale of
billions, brute-force virtual screening using traditional tools such as docking
becomes infeasible in terms of time and computational resources. Active
learning and Bayesian optimization has recently been proven as effective
methods of narrowing down the search space. An essential component in those
methods is a surrogate machine learning model that is trained with a small
subset of the library to predict the desired properties of compounds. Accurate
model can achieve high sample efficiency by finding the most promising
compounds with only a fraction of the whole library being virtually screened.
In this study, we examined the performance of pretrained transformer-based
language model and graph neural network in Bayesian optimization active
learning framework. The best pretrained models identifies 58.97% of the
top-50000 by docking score after screening only 0.6% of an ultra-large library
containing 99.5 million compounds, improving 8% over previous state-of-the-art
baseline. Through extensive benchmarks, we show that the superior performance
of pretrained models persists in both structure-based and ligand-based drug
discovery. Such model can serve as a boost to the accuracy and sample
efficiency of active learning based molecule virtual screening.",http://arxiv.org/pdf/2309.11687v1
2309.11674v1,cs.CL,A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models,2023-09-20 22:53:15+00:00,"Generative Large Language Models (LLMs) have achieved remarkable advancements
in various NLP tasks. However, these advances have not been reflected in the
translation task, especially those with moderate model sizes (i.e., 7B or 13B
parameters), which still lag behind conventional supervised encoder-decoder
translation models. Previous studies have attempted to improve the translation
capabilities of these moderate LLMs, but their gains have been limited. In this
study, we propose a novel fine-tuning approach for LLMs that is specifically
designed for the translation task, eliminating the need for the abundant
parallel data that traditional translation models usually depend on. Our
approach consists of two fine-tuning stages: initial fine-tuning on monolingual
data followed by subsequent fine-tuning on a small set of high-quality parallel
data. We introduce the LLM developed through this strategy as Advanced Language
Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our
results show that the model can achieve an average improvement of more than 12
BLEU and 12 COMET over its zero-shot performance across 10 translation
directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test
datasets. The performance is significantly better than all prior work and even
superior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or
13B parameters. This method establishes the foundation for a novel training
paradigm in machine translation.",http://arxiv.org/pdf/2309.11674v1
2309.11671v1,cs.IR,Popularity Degradation Bias in Local Music Recommendation,2023-09-20 22:36:33+00:00,"In this paper, we study the effect of popularity degradation bias in the
context of local music recommendations. Specifically, we examine how accurate
two top-performing recommendation algorithms, Weight Relevance Matrix
Factorization (WRMF) and Multinomial Variational Autoencoder (Mult-VAE), are at
recommending artists as a function of artist popularity. We find that both
algorithms improve recommendation performance for more popular artists and, as
such, exhibit popularity degradation bias. While both algorithms produce a
similar level of performance for more popular artists, Mult-VAE shows better
relative performance for less popular artists. This suggests that this
algorithm should be preferred for local (long-tail) music artist
recommendation.",http://arxiv.org/pdf/2309.11671v1
2309.11668v1,cs.CL,Towards Effective Disambiguation for Machine Translation with Large Language Models,2023-09-20 22:22:52+00:00,"Resolving semantic ambiguity has long been recognised as a central challenge
in the field of machine translation. Recent work on benchmarking translation
performance on ambiguous sentences has exposed the limitations of conventional
Neural Machine Translation (NMT) systems, which fail to capture many of these
cases. Large language models (LLMs) have emerged as a promising alternative,
demonstrating comparable performance to traditional NMT models while
introducing new paradigms for controlling the target outputs. In this paper, we
study the capabilities of LLMs to translate ambiguous sentences containing
polysemous words and rare word senses. We also propose two ways to improve the
handling of such ambiguity through in-context learning and fine-tuning on
carefully curated ambiguous datasets. Experiments show that our methods can
match or outperform state-of-the-art systems such as DeepL and NLLB in four out
of five language directions. Our research provides valuable insights into
effectively adapting LLMs for disambiguation during machine translation.",http://arxiv.org/pdf/2309.11668v1
2309.11667v1,cs.CV,Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation,2023-09-20 22:22:21+00:00,"As 3D human pose estimation can now be achieved with very high accuracy in
the supervised learning scenario, tackling the case where 3D pose annotations
are not available has received increasing attention. In particular, several
methods have proposed to learn image representations in a self-supervised
fashion so as to disentangle the appearance information from the pose one. The
methods then only need a small amount of supervised data to train a pose
regressor using the pose-related latent vector as input, as it should be free
of appearance information. In this paper, we carry out in-depth analysis to
understand to what degree the state-of-the-art disentangled representation
learning methods truly separate the appearance information from the pose one.
First, we study disentanglement from the perspective of the self-supervised
network, via diverse image synthesis experiments. Second, we investigate
disentanglement with respect to the 3D pose regressor following an adversarial
attack perspective. Specifically, we design an adversarial strategy focusing on
generating natural appearance changes of the subject, and against which we
could expect a disentangled network to be robust. Altogether, our analyses show
that disentanglement in the three state-of-the-art disentangled representation
learning frameworks if far from complete, and that their pose codes contain
significant appearance information. We believe that our approach provides a
valuable testbed to evaluate the degree of disentanglement of pose from
appearance in self-supervised 3D human pose estimation.",http://arxiv.org/pdf/2309.11667v1
2309.11661v1,cs.CV,Neural Image Compression Using Masked Sparse Visual Representation,2023-09-20 21:59:23+00:00,"We study neural image compression based on the Sparse Visual Representation
(SVR), where images are embedded into a discrete latent space spanned by
learned visual codebooks. By sharing codebooks with the decoder, the encoder
transfers integer codeword indices that are efficient and cross-platform
robust, and the decoder retrieves the embedded latent feature using the indices
for reconstruction. Previous SVR-based compression lacks effective mechanism
for rate-distortion tradeoffs, where one can only pursue either high
reconstruction quality or low transmission bitrate. We propose a Masked
Adaptive Codebook learning (M-AdaCode) method that applies masks to the latent
feature subspace to balance bitrate and reconstruction quality. A set of
semantic-class-dependent basis codebooks are learned, which are weighted
combined to generate a rich latent feature for high-quality reconstruction. The
combining weights are adaptively derived from each input image, providing
fidelity information with additional transmission costs. By masking out
unimportant weights in the encoder and recovering them in the decoder, we can
trade off reconstruction quality for transmission bits, and the masking rate
controls the balance between bitrate and distortion. Experiments over the
standard JPEG-AI dataset demonstrate the effectiveness of our M-AdaCode
approach.",http://arxiv.org/pdf/2309.11661v1
2309.11657v1,cs.DS,GLM Regression with Oblivious Corruptions,2023-09-20 21:41:59+00:00,"We demonstrate the first algorithms for the problem of regression for
generalized linear models (GLMs) in the presence of additive oblivious noise.
We assume we have sample access to examples $(x, y)$ where $y$ is a noisy
measurement of $g(w^* \cdot x)$. In particular, \new{the noisy labels are of
the form} $y = g(w^* \cdot x) + \xi + \epsilon$, where $\xi$ is the oblivious
noise drawn independently of $x$ \new{and satisfies} $\Pr[\xi = 0] \geq o(1)$,
and $\epsilon \sim \mathcal N(0, \sigma^2)$. Our goal is to accurately recover
a \new{parameter vector $w$ such that the} function $g(w \cdot x)$ \new{has}
arbitrarily small error when compared to the true values $g(w^* \cdot x)$,
rather than the noisy measurements $y$.
  We present an algorithm that tackles \new{this} problem in its most general
distribution-independent setting, where the solution may not \new{even} be
identifiable. \new{Our} algorithm returns \new{an accurate estimate of} the
solution if it is identifiable, and otherwise returns a small list of
candidates, one of which is close to the true solution. Furthermore, we
\new{provide} a necessary and sufficient condition for identifiability, which
holds in broad settings. \new{Specifically,} the problem is identifiable when
the quantile at which $\xi + \epsilon = 0$ is known, or when the family of
hypotheses does not contain candidates that are nearly equal to a translated
$g(w^* \cdot x) + A$ for some real number $A$, while also having large error
when compared to $g(w^* \cdot x)$.
  This is the first \new{algorithmic} result for GLM regression \new{with
oblivious noise} which can handle more than half the samples being arbitrarily
corrupted. Prior work focused largely on the setting of linear regression, and
gave algorithms under restrictive assumptions.",http://arxiv.org/pdf/2309.11657v1
2309.11655v1,cs.RO,Achieving Autonomous Cloth Manipulation with Optimal Control via Differentiable Physics-Aware Regularization and Safety Constraints,2023-09-20 21:41:01+00:00,"Cloth manipulation is a category of deformable object manipulation of great
interest to the robotics community, from applications of automated
laundry-folding and home organizing and cleaning to textiles and flexible
manufacturing. Despite the desire for automated cloth manipulation, the
thin-shell dynamics and under-actuation nature of cloth present significant
challenges for robots to effectively interact with them. Many recent works omit
explicit modeling in favor of learning-based methods that may yield control
policies directly. However, these methods require large training sets that must
be collected and curated. In this regard, we create a framework for
differentiable modeling of cloth dynamics leveraging an Extended Position-based
Dynamics (XPBD) algorithm. Together with the desired control objective,
physics-aware regularization terms are designed for better results, including
trajectory smoothness and elastic potential energy. In addition, safety
constraints, such as avoiding obstacles, can be specified using signed distance
functions (SDFs). We formulate the cloth manipulation task with safety
constraints as a constrained optimization problem, which can be effectively
solved by mainstream gradient-based optimizers thanks to the end-to-end
differentiability of our framework. Finally, we assess the proposed framework
for manipulation tasks with various safety thresholds and demonstrate the
feasibility of result trajectories on a surgical robot. The effects of the
regularization terms are analyzed in an additional ablation study.",http://arxiv.org/pdf/2309.11655v1
2309.11652v1,astro-ph.GA,Selection of powerful radio galaxies with machine learning,2023-09-20 21:33:17+00:00,"We developed and trained a pipeline of three machine learning (ML) models
than can predict which sources are more likely to be an AGN and to be detected
in specific radio surveys. Also, it can estimate redshift values for predicted
radio-detectable AGNs. These models, which combine predictions from tree-based
and gradient-boosting algorithms, have been trained with multi-wavelength data
from near-infrared-selected sources in the Hobby-Eberly Telescope Dark Energy
Experiment (HETDEX) Spring field. Training, testing, calibration, and
validation were carried out in the HETDEX field. Further validation was
performed on near-infrared-selected sources in the Stripe 82 field. In the
HETDEX validation subset, our pipeline recovers 96% of the initially labelled
AGNs and, from AGNs candidates, we recover 50% of previously detected radio
sources. For Stripe 82, these numbers are 94% and 55%. Compared to random
selection, these rates are two and four times better for HETDEX, and 1.2 and 12
times better for Stripe 82. The pipeline can also recover the redshift
distribution of these sources with $\sigma_{\mathrm{NMAD}}$ = 0.07 for HETDEX
($\sigma_{\mathrm{NMAD}}$ = 0.09 for Stripe 82) and an outlier fraction of 19%
(25% for Stripe 82), compatible with previous results based on broad-band
photometry. Feature importance analysis stresses the relevance of near- and
mid-infrared colours to select AGNs and identify their radio and redshift
nature. Combining different algorithms in ML models shows an improvement in the
prediction power of our pipeline over a random selection of sources. Tree-based
ML models (in contrast to deep learning techniques) facilitate the analysis of
the impact that features have on the predictions. This prediction can give
insight into the potential physical interplay between the properties of radio
AGNs (e.g. mass of black hole and accretion rate).",http://arxiv.org/pdf/2309.11652v1
2309.11651v1,eess.SY,Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks,2023-09-20 21:32:58+00:00,"Motivated by applications in queueing theory, we consider a stochastic
control problem whose state space is the $d$-dimensional positive orthant. The
controlled process $Z$ evolves as a reflected Brownian motion whose covariance
matrix is exogenously specified, as are its directions of reflection from the
orthant's boundary surfaces. A system manager chooses a drift vector
$\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at
time $t$ depends on both $Z(t)$ and $\theta(t)$. In our initial problem
formulation, the objective is to minimize expected discounted cost over an
infinite planning horizon, after which we treat the corresponding ergodic
control problem. Extending earlier work by Han et al. (Proceedings of the
National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a
simulation-based computational method that relies heavily on deep neural
network technology. For test problems studied thus far, our method is accurate
to within a fraction of one percent, and is computationally feasible in
dimensions up to at least $d=30$.",http://arxiv.org/pdf/2309.11651v1
2309.11647v1,quant-ph,Potential and limitations of random Fourier features for dequantizing quantum machine learning,2023-09-20 21:23:52+00:00,"Quantum machine learning is arguably one of the most explored applications of
near-term quantum devices. Much focus has been put on notions of variational
quantum machine learning where parameterized quantum circuits (PQCs) are used
as learning models. These PQC models have a rich structure which suggests that
they might be amenable to efficient dequantization via random Fourier features
(RFF). In this work, we establish necessary and sufficient conditions under
which RFF does indeed provide an efficient dequantization of variational
quantum machine learning for regression. We build on these insights to make
concrete suggestions for PQC architecture design, and to identify structures
which are necessary for a regression problem to admit a potential quantum
advantage via PQC based optimization.",http://arxiv.org/pdf/2309.11647v1
2309.11623v1,cs.IR,Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation,2023-09-20 20:21:13+00:00,"Music streaming services heavily rely on their recommendation engines to
continuously provide content to their consumers. Sequential recommendation
consequently has seen considerable attention in current literature, where state
of the art approaches focus on self-attentive models leveraging contextual
information such as long and short-term user history and item features;
however, most of these studies focus on long-form content domains (retail,
movie, etc.) rather than short-form, such as music. Additionally, many do not
explore incorporating negative session-level feedback during training. In this
study, we investigate the use of transformer-based self-attentive architectures
to learn implicit session-level information for sequential music
recommendation. We additionally propose a contrastive learning task to
incorporate negative feedback (e.g skipped tracks) to promote positive hits and
penalize negative hits. This task is formulated as a simple loss term that can
be incorporated into a variety of deep learning architectures for sequential
recommendation. Our experiments show that this results in consistent
performance gains over the baseline architectures ignoring negative user
feedback.",http://arxiv.org/pdf/2309.11623v1
2309.11617v1,quant-ph,Statistical Complexity of Quantum Learning,2023-09-20 20:04:05+00:00,"Recent years have seen significant activity on the problem of using data for
the purpose of learning properties of quantum systems or of processing
classical or quantum data via quantum computing. As in classical learning,
quantum learning problems involve settings in which the mechanism generating
the data is unknown, and the main goal of a learning algorithm is to ensure
satisfactory accuracy levels when only given access to data and, possibly, side
information such as expert knowledge. This article reviews the complexity of
quantum learning using information-theoretic techniques by focusing on data
complexity, copy complexity, and model complexity. Copy complexity arises from
the destructive nature of quantum measurements, which irreversibly alter the
state to be processed, limiting the information that can be extracted about
quantum data. For example, in a quantum system, unlike in classical machine
learning, it is generally not possible to evaluate the training loss
simultaneously on multiple hypotheses using the same quantum data. To make the
paper self-contained and approachable by different research communities, we
provide extensive background material on classical results from statistical
learning theory, as well as on the distinguishability of quantum states.
Throughout, we highlight the differences between quantum and classical learning
by addressing both supervised and unsupervised learning, and we provide
extensive pointers to the literature.",http://arxiv.org/pdf/2309.11617v1
2309.11612v1,eess.SY,Brief Architectural Survey of Biopotential Recording Front-Ends since the 1970s,2023-09-20 19:57:32+00:00,"Measuring the bioelectric signals is one of the key functions in wearable
healthcare devices and implantable medical devices. The use of wearable
healthcare devices has made continuous and immediate monitoring of personal
health status possible. Implantable medical devices have played an important
role throughout the fields of neuroscience, brain-machine (or brain-computer)
interface, and rehabilitation technology. Over the last five decades, the
bioelectric signals have been observed through a variety of biopotential
recording front-ends, along with advances in semiconductor technology scaling
and circuit techniques. Also, for reliable and continuous signal acquisition,
the front-end architectures have evolved while maintaining low power and low
noise performance. In this article, the architecture history of the
biopotential recording front-ends developed since the 1970s is surveyed, and
overall key circuit techniques are discussed. Depending on the bioelectric
signals being measured, appropriate front-end architecture needs to be chosen,
and the characteristics and challenges of each architecture are also covered in
this article.",http://arxiv.org/pdf/2309.11612v1
2309.11611v1,cs.CL,Hate speech detection in algerian dialect using deep learning,2023-09-20 19:54:48+00:00,"With the proliferation of hate speech on social networks under different
formats, such as abusive language, cyberbullying, and violence, etc., people
have experienced a significant increase in violence, putting them in
uncomfortable situations and threats. Plenty of efforts have been dedicated in
the last few years to overcome this phenomenon to detect hate speech in
different structured languages like English, French, Arabic, and others.
However, a reduced number of works deal with Arabic dialects like Tunisian,
Egyptian, and Gulf, mainly the Algerian ones. To fill in the gap, we propose in
this work a complete approach for detecting hate speech on online Algerian
messages. Many deep learning architectures have been evaluated on the corpus we
created from some Algerian social networks (Facebook, YouTube, and Twitter).
This corpus contains more than 13.5K documents in Algerian dialect written in
Arabic, labeled as hateful or non-hateful. Promising results are obtained,
which show the efficiency of our approach.",http://arxiv.org/pdf/2309.11611v1
