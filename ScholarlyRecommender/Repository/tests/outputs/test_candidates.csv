Id,Category,Title,Published,Abstract,URL
2309.13043v1,cs.RO,E(2)-Equivariant Graph Planning for Navigation,2023-09-22 17:59:48+00:00,"Learning for robot navigation presents a critical and challenging task. The
scarcity and costliness of real-world datasets necessitate efficient learning
approaches. In this letter, we exploit Euclidean symmetry in planning for 2D
navigation, which originates from Euclidean transformations between reference
frames and enables parameter sharing. To address the challenges of unstructured
environments, we formulate the navigation problem as planning on a geometric
graph and develop an equivariant message passing network to perform value
iteration. Furthermore, to handle multi-camera input, we propose a learnable
equivariant layer to lift features to a desired space. We conduct comprehensive
evaluations across five diverse tasks encompassing structured and unstructured
environments, along with maps of known and unknown, given point goals or
semantic goals. Our experiments confirm the substantial benefits on training
efficiency, stability, and generalization.",http://arxiv.org/pdf/2309.13043v1
2309.13042v1,cs.CV,MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation,2023-09-22 17:59:42+00:00,"We present MosaicFusion, a simple yet effective diffusion-based data
augmentation approach for large vocabulary instance segmentation. Our method is
training-free and does not rely on any label supervision. Two key designs
enable us to employ an off-the-shelf text-to-image diffusion model as a useful
dataset generator for object instances and mask annotations. First, we divide
an image canvas into several regions and perform a single round of diffusion
process to generate multiple instances simultaneously, conditioning on
different text prompts. Second, we obtain corresponding instance masks by
aggregating cross-attention maps associated with object prompts across layers
and diffusion time steps, followed by simple thresholding and edge-aware
refinement processing. Without bells and whistles, our MosaicFusion can produce
a significant amount of synthetic labeled data for both rare and novel
categories. Experimental results on the challenging LVIS long-tailed and
open-vocabulary benchmarks demonstrate that MosaicFusion can significantly
improve the performance of existing instance segmentation models, especially
for rare and novel categories. Code will be released at
https://github.com/Jiahao000/MosaicFusion.",http://arxiv.org/pdf/2309.13042v1
2309.13029v1,eess.AS,Memory-augmented conformer for improved end-to-end long-form ASR,2023-09-22 17:44:58+00:00,"Conformers have recently been proposed as a promising modelling approach for
automatic speech recognition (ASR), outperforming recurrent neural
network-based approaches and transformers. Nevertheless, in general, the
performance of these end-to-end models, especially attention-based models, is
particularly degraded in the case of long utterances. To address this
limitation, we propose adding a fully-differentiable memory-augmented neural
network between the encoder and decoder of a conformer. This external memory
can enrich the generalization for longer utterances since it allows the system
to store and retrieve more information recurrently. Notably, we explore the
neural Turing machine (NTM) that results in our proposed Conformer-NTM model
architecture for ASR. Experimental results using Librispeech train-clean-100
and train-960 sets show that the proposed system outperforms the baseline
conformer without memory for long utterances.",http://arxiv.org/pdf/2309.13029v1
2309.13021v1,cs.LG,A Hybrid Deep Learning-based Approach for Optimal Genotype by Environment Selection,2023-09-22 17:31:47+00:00,"Precise crop yield prediction is essential for improving agricultural
practices and ensuring crop resilience in varying climates. Integrating weather
data across the growing season, especially for different crop varieties, is
crucial for understanding their adaptability in the face of climate change. In
the MLCAS2021 Crop Yield Prediction Challenge, we utilized a dataset comprising
93,028 training records to forecast yields for 10,337 test records, covering
159 locations across 28 U.S. states and Canadian provinces over 13 years
(2003-2015). This dataset included details on 5,838 distinct genotypes and
daily weather data for a 214-day growing season, enabling comprehensive
analysis. As one of the winning teams, we developed two novel convolutional
neural network (CNN) architectures: the CNN-DNN model, combining CNN and
fully-connected networks, and the CNN-LSTM-DNN model, with an added LSTM layer
for weather variables. Leveraging the Generalized Ensemble Method (GEM), we
determined optimal model weights, resulting in superior performance compared to
baseline models. The GEM model achieved lower RMSE (5.55% to 39.88%), reduced
MAE (5.34% to 43.76%), and higher correlation coefficients (1.1% to 10.79%)
when evaluated on test data. We applied the CNN-DNN model to identify
top-performing genotypes for various locations and weather conditions, aiding
genotype selection based on weather variables. Our data-driven approach is
valuable for scenarios with limited testing years. Additionally, a feature
importance analysis using RMSE change highlighted the significance of location,
MG, year, and genotype, along with the importance of weather variables MDNI and
AP.",http://arxiv.org/pdf/2309.13021v1
2309.13019v1,cs.CE,Differential Evolution Algorithm Based Hyperparameter Selection of Gated Recurrent Unit for Electrical Load Forecasting,2023-09-22 17:30:29+00:00,"Accurate load forecasting remains a formidable challenge in numerous sectors,
given the intricate dynamics of dynamic power systems, which often defy
conventional statistical models. As a response, time-series methodologies like
ARIMA and sophisticated deep learning techniques such as Artificial Neural
Networks (ANN) and Long Short-Term Memory (LSTM) networks have demonstrated
their mettle by achieving enhanced predictive performance. In our
investigation, we delve into the efficacy of the relatively recent Gated
Recurrent Network (GRU) model within the context of load forecasting. GRU
models are garnering attention due to their inherent capacity to adeptly
capture and model temporal dependencies within data streams. Our methodology
entails harnessing the power of Differential Evolution, a versatile
optimization technique renowned for its prowess in delivering scalable, robust,
and globally optimal solutions, especially in scenarios involving
non-differentiable, multi-objective, or constrained optimization challenges.
Through rigorous analysis, we undertake a comparative assessment of the
proposed Gated Recurrent Network model, collaboratively fused with various
metaheuristic algorithms, evaluating their performance by leveraging
established numerical benchmarks such as Mean Squared Error (MSE) and Mean
Absolute Percentage Error (MAPE). Our empirical investigations are conducted
using power load data originating from the Ontario province, Canada. Our
research findings cast a spotlight on the remarkable potential of
metaheuristic-augmented Gated Recurrent Network models in substantially
augmenting load forecasting precision, offering tailored, optimal
hyperparameter configurations uniquely suited to each model's performance
characteristics.",http://arxiv.org/pdf/2309.13019v1
2309.13015v1,cs.LG,"Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design",2023-09-22 17:26:19+00:00,"Sparse training is one of the promising techniques to reduce the
computational cost of DNNs while retaining high accuracy. In particular, N:M
fine-grained structured sparsity, where only N out of consecutive M elements
can be nonzero, has attracted attention due to its hardware-friendly pattern
and capability of achieving a high sparse ratio. However, the potential to
accelerate N:M sparse DNN training has not been fully exploited, and there is a
lack of efficient hardware supporting N:M sparse training. To tackle these
challenges, this paper presents a computation-efficient training scheme for N:M
sparse DNNs using algorithm, architecture, and dataflow co-design. At the
algorithm level, a bidirectional weight pruning method, dubbed BDWP, is
proposed to leverage the N:M sparsity of weights during both forward and
backward passes of DNN training, which can significantly reduce the
computational cost while maintaining model accuracy. At the architecture level,
a sparse accelerator for DNN training, namely SAT, is developed to neatly
support both the regular dense operations and the computation-efficient N:M
sparse operations. At the dataflow level, multiple optimization methods ranging
from interleave mapping, pre-generation of N:M sparse weights, and offline
scheduling, are proposed to boost the computational efficiency of SAT. Finally,
the effectiveness of our training scheme is evaluated on a Xilinx VCU1525 FPGA
card using various DNN models and datasets. Experimental results show the SAT
accelerator with the BDWP sparse training method under 2:8 sparse ratio
achieves an average speedup of 1.75x over that with the dense training,
accompanied by a negligible accuracy loss of 0.56% on average. Furthermore, our
proposed training scheme significantly improves the training throughput by
2.97~25.22x and the energy efficiency by 1.36~3.58x over prior FPGA-based
accelerators.",http://arxiv.org/pdf/2309.13015v1
2309.13007v1,cs.CL,ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs,2023-09-22 17:12:45+00:00,"Large Language Models (LLMs) still struggle with complex reasoning tasks.
Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a
multi-model multi-agent framework designed as a round table conference among
diverse LLM agents to foster diverse thoughts and discussion for improved
consensus. ReConcile enhances the reasoning capabilities of LLMs by holding
multiple rounds of discussion, learning to convince other agents to improve
their answers, and employing a confidence-weighted voting mechanism. In each
round, ReConcile initiates discussion between agents via a 'discussion prompt'
that consists of (a) grouped answers and explanations generated by each agent
in the previous round, (b) their uncertainties, and (c) demonstrations of
answer-rectifying human explanations, used for convincing other agents. This
discussion prompt enables each agent to revise their responses in light of
insights from other agents. Once a consensus is reached and the discussion
ends, ReConcile determines the final answer by leveraging the confidence of
each agent in a weighted voting scheme. We implement ReConcile with ChatGPT,
Bard, and Claude2 as the three agents. Our experimental results on various
benchmarks demonstrate that ReConcile significantly enhances the reasoning
performance of the agents (both individually and as a team), surpassing prior
single-agent and multi-agent baselines by 7.7% and also outperforming GPT-4 on
some of these datasets. We also experiment with GPT-4 itself as one of the
agents in ReConcile and demonstrate that its initial performance also improves
by absolute 10.0% through discussion and feedback from other agents. Finally,
we also analyze the accuracy after every round and observe that ReConcile
achieves better and faster consensus between agents, compared to a multi-agent
debate baseline. Our code is available at: https://github.com/dinobby/ReConcile",http://arxiv.org/pdf/2309.13007v1
2309.13005v1,cs.LG,Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains,2023-09-22 17:08:20+00:00,"Recognizing the prevalence of domain shift as a common challenge in machine
learning, various domain generalization (DG) techniques have been developed to
enhance the performance of machine learning systems when dealing with
out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data
distributions can gradually change across a sequence of sequential domains.
While current methodologies primarily focus on improving model effectiveness
within these new domains, they often overlook fairness issues throughout the
learning process. In response, we introduce an innovative framework called
Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder
(CDSAE). This approach effectively separates environmental information and
sensitive attributes from the embedded representation of classification
features. This concurrent separation not only greatly improves model
generalization across diverse and unfamiliar domains but also effectively
addresses challenges related to unfair classification. Our strategy is rooted
in the principles of causal inference to tackle these dual issues. To examine
the intricate relationship between semantic information, sensitive attributes,
and environmental cues, we systematically categorize exogenous uncertainty
factors into four latent variables: 1) semantic information influenced by
sensitive attributes, 2) semantic information unaffected by sensitive
attributes, 3) environmental cues influenced by sensitive attributes, and 4)
environmental cues unaffected by sensitive attributes. By incorporating
fairness regularization, we exclusively employ semantic information for
classification purposes. Empirical validation on synthetic and real-world
datasets substantiates the effectiveness of our approach, demonstrating
improved accuracy levels while ensuring the preservation of fairness in the
evolving landscape of continuous domains.",http://arxiv.org/pdf/2309.13005v1
2309.12998v1,cs.CL,Audience-specific Explanations for Machine Translation,2023-09-22 17:00:45+00:00,"In machine translation, a common problem is that the translation of certain
words even if translated can cause incomprehension of the target language
audience due to different cultural backgrounds. A solution to solve this
problem is to add explanations for these words. In a first step, we therefore
need to identify these words or phrases. In this work we explore techniques to
extract example explanations from a parallel corpus. However, the sparsity of
sentences containing words that need to be explained makes building the
training dataset extremely difficult. In this work, we propose a semi-automatic
technique to extract these explanations from a large parallel corpus.
Experiments on English->German language pair show that our method is able to
extract sentence so that more than 10% of the sentences contain explanation,
while only 1.9% of the original sentences contain explanations. In addition,
experiments on English->French and English->Chinese language pairs also show
similar conclusions. This is therefore an essential first automatic step to
create a explanation dataset. Furthermore we show that the technique is robust
for all three language pairs.",http://arxiv.org/pdf/2309.12998v1
2309.12977v1,cs.IT,Performance Evaluation for Subarray-based Reconfigurable Intelligent Surface-Aided Wireless Communication Systems,2023-09-22 16:18:33+00:00,"Reconfigurable intelligent surfaces (RISs) have received extensive concern to
improve the performance of wireless communication systems. In this paper, a
subarray-based scheme is investigated in terms of its effects on ergodic
spectral efficiency (SE) and energy efficiency (EE) in RIS-assisted systems. In
this scheme, the adjacent elements divided into a subarray are controlled by
one signal and share the same reflection coefficient. An upper bound of ergodic
SE is derived and an optimal phase shift design is proposed for the
subarray-based RIS. Based on the upper bound and optimal design, we obtain the
maximum of the upper bound. In particular, we analytically evaluate the effect
of the subarray-based RIS on EE since it reduces SE and power consumption
simultaneously. Numerical results verify the tightness of the upper bound,
demonstrate the effectiveness of the optimal phase shift design for the
subarray-based RIS, and reveal the effects of the subarray-based scheme on SE
and EE.",http://arxiv.org/pdf/2309.12977v1
2309.12971v1,cs.LG,Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes,2023-09-22 16:11:17+00:00,"Despite the recent successes of vanilla Graph Neural Networks (GNNs) on many
tasks, their foundation on pairwise interaction networks inherently limits
their capacity to discern latent higher-order interactions in complex systems.
To bridge this capability gap, we propose a novel approach exploiting the rich
mathematical theory of simplicial complexes (SCs) - a robust tool for modeling
higher-order interactions. Current SC-based GNNs are burdened by high
complexity and rigidity, and quantifying higher-order interaction strengths
remains challenging. Innovatively, we present a higher-order Flower-Petals (FP)
model, incorporating FP Laplacians into SCs. Further, we introduce a
Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians,
capable of discerning intrinsic features across varying topological scales. By
employing learnable graph filters, a parameter group within each FP Laplacian
domain, we can identify diverse patterns where the filters' weights serve as a
quantifiable measure of higher-order interaction strengths. The theoretical
underpinnings of HiGCN's advanced expressiveness are rigorously demonstrated.
Additionally, our empirical investigations reveal that the proposed model
accomplishes state-of-the-art (SOTA) performance on a range of graph tasks and
provides a scalable and flexible solution to explore higher-order interactions
in graphs.",http://arxiv.org/pdf/2309.12971v1
2309.12941v1,cs.SE,Trusta: Reasoning about Assurance Cases with Formal Methods and Large Language Models,2023-09-22 15:42:43+00:00,"Assurance cases can be used to argue for the safety of products in safety
engineering. In safety-critical areas, the construction of assurance cases is
indispensable. Trustworthiness Derivation Trees (TDTs) enhance assurance cases
by incorporating formal methods, rendering it possible for automatic reasoning
about assurance cases. We present Trustworthiness Derivation Tree Analyzer
(Trusta), a desktop application designed to automatically construct and verify
TDTs. The tool has a built-in Prolog interpreter in its backend, and is
supported by the constraint solvers Z3 and MONA. Therefore, it can solve
constraints about logical formulas involving arithmetic, sets, Horn clauses
etc. Trusta also utilizes large language models to make the creation and
evaluation of assurance cases more convenient. It allows for interactive human
examination and modification. We evaluated top language models like
ChatGPT-3.5, ChatGPT-4, and PaLM 2 for generating assurance cases. Our tests
showed a 50%-80% similarity between machine-generated and human-created cases.
In addition, Trusta can extract formal constraints from text in natural
languages, facilitating an easier interpretation and validation process. This
extraction is subject to human review and correction, blending the best of
automated efficiency with human insight. To our knowledge, this marks the first
integration of large language models in automatic creating and reasoning about
assurance cases, bringing a novel approach to a traditional challenge. Through
several industrial case studies, Trusta has proven to quickly find some subtle
issues that are typically missed in manual inspection, demonstrating its
practical value in enhancing the assurance case development process.",http://arxiv.org/pdf/2309.12941v1
2309.12940v1,cs.CL,Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models,2023-09-22 15:41:34+00:00,"Task-oriented dialogue (TOD) systems facilitate users in executing various
activities via multi-turn dialogues, but Large Language Models (LLMs) often
struggle to comprehend these intricate contexts. In this study, we propose a
novel ""Self-Explanation"" prompting strategy to enhance the comprehension
abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires
the model to analyze each dialogue utterance before task execution, thereby
improving performance across various dialogue-centric tasks. Experimental
results from six benchmark datasets confirm that our method consistently
outperforms other zero-shot prompts and matches or exceeds the efficacy of
few-shot prompts, demonstrating its potential as a powerful tool in enhancing
LLMs' comprehension in complex dialogue tasks.",http://arxiv.org/pdf/2309.12940v1
2309.12938v1,cs.AI,Frustrated with Code Quality Issues? LLMs can Help!,2023-09-22 15:37:07+00:00,"As software projects progress, quality of code assumes paramount importance
as it affects reliability, maintainability and security of software. For this
reason, static analysis tools are used in developer workflows to flag code
quality issues. However, developers need to spend extra efforts to revise their
code to improve code quality based on the tool findings. In this work, we
investigate the use of (instruction-following) large language models (LLMs) to
assist developers in revising code to resolve code quality issues. We present a
tool, CORE (short for COde REvisions), architected using a pair of LLMs
organized as a duo comprised of a proposer and a ranker. Providers of static
analysis tools recommend ways to mitigate the tool warnings and developers
follow them to revise their code. The \emph{proposer LLM} of CORE takes the
same set of recommendations and applies them to generate candidate code
revisions. The candidates which pass the static quality checks are retained.
However, the LLM may introduce subtle, unintended functionality changes which
may go un-detected by the static analysis. The \emph{ranker LLM} evaluates the
changes made by the proposer using a rubric that closely follows the acceptance
criteria that a developer would enforce. CORE uses the scores assigned by the
ranker LLM to rank the candidate revisions before presenting them to the
developer. CORE could revise 59.2% Python files (across 52 quality checks) so
that they pass scrutiny by both a tool and a human reviewer. The ranker LLM is
able to reduce false positives by 25.8% in these cases. CORE produced revisions
that passed the static analysis tool in 76.8% Java files (across 10 quality
checks) comparable to 78.3% of a specialized program repair tool, with
significantly much less engineering efforts.",http://arxiv.org/pdf/2309.12938v1
2309.12937v1,cs.RO,Evolving Spiking Neural Networks to Mimic PID Control for Autonomous Blimps,2023-09-22 15:34:18+00:00,"In recent years, Artificial Neural Networks (ANN) have become a standard in
robotic control. However, a significant drawback of large-scale ANNs is their
increased power consumption. This becomes a critical concern when designing
autonomous aerial vehicles, given the stringent constraints on power and
weight. Especially in the case of blimps, known for their extended endurance,
power-efficient control methods are essential. Spiking neural networks (SNN)
can provide a solution, facilitating energy-efficient and asynchronous
event-driven processing. In this paper, we have evolved SNNs for accurate
altitude control of a non-neutrally buoyant indoor blimp, relying solely on
onboard sensing and processing power. The blimp's altitude tracking performance
significantly improved compared to prior research, showing reduced oscillations
and a minimal steady-state error. The parameters of the SNNs were optimized via
an evolutionary algorithm, using a Proportional-Derivative-Integral (PID)
controller as the target signal. We developed two complementary SNN controllers
while examining various hidden layer structures. The first controller responds
swiftly to control errors, mitigating overshooting and oscillations, while the
second minimizes steady-state errors due to non-neutral buoyancy-induced drift.
Despite the blimp's drivetrain limitations, our SNN controllers ensured stable
altitude control, employing only 160 spiking neurons.",http://arxiv.org/pdf/2309.12937v1
2309.12931v1,cs.CL,On Separate Normalization in Self-supervised Transformers,2023-09-22 15:30:53+00:00,"Self-supervised training methods for transformers have demonstrated
remarkable performance across various domains. Previous transformer-based
models, such as masked autoencoders (MAE), typically utilize a single
normalization layer for both the [CLS] symbol and the tokens. We propose in
this paper a simple modification that employs separate normalization layers for
the tokens and the [CLS] symbol to better capture their distinct
characteristics and enhance downstream task performance. Our method aims to
alleviate the potential negative effects of using the same normalization
statistics for both token types, which may not be optimally aligned with their
individual roles. We empirically show that by utilizing a separate
normalization layer, the [CLS] embeddings can better encode the global
contextual information and are distributed more uniformly in its anisotropic
space. When replacing the conventional normalization layer with the two
separate layers, we observe an average 2.7% performance improvement over the
image, natural language, and graph domains.",http://arxiv.org/pdf/2309.12931v1
2309.12913v1,cs.AI,A matter of attitude: Focusing on positive and active gradients to boost saliency maps,2023-09-22 15:00:00+00:00,"Saliency maps have become one of the most widely used interpretability
techniques for convolutional neural networks (CNN) due to their simplicity and
the quality of the insights they provide. However, there are still some doubts
about whether these insights are a trustworthy representation of what CNNs use
to come up with their predictions. This paper explores how rescuing the sign of
the gradients from the saliency map can lead to a deeper understanding of
multi-class classification problems. Using both pretrained and trained from
scratch CNNs we unveil that considering the sign and the effect not only of the
correct class, but also the influence of the other classes, allows to better
identify the pixels of the image that the network is really focusing on.
Furthermore, how occluding or altering those pixels is expected to affect the
outcome also becomes clearer.",http://arxiv.org/pdf/2309.12913v1
2309.12908v1,cs.AI,KG-MDL: Mining Graph Patterns in Knowledge Graphs with the MDL Principle,2023-09-22 14:52:10+00:00,"Nowadays, increasingly more data are available as knowledge graphs (KGs).
While this data model supports advanced reasoning and querying, they remain
difficult to mine due to their size and complexity. Graph mining approaches can
be used to extract patterns from KGs. However this presents two main issues.
First, graph mining approaches tend to extract too many patterns for a human
analyst to interpret (pattern explosion). Second, real-life KGs tend to differ
from the graphs usually treated in graph mining: they are multigraphs, their
vertex degrees tend to follow a power-law, and the way in which they model
knowledge can produce spurious patterns. Recently, a graph mining approach
named GraphMDL+ has been proposed to tackle the problem of pattern explosion,
using the Minimum Description Length (MDL) principle. However, GraphMDL+, like
other graph mining approaches, is not suited for KGs without adaptations. In
this paper we propose KG-MDL, a graph pattern mining approach based on the MDL
principle that, given a KG, generates a human-sized and descriptive set of
graph patterns, and so in a parameter-less and anytime way. We report on
experiments on medium-sized KGs showing that our approach generates sets of
patterns that are both small enough to be interpreted by humans and descriptive
of the KG. We show that the extracted patterns highlight relevant
characteristics of the data: both of the schema used to create the data, and of
the concrete facts it contains. We also discuss the issues related to mining
graph patterns on knowledge graphs, as opposed to other types of graph data.",http://arxiv.org/pdf/2309.12908v1
2309.12901v1,cs.NI,Analytical Model of 5G V2X Mode 2 for Sporadic Traffic,2023-09-22 14:38:14+00:00,"5G Vehicle-to-Everything (V2X) is a promising technology to satisfy the
increasing demands of intelligent transportation systems. Emerging V2X
applications with a high level of automation impose very strict requirements on
latency (less than 10 ms) and reliability (higher than 99.99%). For sporadic
traffic, such demands can be satisfied with a distributed channel access method
called Mode 2. This letter proposes an analytical model of Mode 2 that
estimates the packet loss rate and the network capacity taking into account the
peculiarities of Mode 2 and -- in contrast to the existing models -- provides
the accuracy required in the emerging V2X scenarios. The model can be used to
find the optimal transmission parameters that maximize the network capacity
and/or to select the required bandwidth.",http://arxiv.org/pdf/2309.12901v1
2309.12892v1,cs.CL,ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation Extraction,2023-09-22 14:26:06+00:00,"Event Relation Extraction (ERE) aims to extract multiple kinds of relations
among events in texts. However, existing methods singly categorize event
relations as different classes, which are inadequately capturing the intrinsic
semantics of these relations. To comprehensively understand their intrinsic
semantics, in this paper, we obtain prototype representations for each type of
event relation and propose a Prototype-Enhanced Matching (ProtoEM) framework
for the joint extraction of multiple kinds of event relations. Specifically,
ProtoEM extracts event relations in a two-step manner, i.e., prototype
representing and prototype matching. In the first step, to capture the
connotations of different event relations, ProtoEM utilizes examples to
represent the prototypes corresponding to these relations. Subsequently, to
capture the interdependence among event relations, it constructs a dependency
graph for the prototypes corresponding to these relations and utilized a Graph
Neural Network (GNN)-based module for modeling. In the second step, it obtains
the representations of new event pairs and calculates their similarity with
those prototypes obtained in the first step to evaluate which types of event
relations they belong to. Experimental results on the MAVEN-ERE dataset
demonstrate that the proposed ProtoEM framework can effectively represent the
prototypes of event relations and further obtain a significant improvement over
baseline models.",http://arxiv.org/pdf/2309.12892v1
2309.12881v1,cs.CL,Affect Recognition in Conversations Using Large Language Models,2023-09-22 14:11:23+00:00,"Affect recognition, encompassing emotions, moods, and feelings, plays a
pivotal role in human communication. In the realm of conversational artificial
intelligence (AI), the ability to discern and respond to human affective cues
is a critical factor for creating engaging and empathetic interactions. This
study delves into the capacity of large language models (LLMs) to recognise
human affect in conversations, with a focus on both open-domain chit-chat
dialogues and task-oriented dialogues. Leveraging three diverse datasets,
namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues from
casual conversations to clinical interviews, we evaluated and compared LLMs'
performance in affect recognition. Our investigation explores the zero-shot and
few-shot capabilities of LLMs through in-context learning (ICL) as well as
their model capacities through task-specific fine-tuning. Additionally, this
study takes into account the potential impact of automatic speech recognition
(ASR) errors on LLM predictions. With this work, we aim to shed light on the
extent to which LLMs can replicate human-like affect recognition capabilities
in conversations.",http://arxiv.org/pdf/2309.12881v1
2309.12880v1,astro-ph.SR,The impact of angle-dependent partial frequency redistribution on the scattering polarization of the solar Na i D lines,2023-09-22 14:10:24+00:00,"The long-standing paradox of the linear polarization signal of the Na i D1
line was recently resolved by accounting for the atom's hyperfine structure and
the detailed spectral structure of the incident radiation field. That modeling
relied on the simplifying angle-averaged (AA) approximation for partial
frequency redistribution (PRD) in scattering, which potentially neglects
important angle-frequency couplings. This work aims at evaluating the
suitability of a PRD-AA modeling for the D1 and D2 lines through comparisons
with general angle-dependent (AD) PRD calculations, both in the absence and
presence of magnetic fields. We solved the radiative transfer problem for
polarized radiation in a one-dimensional semi-empirical atmospheric model with
microturbulent and isotropic magnetic fields, accounting for PRD effects,
comparing PRD-AA and PRD-AD modelings. The D1 and D2 lines are modeled
separately as two-level atomic system with hyperfine structure. The numerical
results confirm that a spectrally structured radiation field induces linear
polarization in the D1 line. However, the PRD-AA approximation greatly impacts
the Q/I shape, producing an antisymmetric pattern instead of the more symmetric
PRD-AD one, while presenting a similar sensitivity to magnetic fields between
10 and 200 G. Under the PRD-AA approximation, the Q/I profile of the D2 line
presents an artificial dip in its core, which is not found for the PRD-AD case.
We conclude that accounting for PRD-AD effects is essential to suitably model
the scattering polarization of the Na i D lines. These results bring us closer
to exploiting the full diagnostic potential of these lines for the elusive
chromospheric magnetic fields.",http://arxiv.org/pdf/2309.12880v1
2309.12876v1,cs.CV,Gravity Network for end-to-end small lesion detection,2023-09-22 14:02:22+00:00,"This paper introduces a novel one-stage end-to-end detector specifically
designed to detect small lesions in medical images. Precise localization of
small lesions presents challenges due to their appearance and the diverse
contextual backgrounds in which they are found. To address this, our approach
introduces a new type of pixel-based anchor that dynamically moves towards the
targeted lesion for detection. We refer to this new architecture as GravityNet,
and the novel anchors as gravity points since they appear to be ""attracted"" by
the lesions. We conducted experiments on two well-established medical problems
involving small lesions to evaluate the performance of the proposed approach:
microcalcifications detection in digital mammograms and microaneurysms
detection in digital fundus images. Our method demonstrates promising results
in effectively detecting small lesions in these medical imaging tasks.",http://arxiv.org/pdf/2309.12876v1
2309.12871v1,cs.CL,AnglE-Optimized Text Embeddings,2023-09-22 13:52:42+00:00,"High-quality text embedding is pivotal in improving semantic textual
similarity (STS) tasks, which are crucial components in Large Language Model
(LLM) applications. However, a common challenge existing text embedding models
face is the problem of vanishing gradients, primarily due to their reliance on
the cosine function in the optimization objective, which has saturation zones.
To address this issue, this paper proposes a novel angle-optimized text
embedding model called AnglE. The core idea of AnglE is to introduce angle
optimization in a complex space. This novel approach effectively mitigates the
adverse effects of the saturation zone in the cosine function, which can impede
gradient and hinder optimization processes. To set up a comprehensive STS
evaluation, we experimented on existing short-text STS datasets and a newly
collected long-text STS dataset from GitHub Issues. Furthermore, we examine
domain-specific STS scenarios with limited labeled data and explore how AnglE
works with LLM-annotated data. Extensive experiments were conducted on various
tasks including short-text STS, long-text STS, and domain-specific STS tasks.
The results show that AnglE outperforms the state-of-the-art (SOTA) STS models
that ignore the cosine saturation zone. These findings demonstrate the ability
of AnglE to generate high-quality text embeddings and the usefulness of angle
optimization in STS.",http://arxiv.org/pdf/2309.12871v1
2309.12867v1,cs.CV,Accurate and Fast Compressed Video Captioning,2023-09-22 13:43:22+00:00,"Existing video captioning approaches typically require to first sample video
frames from a decoded video and then conduct a subsequent process (e.g.,
feature extraction and/or captioning model learning). In this pipeline, manual
frame sampling may ignore key information in videos and thus degrade
performance. Additionally, redundant information in the sampled frames may
result in low efficiency in the inference of video captioning. Addressing this,
we study video captioning from a different perspective in compressed domain,
which brings multi-fold advantages over the existing pipeline: 1) Compared to
raw images from the decoded video, the compressed video, consisting of
I-frames, motion vectors and residuals, is highly distinguishable, which allows
us to leverage the entire video for learning without manual sampling through a
specialized model design; 2) The captioning model is more efficient in
inference as smaller and less redundant information is processed. We propose a
simple yet effective end-to-end transformer in the compressed domain for video
captioning that enables learning from the compressed video for captioning. We
show that even with a simple design, our method can achieve state-of-the-art
performance on different benchmarks while running almost 2x faster than
existing approaches. Code is available at https://github.com/acherstyx/CoCap.",http://arxiv.org/pdf/2309.12867v1
2309.12863v1,cs.CL,Domain Adaptation for Arabic Machine Translation: The Case of Financial Texts,2023-09-22 13:37:19+00:00,"Neural machine translation (NMT) has shown impressive performance when
trained on large-scale corpora. However, generic NMT systems have demonstrated
poor performance on out-of-domain translation. To mitigate this issue, several
domain adaptation methods have recently been proposed which often lead to
better translation quality than genetic NMT systems. While there has been some
continuous progress in NMT for English and other European languages, domain
adaption in Arabic has received little attention in the literature. The current
study, therefore, aims to explore the effectiveness of domain-specific
adaptation for Arabic MT (AMT), in yet unexplored domain, financial news
articles. To this end, we developed carefully a parallel corpus for
Arabic-English (AR- EN) translation in the financial domain for benchmarking
different domain adaptation methods. We then fine-tuned several pre-trained NMT
and Large Language models including ChatGPT-3.5 Turbo on our dataset. The
results showed that the fine-tuning is successful using just a few well-aligned
in-domain AR-EN segments. The quality of ChatGPT translation was superior than
other models based on automatic and human evaluations. To the best of our
knowledge, this is the first work on fine-tuning ChatGPT towards financial
domain transfer learning. To contribute to research in domain translation, we
made our datasets and fine-tuned models available at
https://huggingface.co/asas-ai/.",http://arxiv.org/pdf/2309.12863v1
2309.12858v1,cs.IR,Diffusion Augmentation for Sequential Recommendation,2023-09-22 13:31:34+00:00,"Sequential recommendation (SRS) has become the technical foundation in many
applications recently, which aims to recommend the next item based on the
user's historical interactions. However, sequential recommendation often faces
the problem of data sparsity, which widely exists in recommender systems.
Besides, most users only interact with a few items, but existing SRS models
often underperform these users. Such a problem, named the long-tail user
problem, is still to be resolved. Data augmentation is a distinct way to
alleviate these two problems, but they often need fabricated training
strategies or are hindered by poor-quality generated interactions. To address
these problems, we propose a Diffusion Augmentation for Sequential
Recommendation (DiffuASR) for a higher quality generation. The augmented
dataset by DiffuASR can be used to train the sequential recommendation models
directly, free from complex training procedures. To make the best of the
generation ability of the diffusion model, we first propose a diffusion-based
pseudo sequence generation framework to fill the gap between image and sequence
generation. Then, a sequential U-Net is designed to adapt the diffusion noise
prediction model U-Net to the discrete sequence generation task. At last, we
develop two guide strategies to assimilate the preference between generated and
origin sequences. To validate the proposed DiffuASR, we conduct extensive
experiments on three real-world datasets with three sequential recommendation
models. The experimental results illustrate the effectiveness of DiffuASR. As
far as we know, DiffuASR is one pioneer that introduce the diffusion model to
the recommendation.",http://arxiv.org/pdf/2309.12858v1
2309.12832v1,physics.flu-dyn,Accuracy and stability analysis of horizontal discretizations used in unstructured grid ocean models,2023-09-22 12:39:35+00:00,"One important tool at our disposal to evaluate the robustness of Global
Circulation Models (GCMs) is to understand the horizontal discretization of the
dynamical core under a shallow water approximation. Here, we evaluate the
accuracy and stability of different methods used in, or adequate for,
unstructured ocean models considering shallow water models. Our results show
that the schemes have different accuracy capabilities, with the A- (NICAM) and
B-grid (FeSOM 2.0) schemes providing at least 1st order accuracy in most
operators and time integrated variables, while the two C-grid (ICON and MPAS)
schemes display more difficulty in adequately approximating the horizontal
dynamics. Moreover, the theory of the inertia-gravity wave representation on
regular grids can be extended for our unstructured based schemes, where from
least to most accurate we have: A-, B, and C-grid, respectively. Considering
only C-grid schemes, the MPAS scheme has shown a more accurate representation
of inertia-gravity waves than ICON. In terms of stability, we see that both A-
and C-grid MPAS scheme display the best stability properties, but the A-grid
scheme relies on artificial diffusion, while the C-grid scheme doesn't.
Alongside, the B-grid and C-grid ICON schemes are within the least stable.
Finally, in an effort to understand the effects of potential instabilities in
ICON, we note that the full 3D model without a filtering term does not
destabilize as it is integrated in time. However, spurious oscillations are
responsible for decreasing the kinetic energy of the oceanic currents.
Furthermore, an additional decrease of the currents' turbulent kinetic energy
is also observed, creating a spurious mixing, which also plays a role in the
strength decrease of these oceanic currents.",http://arxiv.org/pdf/2309.12832v1
2309.12830v1,cs.AR,AxOCS: Scaling FPGA-based Approximate Operators using Configuration Supersampling,2023-09-22 12:36:40+00:00,"The rising usage of AI and ML-based processing across application domains has
exacerbated the need for low-cost ML implementation, specifically for
resource-constrained embedded systems. To this end, approximate computing, an
approach that explores the power, performance, area (PPA), and behavioral
accuracy (BEHAV) trade-offs, has emerged as a possible solution for
implementing embedded machine learning. Due to the predominance of MAC
operations in ML, designing platform-specific approximate arithmetic operators
forms one of the major research problems in approximate computing. Recently
there has been a rising usage of AI/ML-based design space exploration
techniques for implementing approximate operators. However, most of these
approaches are limited to using ML-based surrogate functions for predicting the
PPA and BEHAV impact of a set of related design decisions. While this approach
leverages the regression capabilities of ML methods, it does not exploit the
more advanced approaches in ML. To this end, we propose AxOCS, a methodology
for designing approximate arithmetic operators through ML-based supersampling.
Specifically, we present a method to leverage the correlation of PPA and BEHAV
metrics across operators of varying bit-widths for generating larger bit-width
operators. The proposed approach involves traversing the relatively smaller
design space of smaller bit-width operators and employing its associated
Design-PPA-BEHAV relationship to generate initial solutions for
metaheuristics-based optimization for larger operators. The experimental
evaluation of AxOCS for FPGA-optimized approximate operators shows that the
proposed approach significantly improves the quality-resulting hypervolume for
multi-objective optimization-of 8x8 signed approximate multipliers.",http://arxiv.org/pdf/2309.12830v1
2309.12829v1,cs.CV,Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography,2023-09-22 12:36:30+00:00,"Accurate segmentation is essential for echocardiography-based assessment of
cardiovascular diseases (CVDs). However, the variability among sonographers and
the inherent challenges of ultrasound images hinder precise segmentation. By
leveraging the joint representation of image and text modalities,
Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual
information, potentially aiding in accurate and explainable segmentation.
However, the lack of readily available data in echocardiography hampers the
training of VLSMs. In this study, we explore using synthetic datasets from
Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography
segmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS)
using seven different kinds of language prompts derived from several
attributes, automatically extracted from echocardiography images, segmentation
masks, and their metadata. Our results show improved metrics and faster
convergence when pretraining VLSMs on SDM-generated synthetic images before
finetuning on real images. The code, configs, and prompts are available at
https://github.com/naamiinepal/synthetic-boost.",http://arxiv.org/pdf/2309.12829v1
2309.12825v1,cs.RO,OmniDrones: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control,2023-09-22 12:26:36+00:00,"In this work, we introduce OmniDrones, an efficient and flexible platform
tailored for reinforcement learning in drone control, built on Nvidia's
Omniverse Isaac Sim. It employs a bottom-up design approach that allows users
to easily design and experiment with various application scenarios on top of
GPU-parallelized simulations. It also offers a range of benchmark tasks,
presenting challenges ranging from single-drone hovering to over-actuated
system tracking. In summary, we propose an open-sourced drone simulation
platform, equipped with an extensive suite of tools for drone learning. It
includes 4 drone models, 5 sensor modalities, 4 control modes, over 10
benchmark tasks, and a selection of widely used RL baselines. To showcase the
capabilities of OmniDrones and to support future research, we also provide
preliminary results on these benchmark tasks. We hope this platform will
encourage further studies on applying RL to practical drone systems.",http://arxiv.org/pdf/2309.12825v1
2309.12821v1,q-bio.NC,A Spectral Theory of Neural Prediction and Alignment,2023-09-22 12:24:06+00:00,"The representations of neural networks are often compared to those of
biological systems by performing regression between the neural network
responses and those measured from biological systems. Many different
state-of-the-art deep neural networks yield similar neural predictions, but it
remains unclear how to differentiate among models that perform equally well at
predicting neural responses. To gain insight into this, we use a recent
theoretical framework that relates the generalization error from regression to
the spectral bias of the model activations and the alignment of the neural
responses onto the learnable subspace of the model. We extend this theory to
the case of regression between model activations and neural responses, and
define geometrical properties describing the error embedding geometry. We test
a large number of deep neural networks that predict visual cortical activity
and show that there are multiple types of geometries that result in low neural
prediction error as measured via regression. The work demonstrates that
carefully decomposing representational metrics can provide interpretability of
how models are capturing neural activity and points the way towards improved
models of neural activity.",http://arxiv.org/pdf/2309.12821v1
2309.12781v1,cs.MA,AgentChat: Multi-Agent Collaborative Logistics for Carbon Reduction,2023-09-22 10:46:45+00:00,"Heavy Good Vehicles (HGVs) are the second largest source of greenhouse gas
emissions in transportation, after cars and taxis. However, HGVs are
inefficiently utilised, with more than one-third of their weight capacity not
being used during travel. We, thus, in this paper address collaborative
logistics, an effective pathway to enhance HGVs' utilisation and reduce carbon
emissions. We investigate a multi-agent system approach to facilitate
collaborative logistics, particularly carrier collaboration. We propose a
simple yet effective multi-agent collaborative logistics (MACL) framework,
representing key stakeholders as intelligent agents. Furthermore, we utilise
the MACL framework in conjunction with a proposed system architecture to create
an integrated collaborative logistics testbed. This testbed, consisting of a
physical system and its digital replica, is a tailored cyber-physical system or
digital twin for collaborative logistics. Through a demonstration, we show the
utility of the testbed for studying collaborative logistics.",http://arxiv.org/pdf/2309.12781v1
2309.12775v1,cs.MM,Semantic Change Driven Generative Semantic Communication Framework,2023-09-22 10:34:11+00:00,"The burgeoning generative artificial intelligence technology offers novel
insights into the development of semantic communication (SemCom) frameworks.
These frameworks hold the potential to address the challenges associated with
the black-box nature inherent in existing end-to-end training manner for the
existing SemCom framework, as well as deterioration of the user experience
caused by the inevitable error floor in deep learning-based semantic
communication. In this paper, we focus on the widespread remote monitoring
scenario, and propose a semantic change driven generative SemCom framework.
Therein, the semantic encoder and semantic decoder can be optimized
independently. Specifically, we develop a modular semantic encoder with value
of information based semantic sampling function. In addition, we propose a
conditional denoising diffusion probabilistic mode-assisted semantic decoder
that relies on received semantic information from the source, namely, the
semantic map, and the local static scene information to remotely regenerate
scenes. Moreover, we demonstrate the effectiveness of the proposed semantic
encoder and decoder as well as the considerable potential in reducing energy
consumption through simulation. The code is available at
https://github.com/wty2011jl/SCDGSC.git",http://arxiv.org/pdf/2309.12775v1
2309.12766v1,eess.AS,A Study on Incorporating Whisper for Robust Speech Assessment,2023-09-22 10:11:05+00:00,"This research introduces an enhanced version of the multi-objective speech
assessment model, called MOSA-Net+, by leveraging the acoustic features from
large pre-trained weakly supervised models, namely Whisper, to create embedding
features. The first part of this study investigates the correlation between the
embedding features of Whisper and two self-supervised learning (SSL) models
with subjective quality and intelligibility scores. The second part evaluates
the effectiveness of Whisper in deploying a more robust speech assessment
model. Third, the possibility of combining representations from Whisper and SSL
models while deploying MOSA-Net+ is analyzed. The experimental results reveal
that Whisper's embedding features correlate more strongly with subjective
quality and intelligibility than other SSL's embedding features, contributing
to more accurate prediction performance achieved by MOSA-Net+. Moreover,
combining the embedding features from Whisper and SSL models only leads to
marginal improvement. As compared to MOSA-Net and other SSL-based speech
assessment models, MOSA-Net+ yields notable improvements in estimating
subjective quality and intelligibility scores across all evaluation metrics. We
further tested MOSA-Net+ on Track 3 of the VoiceMOS Challenge 2023 and obtained
the top-ranked performance.",http://arxiv.org/pdf/2309.12766v1
2309.12765v1,cs.LG,An Intelligent Approach to Detecting Novel Fault Classes for Centrifugal Pumps Based on Deep CNNs and Unsupervised Methods,2023-09-22 10:10:30+00:00,"Despite the recent success in data-driven fault diagnosis of rotating
machines, there are still remaining challenges in this field. Among the issues
to be addressed, is the lack of information about variety of faults the system
may encounter in the field. In this paper, we assume a partial knowledge of the
system faults and use the corresponding data to train a convolutional neural
network. A combination of t-SNE method and clustering techniques is then
employed to detect novel faults. Upon detection, the network is augmented using
the new data. Finally, a test setup is used to validate this two-stage
methodology on a centrifugal pump and experimental results show high accuracy
in detecting novel faults.",http://arxiv.org/pdf/2309.12765v1
2309.12757v1,cs.CV,"Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where",2023-09-22 09:58:38+00:00,"While image data starts to enjoy the simple-but-effective self-supervised
learning scheme built upon masking and self-reconstruction objective thanks to
the introduction of tokenization procedure and vision transformer backbone,
convolutional neural networks as another important and widely-adopted
architecture for image data, though having contrastive-learning techniques to
drive the self-supervised learning, still face the difficulty of leveraging
such straightforward and general masking operation to benefit their learning
process significantly. In this work, we aim to alleviate the burden of
including masking operation into the contrastive-learning framework for
convolutional neural networks as an extra augmentation method. In addition to
the additive but unwanted edges (between masked and unmasked regions) as well
as other adverse effects caused by the masking operations for ConvNets, which
have been discussed by prior works, we particularly identify the potential
problem where for one view in a contrastive sample-pair the randomly-sampled
masking regions could be overly concentrated on important/salient objects thus
resulting in misleading contrastiveness to the other view. To this end, we
propose to explicitly take the saliency constraint into consideration in which
the masked regions are more evenly distributed among the foreground and
background for realizing the masking-based augmentation. Moreover, we introduce
hard negative samples by masking larger regions of salient patches in an input
image. Extensive experiments conducted on various datasets, contrastive
learning mechanisms, and downstream tasks well verify the efficacy as well as
the superior performance of our proposed method with respect to several
state-of-the-art baselines.",http://arxiv.org/pdf/2309.12757v1
2309.12756v1,cs.SE,Towards an MLOps Architecture for XAI in Industrial Applications,2023-09-22 09:56:25+00:00,"Machine learning (ML) has become a popular tool in the industrial sector as
it helps to improve operations, increase efficiency, and reduce costs. However,
deploying and managing ML models in production environments can be complex.
This is where Machine Learning Operations (MLOps) comes in. MLOps aims to
streamline this deployment and management process. One of the remaining MLOps
challenges is the need for explanations. These explanations are essential for
understanding how ML models reason, which is key to trust and acceptance.
Better identification of errors and improved model accuracy are only two
resulting advantages. An often neglected fact is that deployed models are
bypassed in practice when accuracy and especially explainability do not meet
user expectations. We developed a novel MLOps software architecture to address
the challenge of integrating explanations and feedback capabilities into the ML
development and deployment processes. In the project EXPLAIN, our architecture
is implemented in a series of industrial use cases. The proposed MLOps software
architecture has several advantages. It provides an efficient way to manage ML
models in production environments. Further, it allows for integrating
explanations into the development and deployment processes.",http://arxiv.org/pdf/2309.12756v1
2309.12732v1,cs.AI,OpenAi's GPT4 as coding assistant,2023-09-22 09:31:39+00:00,"Lately, Large Language Models have been widely used in code generation. GPT4
is considered the most potent Large Language Model from Openai. In this paper,
we examine GPT3.5 and GPT4 as coding assistants. More specifically, we have
constructed appropriate tests to check whether the two systems can a) answer
typical questions that can arise during the code development, b) produce
reliable code, and c) contribute to code debugging. The test results are
impressive. The performance of GPT4 is outstanding and signals an increase in
the productivity of programmers and the reorganization of software development
procedures based on these new tools.",http://arxiv.org/pdf/2309.12732v1
2309.12731v1,cs.AI,Defeasible Reasoning with Knowledge Graphs,2023-09-22 09:27:26+00:00,"Human knowledge is subject to uncertainties, imprecision, incompleteness and
inconsistencies. Moreover, the meaning of many everyday terms is dependent on
the context. That poses a huge challenge for the Semantic Web. This paper
introduces work on an intuitive notation and model for defeasible reasoning
with imperfect knowledge, and relates it to previous work on argumentation
theory. PKN is to N3 as defeasible reasoning is to deductive logic. Further
work is needed on an intuitive syntax for describing reasoning strategies and
tactics in declarative terms, drawing upon the AIF ontology for inspiration.
The paper closes with observations on symbolic approaches in the era of large
language models.",http://arxiv.org/pdf/2309.12731v1
2309.12727v1,cs.AI,In-context Interference in Chat-based Large Language Models,2023-09-22 09:18:55+00:00,"Large language models (LLMs) have had a huge impact on society due to their
impressive capabilities and vast knowledge of the world. Various applications
and tools have been created that allow users to interact with these models in a
black-box scenario. However, one limitation of this scenario is that users
cannot modify the internal knowledge of the model, and the only way to add or
modify internal knowledge is by explicitly mentioning it to the model during
the current interaction. This learning process is called in-context training,
and it refers to training that is confined to the user's current session or
context. In-context learning has significant applications, but also has
limitations that are seldom studied. In this paper, we present a study that
shows how the model can suffer from interference between information that
continually flows in the context, causing it to forget previously learned
knowledge, which can reduce the model's performance. Along with showing the
problem, we propose an evaluation benchmark based on the bAbI dataset.",http://arxiv.org/pdf/2309.12727v1
2309.12716v1,cs.LG,H2O+: An Improved Framework for Hybrid Offline-and-Online RL with Dynamics Gaps,2023-09-22 08:58:22+00:00,"Solving real-world complex tasks using reinforcement learning (RL) without
high-fidelity simulation environments or large amounts of offline data can be
quite challenging. Online RL agents trained in imperfect simulation
environments can suffer from severe sim-to-real issues. Offline RL approaches
although bypass the need for simulators, often pose demanding requirements on
the size and quality of the offline datasets. The recently emerged hybrid
offline-and-online RL provides an attractive framework that enables joint use
of limited offline data and imperfect simulator for transferable policy
learning. In this paper, we develop a new algorithm, called H2O+, which offers
great flexibility to bridge various choices of offline and online learning
methods, while also accounting for dynamics gaps between the real and
simulation environment. Through extensive simulation and real-world robotics
experiments, we demonstrate superior performance and flexibility over advanced
cross-domain online and offline RL algorithms.",http://arxiv.org/pdf/2309.12716v1
2309.12711v1,cs.AI,The Mathematical Game,2023-09-22 08:43:57+00:00,"Monte Carlo Tree Search can be used for automated theorem proving. Holophrasm
is a neural theorem prover using MCTS combined with neural networks for the
policy and the evaluation. In this paper we propose to improve the performance
of the Holophrasm theorem prover using other game tree search algorithms.",http://arxiv.org/pdf/2309.12711v1
2309.12708v1,cs.CV,PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion,2023-09-22 08:39:16+00:00,"Semantic Scene Completion (SSC) aims to jointly generate space occupancies
and semantic labels for complex 3D scenes. Most existing SSC models focus on
volumetric representations, which are memory-inefficient for large outdoor
spaces. Point clouds provide a lightweight alternative but existing benchmarks
lack outdoor point cloud scenes with semantic labels. To address this, we
introduce PointSSC, the first cooperative vehicle-infrastructure point cloud
benchmark for semantic scene completion. These scenes exhibit long-range
perception and minimal occlusion. We develop an automated annotation pipeline
leveraging Segment Anything to efficiently assign semantics. To benchmark
progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for
global and local feature extraction and a Completion and Segmentation
Cooperative Module for joint completion and segmentation. PointSSC provides a
challenging testbed to drive advances in semantic point cloud completion for
real-world navigation.",http://arxiv.org/pdf/2309.12708v1
2309.12706v1,cs.LG,Multi-Label Noise Transition Matrix Estimation with Label Correlations: Theory and Algorithm,2023-09-22 08:35:38+00:00,"Noisy multi-label learning has garnered increasing attention due to the
challenges posed by collecting large-scale accurate labels, making noisy labels
a more practical alternative. Motivated by noisy multi-class learning, the
introduction of transition matrices can help model multi-label noise and enable
the development of statistically consistent algorithms for noisy multi-label
learning. However, estimating multi-label noise transition matrices remains a
challenging task, as most existing estimators in noisy multi-class learning
rely on anchor points and accurate fitting of noisy class posteriors, which is
hard to satisfy in noisy multi-label learning. In this paper, we address this
problem by first investigating the identifiability of class-dependent
transition matrices in noisy multi-label learning. Building upon the
identifiability results, we propose a novel estimator that leverages label
correlations without the need for anchor points or precise fitting of noisy
class posteriors. Specifically, we first estimate the occurrence probability of
two noisy labels to capture noisy label correlations. Subsequently, we employ
sample selection techniques to extract information implying clean label
correlations, which are then used to estimate the occurrence probability of one
noisy label when a certain clean label appears. By exploiting the mismatches in
label correlations implied by these occurrence probabilities, we demonstrate
that the transition matrix becomes identifiable and can be acquired by solving
a bilinear decomposition problem. Theoretically, we establish an estimation
error bound for our multi-label transition matrix estimator and derive a
generalization error bound for our statistically consistent algorithm.
Empirically, we validate the effectiveness of our estimator in estimating
multi-label noise transition matrices, leading to excellent classification
performance.",http://arxiv.org/pdf/2309.12706v1
2309.12696v1,cs.AI,Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning,2023-09-22 08:10:25+00:00,"Offline multi-agent reinforcement learning is challenging due to the coupling
effect of both distribution shift issue common in offline setting and the high
dimension issue common in multi-agent setting, making the action
out-of-distribution (OOD) and value overestimation phenomenon excessively
severe. Tomitigate this problem, we propose a novel multi-agent offline RL
algorithm, named CounterFactual Conservative Q-Learning (CFCQL) to conduct
conservative value estimation. Rather than regarding all the agents as a high
dimensional single one and directly applying single agent methods to it, CFCQL
calculates conservative regularization for each agent separately in a
counterfactual way and then linearly combines them to realize an overall
conservative value estimation. We prove that it still enjoys the
underestimation property and the performance guarantee as those single agent
conservative methods do, but the induced regularization and safe policy
improvement bound are independent of the agent number, which is therefore
theoretically superior to the direct treatment referred to above, especially
when the agent number is large. We further conduct experiments on four
environments including both discrete and continuous action settings on both
existing and our man-made datasets, demonstrating that CFCQL outperforms
existing methods on most datasets and even with a remarkable margin on some of
them.",http://arxiv.org/pdf/2309.12696v1
2309.12692v1,cs.RO,Enhancing Graph Representation of the Environment through Local and Cloud Computation,2023-09-22 08:05:32+00:00,"Enriching the robot representation of the operational environment is a
challenging task that aims at bridging the gap between low-level sensor
readings and high-level semantic understanding. Having a rich representation
often requires computationally demanding architectures and pure point cloud
based detection systems that struggle when dealing with everyday objects that
have to be handled by the robot. To overcome these issues, we propose a
graph-based representation that addresses this gap by providing a semantic
representation of robot environments from multiple sources. In fact, to acquire
information from the environment, the framework combines classical computer
vision tools with modern computer vision cloud services, ensuring computational
feasibility on onboard hardware. By incorporating an ontology hierarchy with
over 800 object classes, the framework achieves cross-domain adaptability,
eliminating the need for environment-specific tools. The proposed approach
allows us to handle also small objects and integrate them into the semantic
representation of the environment. The approach is implemented in the Robot
Operating System (ROS) using the RViz visualizer for environment
representation. This work is a first step towards the development of a
general-purpose framework, to facilitate intuitive interaction and navigation
across different domains.",http://arxiv.org/pdf/2309.12692v1
2309.12678v1,quant-ph,QAL-BP: An Augmented Lagrangian Quantum Approach for Bin Packing Problem,2023-09-22 07:37:20+00:00,"The bin packing is a well-known NP-Hard problem in the domain of artificial
intelligence, posing significant challenges in finding efficient solutions.
Conversely, recent advancements in quantum technologies have shown promising
potential for achieving substantial computational speedup, particularly in
certain problem classes, such as combinatorial optimization. In this study, we
introduce QAL-BP, a novel Quadratic Unconstrained Binary Optimization (QUBO)
formulation designed specifically for bin packing and suitable for quantum
computation. QAL-BP utilizes the augmented Lagrangian method to incorporate the
bin packing constraints into the objective function while also facilitating an
analytical estimation of heuristic, but empirically robust, penalty
multipliers. This approach leads to a more versatile and generalizable model
that eliminates the need for empirically calculating instance-dependent
Lagrangian coefficients, a requirement commonly encountered in alternative QUBO
formulations for similar problems. To assess the effectiveness of our proposed
approach, we conduct experiments on a set of bin-packing instances using a real
Quantum Annealing device. Additionally, we compare the results with those
obtained from two different classical solvers, namely simulated annealing and
Gurobi. The experimental findings not only confirm the correctness of the
proposed formulation but also demonstrate the potential of quantum computation
in effectively solving the bin-packing problem, particularly as more reliable
quantum technology becomes available.",http://arxiv.org/pdf/2309.12678v1
2309.12677v1,cs.AI,TrTr: A Versatile Pre-Trained Large Traffic Model based on Transformer for Capturing Trajectory Diversity in Vehicle Population,2023-09-22 07:36:22+00:00,"Understanding trajectory diversity is a fundamental aspect of addressing
practical traffic tasks. However, capturing the diversity of trajectories
presents challenges, particularly with traditional machine learning and
recurrent neural networks due to the requirement of large-scale parameters. The
emerging Transformer technology, renowned for its parallel computation
capabilities enabling the utilization of models with hundreds of millions of
parameters, offers a promising solution. In this study, we apply the
Transformer architecture to traffic tasks, aiming to learn the diversity of
trajectories within vehicle populations. We analyze the Transformer's attention
mechanism and its adaptability to the goals of traffic tasks, and subsequently,
design specific pre-training tasks. To achieve this, we create a data structure
tailored to the attention mechanism and introduce a set of noises that
correspond to spatio-temporal demands, which are incorporated into the
structured data during the pre-training process. The designed pre-training
model demonstrates excellent performance in capturing the spatial distribution
of the vehicle population, with no instances of vehicle overlap and an RMSE of
0.6059 when compared to the ground truth values. In the context of time series
prediction, approximately 95% of the predicted trajectories' speeds closely
align with the true speeds, within a deviation of 7.5144m/s. Furthermore, in
the stability test, the model exhibits robustness by continuously predicting a
time series ten times longer than the input sequence, delivering smooth
trajectories and showcasing diverse driving behaviors. The pre-trained model
also provides a good basis for downstream fine-tuning tasks. The number of
parameters of our model is over 50 million.",http://arxiv.org/pdf/2309.12677v1
2309.12675v1,cs.AI,Vision Transformers for Computer Go,2023-09-22 07:35:37+00:00,"Motivated by the success of transformers in various fields, such as language
understanding and image analysis, this investigation explores their application
in the context of the game of Go. In particular, our study focuses on the
analysis of the Transformer in Vision. Through a detailed analysis of numerous
points such as prediction accuracy, win rates, memory, speed, size, or even
learning rate, we have been able to highlight the substantial role that
transformers can play in the game of Go. This study was carried out by
comparing them to the usual Residual Networks.",http://arxiv.org/pdf/2309.12675v1
2309.12673v1,cs.LG,On Sparse Modern Hopfield Model,2023-09-22 07:32:45+00:00,"We introduce the sparse modern Hopfield model as a sparse extension of the
modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield
model equips a memory-retrieval dynamics whose one-step approximation
corresponds to the sparse attention mechanism. Theoretically, our key
contribution is a principled derivation of a closed-form sparse Hopfield energy
using the convex conjugate of the sparse entropic regularizer. Building upon
this, we derive the sparse memory retrieval dynamics from the sparse energy
function and show its one-step approximation is equivalent to the
sparse-structured attention. Importantly, we provide a sparsity-dependent
memory retrieval error bound which is provably tighter than its dense analog.
The conditions for the benefits of sparsity to arise are therefore identified
and discussed. In addition, we show that the sparse modern Hopfield model
maintains the robust theoretical properties of its dense counterpart, including
rapid fixed point convergence and exponential memory capacity. Empirically, we
use both synthetic and real-world datasets to demonstrate that the sparse
Hopfield model outperforms its dense counterpart in many situations.",http://arxiv.org/pdf/2309.12673v1
2309.12671v1,cs.LG,How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization,2023-09-22 07:27:32+00:00,"Designing and deriving effective model-based reinforcement learning (MBRL)
algorithms with a performance improvement guarantee is challenging, mainly
attributed to the high coupling between model learning and policy optimization.
Many prior methods that rely on return discrepancy to guide model learning
ignore the impacts of model shift, which can lead to performance deterioration
due to excessive model updates. Other methods use performance difference bound
to explicitly consider model shift. However, these methods rely on a fixed
threshold to constrain model shift, resulting in a heavy dependence on the
threshold and a lack of adaptability during the training process. In this
paper, we theoretically derive an optimization objective that can unify model
shift and model bias and then formulate a fine-tuning process. This process
adaptively adjusts the model updates to get a performance improvement guarantee
while avoiding model overfitting. Based on these, we develop a straightforward
algorithm USB-PO (Unified model Shift and model Bias Policy Optimization).
Empirical results show that USB-PO achieves state-of-the-art performance on
several challenging benchmark tasks.",http://arxiv.org/pdf/2309.12671v1
2309.12655v1,cs.AI,Natural revision is contingently-conditionalized revision,2023-09-22 06:52:30+00:00,"Natural revision seems so natural: it changes beliefs as little as possible
to incorporate new information. Yet, some counterexamples show it wrong. It is
so conservative that it never fully believes. It only believes in the current
conditions. This is right in some cases and wrong in others. Which is which?
The answer requires extending natural revision from simple formulae expressing
universal truths (something holds) to conditionals expressing conditional truth
(something holds in certain conditions). The extension is based on the basic
principles natural revision follows, identified as minimal change, indifference
and naivety: change beliefs as little as possible; equate the likeliness of
scenarios by default; believe all until contradicted. The extension says that
natural revision restricts changes to the current conditions. A comparison with
an unrestricting revision shows what exactly the current conditions are. It is
not what currently considered true if it contradicts the new information. It
includes something more and more unlikely until the new information is at least
possible.",http://arxiv.org/pdf/2309.12655v1
2309.12632v1,cs.LG,Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?,2023-09-22 05:57:25+00:00,"Following the great success of various deep learning methods in image and
object classification, the biomedical image processing society is also
overwhelmed with their applications to various automatic diagnosis cases.
Unfortunately, most of the deep learning-based classification attempts in the
literature solely focus on the aim of extreme accuracy scores, without
considering interpretability, or patient-wise separation of training and test
data. For example, most lung nodule classification papers using deep learning
randomly shuffle data and split it into training, validation, and test sets,
causing certain images from the CT scan of a person to be in the training set,
while other images of the exact same person to be in the validation or testing
image sets. This can result in reporting misleading accuracy rates and the
learning of irrelevant features, ultimately reducing the real-life usability of
these models. When the deep neural networks trained on the traditional, unfair
data shuffling method are challenged with new patient images, it is observed
that the trained models perform poorly. In contrast, deep neural networks
trained with strict patient-level separation maintain their accuracy rates even
when new patient images are tested. Heat-map visualizations of the activations
of the deep neural networks trained with strict patient-level separation
indicate a higher degree of focus on the relevant nodules. We argue that the
research question posed in the title has a positive answer only if the deep
neural networks are trained with images of patients that are strictly isolated
from the validation and testing patient sets.",http://arxiv.org/pdf/2309.12632v1
2309.12627v1,cs.AI,A Quantum Computing-based System for Portfolio Optimization using Future Asset Values and Automatic Reduction of the Investment Universe,2023-09-22 05:27:23+00:00,"One of the problems in quantitative finance that has received the most
attention is the portfolio optimization problem. Regarding its solving, this
problem has been approached using different techniques, with those related to
quantum computing being especially prolific in recent years. In this study, we
present a system called Quantum Computing-based System for Portfolio
Optimization with Future Asset Values and Automatic Universe Reduction
(Q4FuturePOP), which deals with the Portfolio Optimization Problem considering
the following innovations: i) the developed tool is modeled for working with
future prediction of assets, instead of historical values; and ii) Q4FuturePOP
includes an automatic universe reduction module, which is conceived to
intelligently reduce the complexity of the problem. We also introduce a brief
discussion about the preliminary performance of the different modules that
compose the prototypical version of Q4FuturePOP.",http://arxiv.org/pdf/2309.12627v1
2309.12626v1,cs.AI,Construction contract risk identification based on knowledge-augmented language model,2023-09-22 05:27:06+00:00,"Contract review is an essential step in construction projects to prevent
potential losses. However, the current methods for reviewing construction
contracts lack effectiveness and reliability, leading to time-consuming and
error-prone processes. While large language models (LLMs) have shown promise in
revolutionizing natural language processing (NLP) tasks, they struggle with
domain-specific knowledge and addressing specialized issues. This paper
presents a novel approach that leverages LLMs with construction contract
knowledge to emulate the process of contract review by human experts. Our
tuning-free approach incorporates construction contract domain knowledge to
enhance language models for identifying construction contract risks. The use of
a natural language when building the domain knowledge base facilitates
practical implementation. We evaluated our method on real construction
contracts and achieved solid performance. Additionally, we investigated how
large language models employ logical thinking during the task and provide
insights and recommendations for future research.",http://arxiv.org/pdf/2309.12626v1
2309.12625v1,cs.AI,DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients,2023-09-22 05:18:54+00:00,"In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) plays
a key role but its current assignment process is time-consuming. We introduce
DRG-LLaMA, a large language model (LLM) fine-tuned on clinical notes for
improved DRG prediction. Using Meta's LLaMA as the base model, we optimized it
with Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries. With
an input token length of 512, DRG-LLaMA-7B achieved a macro-averaged F1 score
of 0.327, a top-1 prediction accuracy of 52.0% and a macro-averaged Area Under
the Curve (AUC) of 0.986. Impressively, DRG-LLaMA-7B surpassed previously
reported leading models on this task, demonstrating a relative improvement in
macro-averaged F1 score of 40.3% compared to ClinicalBERT and 35.7% compared to
CAML. When DRG-LLaMA is applied to predict base DRGs and complication or
comorbidity (CC) / major complication or comorbidity (MCC), the top-1
prediction accuracy reached 67.8% for base DRGs and 67.5% for CC/MCC status.
DRG-LLaMA performance exhibits improvements in correlation with larger model
parameters and longer input context lengths. Furthermore, usage of LoRA enables
training even on smaller GPUs with 48 GB of VRAM, highlighting the viability of
adapting LLMs for DRGs prediction.",http://arxiv.org/pdf/2309.12625v1
2309.12589v1,cs.RO,A Multi-Robot Task Assignment Framework for Search and Rescue with Heterogeneous Teams,2023-09-22 02:30:46+00:00,"In post-disaster scenarios, efficient search and rescue operations involve
collaborative efforts between robots and humans. Existing planning approaches
focus on specific aspects but overlook crucial elements like information
gathering, task assignment, and planning. Furthermore, previous methods
considering robot capabilities and victim requirements suffer from time
complexity due to repetitive planning steps. To overcome these challenges, we
introduce a comprehensive framework__the Multi-Stage Multi-Robot Task
Assignment. This framework integrates scouting, task assignment, and
path-planning stages, optimizing task allocation based on robot capabilities,
victim requirements, and past robot performance. Our iterative approach ensures
objective fulfillment within problem constraints. Evaluation across four maps,
comparing with a state-of-the-art baseline, demonstrates our algorithm's
superiority with a remarkable 97 percent performance increase. Our code is
open-sourced to enable result replication.",http://arxiv.org/pdf/2309.12589v1
2309.12579v1,cs.AI,From Text to Trends: A Unique Garden Analytics Perspective on the Future of Modern Agriculture,2023-09-22 02:15:12+00:00,"Data-driven insights are essential for modern agriculture. This research
paper introduces a machine learning framework designed to improve how we
educate and reach out to people in the field of horticulture. The framework
relies on data from the Horticulture Online Help Desk (HOHD), which is like a
big collection of questions from people who love gardening and are part of the
Extension Master Gardener Program (EMGP). This framework has two main parts.
First, it uses special computer programs (machine learning models) to sort
questions into categories. This helps us quickly send each question to the
right expert, so we can answer it faster. Second, it looks at when questions
are asked and uses that information to guess how many questions we might get in
the future and what they will be about. This helps us plan on topics that will
be really important. It's like knowing what questions will be popular in the
coming months. We also take into account where the questions come from by
looking at the Zip Code. This helps us make research that fits the challenges
faced by gardeners in different places. In this paper, we demonstrate the
potential of machine learning techniques to predict trends in horticulture by
analyzing textual queries from homeowners. We show that NLP, classification,
and time series analysis can be used to identify patterns in homeowners'
queries and predict future trends in horticulture. Our results suggest that
machine learning could be used to predict trends in other agricultural sectors
as well. If large-scale agriculture industries curate and maintain a comparable
repository of textual data, the potential for trend prediction and strategic
agricultural planning could be revolutionized. This convergence of technology
and agriculture offers a promising pathway for the future of sustainable
farming and data-informed agricultural practices",http://arxiv.org/pdf/2309.12579v1
2309.12576v1,cs.AI,Understanding Patterns of Deep Learning ModelEvolution in Network Architecture Search,2023-09-22 02:12:47+00:00,"Network Architecture Search and specifically Regularized Evolution is a
common way to refine the structure of a deep learning model.However, little is
known about how models empirically evolve over time which has design
implications for designing caching policies, refining the search algorithm for
particular applications, and other important use cases.In this work, we
algorithmically analyze and quantitatively characterize the patterns of model
evolution for a set of models from the Candle project and the Nasbench-201
search space.We show how the evolution of the model structure is influenced by
the regularized evolution algorithm. We describe how evolutionary patterns
appear in distributed settings and opportunities for caching and improved
scheduling. Lastly, we describe the conditions that affect when particular
model architectures rise and fall in popularity based on their frequency of
acting as a donor in a sliding window.",http://arxiv.org/pdf/2309.12576v1
2309.12572v1,eess.IV,Interpretable 3D Multi-Modal Residual Convolutional Neural Network for Mild Traumatic Brain Injury Diagnosis,2023-09-22 01:58:27+00:00,"Mild Traumatic Brain Injury (mTBI) is a significant public health challenge
due to its high prevalence and potential for long-term health effects. Despite
Computed Tomography (CT) being the standard diagnostic tool for mTBI, it often
yields normal results in mTBI patients despite symptomatic evidence. This fact
underscores the complexity of accurate diagnosis. In this study, we introduce
an interpretable 3D Multi-Modal Residual Convolutional Neural Network (MRCNN)
for mTBI diagnostic model enhanced with Occlusion Sensitivity Maps (OSM). Our
MRCNN model exhibits promising performance in mTBI diagnosis, demonstrating an
average accuracy of 82.4%, sensitivity of 82.6%, and specificity of 81.6%, as
validated by a five-fold cross-validation process. Notably, in comparison to
the CT-based Residual Convolutional Neural Network (RCNN) model, the MRCNN
shows an improvement of 4.4% in specificity and 9.0% in accuracy. We show that
the OSM offers superior data-driven insights into CT images compared to the
Grad-CAM approach. These results highlight the efficacy of the proposed
multi-modal model in enhancing the diagnostic precision of mTBI.",http://arxiv.org/pdf/2309.12572v1
2309.12570v1,cs.HC,Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers,2023-09-22 01:49:36+00:00,"The development of large language models (LLMs) capable of following
instructions and engaging in conversational interactions sparked increased
interest in their utilization across various support tools. We investigate the
utility of modern LLMs in assisting professional writers via an empirical user
study (n=30). The design of our collaborative writing interface is grounded in
the cognitive process model of writing that views writing as a goal-oriented
thinking process encompassing non-linear cognitive activities: planning,
translating, and reviewing. Participants are asked to submit a post-completion
survey to provide feedback on the potential and pitfalls of LLMs as writing
collaborators. Upon analyzing the writer-LLM interactions, we find that while
writers seek LLM's help across all three types of cognitive activities, they
find LLMs more helpful in translation and reviewing. Our findings from
analyzing both the interactions and the survey responses highlight future
research directions in creative writing assistance using LLMs.",http://arxiv.org/pdf/2309.12570v1
2309.12568v1,cs.RO,A Study on Learning Social Robot Navigation with Multimodal Perception,2023-09-22 01:47:47+00:00,"Autonomous mobile robots need to perceive the environments with their onboard
sensors (e.g., LiDARs and RGB cameras) and then make appropriate navigation
decisions. In order to navigate human-inhabited public spaces, such a
navigation task becomes more than only obstacle avoidance, but also requires
considering surrounding humans and their intentions to somewhat change the
navigation behavior in response to the underlying social norms, i.e., being
socially compliant. Machine learning methods are shown to be effective in
capturing those complex and subtle social interactions in a data-driven manner,
without explicitly hand-crafting simplified models or cost functions.
Considering multiple available sensor modalities and the efficiency of learning
methods, this paper presents a comprehensive study on learning social robot
navigation with multimodal perception using a large-scale real-world dataset.
The study investigates social robot navigation decision making on both the
global and local planning levels and contrasts unimodal and multimodal learning
against a set of classical navigation approaches in different social scenarios,
while also analyzing the training and generalizability performance from the
learning perspective. We also conduct a human study on how learning with
multimodal perception affects the perceived social compliance. The results show
that multimodal learning has a clear advantage over unimodal learning in both
dataset and human studies. We open-source our code for the community's future
use to study multimodal perception for learning social robot navigation.",http://arxiv.org/pdf/2309.12568v1
2309.12563v1,cs.IT,Passive Reflection Codebook Design for IRS-Integrated Access Point,2023-09-22 01:24:21+00:00,"Intelligent reflecting surface (IRS) has emerged as a promising technique to
extend the wireless signal coverage of access point (AP) and improve the
communication performance cost-effectively. In order to reduce the path-loss of
the cascaded user-IRS-AP channels, the IRS-integrated AP architecture has been
proposed to deploy the IRSs and the antenna array of the AP within the same
antenna radome. To reduce the pilot overhead for estimating all IRS-involved
channels, in this paper, we propose a novel codebook-based IRS reflection
design for the IRS-integrated AP to enhance the coverage performance in a given
area. In particular, the codebook consisting of a small number of codewords is
designed offline by employing an efficient sector division strategy based on
the azimuth angle. To ensure the performance of each sector, we optimize its
corresponding codeword for IRS reflection pattern to maximize the
sector-min-average-effective-channel-power (SMAECP) by applying the alternating
optimization (AO) and semidefinite relaxation (SDR) methods. With the designed
codebook, the AP performs the IRS reflection training by sequentially applying
all codewords and selects the one achieving the best communication performance
for data transmission. Numerical results show that our proposed codebook design
can enhance the average channel power of the whole coverage area, as compared
to the system without IRS. Moreover, our proposed codebook-based IRS reflection
design is shown to achieve significant performance gain over other benchmark
schemes in both single-user and multi-user transmissions.",http://arxiv.org/pdf/2309.12563v1
2309.12562v1,cs.RO,Cognitive Approach to Hierarchical Task Selection for Human-Robot Interaction in Dynamic Environments,2023-09-22 01:22:51+00:00,"In an efficient and flexible human-robot collaborative work environment, a
robot team member must be able to recognize both explicit requests and implied
actions from human users. Identifying ""what to do"" in such cases requires an
agent to have the ability to construct associations between objects, their
actions, and the effect of actions on the environment. In this regard, semantic
memory is being introduced to understand the explicit cues and their
relationships with available objects and required skills to make ""tea"" and
""sandwich"". We have extended our previous hierarchical robot control
architecture to add the capability to execute the most appropriate task based
on both feedback from the user and the environmental context. To validate this
system, two types of skills were implemented in the hierarchical task tree: 1)
Tea making skills and 2) Sandwich making skills. During the conversation
between the robot and the human, the robot was able to determine the hidden
context using ontology and began to act accordingly. For instance, if the
person says ""I am thirsty"" or ""It is cold outside"" the robot will start to
perform the tea-making skill. In contrast, if the person says, ""I am hungry"" or
""I need something to eat"", the robot will make the sandwich. A humanoid robot
Baxter was used for this experiment. We tested three scenarios with objects at
different positions on the table for each skill. We observed that in all cases,
the robot used only objects that were relevant to the skill.",http://arxiv.org/pdf/2309.12562v1
2309.12560v1,cs.RO,Machine Learning Meets Advanced Robotic Manipulation,2023-09-22 01:06:32+00:00,"Automated industries lead to high quality production, lower manufacturing
cost and better utilization of human resources. Robotic manipulator arms have
major role in the automation process. However, for complex manipulation tasks,
hard coding efficient and safe trajectories is challenging and time consuming.
Machine learning methods have the potential to learn such controllers based on
expert demonstrations. Despite promising advances, better approaches must be
developed to improve safety, reliability, and efficiency of ML methods in both
training and deployment phases. This survey aims to review cutting edge
technologies and recent trends on ML methods applied to real-world manipulation
tasks. After reviewing the related background on ML, the rest of the paper is
devoted to ML applications in different domains such as industry, healthcare,
agriculture, space, military, and search and rescue. The paper is closed with
important research directions for future works.",http://arxiv.org/pdf/2309.12560v1
2309.12559v1,cs.LG,Invariant Learning via Probability of Sufficient and Necessary Causes,2023-09-22 01:06:16+00:00,"Out-of-distribution (OOD) generalization is indispensable for learning models
in the wild, where testing distribution typically unknown and different from
the training. Recent methods derived from causality have shown great potential
in achieving OOD generalization. However, existing methods mainly focus on the
invariance property of causes, while largely overlooking the property of
\textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but
insufficient cause (feature) is invariant to distribution shift, yet it may not
have required accuracy. By contrast, a sufficient yet unnecessary cause
(feature) tends to fit specific data well but may have a risk of adapting to a
new domain. To capture the information of sufficient and necessary causes, we
employ a classical concept, the probability of sufficiency and necessary causes
(PNS), which indicates the probability of whether one is the necessary and
sufficient cause. To associate PNS with OOD generalization, we propose PNS risk
and formulate an algorithm to learn representation with a high PNS value. We
theoretically analyze and prove the generalizability of the PNS risk.
Experiments on both synthetic and real-world benchmarks demonstrate the
effectiveness of the proposed method. The details of the implementation can be
found at the GitHub repository: https://github.com/ymy4323460/CaSN.",http://arxiv.org/pdf/2309.12559v1
2309.12555v1,cs.HC,PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models,2023-09-22 00:55:52+00:00,"A personally tailored exercise regimen is crucial to ensuring sufficient
physical activities, yet challenging to create as people have complex schedules
and considerations and the creation of plans often requires iterations with
experts. We present PlanFitting, a conversational AI that assists in
personalized exercise planning. Leveraging generative capabilities of large
language models, PlanFitting enables users to describe various constraints and
queries in natural language, thereby facilitating the creation and refinement
of their weekly exercise plan to suit their specific circumstances while
staying grounded in foundational principles. Through a user study where
participants (N=18) generated a personalized exercise plan using PlanFitting
and expert planners (N=3) evaluated these plans, we identified the potential of
PlanFitting in generating personalized, actionable, and evidence-based exercise
plans. We discuss future design opportunities for AI assistants in creating
plans that better comply with exercise principles and accommodate personal
constraints.",http://arxiv.org/pdf/2309.12555v1
2309.12545v1,cs.LG,Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation,2023-09-22 00:12:09+00:00,"Counterfactual Explanations (CEs) have received increasing interest as a
major methodology for explaining neural network classifiers. Usually, CEs for
an input-output pair are defined as data points with minimum distance to the
input that are classified with a different label than the output. To tackle the
established problem that CEs are easily invalidated when model parameters are
updated (e.g. retrained), studies have proposed ways to certify the robustness
of CEs under model parameter changes bounded by a norm ball. However, existing
methods targeting this form of robustness are not sound or complete, and they
may generate implausible CEs, i.e., outliers wrt the training dataset. In fact,
no existing method simultaneously optimises for proximity and plausibility
while preserving robustness guarantees. In this work, we propose Provably
RObust and PLAusible Counterfactual Explanations (PROPLACE), a method
leveraging on robust optimisation techniques to address the aforementioned
limitations in the literature. We formulate an iterative algorithm to compute
provably robust CEs and prove its convergence, soundness and completeness.
Through a comparative experiment involving six baselines, five of which target
robustness, we show that PROPLACE achieves state-of-the-art performances
against metrics on three evaluation aspects.",http://arxiv.org/pdf/2309.12545v1
2309.12531v1,cs.RO,RCMS: Risk-Aware Crash Mitigation System for Autonomous Vehicles,2023-09-21 23:09:01+00:00,"We propose a risk-aware crash mitigation system (RCMS), to augment any
existing motion planner (MP), that enables an autonomous vehicle to perform
evasive maneuvers in high-risk situations and minimize the severity of
collision if a crash is inevitable. In order to facilitate a smooth transition
between RCMS and MP, we develop a novel activation mechanism that combines
instantaneous as well as predictive collision risk evaluation strategies in a
unified hysteresis-band approach. For trajectory planning, we deploy a modular
receding horizon optimization-based approach that minimizes a smooth
situational risk profile, while adhering to the physical road limits as well as
vehicular actuator limits. We demonstrate the performance of our approach in a
simulation environment.",http://arxiv.org/pdf/2309.12531v1
2309.12529v1,cs.AI,Curriculum Reinforcement Learning via Morphology-Environment Co-Evolution,2023-09-21 22:58:59+00:00,"Throughout long history, natural species have learned to survive by evolving
their physical structures adaptive to the environment changes. In contrast,
current reinforcement learning (RL) studies mainly focus on training an agent
with a fixed morphology (e.g., skeletal structure and joint attributes) in a
fixed environment, which can hardly generalize to changing environments or new
tasks. In this paper, we optimize an RL agent and its morphology through
``morphology-environment co-evolution (MECE)'', in which the morphology keeps
being updated to adapt to the changing environment, while the environment is
modified progressively to bring new challenges and stimulate the improvement of
the morphology. This leads to a curriculum to train generalizable RL, whose
morphology and policy are optimized for different environments. Instead of
hand-crafting the curriculum, we train two policies to automatically change the
morphology and the environment. To this end, (1) we develop two novel and
effective rewards for the two policies, which are solely based on the learning
dynamics of the RL agent; (2) we design a scheduler to automatically determine
when to change the environment and the morphology. In experiments on two
classes of tasks, the morphology and RL policies trained via MECE exhibit
significantly better generalization performance in unseen test environments
than SOTA morphology optimization methods. Our ablation studies on the two MECE
policies further show that the co-evolution between the morphology and
environment is the key to the success.",http://arxiv.org/pdf/2309.12529v1
2309.12526v1,cs.IT,Distributed CSMA/CA MAC Protocol for RIS-Assisted Networks,2023-09-21 22:55:58+00:00,"This paper focuses on achieving optimal multi-user channel access in
distributed networks using a reconfigurable intelligent surface (RIS). The
network includes wireless channels with direct links between users and RIS
links connecting users to the RIS. To maximize average system throughput, an
optimal channel access strategy is proposed, considering the trade-off between
exploiting spatial diversity gain with RIS assistance and the overhead of
channel probing. The paper proposes an optimal distributed Carrier Sense
Multiple Access with Collision Avoidance (CSMA/CA) strategy with opportunistic
RIS assistance, based on statistics theory of optimal sequential observation
planned decision. Each source-destination pair makes decisions regarding the
use of direct links and/or probing source-RIS-destination links. Channel access
occurs in a distributed manner after successful channel contention. The
optimality of the strategy is rigorously derived using multiple-level pure
thresholds. A distributed algorithm, which achieves significantly lower online
complexity at $O(1)$, is developed to implement the proposed strategy.
Numerical simulations verify the theoretical results and demonstrate the
superior performance compared to existing approaches.",http://arxiv.org/pdf/2309.12526v1
2309.12507v1,eess.SP,Deep Reinforcement Learning for Backscatter Communications: Augmenting Intelligence in Future Internet of Things,2023-09-21 22:08:24+00:00,"Backscatter communication (BC) technology offers sustainable solutions for
next-generation Internet-of-Things (IoT) networks, where devices can transmit
data by reflecting and adjusting incident radio frequency signals. In parallel
to BC, deep reinforcement learning (DRL) has recently emerged as a promising
tool to augment intelligence and optimize low-powered IoT devices. This article
commences by elucidating the foundational principles underpinning BC systems,
subsequently delving into the diverse array of DRL techniques and their
respective practical implementations. Subsequently, it investigates potential
domains and presents recent advancements in the realm of DRL-BC systems. A use
case of RIS-aided non-orthogonal multiple access BC systems leveraging DRL is
meticulously examined to highlight its potential. Lastly, this study identifies
and investigates salient challenges and proffers prospective avenues for future
research endeavors.",http://arxiv.org/pdf/2309.12507v1
2309.12501v1,cs.AI,Knowledge Graph Embedding: An Overview,2023-09-21 21:52:42+00:00,"Many mathematical models have been leveraged to design embeddings for
representing Knowledge Graph (KG) entities and relations for link prediction
and many downstream tasks. These mathematically-inspired models are not only
highly scalable for inference in large KGs, but also have many explainable
advantages in modeling different relation patterns that can be validated
through both formal proofs and empirical results. In this paper, we make a
comprehensive overview of the current state of research in KG completion. In
particular, we focus on two main branches of KG embedding (KGE) design: 1)
distance-based methods and 2) semantic matching-based methods. We discover the
connections between recently proposed models and present an underlying trend
that might help researchers invent novel and more effective models. Next, we
delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D
affine operations, respectively. They encompass a broad spectrum of techniques
including distance-based and semantic-based methods. We will also discuss an
emerging approach for KG completion which leverages pre-trained language models
(PLMs) and textual descriptions of entities and relations and offer insights
into the integration of KGE embedding methods with PLMs for KG completion.",http://arxiv.org/pdf/2309.12501v1
2309.12491v1,cs.CL,Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation,2023-09-21 21:21:55+00:00,"We study the effect of tokenization on gender bias in machine translation, an
aspect that has been largely overlooked in previous works. Specifically, we
focus on the interactions between the frequency of gendered profession names in
training data, their representation in the subword tokenizer's vocabulary, and
gender bias. We observe that female and non-stereotypical gender inflections of
profession names (e.g., Spanish ""doctora"" for ""female doctor"") tend to be split
into multiple subword tokens. Our results indicate that the imbalance of gender
forms in the model's training corpus is a major factor contributing to gender
bias and has a greater impact than subword splitting. We show that analyzing
subword splits provides good estimates of gender-form imbalance in the training
data and can be used even when the corpus is not publicly available. We also
demonstrate that fine-tuning just the token embedding layer can decrease the
gap in gender prediction accuracy between female and male forms without
impairing the translation quality.",http://arxiv.org/pdf/2309.12491v1
2309.12485v1,cs.CL,Studying and improving reasoning in humans and machines,2023-09-21 21:02:05+00:00,"In the present study, we investigate and compare reasoning in large language
models (LLM) and humans using a selection of cognitive psychology tools
traditionally dedicated to the study of (bounded) rationality. To do so, we
presented to human participants and an array of pretrained LLMs new variants of
classical cognitive experiments, and cross-compared their performances. Our
results showed that most of the included models presented reasoning errors akin
to those frequently ascribed to error-prone, heuristic-based human reasoning.
Notwithstanding this superficial similarity, an in-depth comparison between
humans and LLMs indicated important differences with human-like reasoning, with
models limitations disappearing almost entirely in more recent LLMs releases.
Moreover, we show that while it is possible to devise strategies to induce
better performance, humans and machines are not equally-responsive to the same
prompting schemes. We conclude by discussing the epistemological implications
and challenges of comparing human and machine behavior for both artificial
intelligence and cognitive psychology.",http://arxiv.org/pdf/2309.12485v1
2309.12482v1,cs.LG,State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding,2023-09-21 20:55:21+00:00,"With more complex AI systems used by non-AI experts to complete daily tasks,
there is an increasing effort to develop methods that produce explanations of
AI decision making understandable by non-AI experts. Towards this effort,
leveraging higher-level concepts and producing concept-based explanations have
become a popular method. Most concept-based explanations have been developed
for classification techniques, and we posit that the few existing methods for
sequential decision making are limited in scope. In this work, we first
contribute a desiderata for defining ""concepts"" in sequential decision making
settings. Additionally, inspired by the Protege Effect which states explaining
knowledge often reinforces one's self-learning, we explore the utility of
concept-based explanations providing a dual benefit to the RL agent by
improving agent learning rate, and to the end-user by improving end-user
understanding of agent decision making. To this end, we contribute a unified
framework, State2Explanation (S2E), that involves learning a joint embedding
model between state-action pairs and concept-based explanations, and leveraging
such learned model to both (1) inform reward shaping during an agent's
training, and (2) provide explanations to end-users at deployment for improved
task performance. Our experimental validations, in Connect 4 and Lunar Lander,
demonstrate the success of S2E in providing a dual-benefit, successfully
informing reward shaping and improving agent learning rate, as well as
significantly improving end user task performance at deployment time.",http://arxiv.org/pdf/2309.12482v1
2309.12474v1,cs.RO,SAVME: Efficient Safety Validation for Autonomous Systems Using Meta-Learning,2023-09-21 20:41:47+00:00,"Discovering potential failures of an autonomous system is important prior to
deployment. Falsification-based methods are often used to assess the safety of
such systems, but the cost of running many accurate simulation can be high. The
validation can be accelerated by identifying critical failure scenarios for the
system under test and by reducing the simulation runtime. We propose a Bayesian
approach that integrates meta-learning strategies with a multi-armed bandit
framework. Our method involves learning distributions over scenario parameters
that are prone to triggering failures in the system under test, as well as a
distribution over fidelity settings that enable fast and accurate simulations.
In the spirit of meta-learning, we also assess whether the learned fidelity
settings distribution facilitates faster learning of the scenario parameter
distributions for new scenarios. We showcase our methodology using a
cutting-edge 3D driving simulator, incorporating 16 fidelity settings for an
autonomous vehicle stack that includes camera and lidar sensors. We evaluate
various scenarios based on an autonomous vehicle pre-crash typology. As a
result, our approach achieves a significant speedup, up to 18 times faster
compared to traditional methods that solely rely on a high-fidelity simulator.",http://arxiv.org/pdf/2309.12474v1
2309.12468v1,cs.CY,Generativism: the new hybrid,2023-09-21 20:23:58+00:00,"Generative Artificial Intelligence (GenAI) in Education has in a few short
months moved from being the topic of discussion around speculative education
futures to a very concrete reality. It is clear that the future of education,
as all industries, is collaboration with GenAI. GenAI attributes make it well
suited for social and constructivist approaches to learning that value
collaboration, community and the construction of knowledge and skills through
active learning. This article presents an approach to designing education in
collaboration with GenAI, based on digital education frameworks adapted for
this new hybrid of the AI age.",http://arxiv.org/pdf/2309.12468v1
2309.12460v1,cs.LG,Multimodal Deep Learning for Scientific Imaging Interpretation,2023-09-21 20:09:22+00:00,"In the domain of scientific imaging, interpreting visual data often demands
an intricate combination of human expertise and deep comprehension of the
subject materials. This study presents a novel methodology to linguistically
emulate and subsequently evaluate human-like interactions with Scanning
Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a
multimodal deep learning framework, our approach distills insights from both
textual and visual data harvested from peer-reviewed articles, further
augmented by the capabilities of GPT-4 for refined data synthesis and
evaluation. Despite inherent challenges--such as nuanced interpretations and
the limited availability of specialized datasets--our model (GlassLLaVA) excels
in crafting accurate interpretations, identifying key features, and detecting
defects in previously unseen SEM images. Moreover, we introduce versatile
evaluation metrics, suitable for an array of scientific imaging applications,
which allows for benchmarking against research-grounded answers. Benefiting
from the robustness of contemporary Large Language Models, our model adeptly
aligns with insights from research papers. This advancement not only
underscores considerable progress in bridging the gap between human and machine
interpretation in scientific imaging, but also hints at expansive avenues for
future research and broader application.",http://arxiv.org/pdf/2309.12460v1
2309.12455v1,cs.CL,LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation,2023-09-21 19:54:54+00:00,"Maintaining factual consistency is a critical issue in abstractive text
summarisation, however, it cannot be assessed by traditional automatic metrics
used for evaluating text summarisation, such as ROUGE scoring. Recent efforts
have been devoted to developing improved metrics for measuring factual
consistency using pre-trained language models, but these metrics have
restrictive token limits, and are therefore not suitable for evaluating long
document text summarisation. Moreover, there is limited research evaluating
whether existing automatic evaluation metrics are fit for purpose when applied
to long document data sets. In this work, we evaluate the efficacy of automatic
metrics at assessing factual consistency in long document text summarisation
and propose a new evaluation framework LongDocFACTScore. This framework allows
metrics to be extended to any length document. This framework outperforms
existing state-of-the-art metrics in its ability to correlate with human
measures of factuality when used to evaluate long document summarisation data
sets. Furthermore, we show LongDocFACTScore has performance comparable to
state-of-the-art metrics when evaluated against human measures of factual
consistency on short document data sets. We make our code and annotated data
publicly available: https://github.com/jbshp/LongDocFACTScore.",http://arxiv.org/pdf/2309.12455v1
2309.12448v1,physics.app-ph,Near Field Optimization Algorithm for Reconfigurable Intelligent Surface,2023-09-21 19:44:53+00:00,"Reconfigurable intelligent surface (RIS) is a type of wireless communication
technology that uses a reconfigurable surface, such as a wall or building that
is able to adjust its properties by an integrated optimization algorithm in
order to optimize the signal propagation for a given communication scenario. As
a reconfiguration algorithm the multidimensional optimization of the GNU
scientific library was analyzed to evaluate the performance of the smart
surface in the quality of signal reception. This analysis took place by means
of electrodynamic simulations based on the finite difference time domain
method. Through these simulations it was possible to observe the efficiency of
the algorithm in the reconfiguration of the RIS, managing to focus the
electromagnetic waves in a remarkable way towards the point of interest.",http://arxiv.org/pdf/2309.12448v1
2309.12445v1,cs.LG,Ensemble Neural Networks for Remaining Useful Life (RUL) Prediction,2023-09-21 19:38:44+00:00,"A core part of maintenance planning is a monitoring system that provides a
good prognosis on health and degradation, often expressed as remaining useful
life (RUL). Most of the current data-driven approaches for RUL prediction focus
on single-point prediction. These point prediction approaches do not include
the probabilistic nature of the failure. The few probabilistic approaches to
date either include the aleatoric uncertainty (which originates from the
system), or the epistemic uncertainty (which originates from the model
parameters), or both simultaneously as a total uncertainty. Here, we propose
ensemble neural networks for probabilistic RUL predictions which considers both
uncertainties and decouples these two uncertainties. These decoupled
uncertainties are vital in knowing and interpreting the confidence of the
predictions. This method is tested on NASA's turbofan jet engine CMAPSS
data-set. Our results show how these uncertainties can be modeled and how to
disentangle the contribution of aleatoric and epistemic uncertainty.
Additionally, our approach is evaluated on different metrics and compared
against the current state-of-the-art methods.",http://arxiv.org/pdf/2309.12445v1
2309.12444v1,cs.CL,Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI,2023-09-21 19:36:48+00:00,"Generative Artificial Intelligence is set to revolutionize healthcare
delivery by transforming traditional patient care into a more personalized,
efficient, and proactive process. Chatbots, serving as interactive
conversational models, will probably drive this patient-centered transformation
in healthcare. Through the provision of various services, including diagnosis,
personalized lifestyle recommendations, and mental health support, the
objective is to substantially augment patient health outcomes, all the while
mitigating the workload burden on healthcare providers. The life-critical
nature of healthcare applications necessitates establishing a unified and
comprehensive set of evaluation metrics for conversational models. Existing
evaluation metrics proposed for various generic large language models (LLMs)
demonstrate a lack of comprehension regarding medical and health concepts and
their significance in promoting patients' well-being. Moreover, these metrics
neglect pivotal user-centered aspects, including trust-building, ethics,
personalization, empathy, user comprehension, and emotional support. The
purpose of this paper is to explore state-of-the-art LLM-based evaluation
metrics that are specifically applicable to the assessment of interactive
conversational models in healthcare. Subsequently, we present an comprehensive
set of evaluation metrics designed to thoroughly assess the performance of
healthcare chatbots from an end-user perspective. These metrics encompass an
evaluation of language processing abilities, impact on real-world clinical
tasks, and effectiveness in user-interactive conversations. Finally, we engage
in a discussion concerning the challenges associated with defining and
implementing these metrics, with particular emphasis on confounding factors
such as the target audience, evaluation methods, and prompt techniques involved
in the evaluation process.",http://arxiv.org/pdf/2309.12444v1
2309.12426v1,cs.CL,Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges,2023-09-21 18:48:02+00:00,"Large Language Models (LLMs) have demonstrated impressive zero shot
performance on a wide range of NLP tasks, demonstrating the ability to reason
and apply commonsense. A relevant application is to use them for creating high
quality synthetic datasets for downstream tasks. In this work, we probe whether
GPT-4 can be used to augment existing extractive reading comprehension
datasets. Automating data annotation processes has the potential to save large
amounts of time, money and effort that goes into manually labelling datasets.
In this paper, we evaluate the performance of GPT-4 as a replacement for human
annotators for low resource reading comprehension tasks, by comparing
performance after fine tuning, and the cost associated with annotation. This
work serves to be the first analysis of LLMs as synthetic data augmenters for
QA systems, highlighting the unique opportunities and challenges. Additionally,
we release augmented versions of low resource datasets, that will allow the
research community to create further benchmarks for evaluation of generated
datasets.",http://arxiv.org/pdf/2309.12426v1
2309.12423v1,cs.AI,Event Prediction using Case-Based Reasoning over Knowledge Graphs,2023-09-21 18:46:29+00:00,"Applying link prediction (LP) methods over knowledge graphs (KG) for tasks
such as causal event prediction presents an exciting opportunity. However,
typical LP models are ill-suited for this task as they are incapable of
performing inductive link prediction for new, unseen event entities and they
require retraining as knowledge is added or changed in the underlying KG. We
introduce a case-based reasoning model, EvCBR, to predict properties about new
consequent events based on similar cause-effect events present in the KG. EvCBR
uses statistical measures to identify similar events and performs path-based
predictions, requiring no training step. To generalize our methods beyond the
domain of event prediction, we frame our task as a 2-hop LP task, where the
first hop is a causal relation connecting a cause event to a new effect event
and the second hop is a property about the new event which we wish to predict.
The effectiveness of our method is demonstrated using a novel dataset of
newsworthy events with causal relations curated from Wikidata, where EvCBR
outperforms baselines including translational-distance-based, GNN-based, and
rule-based LP models.",http://arxiv.org/pdf/2309.12423v1
2309.12421v1,cs.CR,Change Management using Generative Modeling on Digital Twins,2023-09-21 18:43:28+00:00,"A key challenge faced by small and medium-sized business entities is securely
managing software updates and changes. Specifically, with rapidly evolving
cybersecurity threats, changes/updates/patches to software systems are
necessary to stay ahead of emerging threats and are often mandated by
regulators or statutory authorities to counter these. However, security
patches/updates require stress testing before they can be released in the
production system. Stress testing in production environments is risky and poses
security threats. Large businesses usually have a non-production environment
where such changes can be made and tested before being released into
production. Smaller businesses do not have such facilities. In this work, we
show how ""digital twins"", especially for a mix of IT and IoT environments, can
be created on the cloud. These digital twins act as a non-production
environment where changes can be applied, and the system can be securely tested
before patch release. Additionally, the non-production digital twin can be used
to collect system data and run stress tests on the environment, both manually
and automatically. In this paper, we show how using a small sample of real
data/interactions, Generative Artificial Intelligence (AI) models can be used
to generate testing scenarios to check for points of failure.",http://arxiv.org/pdf/2309.12421v1
2309.12415v1,cs.AI,Constraints First: A New MDD-based Model to Generate Sentences Under Constraints,2023-09-21 18:29:52+00:00,"This paper introduces a new approach to generating strongly constrained
texts. We consider standardized sentence generation for the typical application
of vision screening. To solve this problem, we formalize it as a discrete
combinatorial optimization problem and utilize multivalued decision diagrams
(MDD), a well-known data structure to deal with constraints. In our context,
one key strength of MDD is to compute an exhaustive set of solutions without
performing any search. Once the sentences are obtained, we apply a language
model (GPT-2) to keep the best ones. We detail this for English and also for
French where the agreement and conjugation rules are known to be more complex.
Finally, with the help of GPT-2, we get hundreds of bona-fide candidate
sentences. When compared with the few dozen sentences usually available in the
well-known vision screening test (MNREAD), this brings a major breakthrough in
the field of standardized sentence generation. Also, as it can be easily
adapted for other languages, it has the potential to make the MNREAD test even
more valuable and usable. More generally, this paper highlights MDD as a
convincing alternative for constrained text generation, especially when the
constraints are hard to satisfy, but also for many other prospects.",http://arxiv.org/pdf/2309.12415v1
2309.12414v1,physics.app-ph,FDTD Full Wave Simulations of Reconfigurable Intelligent Surfaces,2023-09-21 18:29:07+00:00,"This paper presents the analysis of metasurfaces, here called reconfigurable
intelligent surface. The analysis is performed by numerical simulations that
implement the finite-difference time-domain method. The metasurface has been
modeled by metallic patches interconnected by varactor diodes. The
electromagnetic source consists of randomly generated plane wave. This kind of
analysis allows us to investigate the response of the metasurface when it is
hit by a random source.",http://arxiv.org/pdf/2309.12414v1
2309.12312v1,cs.RO,ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals,2023-09-21 17:59:50+00:00,"We present ForceSight, a system for text-guided mobile manipulation that
predicts visual-force goals using a deep neural network. Given a single RGBD
image combined with a text prompt, ForceSight determines a target end-effector
pose in the camera frame (kinematic goal) and the associated forces (force
goal). Together, these two components form a visual-force goal. Prior work has
demonstrated that deep models outputting human-interpretable kinematic goals
can enable dexterous manipulation by real robots. Forces are critical to
manipulation, yet have typically been relegated to lower-level execution in
these systems. When deployed on a mobile manipulator equipped with an
eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps,
drawer opening, and object handovers with an 81% success rate in unseen
environments with object instances that differed significantly from the
training data. In a separate experiment, relying exclusively on visual servoing
and ignoring force goals dropped the success rate from 90% to 45%,
demonstrating that force goals can significantly enhance performance. The
appendix, videos, code, and trained models are available at
https://force-sight.github.io/.",http://arxiv.org/pdf/2309.12312v1
2309.12311v1,cs.CV,LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent,2023-09-21 17:59:45+00:00,"3D visual grounding is a critical skill for household robots, enabling them
to navigate, manipulate objects, and answer questions based on their
environment. While existing approaches often rely on extensive labeled data or
exhibit limitations in handling complex language queries, we propose
LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model
(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to
decompose complex natural language queries into semantic constituents and
employs a visual grounding tool, such as OpenScene or LERF, to identify objects
in a 3D scene. The LLM then evaluates the spatial and commonsense relations
among the proposed objects to make a final grounding decision. Our method does
not require any labeled training data and can generalize to novel 3D scenes and
arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and
demonstrate state-of-the-art zero-shot grounding accuracy. Our findings
indicate that LLMs significantly improve the grounding capability, especially
for complex language queries, making LLM-Grounder an effective approach for 3D
vision-language tasks in robotics. Videos and interactive demos can be found on
the project website https://chat-with-nerf.github.io/ .",http://arxiv.org/pdf/2309.12311v1
2309.12309v1,cs.HC,Rehearsal: Simulating Conflict to Teach Conflict Resolution,2023-09-21 17:59:20+00:00,"Interpersonal conflict is an uncomfortable but unavoidable fact of life.
Navigating conflict successfully is a skill -- one that can be learned through
deliberate practice -- but few have access to effective training or feedback.
To expand this access, we introduce Rehearsal, a system that allows users to
rehearse conflicts with a believable simulated interlocutor, explore
counterfactual ""what if?"" scenarios to identify alternative conversational
paths, and learn through feedback on how and when to apply specific conflict
strategies. Users can utilize Rehearsal to practice handling a variety of
predefined conflict scenarios, from office disputes to relationship issues, or
they can choose to create their own. To enable Rehearsal, we develop IRP
prompting, a method of conditioning output of a large language model on the
influential Interest-Rights-Power (IRP) theory from conflict resolution.
Rehearsal uses IRP to generate utterances grounded in conflict resolution
theory, guiding users towards counterfactual conflict resolution strategies
that help de-escalate difficult conversations. In a between-subjects
evaluation, 40 participants engaged in an actual conflict with a confederate
after training. Compared to a control group with lecture material covering the
same IRP theory, participants with simulated training from Rehearsal
significantly improved their performance in the unaided conflict: they reduced
their use of escalating competitive strategies by an average of 67%, while
doubling their use of cooperative strategies. Overall, Rehearsal highlights the
potential effectiveness of language models as tools for learning and practicing
interpersonal skills.",http://arxiv.org/pdf/2309.12309v1
2309.12307v1,cs.CL,LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models,2023-09-21 17:59:11+00:00,"We present LongLoRA, an efficient fine-tuning approach that extends the
context sizes of pre-trained large language models (LLMs), with limited
computation cost. Typically, training LLMs with long context sizes is
computationally expensive, requiring extensive training hours and GPU
resources. For example, training on the context length of 8192 needs 16x
computational costs in self-attention layers as that of 2048. In this paper, we
speed up the context extension of LLMs in two aspects. On the one hand,
although dense global attention is needed during inference, fine-tuning the
model can be effectively and efficiently done by sparse local attention. The
proposed shift short attention effectively enables context extension, leading
to non-trivial computation saving with similar performance to fine-tuning with
vanilla attention. Particularly, it can be implemented with only two lines of
code in training, while being optional in inference. On the other hand, we
revisit the parameter-efficient fine-tuning regime for context expansion.
Notably, we find that LoRA for context extension works well under the premise
of trainable embedding and normalization. LongLoRA demonstrates strong
empirical results on various tasks on LLaMA2 models from 7B/13B to 70B.
LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a
single 8x A100 machine. LongLoRA extends models' context while retaining their
original architectures, and is compatible with most existing techniques, like
FlashAttention-2. In addition, to make LongLoRA practical, we collect a
dataset, LongQA, for supervised fine-tuning. It contains more than 3k long
context question-answer pairs.",http://arxiv.org/pdf/2309.12307v1
2309.12301v1,cs.LG,Environment-biased Feature Ranking for Novelty Detection Robustness,2023-09-21 17:58:26+00:00,"We tackle the problem of robust novelty detection, where we aim to detect
novelties in terms of semantic content while being invariant to changes in
other, irrelevant factors. Specifically, we operate in a setup with multiple
environments, where we determine the set of features that are associated more
with the environments, rather than to the content relevant for the task. Thus,
we propose a method that starts with a pretrained embedding and a multi-env
setup and manages to rank the features based on their environment-focus. First,
we compute a per-feature score based on the feature distribution variance
between envs. Next, we show that by dropping the highly scored ones, we manage
to remove spurious correlations and improve the overall performance by up to
6%, both in covariance and sub-population shift cases, both for a real and a
synthetic benchmark, that we introduce for this task.",http://arxiv.org/pdf/2309.12301v1
2309.12300v1,cs.RO,See to Touch: Learning Tactile Dexterity through Visual Incentives,2023-09-21 17:58:13+00:00,"Equipping multi-fingered robots with tactile sensing is crucial for achieving
the precise, contact-rich, and dexterous manipulation that humans excel at.
However, relying solely on tactile sensing fails to provide adequate cues for
reasoning about objects' spatial configurations, limiting the ability to
correct errors and adapt to changing situations. In this paper, we present
Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances
tactile-based dexterity by optimizing dexterous policies using vision-based
rewards. First, we use a contrastive-based objective to learn visual
representations. Next, we construct a reward function using these visual
representations through optimal-transport based matching on one human
demonstration. Finally, we use online reinforcement learning on our robot to
optimize tactile-based policies that maximize the visual reward. On six
challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping
slender objects, TAVI achieves a success rate of 73% using our four-fingered
Allegro robot hand. The increase in performance is 108% higher than policies
using tactile and vision-based rewards and 135% higher than policies without
tactile observational input. Robot videos are best viewed on our project
website: https://see-to-touch.github.io/.",http://arxiv.org/pdf/2309.12300v1
2309.12295v1,cs.CV,Learning to Drive Anywhere,2023-09-21 17:55:36+00:00,"Human drivers can seamlessly adapt their driving decisions across
geographical locations with diverse conditions and rules of the road, e.g.,
left vs. right-hand traffic. In contrast, existing models for autonomous
driving have been thus far only deployed within restricted operational domains,
i.e., without accounting for varying driving behaviors across locations or
model scalability. In this work, we propose AnyD, a single geographically-aware
conditional imitation learning (CIL) model that can efficiently learn from
heterogeneous and globally distributed data with dynamic environmental,
traffic, and social characteristics. Our key insight is to introduce a
high-capacity geo-location-based channel attention mechanism that effectively
adapts to local nuances while also flexibly modeling similarities among regions
in a data-driven manner. By optimizing a contrastive imitation objective, our
proposed approach can efficiently scale across inherently imbalanced data
distributions and location-dependent events. We demonstrate the benefits of our
AnyD agent across multiple datasets, cities, and scalable deployment paradigms,
i.e., centralized, semi-supervised, and distributed agent training.
Specifically, AnyD outperforms CIL baselines by over 14% in open-loop
evaluation and 30% in closed-loop testing on CARLA.",http://arxiv.org/pdf/2309.12295v1
2309.12288v1,cs.CL,"The Reversal Curse: LLMs trained on ""A is B"" fail to learn ""B is A""",2023-09-21 17:52:19+00:00,"We expose a surprising failure of generalization in auto-regressive large
language models (LLMs). If a model is trained on a sentence of the form ""A is
B"", it will not automatically generalize to the reverse direction ""B is A"".
This is the Reversal Curse. For instance, if a model is trained on ""Olaf Scholz
was the ninth Chancellor of Germany"", it will not automatically be able to
answer the question, ""Who was the ninth Chancellor of Germany?"". Moreover, the
likelihood of the correct answer (""Olaf Scholz"") will not be higher than for a
random name. Thus, models exhibit a basic failure of logical deduction and do
not generalize a prevalent pattern in their training set (i.e. if ""A is B''
occurs, ""B is A"" is more likely to occur). We provide evidence for the Reversal
Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as ""Uriah
Hawthorne is the composer of 'Abyssal Melodies'"" and showing that they fail to
correctly answer ""Who composed 'Abyssal Melodies?'"". The Reversal Curse is
robust across model sizes and model families and is not alleviated by data
augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about
real-world celebrities, such as ""Who is Tom Cruise's mother? [A: Mary Lee
Pfeiffer]"" and the reverse ""Who is Mary Lee Pfeiffer's son?"". GPT-4 correctly
answers questions like the former 79% of the time, compared to 33% for the
latter. This shows a failure of logical deduction that we hypothesize is caused
by the Reversal Curse. Code is available at
https://github.com/lukasberglund/reversal_curse.",http://arxiv.org/pdf/2309.12288v1
2309.12284v2,cs.CL,MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models,2023-09-21 17:45:42+00:00,"Large language models (LLMs) have pushed the limits of natural language
understanding and exhibited excellent problem-solving ability. Despite the
great success, most existing open-source LLMs (e.g., LLaMA-2) are still far
away from satisfactory for solving mathematical problem due to the complex
reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned
language model that specializes in mathematical reasoning. Specifically, we
start by bootstrapping mathematical questions by rewriting the question from
multiple perspectives without extra knowledge, which results in a new dataset
called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.
Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for
mathematical reasoning demonstrate that MetaMath outperforms a suite of
open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%
on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same
size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of
82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release the MetaMathQA
dataset, the MetaMath models with different model sizes and the training code
for public use.",http://arxiv.org/pdf/2309.12284v2
2309.12276v1,cs.HC,LLMR: Real-time Prompting of Interactive Worlds using Large Language Models,2023-09-21 17:37:01+00:00,"We present Large Language Model for Mixed Reality (LLMR), a framework for the
real-time creation and modification of interactive Mixed Reality experiences
using LLMs. LLMR leverages novel strategies to tackle difficult cases where
ideal training data is scarce, or where the design goal requires the synthesis
of internal dynamics, intuitive analysis, or advanced interactivity. Our
framework relies on text interaction and the Unity game engine. By
incorporating techniques for scene understanding, task planning,
self-debugging, and memory management, LLMR outperforms the standard GPT-4 by
4x in average error rate. We demonstrate LLMR's cross-platform interoperability
with several example worlds, and evaluate it on a variety of creation and
modification tasks to show that it can produce and edit diverse objects, tools,
and scenes. Finally, we conducted a usability study (N=11) with a diverse set
that revealed participants had positive experiences with the system and would
use it again.",http://arxiv.org/pdf/2309.12276v1
2309.12267v1,cs.CR,Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications,2023-09-21 17:17:28+00:00,"Federated Learning (FL) has revolutionized how we train deep neural networks
by enabling decentralized collaboration while safeguarding sensitive data and
improving model performance. However, FL faces two crucial challenges: the
diverse nature of data held by individual clients and the vulnerability of the
FL system to security breaches. This paper introduces an innovative solution
named Estimated Mean Aggregation (EMA) that not only addresses these challenges
but also provides a fundamental reference point as a $\mathsf{baseline}$ for
advanced aggregation techniques in FL systems. EMA's significance lies in its
dual role: enhancing model security by effectively handling malicious outliers
through trimmed means and uncovering data heterogeneity to ensure that trained
models are adaptable across various client datasets. Through a wealth of
experiments, EMA consistently demonstrates high accuracy and area under the
curve (AUC) compared to alternative methods, establishing itself as a robust
baseline for evaluating the effectiveness and security of FL aggregation
methods. EMA's contributions thus offer a crucial step forward in advancing the
efficiency, security, and versatility of decentralized deep learning in the
context of FL.",http://arxiv.org/pdf/2309.12267v1
2309.12253v1,cs.LG,SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning,2023-09-21 16:57:09+00:00,"We introduce an extension to the CLRS algorithmic learning benchmark,
prioritizing scalability and the utilization of sparse representations. Many
algorithms in CLRS require global memory or information exchange, mirrored in
its execution model, which constructs fully connected (not sparse) graphs based
on the underlying problem. Despite CLRS's aim of assessing how effectively
learned algorithms can generalize to larger instances, the existing execution
model becomes a significant constraint due to its demanding memory requirements
and runtime (hard to scale). However, many important algorithms do not demand a
fully connected graph; these algorithms, primarily distributed in nature, align
closely with the message-passing paradigm employed by Graph Neural Networks.
Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark
specifically with scalability and sparseness in mind. Our approach includes
adapted algorithms from the original CLRS benchmark and introduces new problems
from distributed and randomized algorithms. Moreover, we perform a thorough
empirical evaluation of our benchmark. Code is publicly available at
https://github.com/jkminder/SALSA-CLRS.",http://arxiv.org/pdf/2309.12253v1
2309.12247v1,cs.CL,"Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",2023-09-21 16:47:30+00:00,"Detecting fake news requires both a delicate sense of diverse clues and a
profound understanding of the real-world background, which remains challenging
for detectors based on small language models (SLMs) due to their knowledge and
capability limitations. Recent advances in large language models (LLMs) have
shown remarkable performance in various tasks, but whether and how LLMs could
help with fake news detection remains underexplored. In this paper, we
investigate the potential of LLMs in fake news detection. First, we conduct an
empirical study and find that a sophisticated LLM such as GPT 3.5 could
generally expose fake news and provide desirable multi-perspective rationales
but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis
attributes such a gap to the LLM's inability to select and integrate rationales
properly to conclude. Based on these findings, we propose that current LLMs may
not substitute fine-tuned SLMs in fake news detection but can be a good advisor
for SLMs by providing multi-perspective instructive rationales. To instantiate
this proposal, we design an adaptive rationale guidance network for fake news
detection (ARG), in which SLMs selectively acquire insights on news analysis
from the LLMs' rationales. We further derive a rationale-free version of ARG by
distillation, namely ARG-D, which services cost-sensitive scenarios without
inquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and
ARG-D outperform three types of baseline methods, including SLM-based,
LLM-based, and combinations of small and large language models.",http://arxiv.org/pdf/2309.12247v1
2309.12244v1,cs.HC,ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events,2023-09-21 16:43:17+00:00,"Children typically learn to identify and express emotions through sharing
their stories and feelings with others, particularly their family. However, it
is challenging for parents or siblings to have emotional communication with
children since children are still developing their communication skills. We
present ChaCha, a chatbot that encourages and guides children to share personal
events and associated emotions. ChaCha combines a state machine and large
language models (LLMs) to keep the dialogue on track while carrying on
free-form conversations. Through an exploratory study with 20 children (aged
8-12), we examine how ChaCha prompts children to share personal events and
guides them to describe associated emotions. Participants perceived ChaCha as a
close friend and shared their stories on various topics, such as family trips
and personal achievements. Based on the quantitative and qualitative findings,
we discuss opportunities for leveraging LLMs to design child-friendly chatbots
to support children in sharing their emotions.",http://arxiv.org/pdf/2309.12244v1
2309.12237v1,cs.CR,t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators,2023-09-21 16:30:40+00:00,"Presentation attack (spoofing) detection (PAD) typically operates alongside
biometric verification to improve reliablity in the face of spoofing attacks.
Even though the two sub-systems operate in tandem to solve the single task of
reliable biometric verification, they address different detection tasks and are
hence typically evaluated separately. Evidence shows that this approach is
suboptimal. We introduce a new metric for the joint evaluation of PAD solutions
operating in situ with biometric verification. In contrast to the tandem
detection cost function proposed recently, the new tandem equal error rate
(t-EER) is parameter free. The combination of two classifiers nonetheless leads
to a \emph{set} of operating points at which false alarm and miss rates are
equal and also dependent upon the prevalence of attacks. We therefore introduce
the \emph{concurrent} t-EER, a unique operating point which is invariable to
the prevalence of attacks. Using both modality (and even application) agnostic
simulated scores, as well as real scores for a voice biometrics application, we
demonstrate application of the t-EER to a wide range of biometric system
evaluations under attack. The proposed approach is a strong candidate metric
for the tandem evaluation of PAD systems and biometric comparators.",http://arxiv.org/pdf/2309.12237v1
2309.12177v1,cs.AI,Explainable Artificial Intelligence for Drug Discovery and Development -- A Comprehensive Survey,2023-09-21 15:36:06+00:00,"The field of drug discovery has experienced a remarkable transformation with
the advent of artificial intelligence (AI) and machine learning (ML)
technologies. However, as these AI and ML models are becoming more complex,
there is a growing need for transparency and interpretability of the models.
Explainable Artificial Intelligence (XAI) is a novel approach that addresses
this issue and provides a more interpretable understanding of the predictions
made by machine learning models. In recent years, there has been an increasing
interest in the application of XAI techniques to drug discovery. This review
article provides a comprehensive overview of the current state-of-the-art in
XAI for drug discovery, including various XAI methods, their application in
drug discovery, and the challenges and limitations of XAI techniques in drug
discovery. The article also covers the application of XAI in drug discovery,
including target identification, compound design, and toxicity prediction.
Furthermore, the article suggests potential future research directions for the
application of XAI in drug discovery. The aim of this review article is to
provide a comprehensive understanding of the current state of XAI in drug
discovery and its potential to transform the field.",http://arxiv.org/pdf/2309.12177v1
2309.12161v1,cs.CL,Code Soliloquies for Accurate Calculations in Large Language Models,2023-09-21 15:16:58+00:00,"High-quality conversational datasets are integral to the successful
development of Intelligent Tutoring Systems (ITS) that employ a Large Language
Model (LLM) backend. These datasets, when used to fine-tune the LLM backend,
significantly enhance the quality of interactions between students and ITS. A
common strategy for developing these datasets involves generating synthetic
student-teacher dialogues using advanced GPT-4 models. However, challenges
arise when these dialogues demand complex calculations, common in subjects like
physics. Despite its advanced capabilities, GPT-4's performance falls short in
reliably handling even simple multiplication tasks, marking a significant
limitation in its utility for these subjects. To address these challenges, this
paper introduces an innovative stateful prompt design. Our approach generates a
mock conversation between a student and a tutorbot, both roles simulated by
GPT-4. Each student response triggers a soliloquy (an inner monologue) in the
GPT-tutorbot, which assesses whether its response would necessitate
calculations. If so, it proceeds to script the required code in Python and then
uses the resulting output to construct its response to the student. Our
approach notably enhances the quality of synthetic conversation datasets,
especially for subjects that are calculation-intensive. Our findings show that
our Higgs model -- a LLaMA finetuned with datasets generated through our novel
stateful prompt design -- proficiently utilizes Python for computations.
Consequently, finetuning with our datasets enriched with code soliloquies
enhances not just the accuracy but also the computational reliability of Higgs'
responses.",http://arxiv.org/pdf/2309.12161v1
2309.12382v1,cs.CV,SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap,2023-09-21 15:06:08+00:00,"Inspired by the great success of language model (LM)-based pre-training,
recent studies in visual document understanding have explored LM-based
pre-training methods for modeling text within document images. Among them,
pre-training that reads all text from an image has shown promise, but often
exhibits instability and even fails when applied to broader domains, such as
those involving both visual documents and scene text images. This is a
substantial limitation for real-world scenarios, where the processing of text
image inputs in diverse domains is essential. In this paper, we investigate
effective pre-training tasks in the broader domains and also propose a novel
pre-training method called SCOB that leverages character-wise supervised
contrastive learning with online text rendering to effectively pre-train
document and scene text domains by bridging the domain gap. Moreover, SCOB
enables weakly supervised learning, significantly reducing annotation costs.
Extensive benchmarks demonstrate that SCOB generally improves vanilla
pre-training methods and achieves comparable performance to state-of-the-art
methods. Our findings suggest that SCOB can be served generally and effectively
for read-type pre-training methods. The code will be available at
https://github.com/naver-ai/scob.",http://arxiv.org/pdf/2309.12382v1
2309.12148v1,cs.NE,Neural Modelling of Dynamic Systems with Time Delays Based on an Adjusted NEAT Algorithm,2023-09-21 15:04:42+00:00,"A problem related to the development of an algorithm designed to find an
architecture of artificial neural network used for black-box modelling of
dynamic systems with time delays has been addressed in this paper. The proposed
algorithm is based on a well-known NeuroEvolution of Augmenting Topologies
(NEAT) algorithm. The NEAT algorithm has been adjusted by allowing additional
connections within an artificial neural network and developing original
specialised evolutionary operators. This resulted in a compromise between the
size of neural network and its accuracy in capturing the response of the
mathematical model under which it has been learnt. The research involved an
extended validation study based on data generated from a mathematical model of
an exemplary system as well as the fast processes occurring in a pressurised
water nuclear reactor. The obtaining simulation results demonstrate the high
effectiveness of the devised neural (black-box) models of dynamic systems with
time delays.",http://arxiv.org/pdf/2309.12148v1
2309.12146v1,astro-ph.CO,Search for a possible quasi-periodic structure based on data of the SDSS DR12 LOWZ,2023-09-21 15:04:35+00:00,"We carry out a statistical analysis of the spatial distribution of galaxies
at cosmological redshifts $0.16 \leq z \leq 0.47$ based on the SDSS\ DR12\ LOWZ
catalogue. Our aim is to search and study possible large-scale quasi-regular
structures embedded in the {\it cosmic web}. We calculate projections of the
Cartesian galaxy coordinates on different axes (directions) densely covering
certain regions in the sky to look for special directions along which
one-dimensional distributions of the projections contain significant
quasi-periodic components. These components appear as peaks in the power
spectra and lie in a narrow range of wave numbers $0.05 < k < 0.07$. Particular
attention is paid to the evaluation of the significance of the peaks. It is
found that the significance of the dominant peaks for some selected directions
exceeds $(4 - 5)\sigma$. In order to reduce possible selection effects, we
create a mock homogeneous catalogue of spatial distribution of galaxies by
adding a random set of artificial objects (points) to the real galaxies under
study. The power spectrum of this cumulative model data also demonstrates
significant peak corresponding to approximately the same scale. As a result we
assume the existence of an anisotropic cosmological quasi-periodic structure
with characteristic scale $(116 \pm 10)~h^{-1}$~Mpc.",http://arxiv.org/pdf/2309.12146v1
2309.12140v1,cs.CV,Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features,2023-09-21 15:00:31+00:00,"The rapid development of 3D object detection systems for self-driving cars
has significantly improved accuracy. However, these systems struggle to
generalize across diverse driving environments, which can lead to
safety-critical failures in detecting traffic participants. To address this, we
propose a method that utilizes unlabeled repeated traversals of multiple
locations to adapt object detectors to new driving environments. By
incorporating statistics computed from repeated LiDAR scans, we guide the
adaptation process effectively. Our approach enhances LiDAR-based detection
models using spatial quantized historical features and introduces a lightweight
regression head to leverage the statistics for feature regularization.
Additionally, we leverage the statistics for a novel self-training process to
stabilize the training. The framework is detector model-agnostic and
experiments on real-world datasets demonstrate significant improvements,
achieving up to a 20-point performance gain, especially in detecting
pedestrians and distant objects. Code is available at
https://github.com/zhangtravis/Hist-DA.",http://arxiv.org/pdf/2309.12140v1
2309.12139v1,cs.RO,"On the relationship between Benchmarking, Standards and Certification in Robotics and AI",2023-09-21 14:59:36+00:00,"Benchmarking, standards and certification are closely related processes.
Standards can provide normative requirements that robotics and AI systems may
or may not conform to. Certification generally relies upon conformance with one
or more standards as the key determinant of granting a certificate to operate.
And benchmarks are sets of standardised tests against which robots and AI
systems can be measured. Benchmarks therefore can be thought of as informal
standards. In this paper we will develop these themes with examples from
benchmarking, standards and certification, and argue that these three linked
processes are not only useful but vital to the broader practice of Responsible
Innovation.",http://arxiv.org/pdf/2309.12139v1
2309.12137v1,cs.CL,OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media,2023-09-21 14:58:50+00:00,"While resources for English language are fairly sufficient to understand
content on social media, similar resources in Arabic are still immature. The
main reason that the resources in Arabic are insufficient is that Arabic has
many dialects in addition to the standard version (MSA). Arabs do not use MSA
in their daily communications; rather, they use dialectal versions.
Unfortunately, social users transfer this phenomenon into their use of social
media platforms, which in turn has raised an urgent need for building suitable
AI models for language-dependent applications. Existing machine translation
(MT) systems designed for MSA fail to work well with Arabic dialects. In light
of this, it is necessary to adapt to the informal nature of communication on
social networks by developing MT systems that can effectively handle the
various dialects of Arabic. Unlike for MSA that shows advanced progress in MT
systems, little effort has been exerted to utilize Arabic dialects for MT
systems. While few attempts have been made to build translation datasets for
dialectal Arabic, they are domain dependent and are not OSN cultural-language
friendly. In this work, we attempt to alleviate these limitations by proposing
an online social network-based multidialect Arabic dataset that is crafted by
contextually translating English tweets into four Arabic dialects: Gulf,
Yemeni, Iraqi, and Levantine. To perform the translation, we followed our
proposed guideline framework for content translation, which could be
universally applicable for translation between foreign languages and local
dialects. We validated the authenticity of our proposed dataset by developing
neural MT models for four Arabic dialects. Our results have shown a superior
performance of our NMT models trained using our dataset. We believe that our
dataset can reliably serve as an Arabic multidialectal translation dataset for
informal MT tasks.",http://arxiv.org/pdf/2309.12137v1
2309.12132v1,cs.AI,A knowledge representation approach for construction contract knowledge modeling,2023-09-21 14:53:36+00:00,"The emergence of large language models (LLMs) presents an unprecedented
opportunity to automate construction contract management, reducing human errors
and saving significant time and costs. However, LLMs may produce convincing yet
inaccurate and misleading content due to a lack of domain expertise. To address
this issue, expert-driven contract knowledge can be represented in a structured
manner to constrain the automatic contract management process. This paper
introduces the Nested Contract Knowledge Graph (NCKG), a knowledge
representation approach that captures the complexity of contract knowledge
using a nested structure. It includes a nested knowledge representation
framework, a NCKG ontology built on the framework, and an implementation
method. Furthermore, we present the LLM-assisted contract review pipeline
enhanced with external knowledge in NCKG. Our pipeline achieves a promising
performance in contract risk reviewing, shedding light on the combination of
LLM and KG towards more reliable and interpretable contract management.",http://arxiv.org/pdf/2309.12132v1
2309.12113v1,cs.AI,Incentivizing Massive Unknown Workers for Budget-Limited Crowdsensing: From Off-Line and On-Line Perspectives,2023-09-21 14:30:42+00:00,"Although the uncertainties of the workers can be addressed by the standard
Combinatorial Multi-Armed Bandit (CMAB) framework in existing proposals through
a trade-off between exploration and exploitation, we may not have sufficient
budget to enable the trade-off among the individual workers, especially when
the number of the workers is huge while the budget is limited. Moreover, the
standard CMAB usually assumes the workers always stay in the system, whereas
the workers may join in or depart from the system over time, such that what we
have learnt for an individual worker cannot be applied after the worker leaves.
To address the above challenging issues, in this paper, we first propose an
off-line Context-Aware CMAB-based Incentive (CACI) mechanism. We innovate in
leveraging the exploration-exploitation trade-off in a elaborately partitioned
context space instead of the individual workers, to effectively incentivize the
massive unknown workers with very limited budget. We also extend the above
basic idea to the on-line setting where unknown workers may join in or depart
from the systems dynamically, and propose an on-line version of the CACI
mechanism. Specifically, by the exploitation-exploration trade-off in the
context space, we learn to estimate the sensing ability of any unknown worker
(even it never appeared in the system before) according to its context
information. We perform rigorous theoretical analysis to reveal the upper
bounds on the regrets of our CACI mechanisms and to prove their truthfulness
and individual rationality, respectively. Extensive experiments on both
synthetic and real datasets are also conducted to verify the efficacy of our
mechanisms.",http://arxiv.org/pdf/2309.12113v1
2309.12109v1,cs.CL,PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models,2023-09-21 14:29:23+00:00,"In this era of large language models (LLMs), the traditional training of
models has become increasingly unimaginable for regular users and institutions.
The exploration of efficient fine-tuning for high-resource languages on these
models is an undeniable trend that is gradually gaining popularity. However,
there has been very little exploration for various low-resource languages, such
as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While
there is currently no existing large language model for Tibetan due to its
low-resource nature, that day will undoubtedly arrive. Therefore, research on
efficient fine-tuning for low-resource language models like Tibetan is highly
necessary. Our research can serve as a reference to fill this crucial gap.
Efficient fine-tuning strategies for pre-trained language models (PLMs) in
Tibetan have seen minimal exploration. We conducted three types of efficient
fine-tuning experiments on the publicly available TNCC-title dataset:
""prompt-tuning,"" ""Adapter lightweight fine-tuning,"" and ""prompt-tuning +
Adapter fine-tuning."" The experimental results demonstrate significant
improvements using these methods, providing valuable insights for advancing
Tibetan language applications in the context of pre-trained models.",http://arxiv.org/pdf/2309.12109v1
2309.12089v1,cs.RO,HiCRISP: A Hierarchical Closed-Loop Robotic Intelligent Self-Correction Planner,2023-09-21 13:58:26+00:00,"The integration of Large Language Models (LLMs) into robotics has
revolutionized human-robot interactions and autonomous task planning. However,
these systems are often unable to self-correct during the task execution, which
hinders their adaptability in dynamic real-world environments. To address this
issue, we present a Hierarchical Closed-loop Robotic Intelligent
Self-correction Planner (HiCRISP), an innovative framework that enables robots
to correct errors within individual steps during the task execution. HiCRISP
actively monitors and adapts the task execution process, addressing both
high-level planning and low-level action errors. Extensive benchmark
experiments, encompassing virtual and real-world scenarios, showcase HiCRISP's
exceptional performance, positioning it as a promising solution for robotic
task planning with LLMs.",http://arxiv.org/pdf/2309.12089v1
2309.12081v1,eess.SY,A Framework on Fully Distributed State Estimation and Cooperative Stabilization of LTI Plants,2023-09-21 13:53:12+00:00,"How to realize high-level autonomy of individuals is one of key technical
issues to promote swarm intelligence of multi-agent (node) systems with
collective tasks, while the fully distributed design is a potential way to
achieve this goal. This paper works on the fully distributed state estimation
and cooperative stabilization problem of linear time-invariant (LTI) plants
with multiple nodes communicating over general directed graphs, and is aimed to
provide a fully distributed framework for each node to perform cooperative
stabilization tasks. First, by incorporating a novel adaptive law, a
consensus-based estimator is designed for each node to obtain the plant state
based on its local measurement and local interaction with neighbors, without
using any global information of the communication topology. Subsequently, a
local controller is developed for each node to stabilize the plant
collaboratively with performance guaranteed under mild conditions.
Specifically, the proposed method only requires that the communication graph be
strongly connected, and the plant be collectively controllable and observable.
Further, the proposed method can be applied to pure fully distributed state
estimation scenarios and modified for noise-bounded LTI plants. Finally, two
numerical examples are provided to show the effectiveness of the theoretical
results.",http://arxiv.org/pdf/2309.12081v1
2309.12075v1,cs.CL,Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models,2023-09-21 13:45:32+00:00,"Prompt Tuning is emerging as a scalable and cost-effective method to
fine-tune Pretrained Language Models (PLMs). This study benchmarks the
performance and computational efficiency of Prompt Tuning and baseline methods
on a multi-label text classification task. This is applied to the use case of
classifying companies into an investment firm's proprietary industry taxonomy,
supporting their thematic investment strategy. Text-to-text classification with
PLMs is frequently reported to outperform classification with a classification
head, but has several limitations when applied to a multi-label classification
problem where each label consists of multiple tokens: (a) Generated labels may
not match any label in the industry taxonomy; (b) During fine-tuning, multiple
labels must be provided in an arbitrary order; (c) The model provides a binary
decision for each label, rather than an appropriate confidence score.
Limitation (a) is addressed by applying constrained decoding using Trie Search,
which slightly improves classification performance. All limitations (a), (b),
and (c) are addressed by replacing the PLM's language head with a
classification head. This improves performance significantly, while also
reducing computational costs during inference. The results indicate the
continuing need to adapt state-of-the-art methods to domain-specific tasks,
even in the era of PLMs with strong generalization abilities.",http://arxiv.org/pdf/2309.12075v1
2309.12071v1,cs.AI,Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam,2023-09-21 13:39:54+00:00,"Although Large Language Models (LLMs) represent a revolution in the way we
interact with computers, allowing the construction of complex questions and the
ability to reason over a sequence of statements, their use is restricted due to
the need for dedicated hardware for execution. In this study, we evaluate the
performance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a
quantization process and run on home hardware. The models considered were
Alpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we
developed a database containing 1,006 questions from the ENEM (Brazilian
National Secondary School Exam). Our analysis revealed that the best performing
models achieved an accuracy of approximately 46% for the original texts of the
Portuguese questions and 49% on their English translations. In addition, we
evaluated the computational efficiency of the models by measuring the time
required for execution. On average, the 7 and 13 billion LLMs took
approximately 20 and 50 seconds, respectively, to process the queries on a
machine equipped with an AMD Ryzen 5 3600x processor",http://arxiv.org/pdf/2309.12071v1
2309.12067v1,cs.CV,"Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives",2023-09-21 13:36:57+00:00,"Action scene understanding in soccer is a challenging task due to the complex
and dynamic nature of the game, as well as the interactions between players.
This article provides a comprehensive overview of this task divided into action
recognition, spotting, and spatio-temporal action localization, with a
particular emphasis on the modalities used and multimodal methods. We explore
the publicly available data sources and metrics used to evaluate models'
performance. The article reviews recent state-of-the-art methods that leverage
deep learning techniques and traditional methods. We focus on multimodal
methods, which integrate information from multiple sources, such as video and
audio data, and also those that represent one source in various ways. The
advantages and limitations of methods are discussed, along with their potential
for improving the accuracy and robustness of models. Finally, the article
highlights some of the open research questions and future directions in the
field of soccer action recognition, including the potential for multimodal
methods to advance this field. Overall, this survey provides a valuable
resource for researchers interested in the field of action scene understanding
in soccer.",http://arxiv.org/pdf/2309.12067v1
2309.12058v1,cs.LG,An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM,2023-09-21 13:25:11+00:00,"Anticancer peptides (ACPs) are a group of peptides that exhibite
antineoplastic properties. The utilization of ACPs in cancer prevention can
present a viable substitute for conventional cancer therapeutics, as they
possess a higher degree of selectivity and safety. Recent scientific
advancements generate an interest in peptide-based therapies which offer the
advantage of efficiently treating intended cells without negatively impacting
normal cells. However, as the number of peptide sequences continues to increase
rapidly, developing a reliable and precise prediction model becomes a
challenging task. In this work, our motivation is to advance an efficient model
for categorizing anticancer peptides employing the consolidation of word
embedding and deep learning models. First, Word2Vec and FastText are evaluated
as word embedding techniques for the purpose of extracting peptide sequences.
Then, the output of word embedding models are fed into deep learning approaches
CNN, LSTM, BiLSTM. To demonstrate the contribution of proposed framework,
extensive experiments are carried on widely-used datasets in the literature,
ACPs250 and Independent. Experiment results show the usage of proposed model
enhances classification accuracy when compared to the state-of-the-art studies.
The proposed combination, FastText+BiLSTM, exhibits 92.50% of accuracy for
ACPs250 dataset, and 96.15% of accuracy for Independent dataset, thence
determining new state-of-the-art.",http://arxiv.org/pdf/2309.12058v1
2309.12056v1,cs.AI,BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision,2023-09-21 13:24:01+00:00,"This paper presents BELT, a novel model and learning framework for the
pivotal topic of brain-to-language translation research. The translation from
noninvasive brain signals into readable natural language has the potential to
promote the application scenario as well as the development of brain-computer
interfaces (BCI) as a whole. The critical problem in brain signal decoding or
brain-to-language translation is the acquisition of semantically appropriate
and discriminative EEG representation from a dataset of limited scale and
quality. The proposed BELT method is a generic and efficient framework that
bootstraps EEG representation learning using off-the-shelf large-scale
pretrained language models (LMs). With a large LM's capacity for understanding
semantic information and zero-shot generalization, BELT utilizes large LMs
trained on Internet-scale datasets to bring significant improvements to the
understanding of EEG signals.
  In particular, the BELT model is composed of a deep conformer encoder and a
vector quantization encoder. Semantical EEG representation is achieved by a
contrastive learning step that provides natural language supervision. We
achieve state-of-the-art results on two featuring brain decoding tasks
including the brain-to-language translation and zero-shot sentiment
classification. Specifically, our model surpasses the baseline model on both
tasks by 5.45% and over 10% and archives a 42.31% BLEU-1 score and 67.32%
precision on the main evaluation metrics for translation and zero-shot
sentiment classification respectively.",http://arxiv.org/pdf/2309.12056v1
2309.12052v1,cs.NI,Optimizing V2V Unicast Communication Transmission with Reinforcement Learning and Vehicle Clustering,2023-09-21 13:17:50+00:00,"Efficient routing algorithms based on vehicular ad hoc networks (VANETs) play
an important role in emerging intelligent transportation systems. This highly
dynamic topology faces a number of wireless communication service challenges.
In this paper, we propose a protocol based on reinforcement learning and
vehicle node clustering, the protocol is called Qucts, solve
vehicle-to-fixed-destination or V2V messaging problems. Improve message
delivery rates with minimal hops and latency, link stability is also taken into
account. The agreement is divided into three levels, first cluster the
vehicles, each cluster head broadcasts its own coordinates and speed, to get
more cluster members. Also when a cluster member receives another cluster head
broadcast message, the cluster head generates a list of surrounding clusters,
find the best cluster to the destination as the next cluster during message
passing. Second, the protocol constructs a Q-value table based on the state
after clustering, used to participate in the selection of messaging clusters.
Finally, we introduce parameters that express the stability of the vehicle
within the cluster, for communication node selection. This protocol hierarchy
makes Qucts an offline and online solution. In order to distinguish unstable
nodes within a cluster, Coding of each road, will have vehicles with planned
routes, For example, car hailing and public bus. Compare the overlap with other
planned paths vehicles in the cluster, low overlap is labeled as unstable
nodes. Vehicle path overlap rate without a planned path is set to the mean
value. Comparing Qucts with existing routing protocols through simulation, Our
proposed Qucts scheme provides large improvements in both data delivery rate
and end-to-end delay reduction.",http://arxiv.org/pdf/2309.12052v1
2309.12038v1,cs.RO,Uncertainty-driven Exploration Strategies for Online Grasp Learning,2023-09-21 13:06:03+00:00,"Existing grasp prediction approaches are mostly based on offline learning,
while, ignored the exploratory grasp learning during online adaptation to new
picking scenarios, i.e., unseen object portfolio, camera and bin settings etc.
In this paper, we present a novel method for online learning of grasp
predictions for robotic bin picking in a principled way. Existing grasp
prediction approaches are mostly based on offline learning, while, ignored the
exploratory grasp learning during online adaptation to new picking scenarios,
i.e., unseen object portfolio, camera and bin settings etc. In this paper, we
present a novel method for online learning of grasp predictions for robotic bin
picking in a principled way. Specifically, the online learning algorithm with
an effective exploration strategy can significantly improve its adaptation
performance to unseen environment settings. To this end, we first propose to
formulate online grasp learning as a RL problem that will allow to adapt both
grasp reward prediction and grasp poses. We propose various uncertainty
estimation schemes based on Bayesian Uncertainty Quantification and
Distributional Ensembles. We carry out evaluations on real-world bin picking
scenes of varying difficulty. The objects in the bin have various challenging
physical and perceptual characteristics that can be characterized by semi- or
total transparency, and irregular or curved surfaces. The results of our
experiments demonstrate a notable improvement in the suggested approach
compared to conventional online learning methods which incorporate only naive
exploration strategies.",http://arxiv.org/pdf/2309.12038v1
2309.12028v1,cs.LG,Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting,2023-09-21 12:44:55+00:00,"This paper studies the problem of traffic flow forecasting, which aims to
predict future traffic conditions on the basis of road networks and traffic
conditions in the past. The problem is typically solved by modeling complex
spatio-temporal correlations in traffic data using spatio-temporal graph neural
networks (GNNs). However, the performance of these methods is still far from
satisfactory since GNNs usually have limited representation capacity when it
comes to complex traffic networks. Graphs, by nature, fall short in capturing
non-pairwise relations. Even worse, existing methods follow the paradigm of
message passing that aggregates neighborhood information linearly, which fails
to capture complicated spatio-temporal high-order interactions. To tackle these
issues, in this paper, we propose a novel model named Dynamic Hypergraph
Structure Learning (DyHSL) for traffic flow prediction. To learn non-pairwise
relationships, our DyHSL extracts hypergraph structural information to model
dynamics in the traffic networks, and updates each node representation by
aggregating messages from its associated hyperedges. Additionally, to capture
high-order spatio-temporal relations in the road network, we introduce an
interactive graph convolution block, which further models the neighborhood
interaction for each node. Finally, we integrate these two views into a
holistic multi-scale correlation extraction module, which conducts temporal
pooling with different scales to model different temporal patterns. Extensive
experiments on four popular traffic benchmark datasets demonstrate the
effectiveness of our proposed DyHSL compared with a broad range of competing
baselines.",http://arxiv.org/pdf/2309.12028v1
2309.12022v1,cs.AI,Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification,2023-09-21 12:39:36+00:00,"In the film industry, movie posters have been an essential part of
advertising and marketing for many decades, and continue to play a vital role
even today in the form of digital posters through online, social media and OTT
platforms. Typically, movie posters can effectively promote and communicate the
essence of a film, such as its genre, visual style/ tone, vibe and storyline
cue/ theme, which are essential to attract potential viewers. Identifying the
genres of a movie often has significant practical applications in recommending
the film to target audiences. Previous studies on movie genre identification
are limited to subtitles, plot synopses, and movie scenes that are mostly
accessible after the movie release. Posters usually contain pre-release
implicit information to generate mass interest. In this paper, we work for
automated multi-label genre identification only from movie poster images,
without any aid of additional textual/meta-data information about movies, which
is one of the earliest attempts of its kind. Here, we present a deep
transformer network with a probabilistic module to identify the movie genres
exclusively from the poster. For experimental analysis, we procured 13882
number of posters of 13 genres from the Internet Movie Database (IMDb), where
our model performances were encouraging and even outperformed some major
contemporary architectures.",http://arxiv.org/pdf/2309.12022v1
2309.12004v1,cs.LG,Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling Based on Energy Consumption,2023-09-21 12:22:11+00:00,"This paper presents a Hierarchical Reinforcement Learning methodology
tailored for optimizing CubeSat task scheduling in Low Earth Orbits (LEO).
Incorporating a high-level policy for global task distribution and a low-level
policy for real-time adaptations as a safety mechanism, our approach integrates
the Similarity Attention-based Encoder (SABE) for task prioritization and an
MLP estimator for energy consumption forecasting. Integrating this mechanism
creates a safe and fault-tolerant system for CubeSat task scheduling.
Simulation results validate the Hierarchical Reinforcement Learning superior
convergence and task success rate, outperforming both the MADDPG model and
traditional random scheduling across multiple CubeSat configurations.",http://arxiv.org/pdf/2309.12004v1
2309.12002v1,cond-mat.mtrl-sci,Lateral Solid Phase Epitaxy of Yttrium Iron Garnet,2023-09-21 12:17:34+00:00,"Solid phase epitaxy is a crystallization technique used to produce high
quality thin films. Lateral solid phase epitaxy furthermore enables the
realization of non-planar structures, which are interesting, e.g., in the field
of spintronics. Here, we demonstrate lateral solid phase epitaxy of yttrium
iron garnet over an artificial edge, such that the crystallization direction is
perpendicular to the initial seed. We use single crystalline garnet seed
substrates partially covered by a \ch{SiO_x} film to study the lateral
crystallization over the \ch{SiO_x} mesa. The yttrium iron garnet layer retains
the crystal orientation of the substrate not only when in direct contact with
the substrate, but also across the edge on top of the \ch{SiO_x} mesa. By
controlling the crystallization dynamics it is possible to almost completely
suppress the formation of polycrystals and to enable epitaxial growth of single
crystalline yttrium iron garnet on top of mesas made from arbitrary materials.
From a series of annealing experiments, we extract an activation energy of
\SI{2.8}{eV} and a velocity prefactor of \SI{5.1e13}{nm/s} for the lateral
epitaxial crystallization along the <$100$> direction. Our results pave the way
to engineer single crystalline non-planar yttrium iron garnet structures with
controlled crystal orientation.",http://arxiv.org/pdf/2309.12002v1
2309.11998v2,cs.CL,LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset,2023-09-21 12:13:55+00:00,"Studying how people interact with large language models (LLMs) in real-world
scenarios is increasingly important due to their widespread use in various
applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset
containing one million real-world conversations with 25 state-of-the-art LLMs.
This dataset is collected from 210K unique IP addresses in the wild on our
Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's
content, including its curation process, basic statistics, and topic
distribution, highlighting its diversity, originality, and scale. We
demonstrate its versatility through four use cases: developing content
moderation models that perform similarly to GPT-4, building a safety benchmark,
training instruction-following models that perform similarly to Vicuna, and
creating challenging benchmark questions. We believe that this dataset will
serve as a valuable resource for understanding and advancing LLM capabilities.
The dataset is publicly available at
https://huggingface.co/datasets/lmsys/lmsys-chat-1m.",http://arxiv.org/pdf/2309.11998v2
2309.11991v1,cs.MA,Quantifying Feature Importance of Games and Strategies via Shapley Values,2023-09-21 12:03:13+00:00,"Recent advances in game informatics have enabled us to find strong strategies
across a diverse range of games. However, these strategies are usually
difficult for humans to interpret. On the other hand, research in Explainable
Artificial Intelligence (XAI) has seen a notable surge in scholarly activity.
Interpreting strong or near-optimal strategies or the game itself can provide
valuable insights. In this paper, we propose two methods to quantify the
feature importance using Shapley values: one for the game itself and another
for individual AIs. We empirically show that our proposed methods yield
intuitive explanations that resonate with and augment human understanding.",http://arxiv.org/pdf/2309.11991v1
2309.11987v1,cs.LG,Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis,2023-09-21 11:54:20+00:00,"Post-hoc explainability methods aim to clarify predictions of black-box
machine learning models. However, it is still largely unclear how well users
comprehend the provided explanations and whether these increase the users
ability to predict the model behavior. We approach this question by conducting
a user study to evaluate comprehensibility and predictability in two widely
used tools: LIME and SHAP. Moreover, we investigate the effect of
counterfactual explanations and misclassifications on users ability to
understand and predict the model behavior. We find that the comprehensibility
of SHAP is significantly reduced when explanations are provided for samples
near a model's decision boundary. Furthermore, we find that counterfactual
explanations and misclassifications can significantly increase the users
understanding of how a machine learning model is making decisions. Based on our
findings, we also derive design recommendations for future post-hoc
explainability methods with increased comprehensibility and predictability.",http://arxiv.org/pdf/2309.11987v1
2309.11984v2,cs.RO,Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study,2023-09-21 11:41:22+00:00,"Choosing an appropriate representation of the environment for the underlying
decision-making process of the RL agent is not always straightforward. The
state representation should be inclusive enough to allow the agent to
informatively decide on its actions and compact enough to increase sample
efficiency for policy training. Given this outlook, this work examines the
effect of various state representations in incentivizing the agent to solve a
specific robotic task: antipodal and planar object grasping. A continuum of
state representation abstractions is defined, starting from a model-based
approach with complete system knowledge, through hand-crafted numerical, to
image-based representations with decreasing level of induced task-specific
knowledge. We examine the effects of each representation in the ability of the
agent to solve the task in simulation and the transferability of the learned
policy to the real robot. The results show that RL agents using numerical
states can perform on par with non-learning baselines. Furthermore, we find
that agents using image-based representations from pre-trained environment
embedding vectors perform better than end-to-end trained agents, and
hypothesize that task-specific knowledge is necessary for achieving convergence
and high success rates in robot control.",http://arxiv.org/pdf/2309.11984v2
2309.11981v2,cs.CL,Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics,2023-09-21 11:34:52+00:00,"In the burgeoning field of artificial intelligence (AI), the unprecedented
progress of large language models (LLMs) in natural language processing (NLP)
offers an opportunity to revisit the entire approach of traditional metrics of
machine intelligence, both in form and content. As the realm of machine
cognitive evaluation has already reached Imitation, the next step is an
efficient Language Acquisition and Understanding. Our paper proposes a paradigm
shift from the established Turing Test towards an all-embracing framework that
hinges on language acquisition, taking inspiration from the recent advancements
in LLMs. The present contribution is deeply tributary of the excellent work
from various disciplines, point out the need to keep interdisciplinary bridges
open, and delineates a more robust and sustainable approach.",http://arxiv.org/pdf/2309.11981v2
2309.11975v1,cs.AI,Inferring Capabilities from Task Performance with Bayesian Triangulation,2023-09-21 11:19:26+00:00,"As machine learning models become more general, we need to characterise them
in richer, more meaningful ways. We describe a method to infer the cognitive
profile of a system from diverse experimental data. To do so, we introduce
measurement layouts that model how task-instance features interact with system
capabilities to affect performance. These features must be triangulated in
complex ways to be able to infer capabilities from non-populational data -- a
challenge for traditional psychometric and inferential tools. Using the
Bayesian probabilistic programming library PyMC, we infer different cognitive
profiles for agents in two scenarios: 68 actual contestants in the AnimalAI
Olympics and 30 synthetic agents for O-PIAAGETS, an object permanence battery.
We showcase the potential for capability-oriented evaluation.",http://arxiv.org/pdf/2309.11975v1
2309.11973v1,math.NA,A review of troubled cell indicators for discontinuous Galerkin method,2023-09-21 11:10:53+00:00,"In this paper, eight different troubled cell indicators (shock detectors) are
reviewed for the solution of nonlinear hyperbolic conservation laws using
discontinuous Galerkin (DG) method and a WENO limiter. Extensive simulations
using one-dimensional and two-dimensional problems for various orders on the
hyperbolic system of Euler equations are used to compare these troubled cell
indicators. For one-dimensional problems, the performance of Fu and Shu
indicator and the modified KXRCF indicator is better than other indicators. For
two-dimensional problems, the performance of the artificial neural network
(ANN) indicator of Ray and Hesthaven is quite good and the Fu and Shu and the
modified KXRCF indicators are also good. These three indicators are suitable
candidates for applications of DGM using WENO limiters though it should be
noted that the ANN indicator is quite expensive and requires a lot of training.",http://arxiv.org/pdf/2309.11973v1
2309.11960v1,cs.AI,A Comprehensive Review on Financial Explainable AI,2023-09-21 10:30:49+00:00,"The success of artificial intelligence (AI), and deep learning models in
particular, has led to their widespread adoption across various industries due
to their ability to process huge amounts of data and learn complex patterns.
However, due to their lack of explainability, there are significant concerns
regarding their use in critical sectors, such as finance and healthcare, where
decision-making transparency is of paramount importance. In this paper, we
provide a comparative survey of methods that aim to improve the explainability
of deep learning models within the context of finance. We categorize the
collection of explainable AI methods according to their corresponding
characteristics, and we review the concerns and challenges of adopting
explainable AI methods, together with future directions we deemed appropriate
and important.",http://arxiv.org/pdf/2309.11960v1
2309.11957v1,cs.HC,Continuous Multi-user Activity Tracking via Room-Scale mmWave Sensing,2023-09-21 10:15:43+00:00,"Continuous detection of human activities and presence is essential for
developing a pervasive interactive smart space. Existing literature lacks
robust wireless sensing mechanisms capable of continuously monitoring multiple
users' activities without prior knowledge of the environment. Developing such a
mechanism requires simultaneous localization and tracking of multiple subjects.
In addition, it requires identifying their activities at various scales, some
being macro-scale activities like walking, squats, etc., while others are
micro-scale activities like typing or sitting, etc. In this paper, we develop a
holistic system called MARS using a single Commercial off the-shelf (COTS)
Millimeter Wave (mmWave) radar, which employs an intelligent model to sense
both macro and micro activities. In addition, it uses a dynamic spatial time
sharing approach to sense different subjects simultaneously. A thorough
evaluation of MARS shows that it can infer activities continuously with a
weighted F1-Score of > 94% and an average response time of approx 2 sec, with 5
subjects and 19 different activities.",http://arxiv.org/pdf/2309.11957v1
2309.11937v1,cs.AI,On the Definition of Appropriate Trust and the Tools that Come with it,2023-09-21 09:52:06+00:00,"Evaluating the efficiency of human-AI interactions is challenging, including
subjective and objective quality aspects. With the focus on the human
experience of the explanations, evaluations of explanation methods have become
mostly subjective, making comparative evaluations almost impossible and highly
linked to the individual user. However, it is commonly agreed that one aspect
of explanation quality is how effectively the user can detect if the
predictions are trustworthy and correct, i.e., if the explanations can increase
the user's appropriate trust in the model. This paper starts with the
definitions of appropriate trust from the literature. It compares the
definitions with model performance evaluation, showing the strong similarities
between appropriate trust and model performance evaluation. The paper's main
contribution is a novel approach to evaluating appropriate trust by taking
advantage of the likenesses between definitions. The paper offers several
straightforward evaluation methods for different aspects of user performance,
including suggesting a method for measuring uncertainty and appropriate trust
in regression.",http://arxiv.org/pdf/2309.11937v1
2309.11932v1,cs.LG,A Machine Learning-oriented Survey on Tiny Machine Learning,2023-09-21 09:47:12+00:00,"The emergence of Tiny Machine Learning (TinyML) has positively revolutionized
the field of Artificial Intelligence by promoting the joint design of
resource-constrained IoT hardware devices and their learning-based software
architectures. TinyML carries an essential role within the fourth and fifth
industrial revolutions in helping societies, economies, and individuals employ
effective AI-infused computing technologies (e.g., smart cities, automotive,
and medical robotics). Given its multidisciplinary nature, the field of TinyML
has been approached from many different angles: this comprehensive survey
wishes to provide an up-to-date overview focused on all the learning algorithms
within TinyML-based solutions. The survey is based on the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow,
allowing for a systematic and complete literature survey. In particular,
firstly we will examine the three different workflows for implementing a
TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly,
we propose a taxonomy that covers the learning panorama under the TinyML lens,
examining in detail the different families of model optimization and design, as
well as the state-of-the-art learning techniques. Thirdly, this survey will
present the distinct features of hardware devices and software tools that
represent the current state-of-the-art for TinyML intelligent edge
applications. Finally, we discuss the challenges and future directions.",http://arxiv.org/pdf/2309.11932v1
2309.11928v1,cs.CV,Video Scene Location Recognition with Neural Networks,2023-09-21 09:42:39+00:00,"This paper provides an insight into the possibility of scene recognition from
a video sequence with a small set of repeated shooting locations (such as in
television series) using artificial neural networks. The basic idea of the
presented approach is to select a set of frames from each scene, transform them
by a pre-trained singleimage pre-processing convolutional network, and classify
the scene location with subsequent layers of the neural network. The considered
networks have been tested and compared on a dataset obtained from The Big Bang
Theory television series. We have investigated different neural network layers
to combine individual frames, particularly AveragePooling, MaxPooling, Product,
Flatten, LSTM, and Bidirectional LSTM layers. We have observed that only some
of the approaches are suitable for the task at hand.",http://arxiv.org/pdf/2309.11928v1
2309.11918v1,eess.SP,Multi-Passive/Active-IRS Enhanced Wireless Coverage: Deployment Optimization and Cost-Performance Trade-off,2023-09-21 09:30:49+00:00,"Both passive and active intelligent reflecting surfaces (IRSs) can be
deployed in complex environments to enhance wireless network coverage by
creating multiple blockage-free cascaded line-of-sight (LoS) links. In this
paper, we study a multi-passive/active-IRS (PIRS/AIRS) aided wireless network
with a multi-antenna base station (BS) in a given region. First, we divide the
region into multiple non-overlapping cells, each of which may contain one
candidate location that can be deployed with a single PIRS or AIRS. Then, we
show several trade-offs between minimizing the total IRS deployment cost and
enhancing the signal-to-noise ratio (SNR) performance over all cells via
direct/cascaded LoS transmission with the BS. To reconcile these trade-offs, we
formulate a joint multi-PIRS/AIRS deployment problem to select an optimal
subset of all candidate locations for deploying IRS and also optimize the
number of passive/active reflecting elements deployed at each selected location
to satisfy a given SNR target over all cells, such that the total deployment
cost is minimized. However, due to the combinatorial optimization involved, the
formulated problem is difficult to be solved optimally. To tackle this
difficulty, we first optimize the reflecting element numbers with given
PIRS/AIRS deployed locations via sequential refinement, followed by a partial
enumeration to determine the PIRS/AIRS locations. Simulation results show that
our proposed algorithm achieves better cost-performance trade-offs than other
baseline deployment strategies.",http://arxiv.org/pdf/2309.11918v1
2309.11907v1,cs.AI,Learning to Recover for Safe Reinforcement Learning,2023-09-21 09:17:38+00:00,"Safety controllers is widely used to achieve safe reinforcement learning.
Most methods that apply a safety controller are using handcrafted safety
constraints to construct the safety controller. However, when the environment
dynamics are sophisticated, handcrafted safety constraints become unavailable.
Therefore, it worth to research on constructing safety controllers by learning
algorithms. We propose a three-stage architecture for safe reinforcement
learning, namely TU-Recovery Architecture. A safety critic and a recovery
policy is learned before task training. They form a safety controller to ensure
safety in task training. Then a phenomenon induced by disagreement between task
policy and recovery policy, called adversarial phenomenon, which reduces
learning efficiency and model performance, is described. Auxiliary reward is
proposed to mitigate adversarial phenomenon, while help the task policy to
learn to recover from high-risk states. A series of experiments are conducted
in a robot navigation environment. Experiments demonstrate that TU-Recovery
outperforms unconstrained counterpart in both reward gaining and constraint
violations during task training, and auxiliary reward further improve
TU-Recovery in reward-to-cost ratio by significantly reduce constraint
violations.",http://arxiv.org/pdf/2309.11907v1
2309.11904v1,astro-ph.EP,SELENA: Semi-analytical Integrator for Lunar Artificial Satellites,2023-09-21 09:15:54+00:00,"The present report summarizes the main theory and implementation steps
associated with SELENA (SEmi-anaLytical intEgrator for a luNar Artificial
satellite), i.e. the semi-analytical propagator for lunar satellite orbits
developed in the framework of the the R&T R-S20/BS-0005-062 CNES research
activity in collaboration between the University of Padova (UniPd), and the
Aristotle University of Thessaloniki (AUTH), both acting as contractors with
CNES.
  A detailed account of the method, algorithms and symbolic manipulations
employed in the derivation of the final theory are described in detail in this
report: they invoke the use of canonical perturbation theory in the form of Lie
series computed in `closed form', i.e., without expansions in the satellite's
orbital eccentricity. These algorithms are provided in the form of a symbolic
package accompanying the present report. The package contains symbolic algebra
programs, as well as explicit data files containing the final Hamiltonian,
equations of motion and transformations (i.e. the coefficients and exponents of
each variable in each term) leading to the averaging of the short-periodic
terms in the satellite's equations of motion.",http://arxiv.org/pdf/2309.11904v1
2309.11899v1,cs.CV,Unlocking the Heart Using Adaptive Locked Agnostic Networks,2023-09-21 09:06:36+00:00,"Supervised training of deep learning models for medical imaging applications
requires a significant amount of labeled data. This is posing a challenge as
the images are required to be annotated by medical professionals. To address
this limitation, we introduce the Adaptive Locked Agnostic Network (ALAN), a
concept involving self-supervised visual feature extraction using a large
backbone model to produce anatomically robust semantic self-segmentation. In
the ALAN methodology, this self-supervised training occurs only once on a large
and diverse dataset. Due to the intuitive interpretability of the segmentation,
downstream models tailored for specific tasks can be easily designed using
white-box models with few parameters. This, in turn, opens up the possibility
of communicating the inner workings of a model with domain experts and
introducing prior knowledge into it. It also means that the downstream models
become less data-hungry compared to fully supervised approaches. These
characteristics make ALAN particularly well-suited for resource-scarce
scenarios, such as costly clinical trials and rare diseases. In this paper, we
apply the ALAN approach to three publicly available echocardiography datasets:
EchoNet-Dynamic, CAMUS, and TMED-2. Our findings demonstrate that the
self-supervised backbone model robustly identifies anatomical subregions of the
heart in an apical four-chamber view. Building upon this, we design two
downstream models, one for segmenting a target anatomical region, and a second
for echocardiogram view classification.",http://arxiv.org/pdf/2309.11899v1
2309.11895v2,cs.SD,Audio Contrastive based Fine-tuning,2023-09-21 08:59:13+00:00,"Audio classification plays a crucial role in speech and sound processing
tasks with a wide range of applications. There still remains a challenge of
striking the right balance between fitting the model to the training data
(avoiding overfitting) and enabling it to generalise well to a new domain.
Leveraging the transferability of contrastive learning, we introduce Audio
Contrastive-based Fine-tuning (AudioConFit), an efficient approach
characterised by robust generalisability. Empirical experiments on a variety of
audio classification tasks demonstrate the effectiveness and robustness of our
approach, which achieves state-of-the-art results in various settings.",http://arxiv.org/pdf/2309.11895v2
2309.11893v1,cs.IT,On the Performance Analysis of RIS-Empowered Communications Over Nakagami-m Fading,2023-09-21 08:56:49+00:00,"In this paper, we study the performance of wireless communications empowered
by Reconfigurable Intelligent Surface (RISs) over Nakagami-m fading channels.
We consider two phase configuration designs for the RIS, one random and another
one based on coherent phase shifting. For both phase configuration cases, we
present single-integral expressions for the outage probability and the bit
error rate of binary modulation schemes, which can be efficiently evaluated
numerically. In addition, we propose accurate closed-form approximations for
the ergodic capacity of the considered system. For all considered metrics, we
have also derived simple analytical expressions that become tight for large
numbers of RIS reflecting elements. Numerically evaluated results compared with
Monte Carlo simulations are presented in order to verify the correctness of the
proposed analysis and showcase the impact of various system settings.",http://arxiv.org/pdf/2309.11893v1
2309.11876v1,cs.CV,Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training,2023-09-21 08:22:44+00:00,"Contrastive learning, which is a powerful technique for learning image-level
representations from unlabeled data, leads a promising direction to dealing
with the dilemma between large-scale pre-training and limited labeled data.
However, most existing contrastive learning strategies are designed mainly for
downstream tasks of natural images, therefore they are sub-optimal and even
worse than learning from scratch when directly applied to medical images whose
downstream tasks are usually segmentation. In this work, we propose a novel
asymmetric contrastive learning framework named JCL for medical image
segmentation with self-supervised pre-training. Specifically, (1) A novel
asymmetric contrastive learning strategy is proposed to pre-train both encoder
and decoder simultaneously in one-stage to provide better initialization for
segmentation models. (2) A multi-level contrastive loss is designed to take the
correspondence among feature-level, image-level and pixel-level projections,
respectively into account to make sure multi-level representations can be
learned by the encoder and decoder during pre-training. (3) Experiments on
multiple medical image datasets indicate our JCL framework outperforms existing
SOTA contrastive learning strategies.",http://arxiv.org/pdf/2309.11876v1
2309.11875v1,cs.LG,Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes,2023-09-21 08:22:12+00:00,"Machine learning models trained with structural health monitoring data have
become a powerful tool for system identification. This paper presents a
physics-informed Gaussian process (GP) model for Timoshenko beam elements. The
model is constructed as a multi-output GP with covariance and cross-covariance
kernels analytically derived based on the differential equations for
deflections, rotations, strains, bending moments, shear forces and applied
loads. Stiffness identification is performed in a Bayesian format by maximising
a posterior model through a Markov chain Monte Carlo method, yielding a
stochastic model for the structural parameters. The optimised GP model is
further employed for probabilistic predictions of unobserved responses.
Additionally, an entropy-based method for physics-informed sensor placement
optimisation is presented, exploiting heterogeneous sensor position information
and structural boundary conditions built into the GP model. Results demonstrate
that the proposed approach is effective at identifying structural parameters
and is capable of fusing data from heterogeneous and multi-fidelity sensors.
Probabilistic predictions of structural responses and internal forces are in
closer agreement with measured data. We validate our model with an experimental
setup and discuss the quality and uncertainty of the obtained results. The
proposed approach has potential applications in the field of structural health
monitoring (SHM) for both mechanical and structural systems.",http://arxiv.org/pdf/2309.11875v1
2309.11858v1,cs.CV,OSNet & MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios,2023-09-21 07:59:58+00:00,"Recently, linear computed tomography (LCT) systems have actively attracted
attention. To weaken projection truncation and image the region of interest
(ROI) for LCT, the backprojection filtration (BPF) algorithm is an effective
solution. However, in BPF for LCT, it is difficult to achieve stable interior
reconstruction, and for differentiated backprojection (DBP) images of LCT,
multiple rotation-finite inversion of Hilbert transform (Hilbert
filtering)-inverse rotation operations will blur the image. To satisfy multiple
reconstruction scenarios for LCT, including interior ROI, complete object, and
exterior region beyond field-of-view (FOV), and avoid the rotation operations
of Hilbert filtering, we propose two types of reconstruction architectures. The
first overlays multiple DBP images to obtain a complete DBP image, then uses a
network to learn the overlying Hilbert filtering function, referred to as the
Overlay-Single Network (OSNet). The second uses multiple networks to train
different directional Hilbert filtering models for DBP images of multiple
linear scannings, respectively, and then overlays the reconstructed results,
i.e., Multiple Networks Overlaying (MNetO). In two architectures, we introduce
a Swin Transformer (ST) block to the generator of pix2pixGAN to extract both
local and global features from DBP images at the same time. We investigate two
architectures from different networks, FOV sizes, pixel sizes, number of
projections, geometric magnification, and processing time. Experimental results
show that two architectures can both recover images. OSNet outperforms BPF in
various scenarios. For the different networks, ST-pix2pixGAN is superior to
pix2pixGAN and CycleGAN. MNetO exhibits a few artifacts due to the differences
among the multiple models, but any one of its models is suitable for imaging
the exterior edge in a certain direction.",http://arxiv.org/pdf/2309.11858v1
2309.11853v1,cs.CL,BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework,2023-09-21 07:55:54+00:00,"Relation triple extraction (RTE) is an essential task in information
extraction and knowledge graph construction. Despite recent advancements,
existing methods still exhibit certain limitations. They just employ
generalized pre-trained models and do not consider the specificity of RTE
tasks. Moreover, existing tagging-based approaches typically decompose the RTE
task into two subtasks, initially identifying subjects and subsequently
identifying objects and relations. They solely focus on extracting relational
triples from subject to object, neglecting that once the extraction of a
subject fails, it fails in extracting all triples associated with that subject.
To address these issues, we propose BitCoin, an innovative Bidirectional
tagging and supervised Contrastive learning based joint relational triple
extraction framework. Specifically, we design a supervised contrastive learning
method that considers multiple positives per anchor rather than restricting it
to just one positive. Furthermore, a penalty term is introduced to prevent
excessive similarity between the subject and object. Our framework implements
taggers in two directions, enabling triples extraction from subject to object
and object to subject. Experimental results show that BitCoin achieves
state-of-the-art results on the benchmark datasets and significantly improves
the F1 score on Normal, SEO, EPO, and multiple relation extraction tasks.",http://arxiv.org/pdf/2309.11853v1
2309.11850v1,cs.IT,Joint Beamforming for RIS Aided Full-Duplex Integrated Sensing and Uplink Communication,2023-09-21 07:48:56+00:00,"This paper studies integrated sensing and communication (ISAC) technology in
a full-duplex (FD) uplink communication system. As opposed to the half-duplex
system, where sensing is conducted in a first-emit-then-listen manner, FD ISAC
system emits and listens simultaneously and hence conducts uninterrupted target
sensing. Besides, impressed by the recently emerging reconfigurable intelligent
surface (RIS) technology, we also employ RIS to improve the self-interference
(SI) suppression and signal processing gain. As will be seen, the joint
beamforming, RIS configuration and mobile users' power allocation is a
difficult optimization problem. To resolve this challenge, via leveraging the
cutting-the-edge majorization-minimization (MM) and penalty-dual-decomposition
(PDD) methods, we develop an iterative solution that optimizes all variables
via using convex optimization techniques. Numerical results demonstrate the
effectiveness of our proposed solution and the great benefit of employing RIS
in the FD ISAC system.",http://arxiv.org/pdf/2309.11850v1
2309.11839v1,cs.CV,MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation,2023-09-21 07:30:21+00:00,"Multi-modal unsupervised domain adaptation (MM-UDA) for 3D semantic
segmentation is a practical solution to embed semantic understanding in
autonomous systems without expensive point-wise annotations. While previous
MM-UDA methods can achieve overall improvement, they suffer from significant
class-imbalanced performance, restricting their adoption in real applications.
This imbalanced performance is mainly caused by: 1) self-training with
imbalanced data and 2) the lack of pixel-wise 2D supervision signals. In this
work, we propose Multi-modal Prior Aided (MoPA) domain adaptation to improve
the performance of rare objects. Specifically, we develop Valid Ground-based
Insertion (VGI) to rectify the imbalance supervision signals by inserting prior
rare objects collected from the wild while avoiding introducing artificial
artifacts that lead to trivial solutions. Meanwhile, our SAM consistency loss
leverages the 2D prior semantic masks from SAM as pixel-wise supervision
signals to encourage consistent predictions for each object in the semantic
mask. The knowledge learned from modal-specific prior is then shared across
modalities to achieve better rare object segmentation. Extensive experiments
show that our method achieves state-of-the-art performance on the challenging
MM-UDA benchmark. Code will be available at https://github.com/AronCao49/MoPA.",http://arxiv.org/pdf/2309.11839v1
2309.11838v1,cs.CL,Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues,2023-09-21 07:28:03+00:00,"In this paper, we investigate the use of large language models (LLMs) like
ChatGPT for document-grounded response generation in the context of
information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus
of task-oriented dialogues in four social service domains previously used in
the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded
in multiple documents providing relevant information. We generate dialogue
completion responses by prompting a ChatGPT model, using two methods:
Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT
model pretraining while LlamaIndex also extracts relevant information from
documents. Observing that document-grounded response generation via LLMs cannot
be adequately assessed by automatic evaluation metrics as they are
significantly more verbose, we perform a human evaluation where annotators rate
the output of the shared task winning system, the two Chat-GPT variants
outputs, and human responses. While both ChatGPT variants are more likely to
include information not present in the relevant segments, possibly including a
presence of hallucinations, they are rated higher than both the shared task
winning system and human responses.",http://arxiv.org/pdf/2309.11838v1
2309.11811v1,eess.SP,Multimodal Transformers for Wireless Communications: A Case Study in Beam Prediction,2023-09-21 06:29:38+00:00,"Wireless communications at high-frequency bands with large antenna arrays
face challenges in beam management, which can potentially be improved by
multimodality sensing information from cameras, LiDAR, radar, and GPS. In this
paper, we present a multimodal transformer deep learning framework for
sensing-assisted beam prediction. We employ a convolutional neural network to
extract the features from a sequence of images, point clouds, and radar raw
data sampled over time. At each convolutional layer, we use transformer
encoders to learn the hidden relations between feature tokens from different
modalities and time instances over abstraction space and produce encoded
vectors for the next-level feature extraction. We train the model on a
combination of different modalities with supervised learning. We try to enhance
the model over imbalanced data by utilizing focal loss and exponential moving
average. We also evaluate data processing and augmentation techniques such as
image enhancement, segmentation, background filtering, multimodal data
flipping, radar signal transformation, and GPS angle calibration. Experimental
results show that our solution trained on image and GPS data produces the best
distance-based accuracy of predicted beams at 78.44%, with effective
generalization to unseen day scenarios near 73% and night scenarios over 84%.
This outperforms using other modalities and arbitrary data processing
techniques, which demonstrates the effectiveness of transformers with feature
fusion in performing radio beam prediction from images and GPS. Furthermore,
our solution could be pretrained from large sequences of multimodality wireless
data, on fine-tuning for multiple downstream radio network tasks.",http://arxiv.org/pdf/2309.11811v1
2309.11810v1,astro-ph.CO,Extragalactic Test of General Relativity from Strong Gravitational Lensing by using Artificial Neural Networks,2023-09-21 06:28:39+00:00,"This study aims to test the validity of general relativity (GR) on kiloparsec
scales by employing a newly compiled galaxy-scale strong gravitational lensing
(SGL) sample. We utilize the distance sum rule within the
Friedmann-Lema\^{\i}tre-Robertson-Walker metric to obtain cosmology-independent
constraints on both the parameterized post-Newtonian parameter $\gamma_{\rm
PPN}$ and the spatial curvature $\Omega_{k}$, which overcomes the circularity
problem induced by the presumption of a cosmological model grounded in GR. To
calibrate the distances in the SGL systems, we introduce a novel nonparametric
approach, Artificial Neural Network (ANN), to reconstruct a smooth
distance--redshift relation from the Pantheon+ sample of type Ia supernovae.
Our results show that $\gamma_{\rm PPN}=1.16_{-0.12}^{+0.15}$ and
$\Omega_k=0.89_{-1.00}^{+1.97}$, indicating a spatially flat universe with the
conservation of GR (i.e., $\Omega_k=0$ and $\gamma_{\rm PPN}=1$) is basically
supported within $1\sigma$ confidence level. Assuming a zero spatial curvature,
we find $\gamma_{\rm PPN}=1.09_{-0.10}^{+0.11}$, representing an agreement with
the prediction of 1 from GR to a 9.6\% precision. If we instead assume GR holds
(i.e., $\gamma_{\rm PPN}=1$), the curvature parameter constraint can be further
improved to be $\Omega_k=0.11_{-0.47}^{+0.78}$. These resulting constraints
demonstrate the effectiveness of our method in testing GR on galactic scales by
combining observations of strong lensing and the distance--redshift relation
reconstructed by ANN.",http://arxiv.org/pdf/2309.11810v1
2309.11805v1,cs.AI,JobRecoGPT -- Explainable job recommendations using LLMs,2023-09-21 06:25:28+00:00,"In today's rapidly evolving job market, finding the right opportunity can be
a daunting challenge. With advancements in the field of AI, computers can now
recommend suitable jobs to candidates. However, the task of recommending jobs
is not same as recommending movies to viewers. Apart from must-have criteria,
like skills and experience, there are many subtle aspects to a job which can
decide if it is a good fit or not for a given candidate. Traditional approaches
can capture the quantifiable aspects of jobs and candidates, but a substantial
portion of the data that is present in unstructured form in the job
descriptions and resumes is lost in the process of conversion to structured
format. As of late, Large Language Models (LLMs) have taken over the AI field
by storm with extraordinary performance in fields where text-based data is
available. Inspired by the superior performance of LLMs, we leverage their
capability to understand natural language for capturing the information that
was previously getting lost during the conversion of unstructured data to
structured form. To this end, we compare performance of four different
approaches for job recommendations namely, (i) Content based deterministic,
(ii) LLM guided, (iii) LLM unguided, and (iv) Hybrid. In this study, we present
advantages and limitations of each method and evaluate their performance in
terms of time requirements.",http://arxiv.org/pdf/2309.11805v1
2309.11795v1,math.NA,An optimal control deep learning method to design artificial viscosities for Discontinuous Galerkin schemes,2023-09-21 05:43:15+00:00,"In this paper, we propose a method for constructing a neural network
viscosity in order to reduce the non-physical oscillations generated by
high-order Discontiuous Galerkin (DG) methods. To this end, the problem is
reformulated as an optimal control problem for which the control is the
viscosity function and the cost function involves comparison with a reference
solution after several compositions of the scheme. The learning process is
strongly based on gradient backpropagation tools. Numerical simulations show
that the artificial viscosities constructed in this way are just as good or
better than those used in the literatur",http://arxiv.org/pdf/2309.11795v1
2309.11793v1,quant-ph,Quantum Circuits for Stabilizer Error Correcting Codes: A Tutorial,2023-09-21 05:42:04+00:00,"Quantum computers have the potential to provide exponential speedups over
their classical counterparts. Quantum principles are being applied to fields
such as communications, information processing, and artificial intelligence to
achieve quantum advantage. However, quantum bits are extremely noisy and prone
to decoherence. Thus, keeping the qubits error free is extremely important
toward reliable quantum computing. Quantum error correcting codes have been
studied for several decades and methods have been proposed to import classical
error correcting codes to the quantum domain. However, circuits for such
encoders and decoders haven't been explored in depth. This paper serves as a
tutorial on designing and simulating quantum encoder and decoder circuits for
stabilizer codes. We present encoding and decoding circuits for five-qubit code
and Steane code, along with verification of these circuits using IBM Qiskit. We
also provide nearest neighbour compliant encoder and decoder circuits for the
five-qubit code.",http://arxiv.org/pdf/2309.11793v1
2309.11782v1,cs.CV,DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning,2023-09-21 05:12:55+00:00,"Self-supervised learning (SSL) has gained remarkable success, for which
contrastive learning (CL) plays a key role. However, the recent development of
new non-CL frameworks has achieved comparable or better performance with high
improvement potential, prompting researchers to enhance these frameworks
further. Assimilating CL into non-CL frameworks has been thought to be
beneficial, but empirical evidence indicates no visible improvements. In view
of that, this paper proposes a strategy of performing CL along the dimensional
direction instead of along the batch direction as done in conventional
contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL
aims to enhance the feature diversity, and it can serve as a regularizer to
prior SSL frameworks. DimCL has been found to be effective, and the
hardness-aware property is identified as a critical reason for its success.
Extensive experimental results reveal that assimilating DimCL into SSL
frameworks leads to performance improvement by a non-trivial margin on various
datasets and backbone architectures.",http://arxiv.org/pdf/2309.11782v1
2309.11755v1,cs.CV,2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation on Point Cloud,2023-09-21 03:32:22+00:00,"Recently, multi-modality models have been introduced because of the
complementary information from different sensors such as LiDAR and cameras. It
requires paired data along with precise calibrations for all modalities, the
complicated calibration among modalities hugely increases the cost of
collecting such high-quality datasets, and hinder it from being applied to
practical scenarios. Inherit from the previous works, we not only fuse the
information from multi-modality without above issues, and also exhaust the
information in the RGB modality. We introduced the 2D Detection Annotations
Transmittable Aggregation(\textbf{2DDATA}), designing a data-specific branch,
called \textbf{Local Object Branch}, which aims to deal with points in a
certain bounding box, because of its easiness of acquiring 2D bounding box
annotations. We demonstrate that our simple design can transmit bounding box
prior information to the 3D encoder model, proving the feasibility of large
multi-modality models fused with modality-specific data.",http://arxiv.org/pdf/2309.11755v1
2309.11753v1,cs.AI,Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language,2023-09-21 03:25:35+00:00,"Reinforcement learning is a powerful technique for learning from trial and
error, but it often requires a large number of interactions to achieve good
performance. In some domains, such as sparse-reward tasks, an oracle that can
provide useful feedback or guidance to the agent during the learning process is
really of great importance. However, querying the oracle too frequently may be
costly or impractical, and the oracle may not always have a clear answer for
every situation. Therefore, we propose a novel method for interacting with the
oracle in a selective and efficient way, using a retrieval-based approach. We
assume that the interaction can be modeled as a sequence of templated questions
and answers, and that there is a large corpus of previous interactions
available. We use a neural network to encode the current state of the agent and
the oracle, and retrieve the most relevant question from the corpus to ask the
oracle. We then use the oracle's answer to update the agent's policy and value
function. We evaluate our method on an object manipulation task. We show that
our method can significantly improve the efficiency of RL by reducing the
number of interactions needed to reach a certain level of performance, compared
to baselines that do not use the oracle or use it in a naive way.",http://arxiv.org/pdf/2309.11753v1
2309.11751v1,cs.CV,How Robust is Google's Bard to Adversarial Image Attacks?,2023-09-21 03:24:30+00:00,"Multimodal Large Language Models (MLLMs) that integrate text and other
modalities (especially vision) have achieved unprecedented performance in
various multimodal tasks. However, due to the unsolved adversarial robustness
problem of vision models, MLLMs can have more severe safety and security risks
by introducing the vision inputs. In this work, we study the adversarial
robustness of Google's Bard, a competitive chatbot to ChatGPT that released its
multimodal capability recently, to better understand the vulnerabilities of
commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs,
the generated adversarial examples can mislead Bard to output wrong image
descriptions with a 22% success rate based solely on the transferability. We
show that the adversarial examples can also attack other MLLMs, e.g., a 26%
attack success rate against Bing Chat and a 86% attack success rate against
ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face
detection and toxicity detection of images. We design corresponding attacks to
evade these defenses, demonstrating that the current defenses of Bard are also
vulnerable. We hope this work can deepen our understanding on the robustness of
MLLMs and facilitate future research on defenses. Our code is available at
https://github.com/thu-ml/Attack-Bard.",http://arxiv.org/pdf/2309.11751v1
2309.12373v1,quant-ph,Systematic Design and Optimization of Quantum Circuits for Stabilizer Codes,2023-09-21 03:21:47+00:00,"Quantum computing is an emerging technology that has the potential to achieve
exponential speedups over their classical counterparts. To achieve quantum
advantage, quantum principles are being applied to fields such as
communications, information processing, and artificial intelligence. However,
quantum computers face a fundamental issue since quantum bits are extremely
noisy and prone to decoherence. Keeping qubits error free is one of the most
important steps towards reliable quantum computing. Different stabilizer codes
for quantum error correction have been proposed in past decades and several
methods have been proposed to import classical error correcting codes to the
quantum domain. However, formal approaches towards the design and optimization
of circuits for these quantum encoders and decoders have so far not been
proposed. In this paper, we propose a formal algorithm for systematic
construction of encoding circuits for general stabilizer codes. This algorithm
is used to design encoding and decoding circuits for an eight-qubit code. Next,
we propose a systematic method for the optimization of the encoder circuit thus
designed. Using the proposed method, we optimize the encoding circuit in terms
of the number of 2-qubit gates used. The proposed optimized eight-qubit encoder
uses 18 CNOT gates and 4 Hadamard gates, as compared to 14 single qubit gates,
33 2-qubit gates, and 6 CCNOT gates in a prior work. The encoder and decoder
circuits are verified using IBM Qiskit. We also present optimized encoder
circuits for Steane code and a 13-qubit code in terms of the number of gates
used.",http://arxiv.org/pdf/2309.12373v1
2309.11748v1,cs.IT,Deep Learning Meets Swarm Intelligence for UAV-Assisted IoT Coverage in Massive MIMO,2023-09-21 03:03:06+00:00,"This study considers a UAV-assisted multi-user massive multiple-input
multiple-output (MU-mMIMO) systems, where a decode-and-forward (DF) relay in
the form of an unmanned aerial vehicle (UAV) facilitates the transmission of
multiple data streams from a base station (BS) to multiple Internet-of-Things
(IoT) users. A joint optimization problem of hybrid beamforming (HBF), UAV
relay positioning, and power allocation (PA) to multiple IoT users to maximize
the total achievable rate (AR) is investigated. The study adopts a
geometry-based millimeter-wave (mmWave) channel model for both links and
proposes three different swarm intelligence (SI)-based algorithmic solutions to
optimize: 1) UAV location with equal PA; 2) PA with fixed UAV location; and 3)
joint PA with UAV deployment. The radio frequency (RF) stages are designed to
reduce the number of RF chains based on the slow time-varying angular
information, while the baseband (BB) stages are designed using the
reduced-dimension effective channel matrices. Then, a novel deep learning
(DL)-based low-complexity joint hybrid beamforming, UAV location and power
allocation optimization scheme (J-HBF-DLLPA) is proposed via fully-connected
deep neural network (DNN), consisting of an offline training phase, and an
online prediction of UAV location and optimal power values for maximizing the
AR. The illustrative results show that the proposed algorithmic solutions can
attain higher capacity and reduce average delay for delay-constrained
transmissions in a UAV-assisted MU-mMIMO IoT systems. Additionally, the
proposed J-HBF-DLLPA can closely approach the optimal capacity while
significantly reducing the runtime by 99%, which makes the DL-based solution a
promising implementation for real-time online applications in UAV-assisted
MU-mMIMO IoT systems.",http://arxiv.org/pdf/2309.11748v1
2309.11737v1,cs.AI,Choice-75: A Dataset on Decision Branching in Script Learning,2023-09-21 02:23:44+00:00,"Script learning studies how daily events unfold. Previous works tend to
consider a script as a linear sequence of events while ignoring the potential
branches that arise due to people's circumstantial choices. We hence propose
Choice-75, the first benchmark that challenges intelligent systems to predict
decisions given descriptive scenarios, containing 75 scripts and more than 600
scenarios. While large language models demonstrate overall decent performances,
there is still notable room for improvement in many hard scenarios.",http://arxiv.org/pdf/2309.11737v1
2309.11729v1,astro-ph.EP,The possibility of detecting our solar system through astrometry,2023-09-21 02:03:05+00:00,"Searching for exoplanets with different methods has always been the focus of
astronomers over the past few years. Among multiple planet detection
techniques, astrometry stands out for its capability to accurately determine
the orbital parameters of exoplanets. In this study, we examine the likelihood
of extraterrestrial intelligent civilizations detecting planets in our solar
system using the astrometry method. By conducting injection-recovery
simulations, we investigate the detectability of the four giant planets in our
solar system under different observing baselines and observational errors. Our
findings indicate that extraterrestrial intelligence could detect and
characterize all four giant planets, provided they are observed for a minimum
of 90 years with signal-noise ratios exceeding 1. For individual planets such
as Jupiter, Saturn, and Neptune, a baseline that surpasses half of their
orbital periods is necessary for detection. However, Uranus requires longer
observing baselines since its orbital period is roughly half of that of
Neptune. If the astrometry precision is equal to or better than 10 $\mu$as, all
8,707 stars located within 30 pcs of our solar system possess the potential to
detect the four giant planets within 100 years. Additionally, our prediction
suggests that over 300 stars positioned within 10 pcs from our solar system
could detect our Earth if they achieve an astrometry precision of 0.3 $\mu$as.",http://arxiv.org/pdf/2309.11729v1
2309.11725v2,cs.SD,FluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency,2023-09-21 01:58:01+00:00,"Text-based speech editing (TSE) techniques are designed to enable users to
edit the output audio by modifying the input text transcript instead of the
audio itself. Despite much progress in neural network-based TSE techniques, the
current techniques have focused on reducing the difference between the
generated speech segment and the reference target in the editing region,
ignoring its local and global fluency in the context and original utterance. To
maintain the speech fluency, we propose a fluency speech editing model, termed
\textit{FluentEditor}, by considering fluency-aware training criterion in the
TSE training. Specifically, the \textit{acoustic consistency constraint} aims
to smooth the transition between the edited region and its neighboring acoustic
segments consistent with the ground truth, while the \textit{prosody
consistency constraint} seeks to ensure that the prosody attributes within the
edited regions remain consistent with the overall style of the original
utterance. The subjective and objective experimental results on VCTK
demonstrate that our \textit{FluentEditor} outperforms all advanced baselines
in terms of naturalness and fluency. The audio samples and code are available
at \url{https://github.com/Ai-S2-Lab/FluentEditor}.",http://arxiv.org/pdf/2309.11725v2
2309.11724v1,cs.AI,Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech,2023-09-21 01:51:10+00:00,"Prosodic phrasing is crucial to the naturalness and intelligibility of
end-to-end Text-to-Speech (TTS). There exist both linguistic and emotional
prosody in natural speech. As the study of prosodic phrasing has been
linguistically motivated, prosodic phrasing for expressive emotion rendering
has not been well studied. In this paper, we propose an emotion-aware prosodic
phrasing model, termed \textit{EmoPP}, to mine the emotional cues of utterance
accurately and predict appropriate phrase breaks. We first conduct objective
observations on the ESD dataset to validate the strong correlation between
emotion and prosodic phrasing. Then the objective and subjective evaluations
show that the EmoPP outperforms all baselines and achieves remarkable
performance in terms of emotion expressiveness. The audio samples and the code
are available at \url{https://github.com/AI-S2-Lab/EmoPP}.",http://arxiv.org/pdf/2309.11724v1
2309.11714v1,eess.SP,A Dynamic Domain Adaptation Deep Learning Network for EEG-based Motor Imagery Classification,2023-09-21 01:34:00+00:00,"There is a correlation between adjacent channels of electroencephalogram
(EEG), and how to represent this correlation is an issue that is currently
being explored. In addition, due to inter-individual differences in EEG
signals, this discrepancy results in new subjects need spend a amount of
calibration time for EEG-based motor imagery brain-computer interface. In order
to solve the above problems, we propose a Dynamic Domain Adaptation Based Deep
Learning Network (DADL-Net). First, the EEG data is mapped to the
three-dimensional geometric space and its temporal-spatial features are learned
through the 3D convolution module, and then the spatial-channel attention
mechanism is used to strengthen the features, and the final convolution module
can further learn the spatial-temporal information of the features. Finally, to
account for inter-subject and cross-sessions differences, we employ a dynamic
domain-adaptive strategy, the distance between features is reduced by
introducing a Maximum Mean Discrepancy loss function, and the classification
layer is fine-tuned by using part of the target domain data. We verify the
performance of the proposed method on BCI competition IV 2a and OpenBMI
datasets. Under the intra-subject experiment, the accuracy rates of 70.42% and
73.91% were achieved on the OpenBMI and BCIC IV 2a datasets.",http://arxiv.org/pdf/2309.11714v1
2309.11691v1,cs.AI,RAI4IoE: Responsible AI for Enabling the Internet of Energy,2023-09-20 23:45:54+00:00,"This paper plans to develop an Equitable and Responsible AI framework with
enabling techniques and algorithms for the Internet of Energy (IoE), in short,
RAI4IoE. The energy sector is going through substantial changes fueled by two
key drivers: building a zero-carbon energy sector and the digital
transformation of the energy infrastructure. We expect to see the convergence
of these two drivers resulting in the IoE, where renewable distributed energy
resources (DERs), such as electric cars, storage batteries, wind turbines and
photovoltaics (PV), can be connected and integrated for reliable energy
distribution by leveraging advanced 5G-6G networks and AI technology. This
allows DER owners as prosumers to participate in the energy market and derive
economic incentives. DERs are inherently asset-driven and face equitable
challenges (i.e., fair, diverse and inclusive). Without equitable access,
privileged individuals, groups and organizations can participate and benefit at
the cost of disadvantaged groups. The real-time management of DER resources not
only brings out the equity problem to the IoE, it also collects highly
sensitive location, time, activity dependent data, which requires to be handled
responsibly (e.g., privacy, security and safety), for AI-enhanced predictions,
optimization and prioritization services, and automated management of flexible
resources. The vision of our project is to ensure equitable participation of
the community members and responsible use of their data in IoE so that it could
reap the benefits of advances in AI to provide safe, reliable and sustainable
energy services.",http://arxiv.org/pdf/2309.11691v1
2309.11688v1,cs.CL,LLM Guided Inductive Inference for Solving Compositional Problems,2023-09-20 23:44:16+00:00,"While large language models (LLMs) have demonstrated impressive performance
in question-answering tasks, their performance is limited when the questions
require knowledge that is not included in the model's training data and can
only be acquired through direct observation or interaction with the real world.
Existing methods decompose reasoning tasks through the use of modules invoked
sequentially, limiting their ability to answer deep reasoning tasks. We
introduce a method, Recursion based extensible LLM (REBEL), which handles
open-world, deep reasoning tasks by employing automated reasoning techniques
like dynamic planning and forward-chaining strategies. REBEL allows LLMs to
reason via recursive problem decomposition and utilization of external tools.
The tools that REBEL uses are specified only by natural language description.
We further demonstrate REBEL capabilities on a set of problems that require a
deeply nested use of external tools in a compositional and conversational
setting.",http://arxiv.org/pdf/2309.11688v1
2309.11682v1,cs.LG,Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework,2023-09-20 23:25:28+00:00,"While training fair machine learning models has been studied extensively in
recent years, most developed methods rely on the assumption that the training
and test data have similar distributions. In the presence of distribution
shifts, fair models may behave unfairly on test data. There have been some
developments for fair learning robust to distribution shifts to address this
shortcoming. However, most proposed solutions are based on the assumption of
having access to the causal graph describing the interaction of different
features. Moreover, existing algorithms require full access to data and cannot
be used when small batches are used (stochastic/batch implementation). This
paper proposes the first stochastic distributionally robust fairness framework
with convergence guarantees that do not require knowledge of the causal graph.
More specifically, we formulate the fair inference in the presence of the
distribution shift as a distributionally robust optimization problem under
$L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual
Information (ERMI) as the measure of fairness violation. We then discuss how
the proposed method can be implemented in a stochastic fashion. We have
evaluated the presented framework's performance and efficiency through
extensive experiments on real datasets consisting of distribution shifts.",http://arxiv.org/pdf/2309.11682v1
2309.11680v1,cs.LG,Federated Learning with Neural Graphical Models,2023-09-20 23:24:22+00:00,"Federated Learning (FL) addresses the need to create models based on
proprietary data in such a way that multiple clients retain exclusive control
over their data, while all benefit from improved model accuracy due to pooled
resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic
Graphical models that utilize the expressive power of neural networks to learn
complex non-linear dependencies between the input features. They learn to
capture the underlying data distribution and have efficient algorithms for
inference and sampling. We develop a FL framework which maintains a global NGM
model that learns the averaged information from the local NGM models while
keeping the training data within the client's environment. Our design, FedNGMs,
avoids the pitfalls and shortcomings of neuron matching frameworks like
Federated Matched Averaging that suffers from model parameter explosion. Our
global model size remains constant throughout the process. In the cases where
clients have local variables that are not part of the combined global
distribution, we propose a `Stitching' algorithm, which personalizes the global
NGM models by merging the additional variables using the client's data. FedNGM
is robust to data heterogeneity, large number of participants, and limited
communication bandwidth.",http://arxiv.org/pdf/2309.11680v1
2309.11672v1,cs.AI,Generative AI in Mafia-like Game Simulation,2023-09-20 22:38:34+00:00,"In this research, we explore the efficacy and potential of Generative AI
models, specifically focusing on their application in role-playing simulations
exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's
advanced capabilities, the study aimed to showcase the model's potential in
understanding, decision-making, and interaction during game scenarios.
Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo,
demonstrated GPT-4's enhanced adaptability to the game environment, with
significant improvements in posing relevant questions and forming human-like
responses. However, challenges such as the model;s limitations in bluffing and
predicting opponent moves emerged. Reflections on game development, financial
constraints, and non-verbal limitations of the study were also discussed. The
findings suggest that while GPT-4 exhibits promising advancements over earlier
models, there remains potential for further development, especially in
instilling more human-like attributes in AI.",http://arxiv.org/pdf/2309.11672v1
2309.11665v1,eess.SP,Channel Reciprocity Attacks Using Intelligent Surfaces with Non-Diagonal Phase Shifts,2023-09-20 22:16:21+00:00,"While reconfigurable intelligent surface (RIS) technology has been shown to
provide numerous benefits to wireless systems, in the hands of an adversary
such technology can also be used to disrupt communication links. This paper
describes and analyzes an RIS-based attack on multi-antenna wireless systems
that operate in time-division duplex mode under the assumption of channel
reciprocity. In particular, we show how an RIS with a non-diagonal (ND) phase
shift matrix (referred to here as an ND-RIS) can be deployed to maliciously
break the channel reciprocity and hence degrade the downlink network
performance. Such an attack is entirely passive and difficult to detect. We
provide a theoretical analysis of the degradation in the sum ergodic rate that
results when an arbitrary malicious ND-RIS is deployed and design an approach
based on the genetic algorithm for optimizing the ND structure under partial
knowledge of the available channel state information. Our simulation results
validate the analysis and demonstrate that an ND-RIS channel reciprocity attack
can dramatically reduce the downlink throughput.",http://arxiv.org/pdf/2309.11665v1
2309.13038v1,cs.CV,Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?,2023-09-22 17:58:04+00:00,"Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used
to evaluate model privacy risk under reconstruction attacks. Under these
metrics, reconstructed images that are determined to resemble the original one
generally indicate more privacy leakage. Images determined as overall
dissimilar, on the other hand, indicate higher robustness against attack.
However, there is no guarantee that these metrics well reflect human opinions,
which, as a judgement for model privacy leakage, are more trustworthy. In this
paper, we comprehensively study the faithfulness of these hand-crafted metrics
to human perception of privacy information from the reconstructed images. On 5
datasets ranging from natural images, faces, to fine-grained classes, we use 4
existing attack methods to reconstruct images from many different
classification models and, for each reconstructed image, we ask multiple human
annotators to assess whether this image is recognizable. Our studies reveal
that the hand-crafted metrics only have a weak correlation with the human
evaluation of privacy leakage and that even these metrics themselves often
contradict each other. These observations suggest risks of current metrics in
the community. To address this potential risk, we propose a learning-based
measure called SemSim to evaluate the Semantic Similarity between the original
and reconstructed images. SemSim is trained with a standard triplet loss, using
an original image as an anchor, one of its recognizable reconstructed images as
a positive sample, and an unrecognizable one as a negative. By training on
human annotations, SemSim exhibits a greater reflection of privacy leakage on
the semantic level. We show that SemSim has a significantly higher correlation
with human judgment compared with existing metrics. Moreover, this strong
correlation generalizes to unseen datasets, models and attack methods.",http://arxiv.org/pdf/2309.13038v1
2309.13033v1,eess.SY,Robust Stability Analysis of a Class of LTV Systems,2023-09-22 17:48:12+00:00,"Many physical systems are inherently time-varying in nature. When these
systems are linearized around a trajectory, generally, the resulting system is
Linear Time-Varying (LTV). LTV systems describe an important class of linear
systems and can be thought of as a natural extension of LTI systems. However,
it is well known that, unlike LTI systems, the eigenvalues of an LTV system do
not determine its stability. In this paper, the stability conditions for a
class of LTV systems are derived. This class is composed of piecewise LTV
systems, i.e. LTV systems that are piecewise linear in time. Sufficient
conditions of stability are derived in the form of linear matrix inequalities
(LMIs) by using the Lyapunov stability criterion. The feasibility of LMIs
guarantees the stability of a given piecewise LTV system. Furthermore,
uncertain piecewise LTV systems with scalar parametric uncertainty are also
considered. Sufficient conditions for robust stability of this case are also
presented, which come out to be quasi-LMIs, which can be optimized using a
bisection algorithm to find the bounds of uncertainty for which the system is
stable. The proposed method is applied to the problem of pitch angle control of
a space launch vehicle, and the results are presented.",http://arxiv.org/pdf/2309.13033v1
2309.13032v1,eess.SY,"Modelling, Simulation, and Control of a Flexible Space Launch Vehicle",2023-09-22 17:47:38+00:00,"Modern Space Launch Vehicles (SLVs), being slender in shape and due to the
use of lightweight materials, are generally flexible in nature. This structural
flexibility, when coupled with sensor and actuator dynamics, can adversely
affect the control of SLV, which may lead to vehicle instability and, in the
worst-case scenario, to structural failure. This work focuses on modelling and
simulation of rigid and flexible dynamics of an SLV and its interactions with
the control system. SpaceX's Falcon 9 has been selected for this study. The
flexible modes are calculated using modal analysis in Ansys. High-fidelity
nonlinear simulation is developed which incorporates the flexible modes and
their interactions with rigid degrees of freedom. Moreover, linearized models
are developed for flexible body dynamics, over the complete trajectory until
the first stage's separation. Using classical control methods, attitude
controllers, that keep the SLV on its desired trajectory, are developed, and
multiple filters are designed to suppress the interactions of flexible
dynamics. The designed controllers along with filters are implemented in the
nonlinear simulation. Furthermore, to demonstrate the robustness of designed
controllers, Monte-Carlo simulations are carried out and results are presented.",http://arxiv.org/pdf/2309.13032v1
2309.13030v1,cs.CE,A numerical framework for simulating progressive failure in composite laminates under high-cycle fatigue loading,2023-09-22 17:45:27+00:00,"In this work, a recently proposed high-cycle fatigue cohesive zone model,
which covers crack initiation and propagation with limited input parameters, is
embedded in a robust and efficient numerical framework for simulating
progressive failure in composite laminates under fatigue loading. The fatigue
cohesive zone model is enhanced with an implicit time integration scheme of the
fatigue damage variable which allows for larger cycle increments and more
efficient analyses. The method is combined with an adaptive strategy for
determining the cycle increment based on global convergence rates. Moreover, a
consistent material tangent stiffness matrix has been derived by fully
linearizing the underlying mixed-mode quasi-static model and the fatigue damage
update. The enhanced fatigue cohesive zone model is used to describe matrix
cracking and delamination in laminates. In order to allow for matrix cracks to
initiate at arbitrary locations and to avoid complex and costly mesh
generation, the phantom node version of the eXtended finite element method
(XFEM) is employed. For the insertion of new crack segments, an XFEM fatigue
crack insertion criterion is presented, which is consistent with the fatigue
cohesive zone formulation. It is shown with numerical examples that the
improved fatigue damage update enhances the accuracy, efficiency and robustness
of the numerical simulations significantly. The numerical framework is applied
to the simulation of progressive fatigue failure in an open-hole
[$\pm$45]-laminate. It is demonstrated that the numerical model is capable of
accurately and efficiently simulating the complete failure process from
distributed damage to localized failure.",http://arxiv.org/pdf/2309.13030v1
2309.13024v1,math.OC,"Zeroth-Order Methods for Nondifferentiable, Nonconvex, and Hierarchical Federated Optimization",2023-09-22 17:38:38+00:00,"Federated learning (FL) has emerged as an enabling framework for
communication-efficient decentralized training. In this paper, we study three
broadly applicable problem classes in FL: (i) Nondifferentiable nonconvex
optimization, e.g., in training of ReLU neural networks; (ii) Federated bilevel
optimization, e.g., in hyperparameter learning; (iii) Federated minimax
problems, e.g., in adversarial training. Research on such problems has been
limited and afflicted by reliance on strong assumptions, including
differentiability and L-smoothness of the implicit function in (ii)-(iii).
Unfortunately, such assumptions may fail to hold in practical settings. We
bridge this gap by making the following contributions. In (i), by leveraging
convolution-based smoothing and Clarke's subdifferential calculus, we devise a
randomized smoothing-enabled zeroth-order FL method and derive communication
and iteration complexity guarantees for computing an approximate Clarke
stationary point. Notably, our scheme allows for local functions that are both
nonconvex and nondifferentiable. In (ii) and (iii), we devise a unifying
randomized implicit zeroth-order FL framework, equipped with explicit
communication and iteration complexities. Importantly, this method employs
single-timescale local steps, resulting in significant reduction in
communication overhead when addressing hierarchical problems. We validate the
theory using numerical experiments on nonsmooth and hierarchical ML problems.",http://arxiv.org/pdf/2309.13024v1
2309.13023v1,hep-ph,Local infrared safety in time-ordered perturbation theory,2023-09-22 17:34:37+00:00,"We develop a general expression for weighted cross sections in leptonic
annihilation to hadrons based on time-ordered perturbation theory (TOPT). The
analytic behavior of the resulting integrals over spatial momenta can be
analyzed in the language of Landau equations and infrared (IR) power counting.
For any infrared-safe weight, the cancellation of infrared divergences is
implemented locally at the integrand level, and in principle can be evaluated
numerically in four dimensions. We go on to show that it is possible to
eliminate unphysical singularities that appear in time-ordered perturbation
theory for arbitrary amplitudes. This is done by reorganizing TOPT into an
equivalent form that combines classes of time orderings into a ``partially
time-ordered perturbation theory"". Applying the formalism to leptonic
annihilation, we show how to derive diagrammatic expressions with only physical
unitarity cuts.",http://arxiv.org/pdf/2309.13023v1
2309.13022v1,cs.LG,Graph Neural Network for Stress Predictions in Stiffened Panels Under Uniform Loading,2023-09-22 17:34:20+00:00,"Machine learning (ML) and deep learning (DL) techniques have gained
significant attention as reduced order models (ROMs) to computationally
expensive structural analysis methods, such as finite element analysis (FEA).
Graph neural network (GNN) is a particular type of neural network which
processes data that can be represented as graphs. This allows for efficient
representation of complex geometries that can change during conceptual design
of a structure or a product. In this study, we propose a novel graph embedding
technique for efficient representation of 3D stiffened panels by considering
separate plate domains as vertices. This approach is considered using Graph
Sampling and Aggregation (GraphSAGE) to predict stress distributions in
stiffened panels with varying geometries. A comparison between a
finite-element-vertex graph representation is conducted to demonstrate the
effectiveness of the proposed approach. A comprehensive parametric study is
performed to examine the effect of structural geometry on the prediction
performance. Our results demonstrate the immense potential of graph neural
networks with the proposed graph embedding method as robust reduced-order
models for 3D structures.",http://arxiv.org/pdf/2309.13022v1
2309.13018v1,eess.AS,Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model,2023-09-22 17:30:28+00:00,"Neural network pruning offers an effective method for compressing a
multilingual automatic speech recognition (ASR) model with minimal performance
loss. However, it entails several rounds of pruning and re-training needed to
be run for each language. In this work, we propose the use of an adaptive
masking approach in two scenarios for pruning a multilingual ASR model
efficiently, each resulting in sparse monolingual models or a sparse
multilingual model (named as Dynamic ASR Pathways). Our approach dynamically
adapts the sub-network, avoiding premature decisions about a fixed sub-network
structure. We show that our approach outperforms existing pruning methods when
targeting sparse monolingual models. Further, we illustrate that Dynamic ASR
Pathways jointly discovers and trains better sub-networks (pathways) of a
single multilingual model by adapting from different sub-network
initializations, thereby reducing the need for language-specific pruning.",http://arxiv.org/pdf/2309.13018v1
2309.13016v1,cs.LG,Understanding Deep Gradient Leakage via Inversion Influence Functions,2023-09-22 17:26:24+00:00,"Deep Gradient Leakage (DGL) is a highly effective attack that recovers
private training images from gradient vectors. This attack casts significant
privacy challenges on distributed learning from clients with sensitive data,
where clients are required to share gradients. Defending against such attacks
requires but lacks an understanding of when and how privacy leakage happens,
mostly because of the black-box nature of deep networks. In this paper, we
propose a novel Inversion Influence Function (I$^2$F) that establishes a
closed-form connection between the recovered images and the private gradients
by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F
is scalable for analyzing deep networks, requiring only oracle access to
gradients and Jacobian-vector products. We empirically demonstrate that I$^2$F
effectively approximated the DGL generally on different model architectures,
datasets, attack implementations, and noise-based defenses. With this novel
tool, we provide insights into effective gradient perturbation directions, the
unfairness of privacy protection, and privacy-preferred model initialization.
Our codes are provided in
https://github.com/illidanlab/inversion-influence-function.",http://arxiv.org/pdf/2309.13016v1
2309.13014v1,quant-ph,"Completeness of qufinite ZXW calculus, a graphical language for mixed-dimensional quantum computing",2023-09-22 17:23:58+00:00,"Finite-dimensional quantum theory serves as the theoretical foundation for
quantum information and computation based on 2-dimensional qubits,
d-dimensional qudits, and their interactions. The qufinite ZX calculus has been
used as a framework for mixed-dimensional quantum computing; however, it lacked
the crucial property of completeness, which ensures that the calculus
incorporates a set of rules rich enough to prove any equation. The ZXW calculus
is a complete language for qudit quantum computing with applications previously
unreachable solely with the ZX or ZW calculus. In this paper, we introduce the
qufinite ZXW calculus, a unification of all qudit ZXW calculi in a single
framework for mixed-dimensional quantum computing. We provide a set of rewrite
rules and a unique normal form that make the calculus complete for
finite-dimensional quantum theory. This work paves the way for the optimization
of mixed dimensional circuits and tensor networks appearing in different areas
of quantum computing including quantum chemistry, compilation, and quantum
many-body systems.",http://arxiv.org/pdf/2309.13014v1
2309.13013v1,eess.IV,Performance Analysis of UNet and Variants for Medical Image Segmentation,2023-09-22 17:20:40+00:00,"Medical imaging plays a crucial role in modern healthcare by providing
non-invasive visualisation of internal structures and abnormalities, enabling
early disease detection, accurate diagnosis, and treatment planning. This study
aims to explore the application of deep learning models, particularly focusing
on the UNet architecture and its variants, in medical image segmentation. We
seek to evaluate the performance of these models across various challenging
medical image segmentation tasks, addressing issues such as image
normalization, resizing, architecture choices, loss function design, and
hyperparameter tuning. The findings reveal that the standard UNet, when
extended with a deep network layer, is a proficient medical image segmentation
model, while the Res-UNet and Attention Res-UNet architectures demonstrate
smoother convergence and superior performance, particularly when handling fine
image details. The study also addresses the challenge of high class imbalance
through careful preprocessing and loss function definitions. We anticipate that
the results of this study will provide useful insights for researchers seeking
to apply these models to new medical imaging problems and offer guidance and
best practices for their implementation.",http://arxiv.org/pdf/2309.13013v1
2309.13010v1,math.SG,Holomorphic discs of negative Maslov index and extended deformations in mirror symmetry,2023-09-22 17:17:47+00:00,"The SYZ approach to mirror symmetry for log Calabi-Yau manifolds starts from
a Lagrangian torus fibration on the complement of an anticanonical divisor. A
mirror space is constructed by gluing local charts (moduli spaces of local
systems on generic torus fibers) via wall-crossing transformations which
account for corrections to the analytic structure of moduli spaces of objects
of the Fukaya category induced by bubbling of Maslov index 0 holomorphic discs,
and made into a Landau-Ginzburg model by equipping it with a regular function
(the superpotential) which enumerates Maslov index 2 holomorphic discs.
  When they occur, holomorphic discs of negative Maslov index deform this
picture by introducing inconsistencies in the wall-crossing transformations, so
that the mirror is no longer an analytic space; the geometric features of the
corrected mirror can be understood in the language of extended deformations of
Landau-Ginzburg models. We illustrate this phenomenon (and show that it
actually occurs) by working through the construction for an explicit example (a
log Calabi-Yau 4-fold obtained by blowing up a toric variety), and discuss a
family Floer approach to the geometry of the corrected mirror in this setting.
Along the way, we introduce a Morse-theoretic model for family Floer theory
which may be of independent interest.",http://arxiv.org/pdf/2309.13010v1
2309.13006v1,cs.CV,Deep3DSketch+: Rapid 3D Modeling from Single Free-hand Sketches,2023-09-22 17:12:13+00:00,"The rapid development of AR/VR brings tremendous demands for 3D content.
While the widely-used Computer-Aided Design (CAD) method requires a
time-consuming and labor-intensive modeling process, sketch-based 3D modeling
offers a potential solution as a natural form of computer-human interaction.
However, the sparsity and ambiguity of sketches make it challenging to generate
high-fidelity content reflecting creators' ideas. Precise drawing from multiple
views or strategic step-by-step drawings is often required to tackle the
challenge but is not friendly to novice users. In this work, we introduce a
novel end-to-end approach, Deep3DSketch+, which performs 3D modeling using only
a single free-hand sketch without inputting multiple sketches or view
information. Specifically, we introduce a lightweight generation network for
efficient inference in real-time and a structural-aware adversarial training
approach with a Stroke Enhancement Module (SEM) to capture the structural
information to facilitate learning of the realistic and fine-detailed shape
structures for high-fidelity performance. Extensive experiments demonstrated
the effectiveness of our approach with the state-of-the-art (SOTA) performance
on both synthetic and real datasets.",http://arxiv.org/pdf/2309.13006v1
2309.13003v1,physics.optics,Observation of light driven band structure via multi-band high harmonic spectroscopy,2023-09-22 17:06:01+00:00,"Intense light-matter interactions have revolutionized our ability to probe
and manipulate quantum systems at sub-femtosecond time scales, opening routes
to all-optical control of electronic currents in solids at petahertz rates.
Such control typically requires electric field amplitudes $\sim V/\AA$, when
the voltage drop across a lattice site becomes comparable to the characteristic
band gap energies. In this regime, intense light-matter interaction induces
significant modifications of electronic and optical properties, dramatically
modifying the crystal band structure. Yet, identifying and characterizing such
modifications remains an outstanding problem. As the oscillating electric field
changes within the driving field's cycle, does the band-structure follow, and
how can it be defined? Here we address this fundamental question, proposing
all-optical spectroscopy to probe laser-induced closing of the band-gap between
adjacent conduction bands. Our work reveals the link between nonlinear light
matter interactions in strongly driven crystals and the sub-cycle modifications
in their effective band structure.",http://arxiv.org/pdf/2309.13003v1
2309.13002v1,quant-ph,Expressive variational quantum circuits provide inherent privacy in federated learning,2023-09-22 17:04:50+00:00,"Federated learning has emerged as a viable distributed solution to train
machine learning models without the actual need to share data with the central
aggregator. However, standard neural network-based federated learning models
have been shown to be susceptible to data leakage from the gradients shared
with the server. In this work, we introduce federated learning with variational
quantum circuit model built using expressive encoding maps coupled with
overparameterized ans\""atze. We show that expressive maps lead to inherent
privacy against gradient inversion attacks, while overparameterization ensures
model trainability. Our privacy framework centers on the complexity of solving
the system of high-degree multivariate Chebyshev polynomials generated by the
gradients of quantum circuit. We present compelling arguments highlighting the
inherent difficulty in solving these equations, both in exact and approximate
scenarios. Additionally, we delve into machine learning-based attack strategies
and establish a direct connection between overparameterization in the original
federated learning model and underparameterization in the attack model.
Furthermore, we provide numerical scaling arguments showcasing that
underparameterization of the expressive map in the attack model leads to the
loss landscape being swamped with exponentially many spurious local minima
points, thus making it extremely hard to realize a successful attack. This
provides a strong claim, for the first time, that the nature of quantum machine
learning models inherently helps prevent data leakage in federated learning.",http://arxiv.org/pdf/2309.13002v1
2309.13000v1,cond-mat.quant-gas,Density engineering via inter-condensate dipole-dipole interactions: axial confinement and supersolids,2023-09-22 17:04:04+00:00,"Exploiting the long-range and anisotropic nature of dipole-dipole
interactions, we show that the density of a {\em target} dipolar Bose-Einstein
condensate can be engineered and axially confined using a trapped {\em control}
dipolar condensate. Increasing the number of control condensates leads to
exotic ground state structures, including supersolids and an incoherent array
of density peaks. Single and double-peaked periodic structures are observed as
a function of spacing between the control condensates. Our ideas may be
generalized to engineer any other dipolar quantum system using another one of a
similar dipole character. For instance, a Rydberg atom with electric dipole
moment may be confined and manipulated using a trapped polar molecule and vice
versa via long-range dipole-dipole interactions.",http://arxiv.org/pdf/2309.13000v1
2309.12999v1,math.GT,"Braid groups, elliptic curves, and resolving the quartic",2023-09-22 17:02:32+00:00,"We show that, up to a natural equivalence relation, the only non-trivial,
non-identity holomorphic maps
$\mathrm{Conf}_n\mathbb{C}\to\mathrm{Conf}_m\mathbb{C}$ between unordered
configuration spaces, where $m\in\{3,4\}$, are the resolving quartic map
$R\colon\mathrm{Conf}_4\mathbb{C}\to\mathrm{Conf}_3\mathbb{C}$, a map
$\Psi_3\colon\mathrm{Conf}_3\mathbb{C}\to\mathrm{Conf}_4\mathbb{C}$ constructed
from the inflection points of elliptic curves in a family, and $\Psi_3\circ R$.
This completes the classification of holomorphic maps
$\mathrm{Conf}_n\mathbb{C}\to\mathrm{Conf}_m\mathbb{C}$ for $m\leq n$,
extending results of Lin, Chen and Salter, and partially resolves a conjecture
of Farb. We also classify the holomorphic families of elliptic curves over
$\mathrm{Conf}_n\mathbb{C}$. To do this we classify homomorphisms between braid
groups with few strands and $\mathrm{PSL}_2\mathbb{Z}$, then apply powerful
results from complex analysis and Teichm\""uller theory. Furthermore, we prove a
conjecture of Castel about the equivalence classes of endomorphisms of the
braid group with three strands.",http://arxiv.org/pdf/2309.12999v1
2309.12994v1,cs.SE,Smart Fuzzing of 5G Wireless Software Implementation,2023-09-22 16:45:42+00:00,"In this paper, we introduce a comprehensive approach to bolstering the
security, reliability, and comprehensibility of OpenAirInterface5G (OAI5G), an
open-source software framework for the exploration, development, and testing of
5G wireless communication systems. Firstly, we employ AFL++, a powerful fuzzing
tool, to fuzzy-test OAI5G with respect to its configuration files rigorously.
This extensive testing process helps identify errors, defects, and security
vulnerabilities that may evade conventional testing methods. Secondly, we
harness the capabilities of Large Language Models such as Google Bard to
automatically decipher and document the meanings of parameters within the OAI5G
codebase that are used in fuzzing. This automated parameter interpretation
streamlines subsequent analyses and facilitates more informed decision-making.
Together, these two techniques contribute to fortifying the OAI5G system,
making it more robust, secure, and understandable for developers and analysts
alike.",http://arxiv.org/pdf/2309.12994v1
2309.12991v1,cond-mat.stat-mech,Deep learning probability flows and entropy production rates in active matter,2023-09-22 16:44:18+00:00,"Active matter systems, from self-propelled colloids to motile bacteria, are
characterized by the conversion of free energy into useful work at the
microscopic scale. These systems generically involve physics beyond the reach
of equilibrium statistical mechanics, and a persistent challenge has been to
understand the nature of their nonequilibrium states. The entropy production
rate and the magnitude of the steady-state probability current provide
quantitative ways to do so by measuring the breakdown of time-reversal symmetry
and the strength of nonequilibrium transport of measure. Yet, their efficient
computation has remained elusive, as they depend on the system's unknown and
high-dimensional probability density. Here, building upon recent advances in
generative modeling, we develop a deep learning framework that estimates the
score of this density. We show that the score, together with the microscopic
equations of motion, gives direct access to the entropy production rate, the
probability current, and their decomposition into local contributions from
individual particles, spatial regions, and degrees of freedom. To represent the
score, we introduce a novel, spatially-local transformer-based network
architecture that learns high-order interactions between particles while
respecting their underlying permutation symmetry. We demonstrate the broad
utility and scalability of the method by applying it to several
high-dimensional systems of interacting active particles undergoing
motility-induced phase separation (MIPS). We show that a single instance of our
network trained on a system of 4096 particles at one packing fraction can
generalize to other regions of the phase diagram, including systems with as
many as 32768 particles. We use this observation to quantify the spatial
structure of the departure from equilibrium in MIPS as a function of the number
of particles and the packing fraction.",http://arxiv.org/pdf/2309.12991v1
2309.12986v1,hep-ex,A precise determination of the strong-coupling constant from the recoil of $Z$ bosons with the ATLAS experiment at $\sqrt{s} = 8$ TeV,2023-09-22 16:31:44+00:00,"The coupling constant of the strong force is determined from the
transverse-momentum distribution of $Z$ bosons produced in 8 TeV proton-proton
collisions at the LHC and recorded by the ATLAS experiment. The $Z$-boson cross
sections are measured in the full phase space of the decay leptons using 15.3
million electron and muon pairs, in a dataset collected in 2012 and
corresponding to an integrated luminosity of 20.2 fb$^{-1}$. The analysis is
based on predictions evaluated at third order in perturbative QCD, supplemented
by the resummation of logarithmically enhanced contributions in the low
transverse-momentum region of the lepton pairs. The determined value of the
strong coupling at the reference scale corresponding to the $Z$-boson mass is
$\alpha_\text{s}(m_Z) = 0.1183 \pm 0.0009$. This is the most precise
experimental determination of $\alpha_\text{s}(m_Z)$ achieved so far.",http://arxiv.org/pdf/2309.12986v1
2309.12976v1,physics.optics,Inverse-designed broadband low-loss grating coupler on thick lithium-niobate-on-insulator platform,2023-09-22 16:18:08+00:00,"A grating coupler on 700-nm-thick Z-cut lithium-niobate-on-insulator platform
with high coupling efficiency, large bandwidth, and high fabrication tolerance
is designed and optimized by inverse design method. The optimized grating
coupler is fabricated with a single set of e-beam lithography and etching
process, and it is experimentally characterized to possess peak coupling
efficiency of -3.8 dB at 1574.93 nm, 1-dB bandwidth of 71.7 nm, and 3-dB
bandwidth of over 120 nm.",http://arxiv.org/pdf/2309.12976v1
2309.12973v1,math.OC,A damped elastodynamics system under the global injectivity condition: A hybrid optimal control problem,2023-09-22 16:13:37+00:00,"The purpose of this paper is to model mathematically certain mechanical
aspects of defibrillation. The time deformation of the heart tissue is modeled
with the elastodynamics equations dealing with the displacement field as main
unknown. These equations are coupled with a pressure whose variations
characterize the defibrillation process. The pressure variable corresponds to a
Lagrange multiplier associated with the so-called global injectivity condition.
We develop a hybrid optimal control approach in a general framework that covers
in particular the maximization of the variations of this pressure, and also the
time the maximum is reached. The control operator is distributed, and can be
described in a form that corresponds to geometric aspects of the modeling. For
mathematical convenience a damping term is added, and mathematical analysis
based on the $L^p$-parabolic maximal regularity is provided for the state
equations and the rigorous derivation of optimality conditions. Numerical
simulations for a toy-model exploit these optimality conditions and illustrate
the capacity of the approach.",http://arxiv.org/pdf/2309.12973v1
2309.12970v1,eess.IV,PI-RADS v2 Compliant Automated Segmentation of Prostate Zones Using co-training Motivated Multi-task Dual-Path CNN,2023-09-22 16:10:21+00:00,"The detailed images produced by Magnetic Resonance Imaging (MRI) provide
life-critical information for the diagnosis and treatment of prostate cancer.
To provide standardized acquisition, interpretation and usage of the complex
MRI images, the PI-RADS v2 guideline was proposed. An automated segmentation
following the guideline facilitates consistent and precise lesion detection,
staging and treatment. The guideline recommends a division of the prostate into
four zones, PZ (peripheral zone), TZ (transition zone), DPU (distal prostatic
urethra) and AFS (anterior fibromuscular stroma). Not every zone shares a
boundary with the others and is present in every slice. Further, the
representations captured by a single model might not suffice for all zones.
This motivated us to design a dual-branch convolutional neural network (CNN),
where each branch captures the representations of the connected zones
separately. Further, the representations from different branches act
complementary to each other at the second stage of training, where they are
fine-tuned through an unsupervised loss. The loss penalises the difference in
predictions from the two branches for the same class. We also incorporate
multi-task learning in our framework to further improve the segmentation
accuracy. The proposed approach improves the segmentation accuracy of the
baseline (mean absolute symmetric distance) by 7.56%, 11.00%, 58.43% and 19.67%
for PZ, TZ, DPU and AFS zones respectively.",http://arxiv.org/pdf/2309.12970v1
2309.12969v1,cs.CV,Detect Every Thing with Few Examples,2023-09-22 16:07:16+00:00,"Open-set object detection aims at detecting arbitrary categories beyond those
seen during training. Most recent advancements have adopted the open-vocabulary
paradigm, utilizing vision-language backbones to represent categories with
language. In this paper, we introduce DE-ViT, an open-set object detector that
employs vision-only DINOv2 backbones and learns new categories through example
images instead of language. To improve general detection ability, we transform
multi-classification tasks into binary classification tasks while bypassing
per-class inference, and propose a novel region propagation technique for
localization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot
object detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the
open-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT
surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and
one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary
SoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at
https://github.com/mlzxy/devit.",http://arxiv.org/pdf/2309.12969v1
2309.12967v1,physics.soc-ph,Predicting Nodal Influence via Local Iterative Metrics,2023-09-22 16:06:20+00:00,"Nodal spreading influence is the capability of a node to activate the rest of
the network when it is the seed of spreading. Combining nodal properties
(centrality metrics) derived from local and global topological information
respectively is shown to better predict nodal influence than a single metric.
In this work, we investigate to what extent local and global topological
information around a node contributes to the prediction of nodal influence and
whether relatively local information is sufficient for the prediction. We show
that by leveraging the iterative process used to derives a classical nodal
centrality such as eigenvector centrality, we can define an iterative metric
set that progressively incorporates more global information around the node. We
propose to predict nodal influence using an iterative metric set that consists
of an iterative metric from order $1$ to $K$ that are produced in an iterative
process, encoding gradually more global information as $K$ increases. Three
iterative metrics are considered, which converge to three classical node
centrality metrics respectively. Our results show that for each of the three
iterative metrics, the prediction quality is close to optimal when the metric
of relatively low orders ($K\sim4$) are included and increases only marginally
when further increasing $K$. The best performing iterative metric set shows
comparable prediction quality to the benchmark that combines seven centrality
metrics, in both real-world networks and synthetic networks with community
structures. Our findings are further explained via the correlation between an
iterative metric and nodal influence, the convergence of iterative metrics and
network properties.",http://arxiv.org/pdf/2309.12967v1
2309.12963v1,eess.AS,Massive End-to-end Models for Short Search Queries,2023-09-22 16:00:50+00:00,"In this work, we investigate two popular end-to-end automatic speech
recognition (ASR) models, namely Connectionist Temporal Classification (CTC)
and RNN-Transducer (RNN-T), for offline recognition of voice search queries,
with up to 2B model parameters. The encoders of our models use the neural
architecture of Google's universal speech model (USM), with additional funnel
pooling layers to significantly reduce the frame rate and speed up training and
inference. We perform extensive studies on vocabulary size, time reduction
strategy, and its generalization performance on long-form test sets. Despite
the speculation that, as the model size increases, CTC can be as good as RNN-T
which builds label dependency into the prediction, we observe that a 900M RNN-T
clearly outperforms a 1.8B CTC and is more tolerant to severe time reduction,
although the WER gap can be largely removed by LM shallow fusion.",http://arxiv.org/pdf/2309.12963v1
2309.12960v1,cs.CL,Nested Event Extraction upon Pivot Element Recogniton,2023-09-22 15:58:06+00:00,"Nested Event Extraction (NEE) aims to extract complex event structures where
an event contains other events as its arguments recursively. Nested events
involve a kind of Pivot Elements (PEs) that simultaneously act as arguments of
outer events and as triggers of inner events, and thus connect them into nested
structures. This special characteristic of PEs brings challenges to existing
NEE methods, as they cannot well cope with the dual identities of PEs.
Therefore, this paper proposes a new model, called PerNee, which extracts
nested events mainly based on recognizing PEs. Specifically, PerNee first
recognizes the triggers of both inner and outer events and further recognizes
the PEs via classifying the relation type between trigger pairs. In order to
obtain better representations of triggers and arguments to further improve NEE
performance, it incorporates the information of both event types and argument
roles into PerNee through prompt learning. Since existing NEE datasets (e.g.,
Genia11) are limited to specific domains and contain a narrow range of event
types with nested structures, we systematically categorize nested events in
generic domain and construct a new NEE dataset, namely ACE2005-Nest.
Experimental results demonstrate that PerNee consistently achieves
state-of-the-art performance on ACE2005-Nest, Genia11 and Genia13.",http://arxiv.org/pdf/2309.12960v1
2309.12957v1,astro-ph.SR,Probabilistic Classification of Infrared-selected targets for SPHEREx mission: In search of YSOs,2023-09-22 15:56:58+00:00,"We apply machine learning algorithms to classify Infrared (IR)-selected
targets for NASA's upcoming SPHEREx mission. In particular, we are interested
in classifying Young Stellar Objects (YSOs), which are essential for
understanding the star formation process. Our approach differs from previous
work, which has relied heavily on broadband color criteria to classify
IR-bright objects, and are typically implemented in color-color and
color-magnitude diagrams. However, these methods do not state the confidence
associated with the classification and the results from these methods are quite
ambiguous due to the overlap of different source types in these diagrams. Here,
we utilize photometric colors and magnitudes from seven near and mid-infrared
bands simultaneously and employ machine and deep learning algorithms to carry
out probabilistic classification of YSOs, Asymptotic Giant Branch (AGB) stars,
Active Galactic Nuclei (AGN) and main-sequence (MS) stars. Our approach also
sub-classifies YSOs into Class I, II, III and flat spectrum YSOs, and AGB stars
into carbon-rich and oxygen-rich AGB stars. We apply our methods to
infrared-selected targets compiled in preparation for SPHEREx which are likely
to include YSOs and other classes of objects. Our classification indicates that
out of $8,308,384$ sources, $1,966,340$ have class prediction with probability
exceeding $90\%$, amongst which $\sim 1.7\%$ are YSOs, $\sim 58.2\%$ are AGB
stars, $\sim 40\%$ are (reddened) MS stars, and $\sim 0.1\%$ are AGN whose red
broadband colors mimic YSOs. We validate our classification using the spatial
distributions of predicted YSOs towards the Cygnus-X star-forming complex, as
well as AGB stars across the Galactic plane.",http://arxiv.org/pdf/2309.12957v1
2309.12953v1,eess.IV,Inter-vendor harmonization of Computed Tomography (CT) reconstruction kernels using unpaired image translation,2023-09-22 15:53:56+00:00,"The reconstruction kernel in computed tomography (CT) generation determines
the texture of the image. Consistency in reconstruction kernels is important as
the underlying CT texture can impact measurements during quantitative image
analysis. Harmonization (i.e., kernel conversion) minimizes differences in
measurements due to inconsistent reconstruction kernels. Existing methods
investigate harmonization of CT scans in single or multiple manufacturers.
However, these methods require paired scans of hard and soft reconstruction
kernels that are spatially and anatomically aligned. Additionally, a large
number of models need to be trained across different kernel pairs within
manufacturers. In this study, we adopt an unpaired image translation approach
to investigate harmonization between and across reconstruction kernels from
different manufacturers by constructing a multipath cycle generative
adversarial network (GAN). We use hard and soft reconstruction kernels from the
Siemens and GE vendors from the National Lung Screening Trial dataset. We use
50 scans from each reconstruction kernel and train a multipath cycle GAN. To
evaluate the effect of harmonization on the reconstruction kernels, we
harmonize 50 scans each from Siemens hard kernel, GE soft kernel and GE hard
kernel to a reference Siemens soft kernel (B30f) and evaluate percent
emphysema. We fit a linear model by considering the age, smoking status, sex
and vendor and perform an analysis of variance (ANOVA) on the emphysema scores.
Our approach minimizes differences in emphysema measurement and highlights the
impact of age, sex, smoking status and vendor on emphysema quantification.",http://arxiv.org/pdf/2309.12953v1
2309.12954v1,hep-ph,Precision unification and the scale of supersymmetry,2023-09-22 15:53:56+00:00,"In this letter, we study the implications of precise gauge coupling
unification on supersymmetric particle masses. We argue that precise
unification favors the superpartner masses that are in the range of several TeV
and well beyond. We demonstrate this in the minimal supersymmetric theory with
a common sparticle mass threshold, and two simple high-scale scenarios: minimal
supergravity and minimal anomaly-mediated supersymmetry. We also identify
candidate models with a Higgsino or a wino dark matter candidate. Finally, the
analysis shows unambiguously that unless one takes foggy naturalness notions
too seriously, the lack of direct superpartner discoveries at the LHC has not
diminished the viability of supersymmetric unified theories in general nor even
precision unification in particular.",http://arxiv.org/pdf/2309.12954v1
2309.12951v1,cs.MA,"Boosting Studies of Multi-Agent Reinforcement Learning on Google Research Football Environment: the Past, Present, and Future",2023-09-22 15:50:07+00:00,"Even though Google Research Football (GRF) was initially benchmarked and
studied as a single-agent environment in its original paper, recent years have
witnessed an increasing focus on its multi-agent nature by researchers
utilizing it as a testbed for Multi-Agent Reinforcement Learning (MARL).
However, the absence of standardized environment settings and unified
evaluation metrics for multi-agent scenarios hampers the consistent
understanding of various studies. Furthermore, the challenging 5-vs-5 and
11-vs-11 full-game scenarios have received limited thorough examination due to
their substantial training complexities. To address these gaps, this paper
extends the original environment by not only standardizing the environment
settings and benchmarking cooperative learning algorithms across different
scenarios, including the most challenging full-game scenarios, but also by
discussing approaches to enhance football AI from diverse perspectives and
introducing related research tools. Specifically, we provide a distributed and
asynchronous population-based self-play framework with diverse pre-trained
policies for faster training, two football-specific analytical tools for deeper
investigation, and an online leaderboard for broader evaluation. The overall
expectation of this work is to advance the study of Multi-Agent Reinforcement
Learning on Google Research Football environment, with the ultimate goal of
benefiting real-world sports beyond virtual games.",http://arxiv.org/pdf/2309.12951v1
2309.12949v1,cs.IT,Guaranteed Private Communication with Secret Block Structure,2023-09-22 15:48:15+00:00,"A novel private communication framework is proposed where privacy is induced
by transmitting over channel instances of linear inverse problems that are
identifiable to the legitimate receiver, but unidentifiable to an eavesdropper.
The gap in identifiability is created in the framework by leveraging secret
knowledge between the transmitter and the legitimate receiver. Specifically,
the case where the legitimate receiver harnesses a secret block structure to
decode a transmitted block-sparse message from underdetermined linear
measurements in conditions where classical compressed sensing would provably
fail is examined. The applicability of the proposed scheme to practical
multiple access wireless communication systems is discussed. The protocol's
privacy is studied under a single transmission, and under multiple
transmissions without refreshing the secret block structure. It is shown that,
under a specific scaling of the channel dimensions and transmission parameters,
the eavesdropper can attempt to overhear the block structure from the
fourth-order moments of the channel output. Computation of a statistical lower
bound, suggests that the proposed fourth-order moment secret block estimation
strategy is near optimal. The performance of a spectral clustering algorithm is
studied to that end, defining scaling laws on the lifespan of the secret key
before the communication is compromised. Finally, numerical experiments
corroborating the theoretical findings are conducted.",http://arxiv.org/pdf/2309.12949v1
2309.12946v1,cond-mat.mtrl-sci,Unraveling Medium-Range Order and Melting Mechanism of ZIF-4 under High Temperature,2023-09-22 15:46:39+00:00,"Glass formation in Zeolitic Imidazolate Frameworks (ZIFs) has garnered
significant attention in the field of Metal-Organic Frameworks (MOFs) in recent
years. Numerous works have been conducted to investigate the microscopic
mechanisms involved in the melting-quenching process of ZIFs. Understanding the
density variations that occur during the melting process of ZIFs is crucial for
comprehending the origins of glass formation. However, conducting large-scale
simulations has been challenging due to limitations in computational resources.
In this work, we utilized deep learning methods to accurately construct a
potential function that describes the atomic-scale melting behavior of Zeolitic
Imidazolate Framework-4 (ZIF-4). The results revealed the spatial heterogeneity
associated with the formation of low-density phases during the melting process
of ZIF-4. This work discusses the advantages and limitations of applying deep
learning simulation methods to complex structures like ZIFs, providing valuable
insights for the development of machine learning approaches in designing
Metal-Organic Framework glasses.",http://arxiv.org/pdf/2309.12946v1
2309.12943v1,cs.CV,Background Activation Suppression for Weakly Supervised Object Localization and Semantic Segmentation,2023-09-22 15:44:10+00:00,"Weakly supervised object localization and semantic segmentation aim to
localize objects using only image-level labels. Recently, a new paradigm has
emerged by generating a foreground prediction map (FPM) to achieve pixel-level
localization. While existing FPM-based methods use cross-entropy to evaluate
the foreground prediction map and to guide the learning of the generator, this
paper presents two astonishing experimental observations on the object
localization learning process: For a trained network, as the foreground mask
expands, 1) the cross-entropy converges to zero when the foreground mask covers
only part of the object region. 2) The activation value continuously increases
until the foreground mask expands to the object boundary. Therefore, to achieve
a more effective localization performance, we argue for the usage of activation
value to learn more object regions. In this paper, we propose a Background
Activation Suppression (BAS) method. Specifically, an Activation Map Constraint
(AMC) module is designed to facilitate the learning of generator by suppressing
the background activation value. Meanwhile, by using foreground region guidance
and area constraint, BAS can learn the whole region of the object. In the
inference phase, we consider the prediction maps of different categories
together to obtain the final localization results. Extensive experiments show
that BAS achieves significant and consistent improvement over the baseline
methods on the CUB-200-2011 and ILSVRC datasets. In addition, our method also
achieves state-of-the-art weakly supervised semantic segmentation performance
on the PASCAL VOC 2012 and MS COCO 2014 datasets. Code and models are available
at https://github.com/wpy1999/BAS-Extension.",http://arxiv.org/pdf/2309.12943v1
2309.12935v1,physics.optics,Fundamentals of polaritons in strongly anisotropic thin crystal layers,2023-09-22 15:33:40+00:00,"Polaritons in strongly anisotropic thin layers have recently captured the
attention in nanophotonics because of their directional propagation at the
nanoscale, which offer unique possibilities for nanooptical applications.
However, exploiting the full potential of anisotropic polaritons requires a
thorough understanding of their properties, including field confinement, energy
and phase propagation direction and losses. Here we fill this critical gap by
providing fundamental insights into the propagation of anisotropic polaritons
in thin biaxial layers. In particular, we introduce a novel methodology that
allows us to represent isofrequency curves of polaritons in strongly
anisotropic materials considering that the real and imaginary parts of the
wavevector are not parallel. In fact, we analytically show that the direction
of the imaginary part of the wavevector is parallel to the group velocity, and
thus to the propagation direction and that the group velocity and the phase
velocity can have different directions. This finding is crucial for accurately
predicting polaritonic phenomena in anisotropic media and for the
interpretation of experimental results, yet it has so far been widely
overlooked in the literature, where it is customary to assume that the real and
imaginary parts of the wavevector are parallel. Additionally, we introduce a
criterion for classifying the polaritonic modes in biaxial layers into volume
and surface categories, and analyze their dispersion, field structure, and
losses. Finally, we discover the existence of anisotropic TE modes, which can
exhibit natural canalization. Taken together, our results shed light on
hitherto unexplored areas of the theory of electromagnetic modes in thin
biaxial layers. Although exemplified for van der Waals \alpha-MoO_3 layers, our
findings are general for polaritons in other strongly anisotropic biaxial
hyperbolic crystals.",http://arxiv.org/pdf/2309.12935v1
2309.12934v1,cs.CL,TopRoBERTa: Topology-Aware Authorship Attribution of Deepfake Texts,2023-09-22 15:32:49+00:00,"Recent advances in Large Language Models (LLMs) have enabled the generation
of open-ended high-quality texts, that are non-trivial to distinguish from
human-written texts. We refer to such LLM-generated texts as \emph{deepfake
texts}. There are currently over 11K text generation models in the huggingface
model repo. As such, users with malicious intent can easily use these
open-sourced LLMs to generate harmful texts and misinformation at scale. To
mitigate this problem, a computational method to determine if a given text is a
deepfake text or not is desired--i.e., Turing Test (TT). In particular, in this
work, we investigate the more general version of the problem, known as
\emph{Authorship Attribution (AA)}, in a multi-class setting--i.e., not only
determining if a given text is a deepfake text or not but also being able to
pinpoint which LLM is the author. We propose \textbf{TopRoBERTa} to improve
existing AA solutions by capturing more linguistic patterns in deepfake texts
by including a Topological Data Analysis (TDA) layer in the RoBERTa model. We
show the benefits of having a TDA layer when dealing with noisy, imbalanced,
and heterogeneous datasets, by extracting TDA features from the reshaped
$pooled\_output$ of RoBERTa as input. We use RoBERTa to capture contextual
representations (i.e., semantic and syntactic linguistic features), while using
TDA to capture the shape and structure of data (i.e., linguistic structures).
Finally, \textbf{TopRoBERTa}, outperforms the vanilla RoBERTa in 2/3 datasets,
achieving up to 7\% increase in Macro F1 score.",http://arxiv.org/pdf/2309.12934v1
2309.12916v1,cond-mat.mtrl-sci,Meso-scale size effects of material heterogeneities on crack propagation in brittle solids,2023-09-22 15:07:19+00:00,"Brittle solids are often toughened by adding a second-phase material. This
practice often results in composites with material heterogeneities on the meso
scale: large compared to the scale of the process zone but small compared to
that of the application. The specific configuration (both geometrical and
mechanical) of this mesoscale heterogeneity is generally recognized as
important in determining crack propagation and, subsequently, the (effective)
toughness of the composite. Here, we systematically investigate how dynamic
crack propagation is affected by mesoscale heterogeneities taking the form of
an array of inclusions. Using a variational phase-field approach, we compute
the apparent crack speed and fracture energy dissipation rate to compare crack
propagation under Mode-I loading across different configurations of these
inclusions. If fixing the volume fraction of inclusions, matching the inclusion
size to the K-dominance zone size gives rise to the best toughening outcome.
Conversely, if varying the volume fraction of inclusions, a lower volume
fraction configuration can lead to a better toughening outcome if and only if
the inclusion size approaches from above the size of the K-dominance zone.
Since the size of the K-dominance zone can be estimated \textit{a priori} given
an understanding of the application scenario and material availability, we can,
in principle, exploit this estimation to design a material's mesoscale
heterogeneity that optimally balances the tradeoff between strength and
toughness. This paves the way for realizing functional (meta-)materials against
crack propagation in extreme environments.",http://arxiv.org/pdf/2309.12916v1
2309.12914v1,eess.AS,VIC-KD: Variance-Invariance-Covariance Knowledge Distillation to Make Keyword Spotting More Robust Against Adversarial Attacks,2023-09-22 15:03:41+00:00,"Keyword spotting (KWS) refers to the task of identifying a set of predefined
words in audio streams. With the advances seen recently with deep neural
networks, it has become a popular technology to activate and control small
devices, such as voice assistants. Relying on such models for edge devices,
however, can be challenging due to hardware constraints. Moreover, as
adversarial attacks have increased against voice-based technologies, developing
solutions robust to such attacks has become crucial. In this work, we propose
VIC-KD, a robust distillation recipe for model compression and adversarial
robustness. Using self-supervised speech representations, we show that imposing
geometric priors to the latent representations of both Teacher and Student
models leads to more robust target models. Experiments on the Google Speech
Commands datasets show that the proposed methodology improves upon current
state-of-the-art robust distillation methods, such as ARD and RSLAD, by 12% and
8% in robust accuracy, respectively.",http://arxiv.org/pdf/2309.12914v1
2309.12912v1,cs.CC,Symmetric Exponential Time Requires Near-Maximum Circuit Size,2023-09-22 14:56:59+00:00,"We show that there is a language in $\mathsf{S}_2\mathsf{E}/_1$ (symmetric
exponential time with one bit of advice) with circuit complexity at least
$2^n/n$. In particular, the above also implies the same near-maximum circuit
lower bounds for the classes $\Sigma_2\mathsf{E}$,
$(\Sigma_2\mathsf{E}\cap\Pi_2\mathsf{E})/_1$, and
$\mathsf{ZPE}^{\mathsf{NP}}/_1$. Previously, only ""half-exponential"" circuit
lower bounds for these complexity classes were known, and the smallest
complexity class known to require exponential circuit complexity was
$\Delta_3\mathsf{E} = \mathsf{E}^{\Sigma_2\mathsf{P}}$ (Miltersen,
Vinodchandran, and Watanabe COCOON'99).
  Our circuit lower bounds are corollaries of an unconditional zero-error
pseudodeterministic algorithm with an $\mathsf{NP}$ oracle and one bit of
advice ($\mathsf{FZPP}^{\mathsf{NP}}/_1$) that solves the range avoidance
problem infinitely often. This algorithm also implies unconditional
infinitely-often pseudodeterministic $\mathsf{FZPP}^{\mathsf{NP}}/_1$
constructions for Ramsey graphs, rigid matrices, two-source extractors, linear
codes, and $\mathrm{K}^{\mathrm{poly}}$-random strings with nearly optimal
parameters.
  Our proofs relativize. The two main technical ingredients are (1) Korten's
$\mathsf{P}^{\mathsf{NP}}$ reduction from the range avoidance problem to
constructing hard truth tables (FOCS'21), which was in turn inspired by a
result of Je\v{r}\'abek on provability in Bounded Arithmetic (Ann. Pure Appl.
Log. 2004); and (2) the recent iterative win-win paradigm of Chen, Lu,
Oliveira, Ren, and Santhanam (FOCS'23).",http://arxiv.org/pdf/2309.12912v1
2309.12910v1,hep-ex,Measurement of the pion formfactor with CMD-3 detector and its implication to the hadronic contribution to muon (g-2),2023-09-22 14:55:31+00:00,"The cross section of the process $e^+e^-\to\pi^+\pi^-$ has been measured in
the center of mass energy range from 0.32 to 1.2 GeV with the CMD-3 detector at
the electron-positron collider VEPP-2000. The measurement is based on an
integrated luminosity of about 88 pb$^{-1}$ out of which 62 pb$^{-1}$
constitutes a full dataset collected by CMD-3 at center-of-mass energies below
1 GeV. In the dominant region near $\rho$-resonance a systematic uncertainty of
0.7% has been reached. The impact of presented results on the evaluation of the
hadronic contribution to the anomalous magnetic moment of muon is discussed.",http://arxiv.org/pdf/2309.12910v1
2309.12905v1,quant-ph,Nonadiabatic dynamics near metal surfaces with periodic drivings: A generalized surface hopping in Floquet representation,2023-09-22 14:48:22+00:00,"With light-matter interaction extending into strong regime, as well as rapid
development of laser technology, systems subjecting to a time-periodic
perturbation are attracted broad attention. Floquet theorem and Floquet
time-independent Hamiltonian are powerful theoretical framework to investigate
the systems subjecting to time-periodic drivings. In this study, we extend the
previous generalized SH algorithm near metal surface (J. Chem. Theory Comput.
2017, 13, 6, 2430-2439) to the Floquet space, and hence, we develop a
generalized Floquet representation based surface hopping (FR-SH) algorithm.
Here, we consider open quantum system with fast drivings. We expect that the
present algorithm will be useful for understanding the chemical processes of
molecules under time-periodic drivings near the metal surface.",http://arxiv.org/pdf/2309.12905v1
2309.12904v1,hep-ph,Flavor anomalies in leptoquark model with gauged $U(1)_{L_-L_}$,2023-09-22 14:46:47+00:00,"Leptoquarks (LQs) have been extensively studied in the context of $B$
anomalies. When $U(1)_{L_\mu-L_\tau}$ is introduced to a scalar LQ model with
the LQ $S_1$ charged under the new symmetry, $S_1$ primarily couples to the
third-generation leptons while its couplings to first and second-generation
leptons are naturally suppressed. Furthermore, only $S_1$ in the scalar LQ
models has the feature that down-type quarks merely couple to neutrinos but not
the charged leptons, avoiding strict restrictions from $b\to s \mu^+ \mu^-$.
With this distinctive characteristic of $S_1$, we investigate its impact on
rare processes involving the $d_i \to d_j \nu \bar\nu$ transitions. Under the
dominant constraints from $\Delta F=2$ processes, we find that the $S_1$
contributions to the branching ratios (BRs) of $B\to K(K^*) \nu \bar\nu$ and
$K_L \to \pi^0 \nu \bar\nu$ can be factorized into the same multiplicative
factor multiplying the standard model predictions. Enhancement in the BRs can
possibly exceed a factor of 2. In particular, ${\cal B}(K^+\to \pi^+ \nu
\bar\nu)$ can reach the upper $1\sigma$ error of the experimental value, i.e.,
$\simeq 15.4 \times 10^{-11}$. We also show that the model can fit the new
world averages of $R(D)$ and $R(D^*)$.",http://arxiv.org/pdf/2309.12904v1
2309.12903v1,astro-ph.SR,Latitudinal Propagation of Thermal Rossby Waves in Stellar Convection Zones,2023-09-22 14:39:34+00:00,"Using an analytic model, we derive the eigenfrequencies for thermal Rossby
waves that are trapped radially and latitudinally in an isentropically
stratified atmosphere. We ignore the star's curvature and work in an equatorial
f-plane geometry. The propagation of inertial waves is found to be sensitive to
the relative direction of the wave vector to the zonal direction. Prograde
propagating thermal Rossby waves are naturally trapped in the radial direction
for frequencies above a critical threshold, which depends on the angle of
propagation. Below the threshold frequency, there exists a continuous spectrum
of prograde and retrograde inertial waves that are untrapped in an isentropic
atmosphere, but can be trapped by gradients in the specific entropy density
such as occurs in a stellar convection zone. Finally, we discuss the
implications of these waves on recent observations of inertial oscillations in
the Sun, as well as in numerical simulations.",http://arxiv.org/pdf/2309.12903v1
2309.12900v1,math.PR,Quantitative homogenization and large-scale regularity of Poisson point clouds,2023-09-22 14:37:59+00:00,"We prove quantitative homogenization results for harmonic functions on
supercritical continuum percolation clusters--that is, Poisson point clouds
with edges connecting points which are closer than some fixed distance. We show
that, on large scales, harmonic functions resemble harmonic functions in
Euclidean space with sharp quantitative bounds on their difference. In
particular, for every point cloud which is supercritical (meaning that the
intensity of the Poisson process is larger than the critical parameter which
guarantees the existence of an infinite connected component), we obtain optimal
corrector bounds, homogenization error estimates and large-scale regularity
results.",http://arxiv.org/pdf/2309.12900v1
2309.12899v1,cs.GR,OptCtrlPoints: Finding the Optimal Control Points for Biharmonic 3D Shape Deformation,2023-09-22 14:37:05+00:00,"We propose OptCtrlPoints, a data-driven framework designed to identify the
optimal sparse set of control points for reproducing target shapes using
biharmonic 3D shape deformation. Control-point-based 3D deformation methods are
widely utilized for interactive shape editing, and their usability is enhanced
when the control points are sparse yet strategically distributed across the
shape. With this objective in mind, we introduce a data-driven approach that
can determine the most suitable set of control points, assuming that we have a
given set of possible shape variations. The challenges associated with this
task primarily stem from the computationally demanding nature of the problem.
Two main factors contribute to this complexity: solving a large linear system
for the biharmonic weight computation and addressing the combinatorial problem
of finding the optimal subset of mesh vertices. To overcome these challenges,
we propose a reformulation of the biharmonic computation that reduces the
matrix size, making it dependent on the number of control points rather than
the number of vertices. Additionally, we present an efficient search algorithm
that significantly reduces the time complexity while still delivering a nearly
optimal solution. Experiments on SMPL, SMAL, and DeformingThings4D datasets
demonstrate the efficacy of our method. Our control points achieve better
template-to-target fit than FPS, random search, and neural-network-based
prediction. We also highlight the significant reduction in computation time
from days to approximately 3 minutes.",http://arxiv.org/pdf/2309.12899v1
2309.12896v1,cond-mat.soft,Rheological Investigation of The Network Structure in Mixed Gels of Kappa and Iota Carrageenan,2023-09-22 14:34:27+00:00,"Carrageenans comprise linear sulfated high molecular weight polysaccharides
obtained from seaweeds and are routinely used in food and home/personal care
industries. Various kinds of carrageenans differ from others based on the ester
sulfate group location on the polysaccharide chains. Pure and mixed systems of
Kappa Carrageenan and Iota Carrageenan undergo a three-dimensional gel network
structure formation or dissociation with a change in temperature. During the
sol-gel and gel-sol transitions, the Carrageenan systems pass through a unique
critical gel state, where dynamic moduli are scale-invariant owing to the
self-similar structure of the three-dimensional network. In this work, we
obtain the critical gel state associated with pure and mixed systems of Kappa
and Iota Carrageenan during cooling and heating by exploring the material
behavior for a range of frequencies. Interestingly, on the one hand, the mixed
gels show a higher critical sol-gel transition temperature compared to the pure
systems at equal individual concentrations. On the other hand, the low
temperature moduli of mixed gels are closer to that of Kappa Carrageenan when
the concentration of the same is more than half in the mixture. The rheological
measurements demonstrate that the Kappa Carrageenan strongly affects the nature
of aggregation of double helices of Iota Carrageenan, but Iota Carrageenan does
not have a significant influence on that of Kappa Carrageenan. These results
suggest an associative, interactive network formation between Kappa and Iota
Carrageenan in the mixture, such that the gel behavior is predominantly
influenced by Kappa Carrageenan.",http://arxiv.org/pdf/2309.12896v1
2309.12887v1,quant-ph,New Approaches to Complexity via Quantum Graphs,2023-09-22 14:20:14+00:00,"Problems based on the structure of graphs -- for example finding cliques,
independent sets, or colourings -- are of fundamental importance in classical
complexity. It is well motivated to consider similar problems about quantum
graphs, which are an operator system generalisation of graphs. Defining
well-formulated decision problems for quantum graphs faces several technical
challenges, and consequently the connections between quantum graphs and
complexity have been underexplored.
  In this work, we introduce and study the clique problem for quantum graphs.
Our approach utilizes a well-known connection between quantum graphs and
quantum channels. The inputs for our problems are presented as quantum channels
induced by circuits, which implicitly determine a corresponding quantum graph.
We also use this approach to reimagine the clique and independent set problems
for classical graphs, by taking the inputs to be circuits of deterministic or
noisy channels which implicitly determine confusability graphs. We show that,
by varying the collection of channels in the language, these give rise to
complete problems for the classes $\textsf{NP}$, $\textsf{MA}$, $\textsf{QMA}$,
and $\textsf{QMA}(2)$. In this way, we exhibit a classical complexity problem
whose natural quantisation is $\textsf{QMA}(2)$, rather than $\textsf{QMA}$,
which is commonly assumed.
  To prove the results in the quantum case, we make use of methods inspired by
self-testing. To illustrate the utility of our techniques, we include a new
proof of the reduction of $\textsf{QMA}(k)$ to $\textsf{QMA}(2)$ via cliques
for quantum graphs. We also study the complexity of a version of the
independent set problem for quantum graphs, and provide preliminary evidence
that it may be in general weaker in complexity, contrasting to the classical
case where the clique and independent set problems are equivalent.",http://arxiv.org/pdf/2309.12887v1
2309.12882v1,cond-mat.dis-nn,Floquet-Anderson localization in the Thouless pump and how to avoid it,2023-09-22 14:13:35+00:00,"We investigate numerically how onsite disorder affects conduction in the
periodically driven Rice-Mele model, a prototypical realization of the Thouless
pump. Although the pump is robust against disorder in the fully adiabatic
limit, much less is known about the case of finite period time $T$, which is
relevant also in light of recent experimental realizations. We find that at any
fixed period time and nonzero disorder, increasing the system size $L\to\infty$
always leads to a breakdown of the pump, indicating Anderson localization of
the Floquet states. Our numerics indicate, however, that in a properly defined
thermodynamic limit, where $L/T^\theta$ is kept constant, Anderson localization
can be avoided, and the charge pumped per cycle has a well-defined value -- as
long as the disorder is not too strong. The critical exponent $\theta$ is not
universal, rather, its value depends on the disorder strength. Our findings are
relevant for practical, experimental realizations of the Thouless pump, for
studies investigating the nature of its current-carrying Floquet eigenstates,
as well as the mechanism of the full breakdown of the pump, expected if the
disorder exceeds a critical value.",http://arxiv.org/pdf/2309.12882v1
2309.12862v1,cs.LG,Associative Transformer Is A Sparse Representation Learner,2023-09-22 13:37:10+00:00,"Emerging from the monolithic pairwise attention mechanism in conventional
Transformer models, there is a growing interest in leveraging sparse
interactions that align more closely with biological principles. Approaches
including the Set Transformer and the Perceiver employ cross-attention
consolidated with a latent space that forms an attention bottleneck with
limited capacity. Building upon recent neuroscience studies of Global Workspace
Theory and associative memory, we propose the Associative Transformer (AiT).
AiT induces low-rank explicit memory that serves as both priors to guide
bottleneck attention in the shared workspace and attractors within associative
memory of a Hopfield network. Through joint end-to-end training, these priors
naturally develop module specialization, each contributing a distinct inductive
bias to form attention bottlenecks. A bottleneck can foster competition among
inputs for writing information into the memory. We show that AiT is a sparse
representation learner, learning distinct priors through the bottlenecks that
are complexity-invariant to input quantities and dimensions. AiT demonstrates
its superiority over methods such as the Set Transformer, Vision Transformer,
and Coordination in various vision tasks.",http://arxiv.org/pdf/2309.12862v1
2309.12861v1,cs.IT,A Proof of Concept for OTFS Resilience in Doubly-Selective Channels by GPU-Enabled Real-Time SDR,2023-09-22 13:35:14+00:00,"Orthogonal time frequency space (OTFS) is a modulation technique which is
robust against the disruptive effects of doubly-selective channels. In this
paper, we perform an experimental study of OTFS by a real-time software defined
radio (SDR) setup. Our SDR consists of a Graphical Processing Unit (GPU) for
signal processing programmed using Sionna and TensorFlow, and Universal
Software Radio Peripheral (USRP) devices for air interface. We implement a
low-latency transceiver structure for OTFS and investigate its performance
under various Doppler values. By comparing the performance of OTFS with
Orthogonal Frequency Division Multiplexing (OFDM), we demonstrate that OTFS is
highly robust against the disruptive effects of doubly-selective channels in a
real-time experimental setup.",http://arxiv.org/pdf/2309.12861v1
2309.12856v1,cs.RO,Robotic Handling of Compliant Food Objects by Robust Learning from Demonstration,2023-09-22 13:30:26+00:00,"The robotic handling of compliant and deformable food raw materials,
characterized by high biological variation, complex geometrical 3D shapes, and
mechanical structures and texture, is currently in huge demand in the ocean
space, agricultural, and food industries. Many tasks in these industries are
performed manually by human operators who, due to the laborious and tedious
nature of their tasks, exhibit high variability in execution, with variable
outcomes. The introduction of robotic automation for most complex processing
tasks has been challenging due to current robot learning policies. A more
consistent learning policy involving skilled operators is desired. In this
paper, we address the problem of robot learning when presented with
inconsistent demonstrations. To this end, we propose a robust learning policy
based on Learning from Demonstration (LfD) for robotic grasping of food
compliant objects. The approach uses a merging of RGB-D images and tactile data
in order to estimate the necessary pose of the gripper, gripper finger
configuration and forces exerted on the object in order to achieve effective
robot handling. During LfD training, the gripper pose, finger configurations
and tactile values for the fingers, as well as RGB-D images are saved. We
present an LfD learning policy that automatically removes inconsistent
demonstrations, and estimates the teacher's intended policy. The performance of
our approach is validated and demonstrated for fragile and compliant food
objects with complex 3D shapes. The proposed approach has a vast range of
potential applications in the aforementioned industry sectors.",http://arxiv.org/pdf/2309.12856v1
2309.12855v1,eess.IV,Cross-Modal Translation and Alignment for Survival Analysis,2023-09-22 13:29:14+00:00,"With the rapid advances in high-throughput sequencing technologies, the focus
of survival analysis has shifted from examining clinical indicators to
incorporating genomic profiles with pathological images. However, existing
methods either directly adopt a straightforward fusion of pathological features
and genomic profiles for survival prediction, or take genomic profiles as
guidance to integrate the features of pathological images. The former would
overlook intrinsic cross-modal correlations. The latter would discard
pathological information irrelevant to gene expression. To address these
issues, we present a Cross-Modal Translation and Alignment (CMTA) framework to
explore the intrinsic cross-modal correlations and transfer potential
complementary information. Specifically, we construct two parallel
encoder-decoder structures for multi-modal data to integrate intra-modal
information and generate cross-modal representation. Taking the generated
cross-modal representation to enhance and recalibrate intra-modal
representation can significantly improve its discrimination for comprehensive
survival analysis. To explore the intrinsic crossmodal correlations, we further
design a cross-modal attention module as the information bridge between
different modalities to perform cross-modal interactions and transfer
complementary information. Our extensive experiments on five public TCGA
datasets demonstrate that our proposed framework outperforms the
state-of-the-art methods.",http://arxiv.org/pdf/2309.12855v1
2309.12845v1,astro-ph.EP,Thermodynamic limits on oxygenic photosynthesis around M-dwarf stars: Generalized models and strategies for optimization,2023-09-22 13:11:21+00:00,"We explore the feasibility and potential characteristics of photosynthetic
light-harvesting on exo-planets orbiting in the habitable zone of low mass
stars ($< 1$ M$_{\odot}$). As stellar temperature, $T_{s}$, decreases, the
irradiance maximum red-shifts out of the $400 \textrm{nm} \leq \lambda < 750$
nm range of wavelengths that can be utilized by \emph{oxygenic} photosynthesis
on Earth. However, limited irradiance in this region does not preclude oxygenic
photosynthesis and Earth's plants, algae and cyanobacteria all possess very
efficient \emph{light-harvesting antennae} that facilitate photosynthesis in
very low light. Here we construct general models of photosynthetic
light-harvesting structures to determine how an oxygenic photosystem would
perform in different irradiant spectral fluxes. We illustrate that the process
of light-harvesting, capturing energy over a large antenna and concentrating it
into a small \emph{reaction centre}, must overcome a fundamental \emph{entropic
barrier}. We show that a plant-like antenna cannot be adapted to the light from
stars of $T_{s}<3400$ K, as increasing antenna size offers diminishing returns
on light-harvesting. This can be overcome if one introduces a slight
\emph{enthalpic gradient}, to the antenna. Interestingly, this strategy appears
to have been adopted by Earth's oxygenic cyanobacteria, and we conclude that
\emph{bacterial} oxygenic photosynthesis is feasible around even the lowest
mass M-dwarf stars.",http://arxiv.org/pdf/2309.12845v1
2309.12843v1,math.NT,On the discriminator of Lucas sequences. II,2023-09-22 13:07:40+00:00,"The family of Shallit sequences consists of the Lucas sequences satisfying
the recurrence $U_{n+2}(k)=(4k+2)U_{n+1}(k) -U_n(k),$ with initial values
$U_0(k)=0$ and $U_1(k)=1$ and with $k\ge 1$ arbitrary. For every fixed $k$ the
integers $\{U_n(k)\}_{n\ge 0}$ are distinct, and hence for every $n\ge 1$ there
exists a smallest integer $D_k(n)$, called discriminator, such that
$U_0(k),U_1(k),\ldots,U_{n-1}(k)$ are pairwise incongruent modulo $D_k(n).$ In
part I it was proved that there exists a constant $n_k$ such that $D_{k}(n)$
has a simple characterization for every $n\ge n_k$. Here, we study the values
not following this characterization and provide an upper bound for $n_k$ using
Matveev's theorem and the Koksma-Erdos-Tur\'an inequality. We completely
determine the discriminator $D_{k}(n)$ for every $n\ge 1$ and a set of integers
$k$ of natural density $68/75$. We also correct an omission in the statement of
Theorem 3 in part I.",http://arxiv.org/pdf/2309.12843v1
2309.12842v1,cs.CV,SRFNet: Monocular Depth Estimation with Fine-grained Structure via Spatial Reliability-oriented Fusion of Frames and Events,2023-09-22 12:59:39+00:00,"Monocular depth estimation is a crucial task to measure distance relative to
a camera, which is important for applications, such as robot navigation and
self-driving. Traditional frame-based methods suffer from performance drops due
to the limited dynamic range and motion blur. Therefore, recent works leverage
novel event cameras to complement or guide the frame modality via frame-event
feature fusion. However, event streams exhibit spatial sparsity, leaving some
areas unperceived, especially in regions with marginal light changes.
Therefore, direct fusion methods, e.g., RAMNet, often ignore the contribution
of the most confident regions of each modality. This leads to structural
ambiguity in the modality fusion process, thus degrading the depth estimation
performance. In this paper, we propose a novel Spatial Reliability-oriented
Fusion Network (SRFNet), that can estimate depth with fine-grained structure at
both daytime and nighttime. Our method consists of two key technical
components. Firstly, we propose an attention-based interactive fusion (AIF)
module that applies spatial priors of events and frames as the initial masks
and learns the consensus regions to guide the inter-modal feature fusion. The
fused feature are then fed back to enhance the frame and event feature
learning. Meanwhile, it utilizes an output head to generate a fused mask, which
is iteratively updated for learning consensual spatial priors. Secondly, we
propose the Reliability-oriented Depth Refinement (RDR) module to estimate
dense depth with the fine-grained structure based on the fused features and
masks. We evaluate the effectiveness of our method on the synthetic and
real-world datasets, which shows that, even without pretraining, our method
outperforms the prior methods, e.g., RAMNet, especially in night scenes. Our
project homepage: https://vlislab22.github.io/SRFNet.",http://arxiv.org/pdf/2309.12842v1
2309.12840v1,cond-mat.supr-con,Nature of the magnetic coupling in infinite-layer nickelates versus cuprates,2023-09-22 12:54:36+00:00,"In contrast to the cuprates, where the proximity of antiferromagnetism (AFM)
and superconductivity is well established, first indications for AFM
interactions in superconducting infinite-layer nickelates were only recently
obtained. Here, we explore, based on first-principles simulations, the nature
of the magnetic coupling in NdNiO2 as a function of the on-site Coulomb and
exchange interaction, varying the explicit hole doping and the treatment of the
Nd $4f$ electrons. The $U$-$J$ phase diagrams for undoped nickelates and
cuprates indicate $G$-type ordering, yet show different $U$ dependency. By
either Sr hole doping or explicit treatment of the Nd $4f$ electrons, we find a
transition to a Ni $C$-type AFM ground state. We trace the effect of Sr doping
back to a distinct accommodation of the holes by the Ni versus Cu $e_g$
orbitals. The interaction between Nd $4f$ and Ni $3d$ states stabilizes
$C$-type AFM order on both sublattices. Though spin-orbit interactions induce a
band splitting near the Fermi energy, the bad-metal state is retained even
under epitaxial strain. These results establish the distinct role of the
magnetic interactions in the nickelates versus the cuprates and suggest the
former as a unique platform to investigate the relation to unconventional
superconductivity.",http://arxiv.org/pdf/2309.12840v1
2309.12837v1,math.DS,Pr-feuilletages de co-degr $1$ sur $\mathbb{P}^{2}_{\mathbb{C}}$ ayant une transforme de Legendre plate,2023-09-22 12:52:53+00:00,"A pre-foliation $\mathscr{F}=\ell\boxtimes\mathcal{F}$ of co-degree $1$ and
degree $d$ on $\mathbb{P}^{2}_{\mathbb{C}}$ is the data of a line $\ell$ of
$\mathbb{P}^{2}_{\mathbb{C}}$ and a holomorphic foliation $\mathcal{F}$ on
$\mathbb{P }^{2}_{\mathbb{C}}$ of degree $d-1.$ We study pre-foliations of
co-degree $1$ on $\mathbb{P}^{2}_{\mathbb{ C}}$ with a flat Legendre transform
(dual web). After having established some general results on the flatness of
the dual $d$-web of a homogeneous pre-foliation of co-degree $1$ and degree
$d$, we describe some explicit examples and we show that up to automorphism of
$\mathbb{P}^{2}_{\mathbb{C}}$ there are two families and six examples of
homogeneous pre-foliations of co-degree $1$ and degree $3$ on $\mathbb
{P}^{2}_{\mathbb{C}}$ with a flat dual web. This allows us to prove an analogue
for pre-foliations of co-degree $1$ and degree~$3$ of a result, obtained in
collaboration with D. Mar\'{\i}n, on foliations of degree $3$ with
non-degenerate singularities and a flat Legendre transform. We also show that
the dual web of a reduced convex pre-foliation of co-degree $1$ on
$\mathbb{P}^{2}_{\mathbb{C}}$ is flat. This is an analogue of a result on
foliations of $\mathbb{P}^{2}_{\mathbb{C}}$ due to D. Mar\'{\i}n and J. V.
Pereira.",http://arxiv.org/pdf/2309.12837v1
2309.12834v1,math.ST,A functional central limit theorem for the K-function with an estimated intensity function,2023-09-22 12:46:09+00:00,"The $K$-function is arguably the most important functional summary statistic
for spatial point processes. It is used extensively for goodness-of-fit testing
and in connection with minimum contrast estimation for parametric spatial point
process models. It is thus pertinent to understand the asymptotic properties of
estimates of the $K$-function. In this paper we derive the functional
asymptotic distribution for the $K$-function estimator. Contrary to previous
papers on functional convergence we consider the case of an inhomogeneous
intensity function. We moreover handle the fact that practical $K$-function
estimators rely on plugging in an estimate of the intensity function. This
removes two serious limitations of the existing literature.",http://arxiv.org/pdf/2309.12834v1
2309.12828v1,eess.SP,Multiple Satellites Collaboration for Joint Code-aided CFOs and CPOs Estimation,2023-09-22 12:28:30+00:00,"Low Earth Orbit (LEO) satellites are being extensively researched in the
development of secure Internet of Remote Things (IoRT). In scenarios with
miniaturized terminals, the limited transmission power and long transmission
distance often lead to low Signal-to-Noise Ratio (SNR) at the satellite
receiver, which degrades communication performance. A solution to address this
issue is the utilization of cooperative satellites, which can combine signals
received from multiple satellites, thereby significantly improve SNR. However,
in order to maximize the combination gain, the signal coherent combining is
necessary, which requires the carrier frequency and phase of each receiving
signal to be aligned.
  Under low SNR circumstances, carrier parameter estimation can be a
significant challenge, especially for short burst transmission with no training
sequence. In order to tackle it, we propose an iterative code-aided estimation
algorithm for joint Carrier Frequency Offset (CFO) and Carrier Phase Offset
(CPO). The Cram\'er-Rao Lower Bound (CRLB) is suggested as the limit on the
parameter estimation performance. Simulation results demonstrate that the
proposed algorithm can approach Bit Error Rate (BER) performance bound within
0.4 dB with regards to four-satellite collaboration.",http://arxiv.org/pdf/2309.12828v1
2309.12824v1,physics.app-ph,"Direct in- and out-of-plane writing of metals on insulators by electron-beam-enabled, confined electrodeposition with submicrometer feature size",2023-09-22 12:25:57+00:00,"Additive microfabrication processes based on localized electroplating enable
the one-step deposition of micro-scale metal structures with outstanding
performance, e.g. high electrical conductivity and mechanical strength. They
are therefore evaluated as an exciting and enabling addition to the existing
repertoire of microfabrication technologies. Yet, electrochemical processes are
generally restricted to conductive or semiconductive substrates, precluding
their application in the manufacturing of functional electric devices where
direct deposition onto insulators is often required. Here, we demonstrate the
direct, localized electrodeposition of copper on a variety of insulating
substrates, namely Al2O3, glass and flexible polyethylene, enabled by
electron-beam-induced reduction in a highly confined liquid electrolyte
reservoir. The nanometer-size of the electrolyte reservoir, fed by
electrohydrodynamic ejection, enables a minimal feature size on the order of
200 nm. The fact that the transient reservoir is established and stabilized by
electrohydrodynamic ejection rather than specialized liquid cells could offer
greater flexibility towards deposition on arbitrary substrate geometries and
materials. Installed in a low-vacuum scanning electron microscope, the setup
further allows for operando, nanoscale observation and analysis of the
manufacturing process.",http://arxiv.org/pdf/2309.12824v1
2309.12817v1,hep-ex,Search for flavor-changing neutral $tqH$ interactions with $H\rightarrow $ in $pp$ collisions at $\sqrt{s}$ = 13 TeV using the ATLAS detector,2023-09-22 12:15:23+00:00,"A search for flavour-changing neutral interactions involving the top quark,
the Higgs boson and an up-type quark $q$ ($q = c, u$) is presented. The
proton-proton collision data set used, with an integrated luminosity of 139
fb$^{-1}$, was collected at $\sqrt{s} = 13$\~TeV by the ATLAS experiment at the
Large Hadron Collider. Both the decay process $t \to qH$ and the production
process $pp \to tH$, with the Higgs boson decaying into two photons, are
investigated. No significant excess is observed and upper limits are set on the
$t\rightarrow cH$ and the $t\rightarrow uH$ branching ratios of $4.3\times
10^{-4}$ and $3.8\times 10^{-4}$, respectively, at the 95% confidence level,
while the expected limits in the absence of signal are $4.7\times 10^{-4}$ and
$3.9\times 10^{-4}$. Combining this search with ATLAS searches in the $H \to
\tau^+\tau^-$ and $H \to b\bar{b}$ final states yields observed (expected)
upper limits on the $t\to cH$ branching ratio of $5.8\times 10^{-4}\ (3.0\times
10^{-4})$ at the 95% confidence level. The corresponding observed (expected)
upper limit on the $t\rightarrow uH$ branching ratio is $4.0 \times 10^{-4}\
(2.4 \times 10^{-4})$",http://arxiv.org/pdf/2309.12817v1
2309.12810v1,cs.CL,StyloMetrix: An Open-Source Multilingual Tool for Representing Stylometric Vectors,2023-09-22 11:53:47+00:00,"This work aims to provide an overview on the open-source multilanguage tool
called StyloMetrix. It offers stylometric text representations that cover
various aspects of grammar, syntax and lexicon. StyloMetrix covers four
languages: Polish as the primary language, English, Ukrainian and Russian. The
normalized output of each feature can become a fruitful course for machine
learning models and a valuable addition to the embeddings layer for any deep
learning algorithm. We strive to provide a concise, but exhaustive overview on
the application of the StyloMetrix vectors as well as explain the sets of the
developed linguistic features. The experiments have shown promising results in
supervised content classification with simple algorithms as Random Forest
Classifier, Voting Classifier, Logistic Regression and others. The deep
learning assessments have unveiled the usefulness of the StyloMetrix vectors at
enhancing an embedding layer extracted from Transformer architectures. The
StyloMetrix has proven itself to be a formidable source for the machine
learning and deep learning algorithms to execute different classification
tasks.",http://arxiv.org/pdf/2309.12810v1
2309.12808v1,cs.CL,ChatPRCS: A Personalized Support System for English Reading Comprehension based on ChatGPT,2023-09-22 11:46:44+00:00,"As a common approach to learning English, reading comprehension primarily
entails reading articles and answering related questions. However, the
complexity of designing effective exercises results in students encountering
standardized questions, making it challenging to align with individualized
learners' reading comprehension ability. By leveraging the advanced
capabilities offered by large language models, exemplified by ChatGPT, this
paper presents a novel personalized support system for reading comprehension,
referred to as ChatPRCS, based on the Zone of Proximal Development theory.
ChatPRCS employs methods including reading comprehension proficiency
prediction, question generation, and automatic evaluation, among others, to
enhance reading comprehension instruction. First, we develop a new algorithm
that can predict learners' reading comprehension abilities using their
historical data as the foundation for generating questions at an appropriate
level of difficulty. Second, a series of new ChatGPT prompt patterns is
proposed to address two key aspects of reading comprehension objectives:
question generation, and automated evaluation. These patterns further improve
the quality of generated questions. Finally, by integrating personalized
ability and reading comprehension prompt patterns, ChatPRCS is systematically
validated through experiments. Empirical results demonstrate that it provides
learners with high-quality reading comprehension questions that are broadly
aligned with expert-crafted questions at a statistical level.",http://arxiv.org/pdf/2309.12808v1
2309.12805v1,eess.IV,Automatic view plane prescription for cardiac magnetic resonance imaging via supervision by spatial relationship between views,2023-09-22 11:36:42+00:00,"Background: View planning for the acquisition of cardiac magnetic resonance
(CMR) imaging remains a demanding task in clinical practice. Purpose: Existing
approaches to its automation relied either on an additional volumetric image
not typically acquired in clinic routine, or on laborious manual annotations of
cardiac structural landmarks. This work presents a clinic-compatible,
annotation-free system for automatic CMR view planning. Methods: The system
mines the spatial relationship, more specifically, locates the intersecting
lines, between the target planes and source views, and trains deep networks to
regress heatmaps defined by distances from the intersecting lines. The
intersection lines are the prescription lines prescribed by the technologists
at the time of image acquisition using cardiac landmarks, and retrospectively
identified from the spatial relationship. As the spatial relationship is
self-contained in properly stored data, the need for additional manual
annotation is eliminated. In addition, the interplay of multiple target planes
predicted in a source view is utilized in a stacked hourglass architecture to
gradually improve the regression. Then, a multi-view planning strategy is
proposed to aggregate information from the predicted heatmaps for all the
source views of a target plane, for a globally optimal prescription, mimicking
the similar strategy practiced by skilled human prescribers. Results: The
experiments include 181 CMR exams. Our system yields the mean angular
difference and point-to-plane distance of 5.68 degrees and 3.12 mm,
respectively. It not only achieves superior accuracy to existing approaches
including conventional atlas-based and newer deep-learning-based in prescribing
the four standard CMR planes but also demonstrates prescription of the first
cardiac-anatomy-oriented plane(s) from the body-oriented scout.",http://arxiv.org/pdf/2309.12805v1
2309.12804v1,cs.CV,Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning,2023-09-22 11:35:10+00:00,"Coral reefs are among the most diverse ecosystems on our planet, and are
depended on by hundreds of millions of people. Unfortunately, most coral reefs
are existentially threatened by global climate change and local anthropogenic
pressures. To better understand the dynamics underlying deterioration of reefs,
monitoring at high spatial and temporal resolution is key. However,
conventional monitoring methods for quantifying coral cover and species
abundance are limited in scale due to the extensive manual labor required.
Although computer vision tools have been employed to aid in this process, in
particular SfM photogrammetry for 3D mapping and deep neural networks for image
segmentation, analysis of the data products creates a bottleneck, effectively
limiting their scalability. This paper presents a new paradigm for mapping
underwater environments from ego-motion video, unifying 3D mapping systems that
use machine learning to adapt to challenging conditions under water, combined
with a modern approach for semantic segmentation of images. The method is
exemplified on coral reefs in the northern Gulf of Aqaba, Red Sea,
demonstrating high-precision 3D semantic mapping at unprecedented scale with
significantly reduced required labor costs: a 100 m video transect acquired
within 5 minutes of diving with a cheap consumer-grade camera can be fully
automatically analyzed within 5 minutes. Our approach significantly scales up
coral reef monitoring by taking a leap towards fully automatic analysis of
video transects. The method democratizes coral reef transects by reducing the
labor, equipment, logistics, and computing cost. This can help to inform
conservation policies more efficiently. The underlying computational method of
learning-based Structure-from-Motion has broad implications for fast low-cost
mapping of underwater environments other than coral reefs.",http://arxiv.org/pdf/2309.12804v1
2309.12802v1,cs.SD,Deepfake audio as a data augmentation technique for training automatic speech to text transcription models,2023-09-22 11:33:03+00:00,"To train transcriptor models that produce robust results, a large and diverse
labeled dataset is required. Finding such data with the necessary
characteristics is a challenging task, especially for languages less popular
than English. Moreover, producing such data requires significant effort and
often money. Therefore, a strategy to mitigate this problem is the use of data
augmentation techniques. In this work, we propose a framework that approaches
data augmentation based on deepfake audio. To validate the produced framework,
experiments were conducted using existing deepfake and transcription models. A
voice cloner and a dataset produced by Indians (in English) were selected,
ensuring the presence of a single accent in the dataset. Subsequently, the
augmented data was used to train speech to text models in various scenarios.",http://arxiv.org/pdf/2309.12802v1
2309.12801v1,astro-ph.HE,The role of electron capture decay in the precision era of Galactic cosmic-ray data,2023-09-22 11:29:47+00:00,"Electron capture (EC) decay relies on attachment and stripping
cross-sections, that in turn, depend on the atomic number of the nucleus. We
revisit the impact of EC decay in the context of the high-precision cosmic-ray
fluxes measured by the AMS-02 experiment. We derive the solution of the
steady-state fluxes in a 1D thin disk model including EC decay. We compare our
results with relevant elemental and isotopic fluxes and evaluate the impact of
this process, given the precision of recent AMS-02, ACE-CRIS, SuperTIGER, and
Voyager data. We find this impact to be at the level or larger than the
precision of recently collected data for several species, e.g. $_{31}$Ga and
$_{33}$As, indicating that EC decay must be properly taken into account in the
calculation.",http://arxiv.org/pdf/2309.12801v1
2309.12798v1,physics.soc-ph,Foundations of continuous agent-based modelling frameworks for pedestrian dynamics and their implications,2023-09-22 11:27:23+00:00,"This paper addresses the theoretical foundations of pedestrian models for
crowd dynamics. While the topic gains momentum, widely different mathematical
approaches are actually in use, even if we only consider continuous agent-based
models. To clarify their underpinning, we first rephrase the common
hierarchical decomposition into strategic, tactical, and operational levels and
show the practical interest in preserving the continuity between the latter two
levels by working with a floor field, rather than way-points. Turning to local
navigation, we clarify how three archetypical approaches, namely, reactive
models, anticipatory models based on the idea of times to collision
(exemplified by the recently proposed ANDA model), and game theory, differ in
their extrapolation of future trajectories, and insist on the oft-overlooked
distinction between processes pertaining to decision-making and mechanical
effects in dense settings. The differences are illustrated with a comparison of
the numerical predictions of instances of these models in the simple scenario
of head-on collision avoidance between agents, by varying the walking speed and
the reaction times, notably.",http://arxiv.org/pdf/2309.12798v1
2309.12797v1,cond-mat.str-el,Probing the strongly correlated magnetic state of Co$_2$C nanoparticles at low temperatures using $$SR,2023-09-22 11:25:05+00:00,"Co$_2$C nanoparticles (NPs) are amongst transition metal carbides whose
magnetic properties have not been well explored. A recent study by Nirmal Roy
et al. [1] showed that a collection of Co$_2$C NPs exhibit an exchange bias
(EB) effect below T$_{EB}$ = 50 K and also a spin glass (SG) state below
T$_{SG}$ = 5 K. We use magnetic, electrical transport, specific heat, and muon
spin rotation ($\mu$SR) measurements to explore further the magnetic properties
of these NPs. We uncover the onset of Kondo localization at Kondo temperature
T$_K$ (= 40.1 K), near the onset of EB effect. A crossover from the
Kondo-screened scenario to an RKKY interaction-dominated regime is also
observed for T < T$_K$. Specific heat measurements confirm Kondo localization
and heavy fermionic nature in Co$_2$C at low T. At low T, zero field $\mu$SR
spectra reveal a dominant magnetically disordered fraction with slow relaxation
and a smaller fraction with short-range order exhibiting fast relaxation, with
no evidence of long-range magnetic order. We observe an increase in this fast
relaxation rate between T$_{EB}$ and T$_{SG}$, suggesting a slowing down of the
fluctuating local magnetic environment around muons. Transverse field $\mu$SR
spectra show the emergence of a stable, multi-peaked local magnetic field
distribution below T$_{EB}$. Longitudinal field $\mu$SR spectra shows distinct
changes in the dynamics of fluctuations suggesting the presence of a frozen
glassy like state below 6 K. Our results suggest that below T$_{EB}$, Co$_2$C
NPs pellet develops a magnetic interface, separating disordered and short-range
order fractions. The Exchange interaction that sets in below T$_{EB}$ at the
interface couples them and suppresses the fluctuations. With the suppression of
magnetic fluctuations below T$_{EB}$, strong correlation effects in the
electronic state of Co$_2$C lead to Kondo localization.",http://arxiv.org/pdf/2309.12797v1
2309.12796v1,eess.SP,"Alteration of skeletal muscle energy metabolism assessed by 31P MRS in clinical routine, part 1: Advanced Quality Control pipeline",2023-09-22 11:24:25+00:00,"Background: Implementing a standardized 31P-MRS dynamic acquisition protocol
to evaluate skeletal muscle energy metabolism and monitor muscle
fatigability1,2, while being compatible with various longitudinal clinical
studies on diversified patient cohorts, requires a high level of technicality
and expertise. Furthermore, processing data to obtain reliable results also
demands a great degree of expertise from the operator. In this two-part
article, we present an advanced quality control approach for data acquired
using a dynamic 31P-MRS protocol. The aim is to provide decision support to the
operator in order to assist in data processing and obtain reliable results
based on objective criteria. We present first in part one, an advanced data
quality control (QC) approach of a dynamic 31P-MRS protocol. Part two is an
impact study demonstrating the added value of the QC approach to explore
clinical results derived from two patient populations with significant fatigue:
COVID19 and multiple sclerosis (MS). Experimental: 31P-MRS was performed on a
3T clinical MRI in 175 subjects from clinical and healthy control populations
conducted in a University Hospital. An advanced data QC Score (QCS) was
developed using multiple objective criteria. The criteria were based on current
recommendations from the literature enriched by new proposals based on clinical
experience. The QCS was designed to indicate valid and corrupt data and guide
necessary objective data editing to extract as much valid physiological data as
possible. Dynamic acquisitions using an MR-compatible ergometer ran over a
rest(40s), exercise(2min), and a recovery phase(6min). Results: Using QCS
enabled rapid identification of subjects with data anomalies allowing the user
to correct the data series or reject them partially or entirely as well as
identify fully valid datasets. Overall, the use of the QCS resulted in the
automatic classification of 45% of the subjects including 58 participants that
had data with no criterion violation and 21 participants with violations that
resulted in the rejection of all dynamic data. The remaining datasets were
inspected manually with guidance allowing acceptance of full datasets from an
additional 80 participants and recovery phase data from an additional 16
subjects. Overall, more anomalies occurred with patient data (35% of datasets)
compared to healthy controls (15% of datasets). Conclusion: This paper
describes typical difficulties encountered during the dynamic acquisition of
31P-MRS. Based on these observations, a standardized data quality control
pipeline was created and implemented in both healthy and patient populations.
The QC scoring ensures a standardized data rejection procedure and rigorous
objective analysis of dynamic 31P-MRS data obtained from patients. The
contribution of this methodology contributes to efforts made to standardize the
practices of the 31P-MRS that has been underway for a decade, with the ultimate
goal of making it an empowered tool for clinical research.",http://arxiv.org/pdf/2309.12796v1
2309.12792v1,eess.AS,DurIAN-E: Duration Informed Attention Network For Expressive Text-to-Speech Synthesis,2023-09-22 11:06:04+00:00,"This paper introduces an improved duration informed attention neural network
(DurIAN-E) for expressive and high-fidelity text-to-speech (TTS) synthesis.
Inherited from the original DurIAN model, an auto-regressive model structure in
which the alignments between the input linguistic information and the output
acoustic features are inferred from a duration model is adopted. Meanwhile the
proposed DurIAN-E utilizes multiple stacked SwishRNN-based Transformer blocks
as linguistic encoders. Style-Adaptive Instance Normalization (SAIN) layers are
exploited into frame-level encoders to improve the modeling ability of
expressiveness. A denoiser incorporating both denoising diffusion probabilistic
model (DDPM) for mel-spectrograms and SAIN modules is conducted to further
improve the synthetic speech quality and expressiveness. Experimental results
prove that the proposed expressive TTS model in this paper can achieve better
performance than the state-of-the-art approaches in both subjective mean
opinion score (MOS) and preference tests.",http://arxiv.org/pdf/2309.12792v1
2309.12790v1,cs.CV,NOC: High-Quality Neural Object Cloning with 3D Lifting of Segment Anything,2023-09-22 11:02:57+00:00,"With the development of the neural field, reconstructing the 3D model of a
target object from multi-view inputs has recently attracted increasing
attention from the community. Existing methods normally learn a neural field
for the whole scene, while it is still under-explored how to reconstruct a
certain object indicated by users on-the-fly. Considering the Segment Anything
Model (SAM) has shown effectiveness in segmenting any 2D images, in this paper,
we propose Neural Object Cloning (NOC), a novel high-quality 3D object
reconstruction method, which leverages the benefits of both neural field and
SAM from two aspects. Firstly, to separate the target object from the scene, we
propose a novel strategy to lift the multi-view 2D segmentation masks of SAM
into a unified 3D variation field. The 3D variation field is then projected
into 2D space and generates the new prompts for SAM. This process is iterative
until convergence to separate the target object from the scene. Then, apart
from 2D masks, we further lift the 2D features of the SAM encoder into a 3D SAM
field in order to improve the reconstruction quality of the target object. NOC
lifts the 2D masks and features of SAM into the 3D neural field for
high-quality target object reconstruction. We conduct detailed experiments on
several benchmark datasets to demonstrate the advantages of our method. The
code will be released.",http://arxiv.org/pdf/2309.12790v1
2309.12784v1,cs.RO,Learning to Walk and Fly with Adversarial Motion Priors,2023-09-22 10:51:49+00:00,"Robot multimodal locomotion encompasses the ability to transition between
walking and flying, representing a significant challenge in robotics. This work
presents an approach that enables automatic smooth transitions between legged
and aerial locomotion. Leveraging the concept of Adversarial Motion Priors, our
method allows the robot to imitate motion datasets and accomplish the desired
task without the need for complex reward functions. The robot learns walking
patterns from human-like gaits and aerial locomotion patterns from motions
obtained using trajectory optimization. Through this process, the robot adapts
the locomotion scheme based on environmental feedback using reinforcement
learning, with the spontaneous emergence of mode-switching behavior. The
results highlight the potential for achieving multimodal locomotion in aerial
humanoid robotics through automatic control of walking and flying modes, paving
the way for applications in diverse domains such as search and rescue,
surveillance, and exploration missions. This research contributes to advancing
the capabilities of aerial humanoid robots in terms of versatile locomotion in
various environments.",http://arxiv.org/pdf/2309.12784v1
2309.12783v1,cs.NI,Multi-objective Optimization of Space-Air-Ground Integrated Network Slicing Relying on a Pair of Central and Distributed Learning Algorithms,2023-09-22 10:51:45+00:00,"As an attractive enabling technology for next-generation wireless
communications, network slicing supports diverse customized services in the
global space-air-ground integrated network (SAGIN) with diverse resource
constraints. In this paper, we dynamically consider three typical classes of
radio access network (RAN) slices, namely high-throughput slices, low-delay
slices and wide-coverage slices, under the same underlying physical SAGIN. The
throughput, the service delay and the coverage area of these three classes of
RAN slices are jointly optimized in a non-scalar form by considering the
distinct channel features and service advantages of the terrestrial, aerial and
satellite components of SAGINs. A joint central and distributed multi-agent
deep deterministic policy gradient (CDMADDPG) algorithm is proposed for solving
the above problem to obtain the Pareto optimal solutions. The algorithm first
determines the optimal virtual unmanned aerial vehicle (vUAV) positions and the
inter-slice sub-channel and power sharing by relying on a centralized unit.
Then it optimizes the intra-slice sub-channel and power allocation, and the
virtual base station (vBS)/vUAV/virtual low earth orbit (vLEO) satellite
deployment in support of three classes of slices by three separate distributed
units. Simulation results verify that the proposed method approaches the
Pareto-optimal exploitation of multiple RAN slices, and outperforms the
benchmarkers.",http://arxiv.org/pdf/2309.12783v1
2309.12777v1,cond-mat.soft,The role of the nucleus for cell mechanics: an elastic phase field approach,2023-09-22 10:39:11+00:00,"The nucleus of eukaryotic cells typically makes up around 30 % of the cell
volume and tends to be up to ten times stiffer than the surrounding cytoplasm.
Therefore it is an important element for cell mechanics, but a quantitative
understanding of its mechanical role is largely missing. Here we demonstrate
that elastic phase fields can be used to describe dynamical cell processes in
adhesive or confining environments in which the nucleus plays an important
role. We first introduce and verify our computational method and then study
several applications of large relevance. For cells on adhesive patterns, we
find that nuclear stress is shielded by the adhesive pattern. For cell
compression between two parallel plates, we obtain force-compression curves
that allow us to extract an effective modulus for the cell-nucleus composite.
For micropipette aspiration, the effect of the nucleus on the effective modulus
is found to be much weaker, highlighting the complicated interplay between
extracellular geometry and cell mechanics that is captured by our approach.",http://arxiv.org/pdf/2309.12777v1
2309.12774v1,quant-ph,Dynamical subset sampling of quantum error correcting protocols,2023-09-22 10:32:20+00:00,"Quantum error correcting (QEC) stabilizer codes enable protection of quantum
information against errors during storage and processing. Simulation of noisy
QEC codes is used to identify the noise parameters necessary for advantageous
operation of logical qubits in realistic quantum computing architectures.
Typical quantum error correction techniques contain intermediate measurements
and classical feedback that determine the actual noisy circuit sequence in an
instance of performing the protocol. Dynamical subset sampling enables
efficient simulation of such non-deterministic quantum error correcting
protocols for any type of quantum circuit and incoherent noise of low strength.
As an importance sampling technique, dynamical subset sampling allows one to
effectively make use of computational resources to only sample the most
relevant sequences of quantum circuits in order to estimate a protocol's
logical failure rate with well-defined error bars. We demonstrate the
capabilities of dynamical subset sampling with examples from fault-tolerant
(FT) QEC. We show that, in a typical stabilizer simulation with incoherent
Pauli noise of strength $p = 10^{-3}$, our method can reach a required sampling
accuracy on the logical failure rate with two orders of magnitude fewer samples
than direct Monte Carlo simulation. Furthermore, dynamical subset sampling
naturally allows for efficient simulation of realistic multi-parameter noise
models describing faulty quantum processors. It can be applied not only for QEC
in the circuit model but any noisy quantum computing framework with incoherent
fault operators including measurement-based quantum computation and quantum
networks.",http://arxiv.org/pdf/2309.12774v1
2309.12767v1,cs.CL,Furthest Reasoning with Plan Assessment: Stable Reasoning Path with Retrieval-Augmented Large Language Models,2023-09-22 10:15:13+00:00,"Large Language Models (LLMs), acting as a powerful reasoner and generator,
exhibit extraordinary performance across various natural language tasks, such
as question answering (QA). Among these tasks, Multi-Hop Question Answering
(MHQA) stands as a widely discussed category, necessitating seamless
integration between LLMs and the retrieval of external knowledge. Existing
methods employ LLM to generate reasoning paths and plans, and utilize IR to
iteratively retrieve related knowledge, but these approaches have inherent
flaws. On one hand, Information Retriever (IR) is hindered by the low quality
of generated queries by LLM. On the other hand, LLM is easily misguided by the
irrelevant knowledge by IR. These inaccuracies, accumulated by the iterative
interaction between IR and LLM, lead to a disaster in effectiveness at the end.
To overcome above barriers, in this paper, we propose a novel pipeline for MHQA
called Furthest-Reasoning-with-Plan-Assessment (FuRePA), including an improved
framework (Furthest Reasoning) and an attached module (Plan Assessor). 1)
Furthest reasoning operates by masking previous reasoning path and generated
queries for LLM, encouraging LLM generating chain of thought from scratch in
each iteration. This approach enables LLM to break the shackle built by
previous misleading thoughts and queries (if any). 2) The Plan Assessor is a
trained evaluator that selects an appropriate plan from a group of candidate
plans proposed by LLM. Our methods are evaluated on three highly recognized
public multi-hop question answering datasets and outperform state-of-the-art on
most metrics (achieving a 10%-12% in answer accuracy).",http://arxiv.org/pdf/2309.12767v1
2309.12763v1,eess.AS,"Reduce, Reuse, Recycle: Is Perturbed Data better than Other Language augmentation for Low Resource Self-Supervised Speech Models",2023-09-22 10:09:09+00:00,"Self-supervised representation learning (SSRL) has improved the performance
on downstream phoneme recognition versus supervised models. Training SSRL
models requires a large amount of pre-training data and this poses a challenge
for low resource languages. A common approach is transferring knowledge from
other languages. Instead, we propose to use audio augmentation to pre-train
SSRL models in a low resource condition and evaluate phoneme recognition as
downstream task. We performed a systematic comparison of augmentation
techniques, namely: pitch variation, noise addition, accented target-language
speech and other language speech. We found combined augmentations (noise/pitch)
was the best augmentation strategy outperforming accent and language knowledge
transfer. We compared the performance with various quantities and types of
pre-training data. We examined the scaling factor of augmented data to achieve
equivalent performance to models pre-trained with target domain speech. Our
findings suggest that for resource constrained languages, in-domain synthetic
augmentation can outperform knowledge transfer from accented or other language
speech.",http://arxiv.org/pdf/2309.12763v1
2309.12762v1,math.CV,Variations on a theorem by Edwards,2023-09-22 10:07:40+00:00,"We discuss two variations of Edwards' duality theorem. More precisely, we
prove one version of the theorem for cones not necessarily containing all
constant functions. In particular, we allow the functions in the cone to have a
non-empty common zero set. In the second variation, we replace suprema of point
evaluations and infima over Jensen measures by suprema of other continuous
functionals and infima over a set measures defined through a natural order
relation induced by the cone. As applications, we give some results on
propagation of discontinuities for Perron--Bremermann envelopes in hyperconvex
domains as well as a characterization of minimal elements in the order relation
mentioned above.",http://arxiv.org/pdf/2309.12762v1
2309.12759v1,astro-ph.EP,Dust Emission and Dynamics,2023-09-22 10:03:16+00:00,"When viewed from Earth, most of what we observe of a comet is dust. The
influence of solar radiation pressure on the trajectories of dust particles
depends on their cross-section to mass ratio. Hence solar radiation pressure
acts like a mass spectrometer inside a cometary tail. The appearances of
cometary dust tails have long been studied to obtain information on the dust
properties, such as characteristic particle size and initial velocity when
entering the tail. Over the past two decades, several spacecraft missions to
comets have enabled us to study the dust activity of their targets at much
greater resolution than is possible with a telescope on Earth or in near-Earth
space, and added detail to the results obtained by the spacecraft visiting
comet 1P/Halley in 1986. We now know that the dynamics of dust in the inner
cometary coma is complex and includes a significant fraction of particles that
will eventually fall back to the surface. The filamented structure of the
near-surface coma is thought to result from a combination of topographic
focussing of the gas flow, inhomogeneous distribution of activity across the
surface, and projection effects. It is possible that some
larger-than-centimetre debris contains ice when lifted from the surface, which
can affect its motion. Open questions remain regarding the microphysics of the
process that leads to the detachment and lifting of dust from the surface, the
evolution of the dust while travelling away from the nucleus, and the extent to
which information on the nucleus activity can be retrieved from remote
observations of the outer coma and tail.",http://arxiv.org/pdf/2309.12759v1
2309.12753v1,math.AG,"Bordifications of the moduli spaces of tropical curves and abelian varieties, and unstable cohomology of $\mathrm{GL}_g(\mathbb{Z})$ and $\mathrm{SL}_g(\mathbb{Z})$",2023-09-22 09:53:49+00:00,"We construct bordifications of the moduli spaces of tropical curves and of
tropical abelian varieties, and show that the tropical Torelli map extends to
their bordifications. We prove that the classical bi-invariant differential
forms studied by Cartan extend to these bordifications by studying their
behaviour at infinity, and consequently deduce infinitely many new non-zero
unstable cohomology classes in the cohomology of the general and special linear
groups $\mathrm{GL}_g(\mathbb{Z})$ and $\mathrm{SL}_g(\mathbb{Z})$. In
addition, we completely determine the cohomology of the link of the moduli
space of tropical abelian varieties within a certain range, and show that it
contains the stable cohomology of the general linear group.
  In the process, we define new transcendental invariants associated to the
minimal vectors of quadratic forms, and show that part of the cohomology of the
general linear group $\mathrm{GL}_g(\mathbb{Z})$ admits the structure of a
motive.",http://arxiv.org/pdf/2309.12753v1
2309.12752v1,physics.soc-ph,Insights from exact social contagion dynamics on networks with higher-order structures,2023-09-22 09:53:35+00:00,"Recently there has been an increasing interest in studying dynamical
processes on networks exhibiting higher-order structures, such as simplicial
complexes, where the dynamics acts above and beyond dyadic interactions. Using
simulations or heuristically derived epidemic spreading models it was shown
that new phenomena can emerge, such as bi-stability/multistability. Here, we
show that such new emerging phenomena do not require complex contact patterns,
such as community structures, but naturally result from the higher-order
contagion mechanisms. We show this by deriving an exact higher-order SIS model
and its limiting mean-field equivalent for fully connected simplicial
complexes. Going beyond previous results, we also give the global bifurcation
picture for networks with 3- and 4-body interactions, with the latter allowing
for two non-trivial stable endemic steady states. Differently from previous
approaches, we are able to study systems featuring interactions of arbitrary
order. In addition, we characterise the contributions from higher-order
infections to the endemic equilibrium as perturbations of the pairwise
baseline, finding that these diminish as the pairwise rate of infection
increases. Our approach represents a first step towards a principled
understanding of higher-order contagion processes beyond triads and opens up
further directions for analytical investigations.",http://arxiv.org/pdf/2309.12752v1
2309.12748v1,math.CO,The Reversed Zeckendorf Game,2023-09-22 09:51:13+00:00,"Zeckendorf proved that every natural number $n$ can be expressed uniquely as
a sum of non-consecutive Fibonacci numbers, called its Zeckendorf
decomposition. Baird-Smith, Epstein, Flint, and Miller created the Zeckendorf
game, a two-player game played on partitions of $n$ into Fibonacci numbers
which always terminates at a Zeckendorf decomposition, and proved that Player 2
has a winning strategy for $n\geq 3$. Since their proof was non-constructive,
other authors have studied the game to find a constructive winning strategy,
and lacking success there turned to related problems. For example, Cheigh,
Moura, Jeong, Duke, Milgrim, Miller, and Ngamlamai studied minimum and maximum
game lengths and randomly played games. We explore a new direction and
introduce the reversed Zeckendorf game, which starts at the ending state of the
Zeckendorf game and flips all the moves, so the reversed game ends with all
pieces in the first bin. We show that Player 1 has a winning strategy for $n =
F_{i+1} + F_{i-2}$ and solve various modified games.",http://arxiv.org/pdf/2309.12748v1
2309.12729v1,cs.SI,Coordinated Information Campaigns on Social Media: A Multifaceted Framework for Detection and Analysis,2023-09-22 09:25:18+00:00,"The prevalence of coordinated information campaigns in social media platforms
has significant negative consequences across various domains, including social,
political, and economic processes. This paper proposes a multifaceted framework
for detecting and analysing coordinated message promotion on social media. By
simultaneously considering features related to content, time, and network
dimensions, our framework can capture the diverse nature of coordinated
activity and identify anomalous user accounts who likely engaged in suspicious
behaviour. Unlike existing solutions that rely on specific constraints, our
approach is more flexible as it employs specialised components to extract the
significant structures within a network and to detect the most unusual
interactions. We demonstrate the effectiveness of our framework using two
Twitter datasets, the Russian Internet Research Agency (IRA), and long-term
discussions on Data Science topics. The results demonstrate our framework's
ability to isolate unusual activity from expected normal behaviour and provide
valuable insights for further qualitative investigation.",http://arxiv.org/pdf/2309.12729v1
2309.12724v1,math.NT,A note on the power sums of the number of Fibonacci partitions,2023-09-22 09:14:08+00:00,"For every nonnegative integer $n$, let $r_F(n)$ be the number of ways to
write $n$ as a sum of Fibonacci numbers, where the order of the summands does
not matter. Moreover, for all positive integers $p$ and $N$, let
\begin{equation*} S_{F}^{(p)}(N) := \sum_{n = 0}^{N - 1} \big(r_F(n)\big)^p .
\end{equation*} Chow, Jones, and Slattery determined the order of growth of
$S_{F}^{(p)}(N)$ for $p \in \{1,2\}$. We prove that, for all positive integers
$p$, there exists a real number $\lambda_p > 1$ such that \begin{equation*}
S^{(p)}_F(N) \asymp_p N^{(\log \lambda_p) /\!\log \varphi} \end{equation*} as
$N \to +\infty$. Furthermore, we show that egin{equation*} \lim_{p \to +\infty}
\lambda_p^{1/p} = \varphi^{1/2} , \end{equation*} where $\varphi := (1 +
\sqrt{5})/2$ is the golden ratio. Our proofs employ automata theory and a
result on the generalized spectral radius due to Blondel and Nesterov.",http://arxiv.org/pdf/2309.12724v1
2309.12719v1,quant-ph,An Efficient and Secure Arbitrary N-Party Quantum Key Agreement Protocol Using Bell States,2023-09-22 09:02:18+00:00,"Two quantum key agreement protocols using Bell states and Bell measurement
were recently proposed by Shukla et al.(Quantum Inf. Process. 13(11),
2391-2405, 2014). However, Zhu et al. pointed out that there are some security
flaws and proposed an improved version (Quantum Inf. Process. 14(11),
4245-4254, 2015). In this study, we will show Zhu et al.'s improvement still
exists some security problems, and its efficiency is not high enough. For
solving these problems, we utilize four Pauli operations {I, Z, X, Y } to
encode two bits instead of the original two operations {I,X} to encode one bit,
and then propose an efficient and secure arbitrary N-party quantum key
agreement protocol. In the protocol, the channel checking with decoy single
photons is introduced to avoid the eavesdropper's flip attack, and a
post-measurement mechanism is used to prevent against the collusion attack. The
security analysis shows the present protocol can guarantee the correctness,
security, privacy and fairness of quantum key agreement.",http://arxiv.org/pdf/2309.12719v1
2309.12717v1,cs.CV,Transformer-based Image Compression with Variable Image Quality Objectives,2023-09-22 08:58:28+00:00,"This paper presents a Transformer-based image compression system that allows
for a variable image quality objective according to the user's preference.
Optimizing a learned codec for different quality objectives leads to
reconstructed images with varying visual characteristics. Our method provides
the user with the flexibility to choose a trade-off between two image quality
objectives using a single, shared model. Motivated by the success of
prompt-tuning techniques, we introduce prompt tokens to condition our
Transformer-based autoencoder. These prompt tokens are generated adaptively
based on the user's preference and input image through learning a prompt
generation network. Extensive experiments on commonly used quality metrics
demonstrate the effectiveness of our method in adapting the encoding and/or
decoding processes to a variable quality objective. While offering the
additional flexibility, our proposed method performs comparably to the
single-objective methods in terms of rate-distortion performance.",http://arxiv.org/pdf/2309.12717v1
2309.12714v1,eess.AS,Unsupervised Representations Improve Supervised Learning in Speech Emotion Recognition,2023-09-22 08:54:06+00:00,"Speech Emotion Recognition (SER) plays a pivotal role in enhancing
human-computer interaction by enabling a deeper understanding of emotional
states across a wide range of applications, contributing to more empathetic and
effective communication. This study proposes an innovative approach that
integrates self-supervised feature extraction with supervised classification
for emotion recognition from small audio segments. In the preprocessing step,
to eliminate the need of crafting audio features, we employed a self-supervised
feature extractor, based on the Wav2Vec model, to capture acoustic features
from audio data. Then, the output featuremaps of the preprocessing step are fed
to a custom designed Convolutional Neural Network (CNN)-based model to perform
emotion classification. Utilizing the ShEMO dataset as our testing ground, the
proposed method surpasses two baseline methods, i.e. support vector machine
classifier and transfer learning of a pretrained CNN. comparing the propose
method to the state-of-the-art methods in SER task indicates the superiority of
the proposed method. Our findings underscore the pivotal role of deep
unsupervised feature learning in elevating the landscape of SER, offering
enhanced emotional comprehension in the realm of human-computer interactions.",http://arxiv.org/pdf/2309.12714v1
2309.12712v1,eess.AS,Big model only for hard audios: Sample dependent Whisper model selection for efficient inferences,2023-09-22 08:50:58+00:00,"Recent progress in Automatic Speech Recognition (ASR) has been coupled with a
substantial increase in the model sizes, which may now contain billions of
parameters, leading to slow inferences even with adapted hardware. In this
context, several ASR models exist in various sizes, with different inference
costs leading to different performance levels. Based on the observation that
smaller models perform optimally on large parts of testing corpora, we propose
to train a decision module, that would allow, given an audio sample, to use the
smallest sufficient model leading to a good transcription. We apply our
approach to two Whisper models with different sizes. By keeping the decision
process computationally efficient, we build a decision module that allows
substantial computational savings with reduced performance drops.",http://arxiv.org/pdf/2309.12712v1
2309.12705v1,quant-ph,"Rapid generation of high fidelity, dissipation-stabilized dimerized chain",2023-09-22 08:34:36+00:00,"Despite the many proposals to use dissipation as a resource to prepare
long-lived entangled states, the speed of such entanglement generation is
usually limited by the requirement of perturbatively small driving strengths.
We propose a new scheme to rapidly generate many-body entanglement between
multiple spins coupled to a 1D bath stabilized by the dissipation into the
bath. Our work stands in contrast to the current well known steady state
protocols for entanglement generation in spins coupled to 1D baths that take a
prohibitively long time, and exhibits a speedup over state-of-the-art protocols
by several orders of magnitude. Importantly, the protocol works even with a
local control Hamiltonian, and the timescale is independent of the system size.
Our scheme can be applied to simultaneously generate a large number of spin
dimer pairs, which can serve as a valuable resource for quantum metrology and
teleportation-based information processing.",http://arxiv.org/pdf/2309.12705v1
2309.12698v1,cond-mat.stat-mech,Non-monotonic flow variations in a TASEP-based traffic model featuring cars searching for parking,2023-09-22 08:12:33+00:00,"The Totally Asymmetric Simple Exclusion Process (TASEP) is a paradigm of
out-of-equilibrium Statistical Physics that serves as a simplistic model for
one-way vehicular traffic. Since traffic is perturbed by cars cruising for
parking in many metropolises, we introduce a variant of TASEP, dubbed SFP, in
which particles are initially cruising at a slower speed and aiming to park on
one of the sites adjacent to the main road, described by a unidimensional
lattice. After parking, they pull out at a finite rate and move at a normal
speed. We show that this model, which breaks many of the conservation rules
applicable in other TASEP variants, exhibits singular features, in particular
non-monotonic variations of the steady-state current with the injection rate
and re-entrant transitions in the phase diagram, for some range of parameters.
These features are robust to variations in the update rule and the boundary
conditions.Neither the slow speed of cruising cars nor the perturbation of the
flow due to pull-out maneuvers, taken in isolation, can rationalize these
observations. Instead, they originate in a cramming (or `paper jam') effect
which results from the coupling of these mechanisms: injecting too many cars
into the system saturates the first sites of the road, which prevents parked
cars from pulling out, thus forcing cruising cars to travel farther along the
road.These strong discrepancies with even the qualitative trends of the
baseline TASEP model highlight the importance of considering the effect of
perturbations on traffic.",http://arxiv.org/pdf/2309.12698v1
2309.12697v1,cs.CL,Semantic similarity prediction is better than other semantic similarity measures,2023-09-22 08:11:01+00:00,"Semantic similarity between natural language texts is typically measured
either by looking at the overlap between subsequences (e.g., BLEU) or by using
embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we
are only interested in measuring the semantic similarity, it is better to
directly predict the similarity using a fine-tuned model for such a task. Using
a fine-tuned model for the STS-B from the GLUE benchmark, we define the
STSScore approach and show that the resulting similarity is better aligned with
our expectations on a robust semantic similarity measure than other approaches.",http://arxiv.org/pdf/2309.12697v1
2309.12691v1,physics.soc-ph,Characterizing the temporally stable structure of community evolution in intra-urban origin-destination networks,2023-09-22 08:04:18+00:00,"Intra-urban origin-destination (OD) network communities evolve throughout the
day, indicating changing groups of closely connected regions. Under this
variation, groups of regions with high consistency of community affiliation
characterize the temporally stable structure of the evolution process, aiding
in comprehending urban dynamics. However, how to quantify this consistency and
identify these groups are open questions. In this study, we introduce the
consensus OD network to quantify the consistency of community affiliation among
regions. Furthermore, the temporally stable community decomposition method is
proposed to identify groups of regions with high internal and low external
consistency (named ""stable groups""), where each group consists of temporally
stable cores and attaching peripheries. Wuhan taxi data is used to verify our
methods. On the hourly time scale, eleven stable groups containing 82.9% of
regions are identified. This high percentage suggests that dynamic communities
can be well organized via cores. Moreover, stable groups are spatially closed
and more likely to distribute within a single district and separated by water
bodies. Cores exhibit higher POI entropy and more healthcare and shopping
services than peripheries. Our methods and empirical findings contribute to
some practical issues, such as urban area division, polycentric evaluation and
construction, and infectious disease control.",http://arxiv.org/pdf/2309.12691v1
2309.12689v1,cs.LG,AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer,2023-09-22 08:02:45+00:00,"Mixup is an effective data augmentation method that generates new augmented
samples by aggregating linear combinations of different original samples.
However, if there are noises or aberrant features in the original samples,
Mixup may propagate them to the augmented samples, leading to over-sensitivity
of the model to these outliers . To solve this problem, this paper proposes a
new Mixup method called AMPLIFY. This method uses the Attention mechanism of
Transformer itself to reduce the influence of noises and aberrant values in the
original samples on the prediction results, without increasing additional
trainable parameters, and the computational cost is very low, thereby avoiding
the problem of high resource consumption in common Mixup methods such as
Sentence Mixup . The experimental results show that, under a smaller
computational resource cost, AMPLIFY outperforms other Mixup methods in text
classification tasks on 7 benchmark datasets, providing new ideas and new ways
to further improve the performance of pre-trained models based on the Attention
mechanism, such as BERT, ALBERT, RoBERTa, and GPT. Our code can be obtained at
https://github.com/kiwi-lilo/AMPLIFY.",http://arxiv.org/pdf/2309.12689v1
2309.12688v1,cs.IT,Green Holographic MIMO Communications With A Few Transmit Radio Frequency Chains,2023-09-22 08:02:17+00:00,"Holographic multiple-input multiple-output (MIMO) communications are widely
recognized as a promising candidate for the next-generation air interface. With
holographic MIMO surface, the number of the spatial degrees-of-freedom (DoFs)
considerably increases and also significantly varies as the user moves. To
fully employ the large and varying number of spatial DoFs, the number of
equipped RF chains has to be larger than or equal to the largest number of
spatial DoFs. However, this causes much waste as radio frequency (RF) chains
(especially the transmit RF chains) are costly and power-hungry. To avoid the
heavy burden, this paper investigates green holographic MIMO communications
with a few transmit RF chains under an electromagnetic-based communication
model. We not only look at the fundamental capacity limits but also propose an
effective transmission, namely non-uniform holographic pattern modulation
(NUHPM), to achieve the capacity limit in the high signal-to-noise (SNR)
regime. The analytical result sheds light on the green evaluation of MIMO
communications, which can be realized by increasing the size of the antenna
aperture without increasing the number of transmit RF chains. Numerical results
are provided to verify our analysis and to show the great performance gain by
employing the additional spatial DoFs as modulation resources.",http://arxiv.org/pdf/2309.12688v1
2309.12686v1,physics.soc-ph,Temporal networks provide a unifying understanding of the evolution of cooperation,2023-09-22 07:59:53+00:00,"Understanding the evolution of cooperation in structured populations
represented by networks is a problem of long research interest, and a most
fundamental and widespread property of social networks related to cooperation
phenomena is that the node's degree (i.e., number of edges connected to the
node) is heterogeneously distributed. Previous results indicate that static
heterogeneous (i.e., degree-heterogeneous) networks promote cooperation in
stationarity compared to static regular (i.e., degree-homogeneous) networks if
equilibrium dynamics starting from many cooperators and defectors is employed.
However, the above conclusion reverses if we employ non-equilibrium stochastic
processes to measure the fixation probability for cooperation, i.e., the
probability that a single cooperator successfully invades a population. Here we
resolve this conundrum by analyzing the fixation of cooperation on temporal
(i.e., time-varying) networks. We theoretically prove and numerically confirm
that on both synthetic and empirical networks, contrary to the case of static
networks, temporal heterogeneous networks can promote cooperation more than
temporal regular networks in terms of the fixation probability of cooperation.
Given that the same conclusion is known for the equilibrium fraction of
cooperators on temporal networks, the present results provide a unified
understanding of the effect of temporal degree heterogeneity on promoting
cooperation across two main analytical frameworks, i.e., equilibrium and
non-equilibrium ones.",http://arxiv.org/pdf/2309.12686v1
2309.12685v1,cs.RO,eWand: A calibration framework for wide baseline frame-based and event-based camera systems,2023-09-22 07:51:17+00:00,"Accurate calibration is crucial for using multiple cameras to triangulate the
position of objects precisely. However, it is also a time-consuming process
that needs to be repeated for every displacement of the cameras. The standard
approach is to use a printed pattern with known geometry to estimate the
intrinsic and extrinsic parameters of the cameras. The same idea can be applied
to event-based cameras, though it requires extra work. By using frame
reconstruction from events, a printed pattern can be detected. A blinking
pattern can also be displayed on a screen. Then, the pattern can be directly
detected from the events. Such calibration methods can provide accurate
intrinsic calibration for both frame- and event-based cameras. However, using
2D patterns has several limitations for multi-camera extrinsic calibration,
with cameras possessing highly different points of view and a wide baseline.
The 2D pattern can only be detected from one direction and needs to be of
significant size to compensate for its distance to the camera. This makes the
extrinsic calibration time-consuming and cumbersome. To overcome these
limitations, we propose eWand, a new method that uses blinking LEDs inside
opaque spheres instead of a printed or displayed pattern. Our method provides a
faster, easier-to-use extrinsic calibration approach that maintains high
accuracy for both event- and frame-based cameras.",http://arxiv.org/pdf/2309.12685v1
2309.12684v1,cs.HC,Visualization According to Statisticians: An Interview Study on the Role of Visualization for Inferential Statistics,2023-09-22 07:47:58+00:00,"Statisticians are not only one of the earliest professional adopters of data
visualization, but also some of its most prolific users. Understanding how
these professionals utilize visual representations in their analytic process
may shed light on best practices for visual sensemaking. We present results
from an interview study involving 18 professional statisticians (19.7 years
average in the profession) on three aspects: (1) their use of visualization in
their daily analytic work; (2) their mental models of inferential statistical
processes; and (3) their design recommendations for how to best represent
statistical inferences. Interview sessions consisted of discussing inferential
statistics, eliciting participant sketches of suitable visual designs, and
finally, a design intervention with our proposed visual designs. We analyzed
interview transcripts using thematic analysis and open coding, deriving
thematic codes on statistical mindset, analytic process, and analytic toolkit.
The key findings for each aspect are as follows: (1) statisticians make
extensive use of visualization during all phases of their work (and not just
when reporting results); (2) their mental models of inferential methods tend to
be mostly visually based; and (3) many statisticians abhor dichotomous
thinking. The latter suggests that a multi-faceted visual display of
inferential statistics that includes a visual indicator of analytically
important effect sizes may help to balance the attributed epistemic power of
traditional statistical testing with an awareness of the uncertainty of
sensemaking.",http://arxiv.org/pdf/2309.12684v1
2309.12676v1,cs.CL,JCoLA: Japanese Corpus of Linguistic Acceptability,2023-09-22 07:35:45+00:00,"Neural language models have exhibited outstanding performance in a range of
downstream tasks. However, there is limited understanding regarding the extent
to which these models internalize syntactic knowledge, so that various datasets
have recently been constructed to facilitate syntactic evaluation of language
models across languages. In this paper, we introduce JCoLA (Japanese Corpus of
Linguistic Acceptability), which consists of 10,020 sentences annotated with
binary acceptability judgments. Specifically, those sentences are manually
extracted from linguistics textbooks, handbooks and journal articles, and split
into in-domain data (86 %; relatively simple acceptability judgments extracted
from textbooks and handbooks) and out-of-domain data (14 %; theoretically
significant acceptability judgments extracted from journal articles), the
latter of which is categorized by 12 linguistic phenomena. We then evaluate the
syntactic knowledge of 9 different types of Japanese language models on JCoLA.
The results demonstrated that several models could surpass human performance
for the in-domain data, while no models were able to exceed human performance
for the out-of-domain data. Error analyses by linguistic phenomena further
revealed that although neural language models are adept at handling local
syntactic dependencies like argument structure, their performance wanes when
confronted with long-distance syntactic dependencies like verbal agreement and
NPI licensing.",http://arxiv.org/pdf/2309.12676v1
2309.12672v1,cs.SD,CrossSinger: A Cross-Lingual Multi-Singer High-Fidelity Singing Voice Synthesizer Trained on Monolingual Singers,2023-09-22 07:29:10+00:00,"It is challenging to build a multi-singer high-fidelity singing voice
synthesis system with cross-lingual ability by only using monolingual singers
in the training stage. In this paper, we propose CrossSinger, which is a
cross-lingual singing voice synthesizer based on Xiaoicesing2. Specifically, we
utilize International Phonetic Alphabet to unify the representation for all
languages of the training data. Moreover, we leverage conditional layer
normalization to incorporate the language information into the model for better
pronunciation when singers meet unseen languages. Additionally, gradient
reversal layer (GRL) is utilized to remove singer biases included in lyrics
since all singers are monolingual, which indicates singer's identity is
implicitly associated with the text. The experiment is conducted on a
combination of three singing voice datasets containing Japanese Kiritan
dataset, English NUS-48E dataset, and one internal Chinese dataset. The result
shows CrossSinger can synthesize high-fidelity songs for various singers with
cross-lingual ability, including code-switch cases.",http://arxiv.org/pdf/2309.12672v1
2309.12669v1,cs.CL,HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering,2023-09-22 07:26:17+00:00,"Answering numerical questions over hybrid contents from the given tables and
text(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs)
have gained significant attention in the NLP community. With the emergence of
large language models, In-Context Learning and Chain-of-Thought prompting have
become two particularly popular research topics in this field. In this paper,
we introduce a new prompting strategy called Hybrid prompt strategy and
Retrieval of Thought for TextTableQA. Through In-Context Learning, we prompt
the model to develop the ability of retrieval thinking when dealing with hybrid
data. Our method achieves superior performance compared to the fully-supervised
SOTA on the MultiHiertt dataset in the few-shot setting.",http://arxiv.org/pdf/2309.12669v1
2309.12665v1,cs.DC,Lovelock: Towards Smart NIC-hosted Clusters,2023-09-22 07:16:42+00:00,"Traditional cluster designs were originally server-centric, and have evolved
recently to support hardware acceleration and storage disaggregation. In
applications that leverage acceleration, the server CPU performs the role of
orchestrating computation and data movement and data-intensive applications
stress the memory bandwidth. Applications that leverage disaggregation can be
adversely affected by the increased PCIe and network bandwidth resulting from
disaggregation. In this paper, we advocate for a specialized cluster design for
important data intensive applications, such as analytics, query processing and
ML training. This design, Lovelock, replaces each server in a cluster with one
or more headless smart NICs. Because smart NICs are significantly cheaper than
servers on bandwidth, the resulting cluster can run these applications without
adversely impacting performance, while obtaining cost and energy savings.",http://arxiv.org/pdf/2309.12665v1
2309.12659v1,cs.LG,OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling,2023-09-22 06:59:14+00:00,"Online updating of time series forecasting models aims to address the concept
drifting problem by efficiently updating forecasting models based on streaming
data. Many algorithms are designed for online time series forecasting, with
some exploiting cross-variable dependency while others assume independence
among variables. Given every data assumption has its own pros and cons in
online time series modeling, we propose \textbf{On}line \textbf{e}nsembling
\textbf{Net}work (OneNet). It dynamically updates and combines two models, with
one focusing on modeling the dependency across the time dimension and the other
on cross-variate dependency. Our method incorporates a reinforcement
learning-based approach into the traditional online convex programming
framework, allowing for the linear combination of the two models with
dynamically adjusted weights. OneNet addresses the main shortcoming of
classical online learning methods that tend to be slow in adapting to the
concept drift. Empirical results show that OneNet reduces online forecasting
error by more than $\mathbf{50\%}$ compared to the State-Of-The-Art (SOTA)
method. The code is available at \url{https://github.com/yfzhang114/OneNet}.",http://arxiv.org/pdf/2309.12659v1
2309.12658v1,cs.LG,Neural Operator Variational Inference based on Regularized Stein Discrepancy for Deep Gaussian Processes,2023-09-22 06:56:35+00:00,"Deep Gaussian Process (DGP) models offer a powerful nonparametric approach
for Bayesian inference, but exact inference is typically intractable,
motivating the use of various approximations. However, existing approaches,
such as mean-field Gaussian assumptions, limit the expressiveness and efficacy
of DGP models, while stochastic approximation can be computationally expensive.
To tackle these challenges, we introduce Neural Operator Variational Inference
(NOVI) for Deep Gaussian Processes. NOVI uses a neural generator to obtain a
sampler and minimizes the Regularized Stein Discrepancy in L2 space between the
generated distribution and true posterior. We solve the minimax problem using
Monte Carlo estimation and subsampling stochastic optimization techniques. We
demonstrate that the bias introduced by our method can be controlled by
multiplying the Fisher divergence with a constant, which leads to robust error
control and ensures the stability and precision of the algorithm. Our
experiments on datasets ranging from hundreds to tens of thousands demonstrate
the effectiveness and the faster convergence rate of the proposed method. We
achieve a classification accuracy of 93.56 on the CIFAR10 dataset,
outperforming SOTA Gaussian process methods. Furthermore, our method guarantees
theoretically controlled prediction error for DGP models and demonstrates
remarkable performance on various datasets. We are optimistic that NOVI has the
potential to enhance the performance of deep Bayesian nonparametric models and
could have significant implications for various practical applications",http://arxiv.org/pdf/2309.12658v1
2309.12657v1,cs.CV,Exploiting Modality-Specific Features For Multi-Modal Manipulation Detection And Grounding,2023-09-22 06:55:41+00:00,"AI-synthesized text and images have gained significant attention,
particularly due to the widespread dissemination of multi-modal manipulations
on the internet, which has resulted in numerous negative impacts on society.
Existing methods for multi-modal manipulation detection and grounding primarily
focus on fusing vision-language features to make predictions, while overlooking
the importance of modality-specific features, leading to sub-optimal results.
In this paper, we construct a simple and novel transformer-based framework for
multi-modal manipulation detection and grounding tasks. Our framework
simultaneously explores modality-specific features while preserving the
capability for multi-modal alignment. To achieve this, we introduce
visual/language pre-trained encoders and dual-branch cross-attention (DCA) to
extract and fuse modality-unique features. Furthermore, we design decoupled
fine-grained classifiers (DFC) to enhance modality-specific feature mining and
mitigate modality competition. Moreover, we propose an implicit manipulation
query (IMQ) that adaptively aggregates global contextual cues within each
modality using learnable queries, thereby improving the discovery of forged
details. Extensive experiments on the $\rm DGM^4$ dataset demonstrate the
superior performance of our proposed model compared to state-of-the-art
approaches.",http://arxiv.org/pdf/2309.12657v1
2309.12656v1,eess.AS,"NTT speaker diarization system for CHiME-7: multi-domain, multi-microphone End-to-end and vector clustering diarization",2023-09-22 06:53:34+00:00,"This paper details our speaker diarization system designed for multi-domain,
multi-microphone casual conversations. The proposed diarization pipeline uses
weighted prediction error (WPE)-based dereverberation as a front end, then
applies end-to-end neural diarization with vector clustering (EEND-VC) to each
channel separately. It integrates the diarization result obtained from each
channel using diarization output voting error reduction plus overlap
(DOVER-LAP). To harness the knowledge from the target domain and results
integrated across all channels, we apply self-supervised adaptation for each
session by retraining the EEND-VC with pseudo-labels derived from DOVER-LAP.
The proposed system was incorporated into NTT's submission for the distant
automatic speech recognition task in the CHiME-7 challenge. Our system achieved
65 % and 62 % relative improvements on development and eval sets compared to
the organizer-provided VC-based baseline diarization system, securing third
place in diarization performance.",http://arxiv.org/pdf/2309.12656v1
2309.12654v1,math.PR,Some extreme value theory for $$-expansions,2023-09-22 06:51:31+00:00,"The main aim of this paper is to develop extreme value theory for
$\theta$-expansions. We get the limit distribution of the largest value of
$\theta$-continued fraction mixing stationary stochastic process and some
related results. These are analogous to J.Galambos and W.Philipp theorems for
the regular continued fractions. We also have to note that a Borel-Bernstein
type theorem plays an important role.",http://arxiv.org/pdf/2309.12654v1
2309.12646v1,cs.CL,Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding,2023-09-22 06:37:45+00:00,"Recent advancements in Natural Language Processing (NLP) have highlighted the
potential of sentence embeddings in measuring semantic similarity. Yet, its
application in analyzing real-world dyadic interactions and predicting the
affect of conversational participants remains largely uncharted. To bridge this
gap, the present study utilizes verbal conversations within 50 married couples
talking about conflicts and pleasant activities. Transformer-based model
all-MiniLM-L6-v2 was employed to obtain the embeddings of the utterances from
each speaker. The overall similarity of the conversation was then quantified by
the average cosine similarity between the embeddings of adjacent utterances.
Results showed that semantic similarity had a positive association with wives'
affect during conflict (but not pleasant) conversations. Moreover, this
association was not observed with husbands' affect regardless of conversation
types. Two validation checks further provided support for the validity of the
similarity measure and showed that the observed patterns were not mere
artifacts of data. The present study underscores the potency of sentence
embeddings in understanding the association between interpersonal dynamics and
individual affect, paving the way for innovative applications in affective and
relationship sciences.",http://arxiv.org/pdf/2309.12646v1
2309.12638v1,eess.IV,Auto-Lesion Segmentation with a Novel Intensity Dark Channel Prior for COVID-19 Detection,2023-09-22 06:09:48+00:00,"During the COVID-19 pandemic, medical imaging techniques like computed
tomography (CT) scans have demonstrated effectiveness in combating the rapid
spread of the virus. Therefore, it is crucial to conduct research on
computerized models for the detection of COVID-19 using CT imaging. A novel
processing method has been developed, utilizing radiomic features, to assist in
the CT-based diagnosis of COVID-19. Given the lower specificity of traditional
features in distinguishing between different causes of pulmonary diseases, the
objective of this study is to develop a CT-based radiomics framework for the
differentiation of COVID-19 from other lung diseases. The model is designed to
focus on outlining COVID-19 lesions, as traditional features often lack
specificity in this aspect. The model categorizes images into three classes:
COVID-19, non-COVID-19, or normal. It employs enhancement auto-segmentation
principles using intensity dark channel prior (IDCP) and deep neural networks
(ALS-IDCP-DNN) within a defined range of analysis thresholds. A publicly
available dataset comprising COVID-19, normal, and non-COVID-19 classes was
utilized to validate the proposed model's effectiveness. The best performing
classification model, Residual Neural Network with 50 layers (Resnet-50),
attained an average accuracy, precision, recall, and F1-score of 98.8%, 99%,
98%, and 98% respectively. These results demonstrate the capability of our
model to accurately classify COVID-19 images, which could aid radiologists in
diagnosing suspected COVID-19 patients. Furthermore, our model's performance
surpasses that of more than 10 current state-of-the-art studies conducted on
the same dataset.",http://arxiv.org/pdf/2309.12638v1
2309.12634v1,cs.RO,Learning Actions and Control of Focus of Attention with a Log-Polar-like Sensor,2023-09-22 06:02:58+00:00,"With the long-term goal of reducing the image processing time on an
autonomous mobile robot in mind we explore in this paper the use of log-polar
like image data with gaze control. The gaze control is not done on the
Cartesian image but on the log-polar like image data. For this we start out
from the classic deep reinforcement learning approach for Atari games. We
extend an A3C deep RL approach with an LSTM network, and we learn the policy
for playing three Atari games and a policy for gaze control. While the Atari
games already use low-resolution images of 80 by 80 pixels, we are able to
further reduce the amount of image pixels by a factor of 5 without losing any
gaming performance.",http://arxiv.org/pdf/2309.12634v1
2309.12633v1,cs.MA,Learning to Coordinate with Anyone,2023-09-22 06:01:26+00:00,"In open multi-agent environments, the agents may encounter unexpected
teammates. Classical multi-agent learning approaches train agents that can only
coordinate with seen teammates. Recent studies attempted to generate diverse
teammates to enhance the generalizable coordination ability, but were
restricted by pre-defined teammates. In this work, our aim is to train agents
with strong coordination ability by generating teammates that fully cover the
teammate policy space, so that agents can coordinate with any teammates. Since
the teammate policy space is too huge to be enumerated, we find only dissimilar
teammates that are incompatible with controllable agents, which highly reduces
the number of teammates that need to be trained with. However, it is hard to
determine the number of such incompatible teammates beforehand. We therefore
introduce a continual multi-agent learning process, in which the agent learns
to coordinate with different teammates until no more incompatible teammates can
be found. The above idea is implemented in the proposed Macop (Multi-agent
compatible policy learning) algorithm. We conduct experiments in 8 scenarios
from 4 environments that have distinct coordination patterns. Experiments show
that Macop generates training teammates with much lower compatibility than
previous methods. As a result, in all scenarios Macop achieves the best overall
coordination ability while never significantly worse than the baselines,
showing strong generalization ability.",http://arxiv.org/pdf/2309.12633v1
2309.12631v1,quant-ph,Learning the eigenstructure of quantum dynamics using classical shadows,2023-09-22 05:56:58+00:00,"Learning dynamics from repeated observation of the time evolution of an open
quantum system, namely, the problem of quantum process tomography is an
important task. This task is difficult in general, but, with some additional
constraints could be tractable. This motivates us to look at the problem of
Lindblad operator discovery from observations. We point out that for moderate
size Hilbert spaces, low Kraus rank of the channel, and short time steps, the
eigenvalues of the Choi matrix corresponding to the channel have a special
structure. We use the least-square method for the estimation of a channel
where, for fixed inputs, we estimate the outputs by classical shadows. The
resultant noisy estimate of the channel can then be denoised by diagonalizing
the nominal Choi matrix, truncating some eigenvalues, and altering it to a
genuine Choi matrix. This processed Choi matrix is then compared to the
original one. We see that as the number of samples increases, our
reconstruction becomes more accurate. We also use tools from random matrix
theory to understand the effect of estimation noise in the eigenspectrum of the
estimated Choi matrix.",http://arxiv.org/pdf/2309.12631v1
2309.12628v1,cs.LG,Sequential Action-Induced Invariant Representation for Reinforcement Learning,2023-09-22 05:31:55+00:00,"How to accurately learn task-relevant state representations from
high-dimensional observations with visual distractions is a realistic and
challenging problem in visual reinforcement learning. Recently, unsupervised
representation learning methods based on bisimulation metrics, contrast,
prediction, and reconstruction have shown the ability for task-relevant
information extraction. However, due to the lack of appropriate mechanisms for
the extraction of task information in the prediction, contrast, and
reconstruction-related approaches and the limitations of bisimulation-related
methods in domains with sparse rewards, it is still difficult for these methods
to be effectively extended to environments with distractions. To alleviate
these problems, in the paper, the action sequences, which contain
task-intensive signals, are incorporated into representation learning.
Specifically, we propose a Sequential Action--induced invariant Representation
(SAR) method, in which the encoder is optimized by an auxiliary learner to only
preserve the components that follow the control signals of sequential actions,
so the agent can be induced to learn the robust representation against
distractions. We conduct extensive experiments on the DeepMind Control suite
tasks with distractions while achieving the best performance over strong
baselines. We also demonstrate the effectiveness of our method at disregarding
task-irrelevant information by deploying SAR to real-world CARLA-based
autonomous driving with natural distractions. Finally, we provide the analysis
results of generalization drawn from the generalization decay and t-SNE
visualization. Code and demo videos are available at
https://github.com/DMU-XMU/SAR.git.",http://arxiv.org/pdf/2309.12628v1
2309.12620v1,cs.LG,Data-driven Preference Learning Methods for Multiple Criteria Sorting with Temporal Criteria,2023-09-22 05:08:52+00:00,"The advent of predictive methodologies has catalyzed the emergence of
data-driven decision support across various domains. However, developing models
capable of effectively handling input time series data presents an enduring
challenge. This study presents novel preference learning approaches to multiple
criteria sorting problems in the presence of temporal criteria. We first
formulate a convex quadratic programming model characterized by fixed time
discount factors, operating within a regularization framework. Additionally, we
propose an ensemble learning algorithm designed to consolidate the outputs of
multiple, potentially weaker, optimizers, a process executed efficiently
through parallel computation. To enhance scalability and accommodate learnable
time discount factors, we introduce a novel monotonic Recurrent Neural Network
(mRNN). It is designed to capture the evolving dynamics of preferences over
time while upholding critical properties inherent to MCS problems, including
criteria monotonicity, preference independence, and the natural ordering of
classes. The proposed mRNN can describe the preference dynamics by depicting
marginal value functions and personalized time discount factors along with
time, effectively amalgamating the interpretability of traditional MCS methods
with the predictive potential offered by deep preference learning models.
Comprehensive assessments of the proposed models are conducted, encompassing
synthetic data scenarios and a real-case study centered on classifying valuable
users within a mobile gaming app based on their historical in-app behavioral
sequences. Empirical findings underscore the notable performance improvements
achieved by the proposed models when compared to a spectrum of baseline
methods, spanning machine learning, deep learning, and conventional multiple
criteria sorting approaches.",http://arxiv.org/pdf/2309.12620v1
2309.12619v1,cs.CL,Learning to Diversify Neural Text Generation via Degenerative Model,2023-09-22 04:57:10+00:00,"Neural language models often fail to generate diverse and informative texts,
limiting their applicability in real-world problems. While previous approaches
have proposed to address these issues by identifying and penalizing undesirable
behaviors (e.g., repetition, overuse of frequent words) from language models,
we propose an alternative approach based on an observation: models primarily
learn attributes within examples that are likely to cause degeneration
problems. Based on this observation, we propose a new approach to prevent
degeneration problems by training two models. Specifically, we first train a
model that is designed to amplify undesirable patterns. We then enhance the
diversity of the second model by focusing on patterns that the first model
fails to learn. Extensive experiments on two tasks, namely language modeling
and dialogue generation, demonstrate the effectiveness of our approach.",http://arxiv.org/pdf/2309.12619v1
2309.12617v1,cs.SE,Analyzing the Influence of Processor Speed and Clock Speed on Remaining Useful Life Estimation of Software Systems,2023-09-22 04:46:34+00:00,"Prognostics and Health Management (PHM) is a discipline focused on predicting
the point at which systems or components will cease to perform as intended,
typically measured as Remaining Useful Life (RUL). RUL serves as a vital
decision-making tool for contingency planning, guiding the timing and nature of
system maintenance. Historically, PHM has primarily been applied to hardware
systems, with its application to software only recently explored. In a recent
study we introduced a methodology and demonstrated how changes in software can
impact the RUL of software. However, in practical software development,
real-time performance is also influenced by various environmental attributes,
including operating systems, clock speed, processor performance, RAM, machine
core count and others. This research extends the analysis to assess how changes
in environmental attributes, such as operating system and clock speed, affect
RUL estimation in software. Findings are rigorously validated using real
performance data from controlled test beds and compared with predictive
model-generated data. Statistical validation, including regression analysis,
supports the credibility of the results. The controlled test bed environment
replicates and validates faults from real applications, ensuring a standardized
assessment platform. This exploration yields actionable knowledge for software
maintenance and optimization strategies, addressing a significant gap in the
field of software health management.",http://arxiv.org/pdf/2309.12617v1
2309.12616v1,cs.CL,Unlocking Model Insights: A Dataset for Automated Model Card Generation,2023-09-22 04:46:11+00:00,"Language models (LMs) are no longer restricted to ML community, and
instruction-tuned LMs have led to a rise in autonomous AI agents. As the
accessibility of LMs grows, it is imperative that an understanding of their
capabilities, intended usage, and development cycle also improves. Model cards
are a popular practice for documenting detailed information about an ML model.
To automate model card generation, we introduce a dataset of 500
question-answer pairs for 25 ML models that cover crucial aspects of the model,
such as its training configurations, datasets, biases, architecture details,
and training resources. We employ annotators to extract the answers from the
original paper. Further, we explore the capabilities of LMs in generating model
cards by answering questions. Our initial experiments with ChatGPT-3.5, LLaMa,
and Galactica showcase a significant gap in the understanding of research
papers by these aforementioned LMs as well as generating factual textual
responses. We posit that our dataset can be used to train models to automate
the generation of model cards from paper text and reduce human effort in the
model card curation process. The complete dataset is available on
https://osf.io/hqt7p/?view_only=3b9114e3904c4443bcd9f5c270158d37",http://arxiv.org/pdf/2309.12616v1
2309.12613v1,cs.SI,User Migration across Multiple Social Media Platforms,2023-09-22 04:15:39+00:00,"After Twitter's ownership change and policy shifts, many users reconsidered
their go-to social media outlets and platforms like Mastodon, Bluesky, and
Threads became attractive alternatives in the battle for users. Based on the
data from over 16,000 users who migrated to these platforms within the first
eight weeks after the launch of Threads, our study examines: (1) distinguishing
attributes of Twitter users who migrated, compared to non-migrants; (2)
temporal migration patterns and associated challenges for sustainable migration
faced by each platform; and (3) how these new platforms are perceived in
relation to Twitter. Our research proceeds in three stages. First, we examine
migration from a broad perspective, not just one-to-one migration. Second, we
leverage behavioral analysis to pinpoint the distinct migration pattern of each
platform. Last, we employ a large language model (LLM) to discern stances
towards each platform and correlate them with the platform usage. This in-depth
analysis illuminates migration patterns amid competition across social media
platforms.",http://arxiv.org/pdf/2309.12613v1
2309.12608v1,eess.AS,SPGM: Prioritizing Local Features for enhanced speech separation performance,2023-09-22 03:48:50+00:00,"Dual-path is a popular architecture for speech separation models (e.g.
Sepformer) which splits long sequences into overlapping chunks for its intra-
and inter-blocks that separately model intra-chunk local features and
inter-chunk global relationships. However, it has been found that inter-blocks,
which comprise half a dual-path model's parameters, contribute minimally to
performance. Thus, we propose the Single-Path Global Modulation (SPGM) block to
replace inter-blocks. SPGM is named after its structure consisting of a
parameter-free global pooling module followed by a modulation module comprising
only 2% of the model's total parameters. The SPGM block allows all transformer
layers in the model to be dedicated to local feature modelling, making the
overall model single-path. SPGM achieves 22.1 dB SI-SDRi on WSJ0-2Mix and 20.4
dB SI-SDRi on Libri2Mix, exceeding the performance of Sepformer by 0.5 dB and
0.3 dB respectively and matches the performance of recent SOTA models with up
to 8 times fewer parameters.",http://arxiv.org/pdf/2309.12608v1
2309.12602v1,cs.HC,ViT-MDHGR: Cross-day Reliability and Agility in Dynamic Hand Gesture Prediction via HD-sEMG Signal Decoding,2023-09-22 03:23:42+00:00,"Surface electromyography (sEMG) and high-density sEMG (HD-sEMG) biosignals
have been extensively investigated for myoelectric control of prosthetic
devices, neurorobotics, and more recently human-computer interfaces because of
their capability for hand gesture recognition/prediction in a wearable and
non-invasive manner. High intraday (same-day) performance has been reported.
However, the interday performance (separating training and testing days) is
substantially degraded due to the poor generalizability of conventional
approaches over time, hindering the application of such techniques in real-life
practices. There are limited recent studies on the feasibility of multi-day
hand gesture recognition. The existing studies face a major challenge: the need
for long sEMG epochs makes the corresponding neural interfaces impractical due
to the induced delay in myoelectric control. This paper proposes a compact
ViT-based network for multi-day dynamic hand gesture prediction. We tackle the
main challenge as the proposed model only relies on very short HD-sEMG signal
windows (i.e., 50 ms, accounting for only one-sixth of the convention for
real-time myoelectric implementation), boosting agility and responsiveness. Our
proposed model can predict 11 dynamic gestures for 20 subjects with an average
accuracy of over 71% on the testing day, 3-25 days after training. Moreover,
when calibrated on just a small portion of data from the testing day, the
proposed model can achieve over 92% accuracy by retraining less than 10% of the
parameters for computational efficiency.",http://arxiv.org/pdf/2309.12602v1
2309.12598v1,eess.SY,Incentivizing Private Data Sharing in Vehicular Networks: A Game-Theoretic Approach,2023-09-22 03:01:56+00:00,"In the context of evolving smart cities and autonomous transportation
systems, Vehicular Ad-hoc Networks (VANETs) and the Internet of Vehicles (IoV)
are growing in significance. Vehicles are becoming more than just a means of
transportation; they are collecting, processing, and transmitting massive
amounts of data to make driving safer and more convenient. However, this
advancement ushers in complex issues concerning the centralized structure of
traditional vehicular networks and the privacy and security concerns around
vehicular data. This paper offers a novel, game-theoretic network architecture
to address these challenges. Our approach decentralizes data collection through
distributed servers across the network, aggregating vehicular data into
spatio-temporal maps via secure multi-party computation (SMPC). This strategy
effectively reduces the chances of adversaries reconstructing a vehicle's
complete path, increasing privacy. We also introduce an economic model grounded
in game theory that incentivizes vehicle owners to participate in the network,
balancing the owners' privacy concerns with the monetary benefits of data
sharing. This model aims to maximize the data consumer's utility from the
gathered sensor data by determining the most suitable payment to participating
vehicles, the frequency in which these vehicles share their data, and the total
number of servers in the network. We explore the interdependencies among these
parameters and present our findings accordingly. To define meaningful utility
and loss functions for our study, we utilize a real dataset of vehicular
movement traces.",http://arxiv.org/pdf/2309.12598v1
2309.12596v1,eess.SP,Movable Antenna-Empowered AirComp,2023-09-22 02:54:59+00:00,"A novel over-the-air computation (AirComp) framework, empowered by the
incorporation of movable antennas (MAs), is proposed to significantly enhance
computation accuracy. Within this framework, the joint optimization of transmit
power control, antenna positioning, and receive combining is investigated. An
efficient method is proposed to tackle the problem of computation mean-squared
error (MSE) minimization, capitalizing on the approach of alternating
optimization. Numerical results are provided to substantiate the superior MSE
performance of the proposed framework, which establish its clear advantage over
benchmark systems employing conventional fixed-position antennas (FPAs).",http://arxiv.org/pdf/2309.12596v1
2309.12593v1,cs.LG,Improving Machine Learning Robustness via Adversarial Training,2023-09-22 02:43:04+00:00,"As Machine Learning (ML) is increasingly used in solving various tasks in
real-world applications, it is crucial to ensure that ML algorithms are robust
to any potential worst-case noises, adversarial attacks, and highly unusual
situations when they are designed. Studying ML robustness will significantly
help in the design of ML algorithms. In this paper, we investigate ML
robustness using adversarial training in centralized and decentralized
environments, where ML training and testing are conducted in one or multiple
computers. In the centralized environment, we achieve a test accuracy of 65.41%
and 83.0% when classifying adversarial examples generated by Fast Gradient Sign
Method and DeepFool, respectively. Comparing to existing studies, these results
demonstrate an improvement of 18.41% for FGSM and 47% for DeepFool. In the
decentralized environment, we study Federated learning (FL) robustness by using
adversarial training with independent and identically distributed (IID) and
non-IID data, respectively, where CIFAR-10 is used in this research. In the IID
data case, our experimental results demonstrate that we can achieve such a
robust accuracy that it is comparable to the one obtained in the centralized
environment. Moreover, in the non-IID data case, the natural accuracy drops
from 66.23% to 57.82%, and the robust accuracy decreases by 25% and 23.4% in
C&W and Projected Gradient Descent (PGD) attacks, compared to the IID data
case, respectively. We further propose an IID data-sharing approach, which
allows for increasing the natural accuracy to 85.04% and the robust accuracy
from 57% to 72% in C&W attacks and from 59% to 67% in PGD attacks.",http://arxiv.org/pdf/2309.12593v1
2309.12592v1,cs.DC,ChainsFormer: A Chain Latency-aware Resource Provisioning Approach for Microservices Cluster,2023-09-22 02:41:39+00:00,"The trend towards transitioning from monolithic applications to microservices
has been widely embraced in modern distributed systems and applications. This
shift has resulted in the creation of lightweight, fine-grained, and
self-contained microservices. Multiple microservices can be linked together via
calls and inter-dependencies to form complex functions. One of the challenges
in managing microservices is provisioning the optimal amount of resources for
microservices in the chain to ensure application performance while improving
resource usage efficiency. This paper presents \textit{ChainsFormer}, a
framework that analyzes microservice inter-dependencies to identify critical
chains and nodes, and provision resources based on reinforcement learning. To
analyze chains, ChainsFormer utilizes light-weight machine learning techniques
to address the dynamic nature of microservice chains and workloads. For
resource provisioning, a reinforcement learning approach is used that combines
vertical and horizontal scaling to determine the amount of allocated resources
and the number of replicates. We evaluate the effectiveness of
\textit{ChainsFormer} using realistic applications and traces on a real testbed
based on Kubernetes. Our experimental results demonstrate that
\textit{ChainsFormer} can reduce response time by up to 26% and improve
processed requests per second by 8\% compared with state-of-the-art techniques.",http://arxiv.org/pdf/2309.12592v1
2309.12586v1,math.AG,Arithmetic Counts of Tropical Plane Curves and Their Properties,2023-09-22 02:28:22+00:00,"Recently, the first and third author proved a correspondence theorem which
recovers the Levine-Welschinger invariants of toric del Pezzo surfaces as a
count of tropical curves weighted with arithmetic multiplicities. In this
paper, we study properties of the arithmetic count of plane tropical curves
satisfying point conditions. We prove that this count is independent of the
configuration of point conditions. Moreover, a Caporaso-Harris formula for the
arithmetic count of plane tropical curves is obtained by moving one point to
the very left. Repeating this process until all point conditions are stretched,
we obtain an enriched count of floor diagrams which coincides with the tropical
count. Finally, we prove polynomiality properties for the arithmetic counts
using floor diagrams.",http://arxiv.org/pdf/2309.12586v1
2309.12585v1,cs.CV,BGF-YOLO: Enhanced YOLOv8 with Multiscale Attentional Feature Fusion for Brain Tumor Detection,2023-09-22 02:24:58+00:00,"You Only Look Once (YOLO)-based object detectors have shown remarkable
accuracy for automated brain tumor detection. In this paper, we develop a novel
BGFG-YOLO architecture by incorporating Bi-level Routing Attention (BRA),
Generalized feature pyramid networks (GFPN), Forth detecting head, and
Generalized-IoU (GIoU) bounding box regression loss into YOLOv8. BGFG-YOLO
contains an attention mechanism to focus more on important features, and
feature pyramid networks to enrich feature representation by merging high-level
semantic features with spatial details. Furthermore, we investigate the effect
of different attention mechanisms and feature fusions, detection head
architectures on brain tumor detection accuracy. Experimental results show that
BGFG-YOLO gives a 3.4% absolute increase of mAP50 compared to YOLOv8x, and
achieves state-of-the-art on the brain tumor detection dataset Br35H. The code
is available at https://github.com/mkang315/BGFG-YOLO.",http://arxiv.org/pdf/2309.12585v1
2309.12581v1,eess.AS,Sampling-Frequency-Independent Universal Sound Separation,2023-09-22 02:16:37+00:00,"This paper proposes a universal sound separation (USS) method capable of
handling untrained sampling frequencies (SFs). The USS aims at separating
arbitrary sources of different types and can be the key technique to realize a
source separator that can be universally used as a preprocessor for any
downstream tasks. To realize a universal source separator, there are two
essential properties: universalities with respect to source types and recording
conditions. The former property has been studied in the USS literature, which
has greatly increased the number of source types that can be handled by a
single neural network. However, the latter property (e.g., SF) has received
less attention despite its necessity. Since the SF varies widely depending on
the downstream tasks, the universal source separator must handle a wide variety
of SFs. In this paper, to encompass the two properties, we propose an
SF-independent (SFI) extension of a computationally efficient USS network,
SuDoRM-RF. The proposed network uses our previously proposed SFI convolutional
layers, which can handle various SFs by generating convolutional kernels in
accordance with an input SF. Experiments show that signal resampling can
degrade the USS performance and the proposed method works more consistently
than signal-resampling-based methods for various SFs.",http://arxiv.org/pdf/2309.12581v1
2309.12580v1,physics.flu-dyn,Sustained oscillation of flexible cantilevers without vortex shedding,2023-09-22 02:15:13+00:00,"The present work investigates the fluid-structure interaction (FSI) of a
flexible cylindrical cantilever beam at subcritical Reynolds numbers ($Re$). A
fully-coupled fluid-structure solver based on the three-dimensional (3D)
incompressible Navier-Stokes equations and Euler-Bernoulli beam theory is
employed to numerically examine the coupled dynamics of the beam. We assess the
extent to which such a flexible cylindrical beam could sustain oscillations in
this $Re$ regime when it is either exposed to a steady upstream wake (i.e.,
tandem cylinder configuration) or subjected to an externally applied base
excitation. Our results indicate that within a particular range of reduced
velocity parameter ($U^*$), the beam experiences sustained oscillations in both
scenarios, leading to periodic vortex shedding downstream. The mechanism
governing the sustained oscillations is characterized as synchronization,
during which the frequency of the cross-flow fluid loading matches the beam's
first-mode natural frequency. When the beam is subjected to base excitation,
the critical Reynolds number for vortex shedding ($Re_{c}$) is found to reduce
to $Re_{c}\approx5$. Above this threshold, vortex shedding is found to occur by
stimulating the pair of counter-rotating vortices in the near-wake region. For
the tandem cylinder configuration, the beam is shown to exhibit
figure-eight-shaped tip motion trajectories during its oscillatory response.
However, various patterns of tip motion trajectories, including figure-eight,
and chaotic-type responses, are observed when the beam is under external base
excitation. The findings of this work aim to generalize our understanding of
sustained oscillation in flexible cylindrical cantilevers and have relevance to
the development of bio-inspired cantilever flow sensors.",http://arxiv.org/pdf/2309.12580v1
2309.12574v1,cs.CV,Classification of Alzheimers Disease with Deep Learning on Eye-tracking Data,2023-09-22 02:02:59+00:00,"Existing research has shown the potential of classifying Alzheimers Disease
(AD) from eye-tracking (ET) data with classifiers that rely on task-specific
engineered features. In this paper, we investigate whether we can improve on
existing results by using a Deep-Learning classifier trained end-to-end on raw
ET data. This classifier (VTNet) uses a GRU and a CNN in parallel to leverage
both visual (V) and temporal (T) representations of ET data and was previously
used to detect user confusion while processing visual displays. A main
challenge in applying VTNet to our target AD classification task is that the
available ET data sequences are much longer than those used in the previous
confusion detection task, pushing the limits of what is manageable by
LSTM-based models. We discuss how we address this challenge and show that VTNet
outperforms the state-of-the-art approaches in AD classification, providing
encouraging evidence on the generality of this model to make predictions from
ET data.",http://arxiv.org/pdf/2309.12574v1
2309.12569v1,math.DS,A Study of the Long-Term Behavior of Hybrid Systems with Symmetries via Reduction and the Frobenius-Perron Operator,2023-09-22 01:47:53+00:00,"Hybrid dynamical systems are systems which undergo both continuous and
discrete transitions. As typical in dynamical analysis, an essential goal is to
study the long-term behavior of these systems. In this work, we present two
different novel approaches for studying these systems. The first approach is
based on constructing an analog of the Frobenius-Perron (transport) operator
for hybrid systems. Rather than tracking the evolution of a single trajectory,
this operator encodes the asymptotic nature of an ensemble of trajectories. The
second approach presented applies to an important subclass of hybrid systems,
mechanical impact systems. We develop an analog of Lie-Poisson(-Suslov)
reduction for left-invariant impact systems on Lie groups. In addition to the
Hamiltonian (and constraints) being left-invariant, the impact surface must
also be a right coset of a normal subgroup. This procedure allows a reduction
from a $2n$-dimensional system to an $(n+1)$-dimensional one. We conclude the
paper by presenting numerical results on a diverse array of applications.",http://arxiv.org/pdf/2309.12569v1
2309.12561v1,physics.flu-dyn,Coalescence of immiscible sessile droplets on a partial wetting surface,2023-09-22 01:08:51+00:00,"Droplet coalescence is a common phenomenon and plays an important role in
multi-disciplinary applications. Previous studies mainly consider the
coalescence of miscible liquid, even though the coalescence of immiscible
droplets on a solid surface is a common process. In this study, we explore the
coalescence of two immiscible droplets on a partial wetting surface
experimentally and theoretically. We find that the coalescence process can be
divided into three stages based on the timescales and force interactions
involved, namely (I) the growth of the liquid bridge, (II) the oscillation of
the coalescing sessile droplet, and (III) the formation of a partially-engulfed
compound sessile droplet and the subsequent retraction. In stage I, the
immiscible interface is found not to affect the scaling of the temporal
evolution of the liquid bridge, which follows the same 2/3 power law as that of
miscible droplets. In Stage II, by developing a new capillary timescale
considering both surface and interfacial tensions, we show that the interfacial
tension between the two immiscible liquids functions as a nonnegligible
resistance to the oscillation which decreases the oscillation periods. In Stage
III, a modified Ohnesorge number is developed to characterize the
visco-capillary and inertia-capillary timescales involved during the
displacement of water by oil; a new model based on energy balance is proposed
to analyze the maximum retraction velocity, highlighting that the viscous
resistance is concentrated in a region close to the contact line.",http://arxiv.org/pdf/2309.12561v1
2309.12556v1,math.OC,Relaxed optimal control for the stochastic Landau-Lifshitz-Gilbert equation,2023-09-22 01:00:00+00:00,"We consider the stochastic Landau-Lifshitz-Gilbert equation, perturbed by a
real-valued Wiener process. We add an external control to the effective field
as an attempt to drive the magnetization to a desired state and also to control
thermal fluctuations. We use the theory of Young measures to relax the given
control problem along with the associated cost. We consider a control operator
that can depend (possibly non-linearly) on both the control and the associated
solution. Moreover, we consider a fairly general associated cost functional
without any special convexity assumption. We use certain compactness arguments,
along with the Jakubowski version of the Skorohod Theorem to show that the
relaxed problem admits an optimal control.",http://arxiv.org/pdf/2309.12556v1
2309.12553v1,eess.AS,ICASSP 2023 Acoustic Echo Cancellation Challenge,2023-09-22 00:51:19+00:00,"The ICASSP 2023 Acoustic Echo Cancellation Challenge is intended to stimulate
research in acoustic echo cancellation (AEC), which is an important area of
speech enhancement and is still a top issue in audio communication. This is the
fourth AEC challenge and it is enhanced by adding a second track for
personalized acoustic echo cancellation, reducing the algorithmic + buffering
latency to 20ms, as well as including a full-band version of AECMOS. We open
source two large datasets to train AEC models under both single talk and double
talk scenarios. These datasets consist of recordings from more than 10,000 real
audio devices and human speakers in real environments, as well as a synthetic
dataset. We open source an online subjective test framework and provide an
objective metric for researchers to quickly test their results. The winners of
this challenge were selected based on the average mean opinion score (MOS)
achieved across all scenarios and the word accuracy (WAcc) rate.",http://arxiv.org/pdf/2309.12553v1
2309.12552v1,eess.SY,Adaptive Model Predictive Control for Engine-Driven Ducted Fan Lift Systems using an Associated Linear Parameter Varying Model,2023-09-22 00:49:40+00:00,"Ducted fan lift systems (DFLSs) powered by two-stroke aviation piston engines
present a challenging control problem due to their complex multivariable
dynamics. Current controllers for these systems typically rely on
proportional-integral algorithms combined with data tables, which rely on
accurate models and are not adaptive to handle time-varying dynamics or system
uncertainties. This paper proposes a novel adaptive model predictive control
(AMPC) strategy with an associated linear parameter varying (LPV) model for
controlling the engine-driven DFLS. This LPV model is derived from a global
network model, which is trained off-line with data obtained from a general mean
value engine model for two-stroke aviation engines. Different network models,
including multi-layer perceptron, Elman, and radial basis function (RBF), are
evaluated and compared in this study. The results demonstrate that the RBF
model exhibits higher prediction accuracy and robustness in the DFLS
application. Based on the trained RBF model, the proposed AMPC approach
constructs an associated network that directly outputs the LPV model parameters
as an adaptive, robust, and efficient prediction model. The efficiency of the
proposed approach is demonstrated through numerical simulations of a vertical
take-off thrust preparation process for the DFLS. The simulation results
indicate that the proposed AMPC method can effectively control the DFLS thrust
with a relative error below 3.5%.",http://arxiv.org/pdf/2309.12552v1
2309.12551v1,cs.CL,Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models,2023-09-22 00:47:18+00:00,"Text simplification is a common task where the text is adapted to make it
easier to understand. Similarly, text elaboration can make a passage more
sophisticated, offering a method to control the complexity of reading
comprehension tests. However, text simplification and elaboration tasks are
limited to only relatively alter the readability of texts. It is useful to
directly modify the readability of any text to an absolute target readability
level to cater to a diverse audience. Ideally, the readability of
readability-controlled generated text should be independent of the source text.
Therefore, we propose a novel readability-controlled text modification task.
The task requires the generation of 8 versions at various target readability
levels for each input text. We introduce novel readability-controlled text
modification metrics. The baselines for this task use ChatGPT and Llama-2, with
an extension approach introducing a two-step process (generating paraphrases by
passing through the language model twice). The zero-shot approaches are able to
push the readability of the paraphrases in the desired direction but the final
readability remains correlated with the original text's readability. We also
find greater drops in semantic and lexical similarity between the source and
target texts with greater shifts in the readability.",http://arxiv.org/pdf/2309.12551v1
2309.12546v1,cs.CL,Automatic Answerability Evaluation for Question Generation,2023-09-22 00:13:07+00:00,"Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed
for natural language generation (NLG) tasks, are based on measuring the n-gram
overlap between the generated and reference text. These simple metrics may be
insufficient for more complex tasks, such as question generation (QG), which
requires generating questions that are answerable by the reference answers.
Developing a more sophisticated automatic evaluation metric, thus, remains as
an urgent problem in QG research. This work proposes a Prompting-based Metric
on ANswerability (PMAN), a novel automatic evaluation metric to assess whether
the generated questions are answerable by the reference answers for the QG
tasks. Extensive experiments demonstrate that its evaluation results are
reliable and align with human evaluations. We further apply our metric to
evaluate the performance of QG models, which shows our metric complements
conventional metrics. Our implementation of a ChatGPT-based QG model achieves
state-of-the-art (SOTA) performance in generating answerable questions.",http://arxiv.org/pdf/2309.12546v1
2309.12534v1,cs.LG,Trip Planning for Autonomous Vehicles with Wireless Data Transfer Needs Using Reinforcement Learning,2023-09-21 23:19:16+00:00,"With recent advancements in the field of communications and the Internet of
Things, vehicles are becoming more aware of their environment and are evolving
towards full autonomy. Vehicular communication opens up the possibility for
vehicle-to-infrastructure interaction, where vehicles could share information
with components such as cameras, traffic lights, and signage that support a
countrys road system. As a result, vehicles are becoming more than just a means
of transportation; they are collecting, processing, and transmitting massive
amounts of data used to make driving safer and more convenient. With 5G
cellular networks and beyond, there is going to be more data bandwidth
available on our roads, but it may be heterogeneous because of limitations like
line of sight, infrastructure, and heterogeneous traffic on the road. This
paper addresses the problem of route planning for autonomous vehicles in urban
areas accounting for both driving time and data transfer needs. We propose a
novel reinforcement learning solution that prioritizes high bandwidth roads to
meet a vehicles data transfer requirement, while also minimizing driving time.
We compare this approach to traffic-unaware and bandwidth-unaware baselines to
show how much better it performs under heterogeneous traffic. This solution
could be used as a starting point to understand what good policies look like,
which could potentially yield faster, more efficient heuristics in the future.",http://arxiv.org/pdf/2309.12534v1
2309.13041v1,cs.RO,Robotic Offline RL from Internet Videos via Value-Function Pre-Training,2023-09-22 17:59:14+00:00,"Pre-training on Internet data has proven to be a key ingredient for broad
generalization in many modern ML systems. What would it take to enable such
capabilities in robotic reinforcement learning (RL)? Offline RL methods, which
learn from datasets of robot experience, offer one way to leverage prior data
into the robotic learning pipeline. However, these methods have a ""type
mismatch"" with video data (such as Ego4D), the largest prior datasets available
for robotics, since video offers observation-only experience without the action
or reward annotations needed for RL methods. In this paper, we develop a system
for leveraging large-scale human video datasets in robotic offline RL, based
entirely on learning value functions via temporal-difference learning. We show
that value learning on video datasets learns representations that are more
conducive to downstream robotic offline RL than other approaches for learning
from video data. Our system, called V-PTR, combines the benefits of
pre-training on video data with robotic offline RL approaches that train on
diverse robot data, resulting in value functions and policies for manipulation
tasks that perform better, act robustly, and generalize broadly. On several
manipulation tasks on a real WidowX robot, our framework produces policies that
greatly improve over prior methods. Our video and additional details can be
found at https://dibyaghosh.com/vptr/",http://arxiv.org/pdf/2309.13041v1
2309.13039v1,cs.CV,NeRRF: 3D Reconstruction and View Synthesis for Transparent and Specular Objects with Neural Refractive-Reflective Fields,2023-09-22 17:59:12+00:00,"Neural radiance fields (NeRF) have revolutionized the field of image-based
view synthesis. However, NeRF uses straight rays and fails to deal with
complicated light path changes caused by refraction and reflection. This
prevents NeRF from successfully synthesizing transparent or specular objects,
which are ubiquitous in real-world robotics and A/VR applications. In this
paper, we introduce the refractive-reflective field. Taking the object
silhouette as input, we first utilize marching tetrahedra with a progressive
encoding to reconstruct the geometry of non-Lambertian objects and then model
refraction and reflection effects of the object in a unified framework using
Fresnel terms. Meanwhile, to achieve efficient and effective anti-aliasing, we
propose a virtual cone supersampling technique. We benchmark our method on
different shapes, backgrounds and Fresnel terms on both real-world and
synthetic datasets. We also qualitatively and quantitatively benchmark the
rendering results of various editing applications, including material editing,
object replacement/insertion, and environment illumination estimation. Codes
and data are publicly available at https://github.com/dawning77/NeRRF.",http://arxiv.org/pdf/2309.13039v1
2309.13001v1,stat.ME,Joint $p$-Values for Higher-Powered Bayesian Model Checking with Frequentist Guarantees,2023-09-22 17:04:12+00:00,"We define an extension of the posterior predictive $p$-value for multiple
test statistics and establish a bound on its frequency under the assumption of
model correctness. We argue that the conservativity of the posterior predictive
$p$-value increases with model dimension, and we demonstrate the ability of the
joint $p$-value to overcome this problem in many cases. We also compare the
joint $p$-values to other alternative $p$-values designed to have higher power
and show that the joint $p$-value can achieve similar performance for model
rejection while maintaining more favorable computational and interpretive
properties.",http://arxiv.org/pdf/2309.13001v1
2309.12996v1,cs.LG,Point Cloud Network: An Order of Magnitude Improvement in Linear Layer Parameter Count,2023-09-22 16:56:40+00:00,"This paper introduces the Point Cloud Network (PCN) architecture, a novel
implementation of linear layers in deep learning networks, and provides
empirical evidence to advocate for its preference over the Multilayer
Perceptron (MLP) in linear layers. We train several models, including the
original AlexNet, using both MLP and PCN architectures for direct comparison of
linear layers (Krizhevsky et al., 2012). The key results collected are model
parameter count and top-1 test accuracy over the CIFAR-10 and CIFAR-100
datasets (Krizhevsky, 2009). AlexNet-PCN16, our PCN equivalent to AlexNet,
achieves comparable efficacy (test accuracy) to the original architecture with
a 99.5% reduction of parameters in its linear layers. All training is done on
cloud RTX 4090 GPUs, leveraging pytorch for model construction and training.
Code is provided for anyone to reproduce the trials from this paper.",http://arxiv.org/pdf/2309.12996v1
2309.12985v1,math.HO,Fractal Word Search: How Deep to Delve,2023-09-22 16:30:15+00:00,"We look at the puzzle \textit{In the Details} which appeared in the 2013 MIT
Mystery Hunt and which gained fame as the \textit{fractal word search}. This
seemingly impossible puzzle, whose solution could not fit the memory of a
modern computer if the puzzle were solved using a brute-force approach,
requires an understanding of its fundamental structure to be cracked. In this
paper, we study fractal word searches in a general setting, where we consider
one- and two-dimensional word searches with alphabets of any length and
replacement rules of any size. We prove that the puzzle is solvable within a
finite number of steps under this generalization and give an explicit upper
bound on the latest level on which a word of a given length can appear for the
first time in a given direction.",http://arxiv.org/pdf/2309.12985v1
2309.12979v1,stat.CO,EgoCor: an R package to fit exponential semi-variograms to model the local spatial correlation structure of health outcomes,2023-09-22 16:22:06+00:00,"As an alternative to using administrative areas for the evaluation of
small-area health inequalities, Sauzet et al suggested to take an ego-centred
approach and model the spatial correlation structure of health outcomes at
individual level. Existing tools for the analysis of spatial data in R may
appear too complex to non-specialists which may limit the use of the approach.
We present the R package EgoCor which offers a user-friendly interface
displaying in one function a range of graphics and tables of parameters to
facilitate the decision making about which exponential parameters fit best
either raw data or residuals. This function is based on the functions of the R
package gstat. Moreover, we implemented a function providing the measure of
uncertainty proposed by Dyck and Sauzet. With the R package EgoCor the
modelling of spatial correlation structure of health outcomes with a measure of
uncertainty is made available to non specialists.",http://arxiv.org/pdf/2309.12979v1
2309.12972v1,cs.CV,License Plate Recognition Based On Multi-Angle View Model,2023-09-22 16:12:45+00:00,"In the realm of research, the detection/recognition of text within
images/videos captured by cameras constitutes a highly challenging problem for
researchers. Despite certain advancements achieving high accuracy, current
methods still require substantial improvements to be applicable in practical
scenarios. Diverging from text detection in images/videos, this paper addresses
the issue of text detection within license plates by amalgamating multiple
frames of distinct perspectives. For each viewpoint, the proposed method
extracts descriptive features characterizing the text components of the license
plate, specifically corner points and area. Concretely, we present three
viewpoints: view-1, view-2, and view-3, to identify the nearest neighboring
components facilitating the restoration of text components from the same
license plate line based on estimations of similarity levels and distance
metrics. Subsequently, we employ the CnOCR method for text recognition within
license plates. Experimental results on the self-collected dataset
(PTITPlates), comprising pairs of images in various scenarios, and the publicly
available Stanford Cars Dataset, demonstrate the superiority of the proposed
method over existing approaches.",http://arxiv.org/pdf/2309.12972v1
2309.12959v1,cs.LO,Proceedings 7th Symposium on Working Formal Methods,2023-09-22 15:57:54+00:00,"This volume contains the proceedings of the 7th Working Formal Methods
Symposium, which was held at the University of Bucharest, September 21-22,
2023.",http://arxiv.org/pdf/2309.12959v1
2309.12955v1,cs.CR,On Data Fabrication in Collaborative Vehicular Perception: Attacks and Countermeasures,2023-09-22 15:54:04+00:00,"Collaborative perception, which greatly enhances the sensing capability of
connected and autonomous vehicles (CAVs) by incorporating data from external
resources, also brings forth potential security risks. CAVs' driving decisions
rely on remote untrusted data, making them susceptible to attacks carried out
by malicious participants in the collaborative perception system. However,
security analysis and countermeasures for such threats are absent. To
understand the impact of the vulnerability, we break the ground by proposing
various real-time data fabrication attacks in which the attacker delivers
crafted malicious data to victims in order to perturb their perception results,
leading to hard brakes or increased collision risks. Our attacks demonstrate a
high success rate of over 86\% on high-fidelity simulated scenarios and are
realizable in real-world experiments. To mitigate the vulnerability, we present
a systematic anomaly detection approach that enables benign vehicles to jointly
reveal malicious fabrication. It detects 91.5% of attacks with a false positive
rate of 3% in simulated scenarios and significantly mitigates attack impacts in
real-world scenarios.",http://arxiv.org/pdf/2309.12955v1
2309.12932v1,gr-qc,Different Regular Black Holes: Geodesic Structures of Test Particles,2023-09-22 15:30:57+00:00,"This paper investigates the metric of previously proposed regular black
holes, calculates their effective potentials, and plots the curves of the
effective potentials. By determining the conserved quantities, the dynamical
equations for particles and photons near the black hole are derived. The
analysis encompasses timelike and null geodesics in different spacetimes,
including bound geodesics, unstable circular geodesics, stable circular
geodesics, and escape geodesics. The findings are presented through figures and
tables. Furthermore, the bound geodesics of the four regular black hole
spacetimes are analyzed, examining the average distance of particle orbits from
the center of the event horizon, the precession behavior of the perihelion, and
the probability of particles appearing inside the outer event horizon during
motion. Based on these analyses, a general formula is proposed, which yields
the existing metrics when specific parameter values are chosen. The impact of
parameter variations on the effective potential and geodesics is then computed
using this new formula.",http://arxiv.org/pdf/2309.12932v1
2309.12928v1,cs.LG,BayesDLL: Bayesian Deep Learning Library,2023-09-22 15:27:54+00:00,"We release a new Bayesian neural network library for PyTorch for large-scale
deep networks. Our library implements mainstream approximate Bayesian inference
algorithms: variational inference, MC-dropout, stochastic-gradient MCMC, and
Laplace approximation. The main differences from other existing Bayesian neural
network libraries are as follows: 1) Our library can deal with very large-scale
deep networks including Vision Transformers (ViTs). 2) We need virtually zero
code modifications for users (e.g., the backbone network definition codes do
not neet to be modified at all). 3) Our library also allows the pre-trained
model weights to serve as a prior mean, which is very useful for performing
Bayesian inference with the large-scale foundation models like ViTs that are
hard to optimise from scratch with the downstream data alone. Our code is
publicly available at: \url{https://github.com/SamsungLabs/BayesDLL}\footnote{A
mirror repository is also available at:
\url{https://github.com/minyoungkim21/BayesDLL}.}.",http://arxiv.org/pdf/2309.12928v1
2309.12927v1,cs.NE,Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks,2023-09-22 15:26:49+00:00,"Recurrent neural networks (RNNs) in the brain and in silico excel at solving
tasks with intricate temporal dependencies. Long timescales required for
solving such tasks can arise from properties of individual neurons
(single-neuron timescale, $\tau$, e.g., membrane time constant in biological
neurons) or recurrent interactions among them (network-mediated timescale).
However, the contribution of each mechanism for optimally solving
memory-dependent tasks remains poorly understood. Here, we train RNNs to solve
$N$-parity and $N$-delayed match-to-sample tasks with increasing memory
requirements controlled by $N$ by simultaneously optimizing recurrent weights
and $\tau$s. We find that for both tasks RNNs develop longer timescales with
increasing $N$, but depending on the learning objective, they use different
mechanisms. Two distinct curricula define learning objectives: sequential
learning of a single-$N$ (single-head) or simultaneous learning of multiple
$N$s (multi-head). Single-head networks increase their $\tau$ with $N$ and are
able to solve tasks for large $N$, but they suffer from catastrophic
forgetting. However, multi-head networks, which are explicitly required to hold
multiple concurrent memories, keep $\tau$ constant and develop longer
timescales through recurrent connectivity. Moreover, we show that the
multi-head curriculum increases training speed and network stability to
ablations and perturbations, and allows RNNs to generalize better to tasks
beyond their training regime. This curriculum also significantly improves
training GRUs and LSTMs for large-$N$ tasks. Our results suggest that adapting
timescales to task requirements via recurrent interactions allows learning more
complex objectives and improves the RNN's performance.",http://arxiv.org/pdf/2309.12927v1
2309.12917v1,cs.AR,Platform-Aware FPGA System Architecture Generation based on MLIR,2023-09-22 15:13:47+00:00,"FPGA acceleration is becoming increasingly important to meet the performance
demands of modern computing, particularly in big data or machine learning
applications. As such, significant effort is being put into the optimization of
the hardware accelerators. However, integrating accelerators into modern FPGA
platforms, with key features such as high bandwidth memory (HBM), requires
manual effort from a platform expert for every new application. We propose the
Olympus multi-level intermediate representation (MLIR) dialect and Olympus-opt,
a series of analysis and transformation passes on this dialect, for
representing and optimizing platform aware system level FPGA architectures. By
leveraging MLIR, our automation will be extensible and reusable both between
many sources of input and many platform-specific back-ends.",http://arxiv.org/pdf/2309.12917v1
2309.12907v1,quant-ph,Certifying the topology of quantum networks: theory and experiment,2023-09-22 14:50:38+00:00,"Distributed quantum information in networks is paramount for global secure
quantum communication. Moreover, it finds applications as a resource for
relevant tasks, such as clock synchronization, magnetic field sensing, and
blind quantum computation. For quantum network analysis and benchmarking of
implementations, however, it is crucial to characterize the topology of
networks in a way that reveals the nodes between which entanglement can be
reliably distributed. Here, we demonstrate an efficient scheme for this
topology certification. Our scheme allows for distinguishing, in a scalable
manner, different networks consisting of bipartite and multipartite
entanglement sources, for different levels of trust in the measurement devices
and network nodes. We experimentally demonstrate our approach by certifying the
topology of different six-qubit networks generated with polarized photons,
employing active feed-forward and time multiplexing. Our methods can be used
for general simultaneous tests of multiple hypotheses with few measurements,
being useful for other certification scenarios in quantum technologies.",http://arxiv.org/pdf/2309.12907v1
2309.12897v1,cs.DC,Distributed Optimisation with Linear Equality and Inequality Constraints using PDMM,2023-09-22 14:36:08+00:00,"In this paper, we consider the problem of distributed optimisation of a
separable convex cost function over a graph, where every edge and node in the
graph could carry both linear equality and/or inequality constraints. We show
how to modify the primal-dual method of multipliers (PDMM), originally designed
for linear equality constraints, such that it can handle inequality constraints
as well. In contrast to most existing algorithms for optimisation with
inequality constraints, the proposed algorithm does not need any slack
variables. Using convex analysis, monotone operator theory and fixed-point
theory, we show how to derive the update equations of the modified PDMM
algorithm by applying Peaceman-Rachford splitting to the monotonic inclusion
related to the extended dual problem. To incorporate the inequality
constraints, we impose a non-negativity constraint on the associated dual
variables. This additional constraint results in the introduction of a
reflection operator to model the data exchange in the network, instead of a
permutation operator as derived for equality constraint PDMM. Convergence for
both synchronous and stochastic update schemes of PDMM are provided. The latter
includes asynchronous update schemes and update schemes with transmission
losses.",http://arxiv.org/pdf/2309.12897v1
2309.12895v1,physics.geo-ph,Thermal modeling of subduction zones with prescribed and evolving 2D and 3D slab geometries,2023-09-22 14:34:22+00:00,"The determination of the temperature in and above the slab in subduction
zones, using models where the top of the slab is precisely known, is important
to test hypotheses regarding the causes of arc volcanism and intermediate-depth
seismicity. While 2D and 3D models can predict the thermal structure with high
precision for fixed slab geometries, a number of regions are characterized by
relatively large geometrical changes. Examples include the flat slab segments
in South America that evolved from more steeply dipping geometries to the
present day flat slab geometry. We devise, implement, and test a numerical
approach to model the thermal evolution of a subduction zone with prescribed
changes in slab geometry over time. Our numerical model approximates the
subduction zone geometry by employing time dependent deformation of a B\'ezier
spline which is used as the slab interface in a finite element discretization
of the Stokes and heat equations. We implement the numerical model using the
FEniCS open source finite element suite and describe the means by which we
compute approximations of the subduction zone velocity, temperature, and
pressure fields. We compute and compare the 3D time evolving numerical model
with its 2D analogy at cross-sections for slabs that evolve to the present-day
structure of a flat segment of the subducting Nazca plate.",http://arxiv.org/pdf/2309.12895v1
2309.12891v1,q-fin.TR,EarnHFT: Efficient Hierarchical Reinforcement Learning for High Frequency Trading,2023-09-22 14:25:03+00:00,"High-frequency trading (HFT) uses computer algorithms to make trading
decisions in short time scales (e.g., second-level), which is widely used in
the Cryptocurrency (Crypto) market (e.g., Bitcoin). Reinforcement learning (RL)
in financial research has shown stellar performance on many quantitative
trading tasks. However, most methods focus on low-frequency trading, e.g.,
day-level, which cannot be directly applied to HFT because of two challenges.
First, RL for HFT involves dealing with extremely long trajectories (e.g., 2.4
million steps per month), which is hard to optimize and evaluate. Second, the
dramatic price fluctuations and market trend changes of Crypto make existing
algorithms fail to maintain satisfactory performance. To tackle these
challenges, we propose an Efficient hieArchical Reinforcement learNing method
for High Frequency Trading (EarnHFT), a novel three-stage hierarchical RL
framework for HFT. In stage I, we compute a Q-teacher, i.e., the optimal action
value based on dynamic programming, for enhancing the performance and training
efficiency of second-level RL agents. In stage II, we construct a pool of
diverse RL agents for different market trends, distinguished by return rates,
where hundreds of RL agents are trained with different preferences of return
rates and only a tiny fraction of them will be selected into the pool based on
their profitability. In stage III, we train a minute-level router which
dynamically picks a second-level agent from the pool to achieve stable
performance across different markets. Through extensive experiments in various
market trends on Crypto markets in a high-fidelity simulation trading
environment, we demonstrate that EarnHFT significantly outperforms 6
state-of-art baselines in 6 popular financial criteria, exceeding the runner-up
by 30% in profitability.",http://arxiv.org/pdf/2309.12891v1
2309.12889v1,cond-mat.mtrl-sci,Influence of surface energy anisotropy on nucleation and crystallographic texture of polycrystalline deposits,2023-09-22 14:21:42+00:00,"This paper aims to elucidate the role of interface energy anisotropy in
orientation selection during nucleation of new grains in a polycrystalline film
growth. An assessment of (heterogeneous) nucleation probability as function of
orientation of both the bottom grain and of the nucleus was developed (using
the concepts of classical nucleation theory). Novel solutions to the
generalized Winterbottom construction were described in cases of very strong
anisotropy and arbitrary orientations. In order to demonstrate the effect on
the film crystallographic texture, a 2D Monte Carlo algorithm for anisotropic
polycrystalline growth was used to simulate growth of films with columnar
microstructure. The effect of strength of anisotropy, the deposition rate and
initial texture were investigated. Results showed that with larger strength of
anisotropy, the nucleation rate is less dependent on the driving force, but
more dependent on the initial texture. With certain initial textures, the
anisotropic nucleation may even be either impossible or having probability
close to one irrespective of the driving force. Depending on the conditions,
the anisotropic nucleation could hasten the evolution towards the
interface-energy minimizing texture or retard it. Based on these insights, a
hypothesis was offered to explain a peculiar texture evolution in
electrodeposited nickel.",http://arxiv.org/pdf/2309.12889v1
2309.12888v1,math.AG,The algebra of symmetric tensors on smooth projective varieties,2023-09-22 14:21:25+00:00,"We discuss in this note the algebra H^0(X, Sym*TX) for a smooth complex
projective variety X . We compute it in some simple examples, and survey a few
cases where it is trivial.",http://arxiv.org/pdf/2309.12888v1
2309.12885v1,cs.CY,Do Digital Jobs Need an Image Filter? Factors Contributing to Negative Attitudes,2023-09-22 14:17:37+00:00,"The rapid expansion of high-speed internet has led to the emergence of new
digital jobs, such as digital influencers, fitness models, and adult models who
share content on subscription-based social media platforms. Across two
experiments involving 1,002 participants, we combined theories from both social
psychology and information systems to investigate perceptions of digital jobs
compared to matched established jobs, and predictors of attitudes toward
digital jobs (e.g., symbolic threat, contact, perceived usefulness). We found
that individuals in digital professions were perceived as less favorably and as
less hard-working than those in matched established jobs. Digital jobs were
also regarded as more threatening to societal values and less useful. The
relation between job type and attitudes toward these jobs was partially
mediated by contact with people working in these jobs, perceived usefulness,
perception of hard-working, and symbolic threat. These effects were consistent
across openness to new experiences, attitudes toward digitalization, political
orientation, and age. Among the nine jobs examined, lecturers were perceived as
the most favorable, while adult models were viewed least favorably. Overall,
our findings demonstrate that integrating theories from social psychology and
information systems can enhance our understanding of how attitudes are formed.",http://arxiv.org/pdf/2309.12885v1
2309.12884v1,cs.DC,A Survey of FPGA Optimization Methods for Data Center Energy Efficiency,2023-09-22 14:17:02+00:00,"This article provides a survey of academic literature about field
programmable gate array (FPGA) and their utilization for energy efficiency
acceleration in data centers. The goal is to critically present the existing
FPGA energy optimization techniques and discuss how they can be applied to such
systems. To do so, the article explores current energy trends and their
projection to the future with particular attention to the requirements set out
by the European Code of Conduct for Data Center Energy Efficiency. The article
then proposes a complete analysis of over ten years of research in energy
optimization techniques, classifying them by purpose, method of application,
and impacts on the sources of consumption. Finally, we conclude with the
challenges and possible innovations we expect for this sector.",http://arxiv.org/pdf/2309.12884v1
2309.12877v1,cs.CY,FairComp: Workshop on Fairness and Robustness in Machine Learning for Ubiquitous Computing,2023-09-22 14:04:51+00:00,"How can we ensure that Ubiquitous Computing (UbiComp) research outcomes are
both ethical and fair? While fairness in machine learning (ML) has gained
traction in recent years, fairness in UbiComp remains unexplored. This workshop
aims to discuss fairness in UbiComp research and its social, technical, and
legal implications. From a social perspective, we will examine the relationship
between fairness and UbiComp research and identify pathways to ensure that
ubiquitous technologies do not cause harm or infringe on individual rights.
From a technical perspective, we will initiate a discussion on data practices
to develop bias mitigation approaches tailored to UbiComp research. From a
legal perspective, we will examine how new policies shape our community's work
and future research. We aim to foster a vibrant community centered around the
topic of responsible UbiComp, while also charting a clear path for future
research endeavours in this field.",http://arxiv.org/pdf/2309.12877v1
2309.12875v1,math.NA,"A second-order in time, BGN-based parametric finite element method for geometric flows of curves",2023-09-22 14:00:40+00:00,"Over the last two decades, the field of geometric curve evolutions has
attracted significant attention from scientific computing. One of the most
popular numerical methods for solving geometric flows is the so-called BGN
scheme, which was proposed by Barrett, Garcke, and Nurnberg (J. Comput. Phys.,
222 (2007), pp. 441{467), due to its favorable properties (e.g., its
computational efficiency and the good mesh property). However, the BGN scheme
is limited to first-order accuracy in time, and how to develop a higher-order
numerical scheme is challenging. In this paper, we propose a fully discrete,
temporal second-order parametric finite element method, which incorporates a
mesh regularization technique when necessary, for solving geometric flows of
curves. The scheme is constructed based on the BGN formulation and a
semi-implicit Crank-Nicolson leap-frog time stepping discretization as well as
a linear finite element approximation in space. More importantly, we point out
that the shape metrics, such as manifold distance and Hausdorff distance,
instead of function norms, should be employed to measure numerical errors.
Extensive numerical experiments demonstrate that the proposed BGN-based scheme
is second-order accurate in time in terms of shape metrics. Moreover, by
employing the classical BGN scheme as a mesh regularization technique when
necessary, our proposed second-order scheme exhibits good properties with
respect to the mesh distribution.",http://arxiv.org/pdf/2309.12875v1
2309.12872v1,stat.ME,Deep regression learning with optimal loss function,2023-09-22 13:53:25+00:00,"In this paper, we develop a novel efficient and robust nonparametric
regression estimator under a framework of feedforward neural network. There are
several interesting characteristics for the proposed estimator. First, the loss
function is built upon an estimated maximum likelihood function, who integrates
the information from observed data, as well as the information from data
structure. Consequently, the resulting estimator has desirable optimal
properties, such as efficiency. Second, different from the traditional maximum
likelihood estimation (MLE), the proposed method avoid the specification of the
distribution, hence is flexible to any kind of distribution, such as heavy
tails, multimodal or heterogeneous distribution. Third, the proposed loss
function relies on probabilities rather than direct observations as in least
squares, contributing the robustness in the proposed estimator. Finally, the
proposed loss function involves nonparametric regression function only. This
enables a direct application of existing packages, simplifying the computation
and programming. We establish the large sample property of the proposed
estimator in terms of its excess risk and minimax near-optimal rate. The
theoretical results demonstrate that the proposed estimator is equivalent to
the true MLE in which the density function is known. Our simulation studies
show that the proposed estimator outperforms the existing methods in terms of
prediction accuracy, efficiency and robustness. Particularly, it is comparable
to the true MLE, and even gets better as the sample size increases. This
implies that the adaptive and data-driven loss function from the estimated
density may offer an additional avenue for capturing valuable information. We
further apply the proposed method to four real data examples, resulting in
significantly reduced out-of-sample prediction errors compared to existing
methods.",http://arxiv.org/pdf/2309.12872v1
2309.12865v1,cs.CV,Bridging Sensor Gaps via Single-Direction Tuning for Hyperspectral Image Classification,2023-09-22 13:39:24+00:00,"Recently, some researchers started exploring the use of ViTs in tackling HSI
classification and achieved remarkable results. However, the training of ViT
models requires a considerable number of training samples, while hyperspectral
data, due to its high annotation costs, typically has a relatively small number
of training samples. This contradiction has not been effectively addressed. In
this paper, aiming to solve this problem, we propose the single-direction
tuning (SDT) strategy, which serves as a bridge, allowing us to leverage
existing labeled HSI datasets even RGB datasets to enhance the performance on
new HSI datasets with limited samples. The proposed SDT inherits the idea of
prompt tuning, aiming to reuse pre-trained models with minimal modifications
for adaptation to new tasks. But unlike prompt tuning, SDT is custom-designed
to accommodate the characteristics of HSIs. The proposed SDT utilizes a
parallel architecture, an asynchronous cold-hot gradient update strategy, and
unidirectional interaction. It aims to fully harness the potent representation
learning capabilities derived from training on heterologous, even cross-modal
datasets. In addition, we also introduce a novel Triplet-structured transformer
(Tri-Former), where spectral attention and spatial attention modules are merged
in parallel to construct the token mixing component for reducing computation
cost and a 3D convolution-based channel mixer module is integrated to enhance
stability and keep structure information. Comparison experiments conducted on
three representative HSI datasets captured by different sensors demonstrate the
proposed Tri-Former achieves better performance compared to several
state-of-the-art methods. Homologous, heterologous and cross-modal tuning
experiments verified the effectiveness of the proposed SDT.",http://arxiv.org/pdf/2309.12865v1
2309.12854v1,cs.NE,ThinResNet: A New Baseline for Structured Convolutional Networks Pruning,2023-09-22 13:28:18+00:00,"Pruning is a compression method which aims to improve the efficiency of
neural networks by reducing their number of parameters while maintaining a good
performance, thus enhancing the performance-to-cost ratio in nontrivial ways.
Of particular interest are structured pruning techniques, in which whole
portions of parameters are removed altogether, resulting in easier to leverage
shrunk architectures. Since its growth in popularity in the recent years,
pruning gave birth to countless papers and contributions, resulting first in
critical inconsistencies in the way results are compared, and then to a
collective effort to establish standardized benchmarks. However, said
benchmarks are based on training practices that date from several years ago and
do not align with current practices. In this work, we verify how results in the
recent literature of pruning hold up against networks that underwent both
state-of-the-art training methods and trivial model scaling. We find that the
latter clearly and utterly outperform all the literature we compared to,
proving that updating standard pruning benchmarks and re-evaluating classical
methods in their light is an absolute necessity. We thus introduce a new
challenging baseline to compare structured pruning to: ThinResNet.",http://arxiv.org/pdf/2309.12854v1
2309.12836v1,cond-mat.mes-hall,Using superconductivity to control magnetism: a facet of superconducting spintronics,2023-09-22 12:51:42+00:00,"Magnets are used in electronics to store and read information. A magnetic
moment is rotated to a desired direction, so that information can later be
retrieved by reading this orientation. Controlling the moment via electric
currents causes resistive losses and heating, a major bottleneck in advancing
computing technologies. Superconducting spintronics can resolve this using the
unique features of superconductors.",http://arxiv.org/pdf/2309.12836v1
2309.12820v1,quant-ph,Almost-Optimal Computational Basis State Transpositions,2023-09-22 12:19:59+00:00,"We give an explicit construction to perform any $n$-qubit computational basis
state transposition using $\Theta(n)$ gates. This nearly coincides with the
lower bound $\Omega(n/\log(nd))$ on worst-case and average-case gate complexity
to perform transpositions using a $d$-element gate-set, which we also prove.",http://arxiv.org/pdf/2309.12820v1
2309.12819v1,stat.ME,Doubly Robust Proximal Causal Learning for Continuous Treatments,2023-09-22 12:18:53+00:00,"Proximal causal learning is a promising framework for identifying the causal
effect under the existence of unmeasured confounders. Within this framework,
the doubly robust (DR) estimator was derived and has shown its effectiveness in
estimation, especially when the model assumption is violated. However, the
current form of the DR estimator is restricted to binary treatments, while the
treatment can be continuous in many real-world applications. The primary
obstacle to continuous treatments resides in the delta function present in the
original DR estimator, making it infeasible in causal effect estimation and
introducing a heavy computational burden in nuisance function estimation. To
address these challenges, we propose a kernel-based DR estimator that can well
handle continuous treatments. Equipped with its smoothness, we show that its
oracle form is a consistent approximation of the influence function. Further,
we propose a new approach to efficiently solve the nuisance functions. We then
provide a comprehensive convergence analysis in terms of the mean square error.
We demonstrate the utility of our estimator on synthetic datasets and
real-world applications.",http://arxiv.org/pdf/2309.12819v1
2309.12814v1,cs.CV,Domain Adaptive Few-Shot Open-Set Learning,2023-09-22 12:04:47+00:00,"Few-shot learning has made impressive strides in addressing the crucial
challenges of recognizing unknown samples from novel classes in target query
sets and managing visual shifts between domains. However, existing techniques
fall short when it comes to identifying target outliers under domain shifts by
learning to reject pseudo-outliers from the source domain, resulting in an
incomplete solution to both problems. To address these challenges
comprehensively, we propose a novel approach called Domain Adaptive Few-Shot
Open Set Recognition (DA-FSOS) and introduce a meta-learning-based architecture
named DAFOSNET. During training, our model learns a shared and discriminative
embedding space while creating a pseudo open-space decision boundary, given a
fully-supervised source domain and a label-disjoint few-shot target domain. To
enhance data density, we use a pair of conditional adversarial networks with
tunable noise variances to augment both domains closed and pseudo-open spaces.
Furthermore, we propose a domain-specific batch-normalized class prototypes
alignment strategy to align both domains globally while ensuring
class-discriminativeness through novel metric objectives. Our training approach
ensures that DAFOS-NET can generalize well to new scenarios in the target
domain. We present three benchmarks for DA-FSOS based on the Office-Home,
mini-ImageNet/CUB, and DomainNet datasets and demonstrate the efficacy of
DAFOS-NET through extensive experimentation",http://arxiv.org/pdf/2309.12814v1
2309.12787v1,cs.CV,EMS: 3D Eyebrow Modeling from Single-view Images,2023-09-22 10:55:11+00:00,"Eyebrows play a critical role in facial expression and appearance. Although
the 3D digitization of faces is well explored, less attention has been drawn to
3D eyebrow modeling. In this work, we propose EMS, the first learning-based
framework for single-view 3D eyebrow reconstruction. Following the methods of
scalp hair reconstruction, we also represent the eyebrow as a set of fiber
curves and convert the reconstruction to fibers growing problem. Three modules
are then carefully designed: RootFinder firstly localizes the fiber root
positions which indicates where to grow; OriPredictor predicts an orientation
field in the 3D space to guide the growing of fibers; FiberEnder is designed to
determine when to stop the growth of each fiber. Our OriPredictor is directly
borrowing the method used in hair reconstruction. Considering the differences
between hair and eyebrows, both RootFinder and FiberEnder are newly proposed.
Specifically, to cope with the challenge that the root location is severely
occluded, we formulate root localization as a density map estimation task.
Given the predicted density map, a density-based clustering method is further
used for finding the roots. For each fiber, the growth starts from the root
point and moves step by step until the ending, where each step is defined as an
oriented line with a constant length according to the predicted orientation
field. To determine when to end, a pixel-aligned RNN architecture is designed
to form a binary classifier, which outputs stop or not for each growing step.
To support the training of all proposed networks, we build the first 3D
synthetic eyebrow dataset that contains 400 high-quality eyebrow models
manually created by artists. Extensive experiments have demonstrated the
effectiveness of the proposed EMS pipeline on a variety of different eyebrow
styles and lengths, ranging from short and sparse to long bushy eyebrows.",http://arxiv.org/pdf/2309.12787v1
2309.12786v1,cs.RO,"CloudGripper: An Open Source Cloud Robotics Testbed for Robotic Manipulation Research, Benchmarking and Data Collection at Scale",2023-09-22 10:54:07+00:00,"We present CloudGripper, an open source cloud robotics testbed, consisting of
a scalable, space and cost-efficient design constructed as a rack of 32 small
robot arm work cells. Each robot work cell is fully enclosed and features
individual lighting, a low-cost custom 5 degree of freedom Cartesian robot arm
with an attached parallel jaw gripper and a dual camera setup for
experimentation. The system design is focused on continuous operation and
features a 10 Gbit/s network connectivity allowing for high throughput
remote-controlled experimentation and data collection for robotic manipulation.
CloudGripper furthermore is intended to form a community testbed to study the
challenges of large scale machine learning and cloud and edge-computing in the
context of robotic manipulation. In this work, we describe the mechanical
design of the system, its initial software stack and evaluate the repeatability
of motions executed by the proposed robot arm design. A local network API
throughput and latency analysis is also provided. CloudGripper-Rope-100, a
dataset of more than a hundred hours of randomized rope pushing interactions
and approximately 4 million camera images is collected and serves as a proof of
concept demonstrating data collection capabilities. A project website with more
information is available at https://cloudgripper.org.",http://arxiv.org/pdf/2309.12786v1
2309.12780v1,cs.CV,LMC: Large Model Collaboration with Cross-assessment for Training-Free Open-Set Object Recognition,2023-09-22 10:43:55+00:00,"Open-set object recognition aims to identify if an object is from a class
that has been encountered during training or not. To perform open-set object
recognition accurately, a key challenge is how to reduce the reliance on
spurious-discriminative features. In this paper, motivated by that different
large models pre-trained through different paradigms can possess very rich
while distinct implicit knowledge, we propose a novel framework named Large
Model Collaboration (LMC) to tackle the above challenge via collaborating
different off-the-shelf large models in a training-free manner. Moreover, we
also incorporate the proposed framework with several novel designs to
effectively extract implicit knowledge from large models. Extensive experiments
demonstrate the efficacy of our proposed framework. Code is available
\href{https://github.com/Harryqu123/LMC}{here}.",http://arxiv.org/pdf/2309.12780v1
2309.12768v1,cs.CV,WiCV@CVPR2023: The Eleventh Women In Computer Vision Workshop at the Annual CVPR Conference,2023-09-22 10:15:38+00:00,"In this paper, we present the details of Women in Computer Vision Workshop -
WiCV 2023, organized alongside the hybrid CVPR 2023 in Vancouver, Canada. WiCV
aims to amplify the voices of underrepresented women in the computer vision
community, fostering increased visibility in both academia and industry. We
believe that such events play a vital role in addressing gender imbalances
within the field. The annual WiCV@CVPR workshop offers a) opportunity for
collaboration between researchers from minority groups, b) mentorship for
female junior researchers, c) financial support to presenters to alleviate
finanacial burdens and d) a diverse array of role models who can inspire
younger researchers at the outset of their careers. In this paper, we present a
comprehensive report on the workshop program, historical trends from the past
WiCV@CVPR events, and a summary of statistics related to presenters, attendees,
and sponsorship for the WiCV 2023 workshop.",http://arxiv.org/pdf/2309.12768v1
2309.12761v1,cs.CV,S3TC: Spiking Separated Spatial and Temporal Convolutions with Unsupervised STDP-based Learning for Action Recognition,2023-09-22 10:05:35+00:00,"Video analysis is a major computer vision task that has received a lot of
attention in recent years. The current state-of-the-art performance for video
analysis is achieved with Deep Neural Networks (DNNs) that have high
computational costs and need large amounts of labeled data for training.
Spiking Neural Networks (SNNs) have significantly lower computational costs
(thousands of times) than regular non-spiking networks when implemented on
neuromorphic hardware. They have been used for video analysis with methods like
3D Convolutional Spiking Neural Networks (3D CSNNs). However, these networks
have a significantly larger number of parameters compared with spiking 2D CSNN.
This, not only increases the computational costs, but also makes these networks
more difficult to implement with neuromorphic hardware. In this work, we use
CSNNs trained in an unsupervised manner with the Spike Timing-Dependent
Plasticity (STDP) rule, and we introduce, for the first time, Spiking Separated
Spatial and Temporal Convolutions (S3TCs) for the sake of reducing the number
of parameters required for video analysis. This unsupervised learning has the
advantage of not needing large amounts of labeled data for training.
Factorizing a single spatio-temporal spiking convolution into a spatial and a
temporal spiking convolution decreases the number of parameters of the network.
We test our network with the KTH, Weizmann, and IXMAS datasets, and we show
that S3TCs successfully extract spatio-temporal information from videos, while
increasing the output spiking activity, and outperforming spiking 3D
convolutions.",http://arxiv.org/pdf/2309.12761v1
2309.12745v1,astro-ph.GA,Revised gas-phase formation network of methyl cyanide: the origin of methyl cyanide and methanol abundance correlation in hot corinos,2023-09-22 09:45:13+00:00,"Methyl cyanide (CH$_3$CN) is one of the most abundant and widely spread
interstellar complex organic molecules (iCOMs). Several studies found that, in
hot corinos, methyl cyanide and methanol abundances are correlated suggesting a
chemical link, often interpreted as a synthesis of them on the interstellar
grain surfaces. In this article, we present a revised network of the reactions
forming methyl cyanide in the gas-phase. We carried out an exhaustive review of
the gas-phase CH$_3$CN formation routes, propose two new reactions and
performed new quantum mechanics computations of several reactions. We found
that 13 of the 15 reactions reported in the databases KIDA and UDfA have
incorrect products and/or rate constants. The new corrected reaction network
contains 10 reactions leading to methyl cyanide. We tested the relative
importance of those reactions in forming CH$_3$CN using our astrochemical
model. We confirm that the radiative association of CH${_3}{^+}$ and HCN,
forming CH$_{3}$CNH$^{+}$, followed by the electron recombination of
CH$_{3}$CNH$^{+}$, is the most important CH$_3$CN formation route in both cold
and warm environments, notwithstanding that we significantly corrected the rate
constants and products of both reactions. The two newly proposed reactions play
an important role in warm environments. Finally, we found a very good agreement
between the CH$_3$CN predicted abundances with those measured in cold ($\sim$10
K) and warm ($\sim$90 K) objects. Unexpectedly, we also found a chemical link
between methanol and methyl cyanide via the CH$_{3}^{+}$ ion, which can explain
the observed correlation between the CH$_3$OH and CH$_3$CN abundances measured
in hot corinos.",http://arxiv.org/pdf/2309.12745v1
2309.12743v1,astro-ph.HE,Cosmic-ray propagation in extragalactic space and secondary messengers,2023-09-22 09:44:55+00:00,"These notes summarize the lectures about ""Cosmic-ray propagation in
extragalactic space and secondary messengers"", focusing in particular on the
interactions of cosmic-ray particles with the background photons in the
Universe, including nuclear species heavier than hydrogen, and on the
analytical computation of the expected cosmic-ray fluxes at Earth. The lectures
were held at the Course 208 of the International School of Physics ""Enrico
Fermi"" on ""Foundations of Cosmic-Ray Astrophysics"", in Varenna (Como, Italy)
from June 23rd to June 29th, 2022. These notes are complementary to the content
of the lectures held by Pasquale Dario Serpico at the same school.",http://arxiv.org/pdf/2309.12743v1
2309.12739v1,hep-th,Tackling Feynman integrals with quantum minimization algorithms,2023-09-22 09:41:09+00:00,"One of the most severe bottlenecks to reach high-precision predictions in QFT
is the calculation of multiloop multileg Feynman integrals. Several new
strategies have been proposed in the last years, allowing impressive results
with deep implications in particle physics. Still, the efficiency of such
techniques starts to drastically decrease when including many loops and legs.
In this talk, we explore the implementation of quantum algorithms to optimize
the integrands of scattering amplitudes. We rely on the manifestly causal
loop-tree duality, which translates the loop into phase-space integrals and
avoids the spurious singularities due to non-causal effects. Then, we built a
Hamiltonian codifying causal-compatible contributions and minimize it using a
Variational Quantum Eigensolver. Our very promising results point towards a
potential speed-up for achieving a more numerically-stable representation of
Feynman integrals by using quantum computers.",http://arxiv.org/pdf/2309.12739v1
2309.12735v1,cs.GT,Optimal Dynamic Fees for Blockchain Resources,2023-09-22 09:34:33+00:00,"We develop a general and practical framework to address the problem of the
optimal design of dynamic fee mechanisms for multiple blockchain resources. Our
framework allows to compute policies that optimally trade-off between adjusting
resource prices to handle persistent demand shifts versus being robust to local
noise in the observed block demand. In the general case with more than one
resource, our optimal policies correctly handle cross-effects (complementarity
and substitutability) in resource demands. We also show how these cross-effects
can be used to inform resource design, i.e. combining resources into bundles
that have low demand-side cross-effects can yield simpler and more efficient
price-update rules. Our framework is also practical, we demonstrate how it can
be used to refine or inform the design of heuristic fee update rules such as
EIP-1559 or EIP-4844 with two case studies. We then estimate a uni-dimensional
version of our model using real market data from the Ethereum blockchain and
empirically compare the performance of our optimal policies to EIP-1559.",http://arxiv.org/pdf/2309.12735v1
2309.12713v1,cs.CR,HammerHead: Leader Reputation for Dynamic Scheduling,2023-09-22 08:51:07+00:00,"The need for high throughput and censorship resistance in blockchain
technology has led to research on DAG-based consensus. The Sui blockchain
protocol uses a variant of the Bullshark consensus algorithm due to its lower
latency, but this leader-based protocol causes performance issues when
candidate leaders crash. In this paper, we explore the ideas pioneered by
Carousel on providing Leader-Utilization and present HammerHead. Unlike
Carousel, which is built with a chained and pipelined consensus protocol in
mind, HammerHead does not need to worry about chain quality as it is directly
provided by the DAG, but needs to make sure that even though validators might
commit blocks in different views the safety and liveness is preserved. Our
implementation of HammerHead shows a slight performance increase in a faultless
setting, and a drastic 2x latency reduction and up to 40% throughput increase
when suffering faults (100 validators, 33 faults).",http://arxiv.org/pdf/2309.12713v1
2309.12710v1,cs.LO,Do Repeat Yourself: Understanding Sufficient Conditions for Restricted Chase Non-Termination,2023-09-22 08:43:24+00:00,"The disjunctive restricted chase is a sound and complete procedure for
solving boolean conjunctive query entailment over knowledge bases of
disjunctive existential rules. Alas, this procedure does not always terminate
and checking if it does is undecidable. However, we can use acyclicity notions
(sufficient conditions that imply termination) to effectively apply the chase
in many real-world cases. To know if these conditions are as general as
possible, we can use cyclicity notions (sufficient conditions that imply
non-termination). In this paper, we discuss some issues with previously
existing cyclicity notions, propose some novel notions for non-termination by
dismantling the original idea, and empirically verify the generality of the new
criteria.",http://arxiv.org/pdf/2309.12710v1
2309.12701v1,cs.LG,Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming,2023-09-22 08:18:08+00:00,"Decision trees are known to be intrinsically interpretable as they can be
inspected and interpreted by humans. Furthermore, recent hardware advances have
rekindled an interest for optimal decision tree algorithms, that produce more
accurate trees than the usual greedy approaches. However, these optimal
algorithms return a single tree optimizing a hand defined
interpretability-performance trade-off, obtained by specifying a maximum number
of decision nodes, giving no further insights about the quality of this
trade-off. In this paper, we propose a new Markov Decision Problem (MDP)
formulation for finding optimal decision trees. The main interest of this
formulation is that we can compute the optimal decision trees for several
interpretability-performance trade-offs by solving a single dynamic program,
letting the user choose a posteriori the tree that best suits their needs.
Empirically, we show that our method is competitive with state-of-the-art
algorithms in terms of accuracy and runtime while returning a whole set of
trees on the interpretability-performance Pareto front.",http://arxiv.org/pdf/2309.12701v1
2309.12700v1,cs.CV,mixed attention auto encoder for multi-class industrial anomaly detection,2023-09-22 08:17:48+00:00,"Most existing methods for unsupervised industrial anomaly detection train a
separate model for each object category. This kind of approach can easily
capture the category-specific feature distributions, but results in high
storage cost and low training efficiency. In this paper, we propose a unified
mixed-attention auto encoder (MAAE) to implement multi-class anomaly detection
with a single model. To alleviate the performance degradation due to the
diverse distribution patterns of different categories, we employ spatial
attentions and channel attentions to effectively capture the global category
information and model the feature distributions of multiple classes.
Furthermore, to simulate the realistic noises on features and preserve the
surface semantics of objects from different categories which are essential for
detecting the subtle anomalies, we propose an adaptive noise generator and a
multi-scale fusion module for the pre-trained features. MAAE delivers
remarkable performances on the benchmark dataset compared with the
state-of-the-art methods.",http://arxiv.org/pdf/2309.12700v1
2309.12699v1,hep-th,Center-symmetric Landau gauge: Further signatures of confinement,2023-09-22 08:14:46+00:00,"In a recent article [1], we have identified new signatures for the Yang-Mills
deconfinement transition, based on the finite-temperature longitudinal or
(chromo-)electric gluon propagator as computed in the center-symmetric Landau
gauge. Here, we generalize these considerations into a systematic study of the
center symmetry identities obeyed by the correlation functions in this gauge.
Any violation of these constraints signals the breaking of center symmetry and
can thus serve as a probe for the deconfinement transition.",http://arxiv.org/pdf/2309.12699v1
2309.12695v1,math.NA,Total positivity and least squares problems in the Lagrange basis,2023-09-22 08:10:21+00:00,"The problem of polynomial least squares fitting in the standard Lagrange
basis is addressed in this work. Although the matrices involved in the
corresponding overdetermined linear systems are not totally positive,
rectangular totally positive Lagrange-Vandermonde matrices are used to take
advantage of total positivity in the construction of accurate algorithms to
solve the considered problem. In particular, a fast and accurate algorithm to
compute the bidiagonal decomposition of such rectangular totally positive
matrices is crucial to solve the problem. This algorithm also allows the
accurate computation of the Moore-Penrose inverse and the projection matrix of
the collocation matrices involved in these problems. Numerical experiments
showing the good behaviour of the proposed algorithms are included.",http://arxiv.org/pdf/2309.12695v1
2309.12680v1,eess.SY,"Can Urban Air Mobility become reality? Opportunities, challenges and selected research results",2023-09-22 07:37:45+00:00,"Urban Air Mobility (UAM) is a new air transportation system for passengers
and cargo in urban environments, enabled by new technologies and integrated
into multimodal transportation systems. The vision of UAM comprises the mass
use in urban and suburban environments, complementing existing transportation
systems and contributing to the decarbonization of the transport sector.
Initial attempts to create a market for urban air transportation in the last
century failed due to lack of profitability and community acceptance.
Technological advances in numerous fields over the past few decades have led to
a renewed interest in urban air transportation. UAM is expected to benefit
users and to also have a positive impact on the economy by creating new markets
and employment opportunities for manufacturing and operation of UAM vehicles
and the construction of related ground infrastructure. However, there are also
concerns about noise, safety and security, privacy and environmental impacts.
Therefore, the UAM system needs to be designed carefully to become safe,
affordable, accessible, environmentally friendly, economically viable and thus
sustainable. This paper provides an overview of selected key research topics
related to UAM and how the German Aerospace Center (DLR) contributed to this
research in the project ""HorizonUAM - Urban Air Mobility Research at the German
Aerospace Center (DLR)"". Selected research results that support the realization
of the UAM vision are briefly presented.",http://arxiv.org/pdf/2309.12680v1
2309.12664v1,stat.CO,Langevin Quasi-Monte Carlo,2023-09-22 07:15:18+00:00,"Langevin Monte Carlo (LMC) and its stochastic gradient versions are powerful
algorithms for sampling from complex high-dimensional distributions. To sample
from a distribution with density $\pi(\theta)\propto \exp(-U(\theta)) $, LMC
iteratively generates the next sample by taking a step in the gradient
direction $\nabla U$ with added Gaussian perturbations. Expectations w.r.t. the
target distribution $\pi$ are estimated by averaging over LMC samples. In
ordinary Monte Carlo, it is well known that the estimation error can be
substantially reduced by replacing independent random samples by quasi-random
samples like low-discrepancy sequences. In this work, we show that the
estimation error of LMC can also be reduced by using quasi-random samples.
Specifically, we propose to use completely uniformly distributed (CUD)
sequences with certain low-discrepancy property to generate the Gaussian
perturbations. Under smoothness and convexity conditions, we prove that LMC
with a low-discrepancy CUD sequence achieves smaller error than standard LMC.
The theoretical analysis is supported by compelling numerical experiments,
which demonstrate the effectiveness of our approach.",http://arxiv.org/pdf/2309.12664v1
2309.12652v1,cond-mat.str-el,Classification of Classical Spin Liquids: Topological Quantum Chemistry and Crystalline Symmetry,2023-09-22 06:45:47+00:00,"Frustrated magnetic systems can host highly interesting phases known as
classical spin liquids (CSLs), which feature {extensive} ground state
degeneracy and lack long-range magnetic order. Recently, Yan and Benton et al.
proposed a classification scheme of CSLs in the large-$\mathcal{N}$ (soft spin)
limit [arXiv.2305.00155, arXiv:2305.19189]. This scheme classifies CSLs into
two categories: the algebraic CSLs and the fragile topological CSLs, each with
their own correlation properties, low energy effective description, and finer
classification frameworks. In this work, we further develop the classification
scheme by considering the role of crystalline symmetry. We present a
mathematical framework for computing the band representation of the flat bands
in the spectrum of these CSLs, which extends beyond the conventional
representation analysis. It allows one to determine whether the algebraic CSLs,
which features gapless points on their bottom flat bands, are protected by
symmetry or not. It also provides more information on the finer classifications
of algebraic and fragile topological CSLs. We demonstrate this framework via
concrete examples and showcase its power by constructing a pinch-line algebraic
CSL protected by symmetry.",http://arxiv.org/pdf/2309.12652v1
2309.12650v1,cs.CV,"FP-PET: Large Model, Multiple Loss And Focused Practice",2023-09-22 06:44:28+00:00,"This study presents FP-PET, a comprehensive approach to medical image
segmentation with a focus on CT and PET images. Utilizing a dataset from the
AutoPet2023 Challenge, the research employs a variety of machine learning
models, including STUNet-large, SwinUNETR, and VNet, to achieve
state-of-the-art segmentation performance. The paper introduces an aggregated
score that combines multiple evaluation metrics such as Dice score, false
positive volume (FPV), and false negative volume (FNV) to provide a holistic
measure of model effectiveness. The study also discusses the computational
challenges and solutions related to model training, which was conducted on
high-performance GPUs. Preprocessing and postprocessing techniques, including
gaussian weighting schemes and morphological operations, are explored to
further refine the segmentation output. The research offers valuable insights
into the challenges and solutions for advanced medical image segmentation.",http://arxiv.org/pdf/2309.12650v1
2309.12642v1,cs.CV,RHINO: Regularizing the Hash-based Implicit Neural Representation,2023-09-22 06:20:41+00:00,"The use of Implicit Neural Representation (INR) through a hash-table has
demonstrated impressive effectiveness and efficiency in characterizing
intricate signals. However, current state-of-the-art methods exhibit
insufficient regularization, often yielding unreliable and noisy results during
interpolations. We find that this issue stems from broken gradient flow between
input coordinates and indexed hash-keys, where the chain rule attempts to model
discrete hash-keys, rather than the continuous coordinates. To tackle this
concern, we introduce RHINO, in which a continuous analytical function is
incorporated to facilitate regularization by connecting the input coordinate
and the network additionally without modifying the architecture of current
hash-based INRs. This connection ensures a seamless backpropagation of
gradients from the network's output back to the input coordinates, thereby
enhancing regularization. Our experimental results not only showcase the
broadened regularization capability across different hash-based INRs like DINER
and Instant NGP, but also across a variety of tasks such as image fitting,
representation of signed distance functions, and optimization of 5D static / 6D
dynamic neural radiance fields. Notably, RHINO outperforms current
state-of-the-art techniques in both quality and speed, affirming its
superiority.",http://arxiv.org/pdf/2309.12642v1
2309.12641v1,cs.CV,Global Context Aggregation Network for Lightweight Saliency Detection of Surface Defects,2023-09-22 06:19:11+00:00,"Surface defect inspection is a very challenging task in which surface defects
usually show weak appearances or exist under complex backgrounds. Most
high-accuracy defect detection methods require expensive computation and
storage overhead, making them less practical in some resource-constrained
defect detection applications. Although some lightweight methods have achieved
real-time inference speed with fewer parameters, they show poor detection
accuracy in complex defect scenarios. To this end, we develop a Global Context
Aggregation Network (GCANet) for lightweight saliency detection of surface
defects on the encoder-decoder structure. First, we introduce a novel
transformer encoder on the top layer of the lightweight backbone, which
captures global context information through a novel Depth-wise Self-Attention
(DSA) module. The proposed DSA performs element-wise similarity in channel
dimension while maintaining linear complexity. In addition, we introduce a
novel Channel Reference Attention (CRA) module before each decoder block to
strengthen the representation of multi-level features in the bottom-up path.
The proposed CRA exploits the channel correlation between features at different
layers to adaptively enhance feature representation. The experimental results
on three public defect datasets demonstrate that the proposed network achieves
a better trade-off between accuracy and running efficiency compared with other
17 state-of-the-art methods. Specifically, GCANet achieves competitive accuracy
(91.79% $F_{\beta}^{w}$, 93.55% $S_\alpha$, and 97.35% $E_\phi$) on
SD-saliency-900 while running 272fps on a single gpu.",http://arxiv.org/pdf/2309.12641v1
2309.12640v1,cs.GT,MEV Makes Everyone Happy under Greedy Sequencing Rule,2023-09-22 06:12:19+00:00,"Trading through decentralized exchanges (DEXs) has become crucial in today's
blockchain ecosystem, enabling users to swap tokens efficiently and
automatically. However, the capacity of miners to strategically order
transactions has led to exploitative practices (e.g., front-running attacks,
sandwich attacks) and gain substantial Maximal Extractable Value (MEV) for
their own advantage. To mitigate such manipulation, Ferreira and Parkes
recently proposed a greedy sequencing rule such that the execution price of
transactions in a block moves back and forth around the starting price.
Utilizing this sequencing rule makes it impossible for miners to conduct
sandwich attacks, consequently mitigating the MEV problem.
  However, no sequencing rule can prevent miners from obtaining risk-free
profits. This paper systemically studies the computation of a miner's optimal
strategy for maximizing MEV under the greedy sequencing rule, where the utility
of miners is measured by the overall value of their token holdings. Our results
unveil a dichotomy between the no trading fee scenario, which can be optimally
strategized in polynomial time, and the scenario with a constant fraction of
trading fee, where finding the optimal strategy is proven NP-hard. The latter
represents a significant challenge for miners seeking optimal MEV.
  Following the computation results, we further show a remarkable phenomenon:
Miner's optimal MEV also benefits users. Precisely, in the scenarios without
trading fees, when miners adopt the optimal strategy given by our algorithm,
all users' transactions will be executed, and each user will receive equivalent
or surpass profits compared to their expectations. This outcome provides
further support for the study and design of sequencing rules in decentralized
exchanges.",http://arxiv.org/pdf/2309.12640v1
2309.12639v1,cs.CV,CINFormer: Transformer network with multi-stage CNN feature injection for surface defect segmentation,2023-09-22 06:12:02+00:00,"Surface defect inspection is of great importance for industrial manufacture
and production. Though defect inspection methods based on deep learning have
made significant progress, there are still some challenges for these methods,
such as indistinguishable weak defects and defect-like interference in the
background. To address these issues, we propose a transformer network with
multi-stage CNN (Convolutional Neural Network) feature injection for surface
defect segmentation, which is a UNet-like structure named CINFormer. CINFormer
presents a simple yet effective feature integration mechanism that injects the
multi-level CNN features of the input image into different stages of the
transformer network in the encoder. This can maintain the merit of CNN
capturing detailed features and that of transformer depressing noises in the
background, which facilitates accurate defect detection. In addition, CINFormer
presents a Top-K self-attention module to focus on tokens with more important
information about the defects, so as to further reduce the impact of the
redundant background. Extensive experiments conducted on the surface defect
datasets DAGM 2007, Magnetic tile, and NEU show that the proposed CINFormer
achieves state-of-the-art performance in defect detection.",http://arxiv.org/pdf/2309.12639v1
2309.12630v1,cs.CV,Decision Fusion Network with Perception Fine-tuning for Defect Classification,2023-09-22 05:41:25+00:00,"Surface defect inspection is an important task in industrial inspection. Deep
learning-based methods have demonstrated promising performance in this domain.
Nevertheless, these methods still suffer from misjudgment when encountering
challenges such as low-contrast defects and complex backgrounds. To overcome
these issues, we present a decision fusion network (DFNet) that incorporates
the semantic decision with the feature decision to strengthen the decision
ability of the network. In particular, we introduce a decision fusion module
(DFM) that extracts a semantic vector from the semantic decision branch and a
feature vector for the feature decision branch and fuses them to make the final
classification decision. In addition, we propose a perception fine-tuning
module (PFM) that fine-tunes the foreground and background during the
segmentation stage. PFM generates the semantic and feature outputs that are
sent to the classification decision stage. Furthermore, we present an
inner-outer separation weight matrix to address the impact of label edge
uncertainty during segmentation supervision. Our experimental results on the
publicly available datasets including KolektorSDD2 (96.1% AP) and
Magnetic-tile-defect-datasets (94.6% mAP) demonstrate the effectiveness of the
proposed method.",http://arxiv.org/pdf/2309.12630v1
2309.12624v1,cs.NI,Quark: A High-Performance Secure Container Runtime for Serverless Computing,2023-09-22 05:11:48+00:00,"Secure container runtimes serve as the foundational layer for creating and
running containers, which is the bedrock of emerging computing paradigms like
microservices and serverless computing. Although existing secure container
runtimes indeed enhance security via running containers over a guest kernel and
a Virtual Machine Monitor (VMM or Hypervisor), they incur performance penalties
in critical areas such as networking, container startup, and I/O system calls.
  In our practice of operating microservices and serverless computing, we build
a high-performance secure container runtime named Quark. Unlike existing
solutions that rely on traditional VM technologies by importing Linux for the
guest kernel and QEMU for the VMM, we take a different approach to building
Quark from the ground up, paving the way for extreme customization to unlock
high performance. Our development centers on co-designing a custom guest kernel
and a VMM for secure containers. To this end, we build a lightweight guest OS
kernel named QKernel and a specialized VMM named QVisor. The QKernel-QVisor
codesign allows us to deliver three key advancements: high-performance
RDMA-based container networking, fast container startup mode, and efficient
mechanisms for executing I/O syscalls. In our practice with real-world apps
like Redis, Quark cuts down P95 latency by 79.3% and increases throughput by
2.43x compared to Kata. Moreover, Quark container startup achieves 96.5% lower
latency than the cold-start mode while saving 81.3% memory cost to the
keep-warm mode. Quark is open-source with an industry-standard codebase in
Rust.",http://arxiv.org/pdf/2309.12624v1
2309.12612v1,cs.DC,WattScope: Non-intrusive Application-level Power Disaggregation in Datacenters,2023-09-22 04:13:46+00:00,"Datacenter capacity is growing exponentially to satisfy the increasing demand
for emerging computationally-intensive applications, such as deep learning.
This trend has led to concerns over datacenters' increasing energy consumption
and carbon footprint. The basic prerequisite for optimizing a datacenter's
energy- and carbon-efficiency is accurately monitoring and attributing energy
consumption to specific users and applications. Since datacenter servers tend
to be multi-tenant, i.e., they host many applications, server- and rack-level
power monitoring alone does not provide insight into their resident
applications' energy usage and carbon emissions. At the same time, current
application-level energy monitoring and attribution techniques are intrusive:
they require privileged access to servers and require coordinated support in
hardware and software, which is not always possible in cloud. To address the
problem, we design WattScope, a system for non-intrusively estimating the power
consumption of individual applications using external measurements of a
server's aggregate power usage without requiring direct access to the server's
operating system or applications. Our key insight is that, based on an analysis
of production traces, the power characteristics of datacenter workloads, e.g.,
low variability, low magnitude, and high periodicity, are highly amenable to
disaggregation of a server's total power consumption into application-specific
values. WattScope adapts and extends a machine learning-based technique for
disaggregating building power and applies it to server- and rack-level power
meter measurements in data centers. We evaluate WattScope's accuracy on a
production workload and show that it yields high accuracy, e.g., often <10%
normalized mean absolute error, and is thus a potentially useful tool for
datacenters in externally monitoring application-level power usage.",http://arxiv.org/pdf/2309.12612v1
2309.12605v1,quant-ph,Privacy-Preserving Quantum Two-Party Geometric Intersection,2023-09-22 03:39:01+00:00,"Privacy-preserving computational geometry is the research area on the
intersection of the domains of secure multi-party computation (SMC) and
computational geometry. As an important field, the privacy-preserving geometric
intersection (PGI) problem is when each of the multiple parties has a private
geometric graph and seeks to determine whether their graphs intersect or not
without revealing their private information. In this study, through
representing Alice's (Bob's) private geometric graph G_A (G_B) as the set of
numbered grids S_A (S_B), an efficient privacy-preserving quantum two-party
geometric intersection (PQGI) protocol is proposed. In the protocol, the oracle
operation O_A (O_B) is firstly utilized to encode the private elements of
S_A=(a_0, a_1, ..., a_(M-1)) (S_B=(b_0, b_1, ..., b_(N-1))) into the quantum
states, and then the oracle operation O_f is applied to obtain a new quantum
state which includes the XOR results between each element of S_A and S_B.
Finally, the quantum counting is introduced to get the amount (t) of the states
|a_i+b_j> equaling to |0>, and the intersection result can be obtained by
judging t>0 or not. Compared with classical PGI protocols, our proposed
protocol not only has higher security, but also holds lower communication
complexity.",http://arxiv.org/pdf/2309.12605v1
2309.12603v1,cond-mat.mtrl-sci,Ultrathin Magnesium-based Coating as an Efficient Oxygen Barrier for Superconducting Circuit Materials,2023-09-22 03:31:51+00:00,"Scaling up superconducting quantum circuits based on transmon qubits
necessitates substantial enhancements in qubit coherence time. Among the
materials considered for transmon qubits, tantalum (Ta) has emerged as a
promising candidate, surpassing conventional counterparts in terms of coherence
time. However, the presence of an amorphous surface Ta oxide layer introduces
dielectric loss, ultimately placing a limit on the coherence time. In this
study, we present a novel approach for suppressing the formation of tantalum
oxide using an ultrathin magnesium (Mg) capping layer deposited on top of
tantalum. Synchrotron-based X-ray photoelectron spectroscopy (XPS) studies
demonstrate that oxide is confined to an extremely thin region directly beneath
the Mg/Ta interface. Additionally, we demonstrate that the superconducting
properties of thin Ta films are improved following the Mg capping, exhibiting
sharper and higher-temperature transitions to superconductive and magnetically
ordered states. Based on the experimental data and computational modeling, we
establish an atomic-scale mechanistic understanding of the role of the capping
layer in protecting Ta from oxidation. This work provides valuable insights
into the formation mechanism and functionality of surface tantalum oxide, as
well as a new materials design principle with the potential to reduce
dielectric loss in superconducting quantum materials. Ultimately, our findings
pave the way for the realization of large-scale, high-performance quantum
computing systems.",http://arxiv.org/pdf/2309.12603v1
2309.12597v1,math.MG,On Axial Symmetry in Convex Bodies,2023-09-22 02:59:38+00:00,"For a two-dimensional convex body, the Kovner-Besicovitch measure of symmetry
is defined as the volume ratio of the largest centrally symmetric body
contained inside the body to the original body. A classical result states that
the Kovner-Besicovitch measure is at least $2/3$ for every convex body and
equals $2/3$ for triangles. Lassak showed that an alternative measure of
symmetry, i.e., symmetry about a line (axiality) has a value of at least $2/3$
for every convex body. However, the smallest known value of the axiality of a
convex body is around $0.81584$, achieved by a convex quadrilateral. We show
that every plane convex body has axiality at least $\frac{2}{41}(10 + 3
\sqrt{2}) \approx 0.69476$, thereby establishing a separation with the central
symmetry measure. Moreover, we find a family of convex quadrilaterals with
axiality approaching $\frac{1}{3}(\sqrt{2}+1) \approx 0.80474$. We also
establish improved bounds for a ``folding"" measure of axial symmetry for plane
convex bodies. Finally, we establish improved bounds for a generalization of
axiality to high-dimensional convex bodies.",http://arxiv.org/pdf/2309.12597v1
2309.12594v1,cs.CV,DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image,2023-09-22 02:46:43+00:00,"Accurate 3D shape abstraction from a single 2D image is a long-standing
problem in computer vision and graphics. By leveraging a set of primitives to
represent the target shape, recent methods have achieved promising results.
However, these methods either use a relatively large number of primitives or
lack geometric flexibility due to the limited expressibility of the primitives.
In this paper, we propose a novel bi-channel Transformer architecture,
integrated with parameterized deformable models, termed DeFormer, to
simultaneously estimate the global and local deformations of primitives. In
this way, DeFormer can abstract complex object shapes while using a small
number of primitives which offer a broader geometry coverage and finer details.
Then, we introduce a force-driven dynamic fitting and a cycle-consistent
re-projection loss to optimize the primitive parameters. Extensive experiments
on ShapeNet across various settings show that DeFormer achieves better
reconstruction accuracy over the state-of-the-art, and visualizes with
consistent semantic correspondences for improved interpretability.",http://arxiv.org/pdf/2309.12594v1
2309.12591v1,cs.CY,Before Blue Birds Became X-tinct: Understanding the Effect of Regime Change on Twitter's Advertising and Compliance of Advertising Policies,2023-09-22 02:39:53+00:00,"Social media platforms, including Twitter (now X), have policies in place to
maintain a safe and trustworthy advertising environment. However, the extent to
which these policies are adhered to and enforced remains a subject of interest
and concern. We present the first large-scale audit of advertising on Twitter
focusing on compliance with the platform's advertising policies, particularly
those related to political and adult content. We investigate the compliance of
advertisements on Twitter with the platform's stated policies and the impact of
recent acquisition on the advertising activity of the platform. By analyzing
34K advertisements from ~6M tweets, collected over six months, we find evidence
of widespread noncompliance with Twitter's political and adult content
advertising policies suggesting a lack of effective ad content moderation. We
also find that Elon Musk's acquisition of Twitter had a noticeable impact on
the advertising landscape, with most existing advertisers either completely
stopping their advertising activity or reducing it. Major brands decreased
their advertising on Twitter, suggesting a negative immediate effect on the
platform's advertising revenue. Our findings underscore the importance of
external audits to monitor compliance and improve transparency in online
advertising.",http://arxiv.org/pdf/2309.12591v1
2309.12582v1,math-ph,On the interplay between vortices and harmonic flows: Hodge decomposition of Euler's equations in 2d,2023-09-22 02:17:22+00:00,"Let $\Sigma$ be a compact manifold without boundary whose first homology is
nontrivial. Hodge decomposition of the incompressible Euler's equation in terms
of 1-forms yields a coupled PDE-ODE system. The $L^2$-orthogonal components are
a `pure' vorticity flow and a potential flow (harmonic, with the dimension of
the homology). In this paper we focus on $N$ point vortices on a compact
Riemann surface without boundary of genus $g$, with a metric chosen in the
conformal class.
  The phase space has finite dimension $2N+ 2g$. We compute a surface of
section for the motion of a single vortex ($N=1$) on a torus ($g=1$) with a
non-flat metric, that shows typical features of non-integrable 2-dof
Hamiltonians. In contradistinction, for flat tori the harmonic part is
constant. Next, we turn to hyperbolic surfaces ($ g \geq 2$), having constant
curvature -1, with discrete symmetries. Fixed points of involutions yield
vortex crystals in the Poincar\'e disk. Finally we consider multiply connected
planar domains. The image method due to Green and Thomson is viewed in the
Schottky double. The Kirchhoff-Routh hamiltonian given in C.C. Lin's celebrated
theorem is recovered by Marsden-Weinstein reduction from $2N+2g$ to $2N$.
  The relation between the electrostatic Green function and the hydrodynamical
Green function is clarified.
  A number of questions are suggested.",http://arxiv.org/pdf/2309.12582v1
2309.12578v1,cs.LG,SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling,2023-09-22 02:14:46+00:00,"Sparsifying the Transformer has garnered considerable interest, as training
the Transformer is very computationally demanding. Prior efforts to sparsify
the Transformer have either used a fixed pattern or data-driven approach to
reduce the number of operations involving the computation of multi-head
attention, which is the main bottleneck of the Transformer. However, existing
methods suffer from inevitable problems, such as the potential loss of
essential sequence features due to the uniform fixed pattern applied across all
layers, and an increase in the model size resulting from the use of additional
parameters to learn sparsity patterns in attention operations. In this paper,
we propose a novel sparsification scheme for the Transformer that integrates
convolution filters and the flood filling method to efficiently capture the
layer-wise sparse pattern in attention operations. Our sparsification approach
reduces the computational complexity and memory footprint of the Transformer
during training. Efficient implementations of the layer-wise sparsified
attention algorithm on GPUs are developed, demonstrating a new SPION that
achieves up to 3.08X speedup over existing state-of-the-art sparse Transformer
models, with better evaluation quality.",http://arxiv.org/pdf/2309.12578v1
2309.12577v1,math.OC,Distributed Optimal Control and Application to Consensus of Multi-Agent Systems,2023-09-22 02:13:01+00:00,"This paper develops a novel approach to the consensus problem of multi-agent
systems by minimizing a weighted state error with neighbor agents via linear
quadratic (LQ) optimal control theory. Existing consensus control algorithms
only utilize the current state of each agent, and the design of distributed
controller depends on nonzero eigenvalues of the communication topology. The
presented optimal consensus controller is obtained by solving Riccati equations
and designing appropriate observers to account for agents' historical state
information. It is shown that the corresponding cost function under the
proposed controllers is asymptotically optimal. Simulation examples demonstrate
the effectiveness of the proposed scheme, and a much faster convergence speed
than the conventional consensus methods. Moreover, the new method avoids
computing nonzero eigenvalues of the communication topology as in the
traditional consensus methods.",http://arxiv.org/pdf/2309.12577v1
2309.12566v1,cs.RO,Recent Advances in Path Integral Control for Trajectory Optimization: An Overview in Theoretical and Algorithmic Perspectives,2023-09-22 01:34:55+00:00,"This paper presents a tutorial overview of path integral (PI) control
approaches for stochastic optimal control and trajectory optimization. We
concisely summarize the theoretical development of path integral control to
compute a solution for stochastic optimal control and provide algorithmic
descriptions of the cross-entropy (CE) method, an open-loop controller using
the receding horizon scheme known as the model predictive path integral (MPPI),
and a parameterized state feedback controller based on the path integral
control theory. We discuss policy search methods based on path integral
control, efficient and stable sampling strategies, extensions to multi-agent
decision-making, and MPPI for the trajectory optimization on manifolds. For
tutorial demonstrations, some PI-based controllers are implemented in MATLAB
and ROS2/Gazebo simulations for trajectory optimization. The simulation
frameworks and source codes are publicly available at
https://github.com/INHA-Autonomous-Systems-Laboratory-ASL/An-Overview-on-Recent-Advances-in-Path-Integral-Control.",http://arxiv.org/pdf/2309.12566v1
2309.12558v1,nucl-th,Some Approaches to the Energy-Loss Straggling Calculation,2023-09-22 01:05:35+00:00,"Some exact and approximate methods commonly used to calculate corrections to
the Bethe stopping formula are modified and adapted by the authors to the
calculation of the energy loss straggling. An intercomparison is carried out
for results of approaches developed in the present work. An excellent agreement
obtained between the results for ELS, calculated with an exact method
previously proposed by one of the authors and the results of the
Lindhard-Sorensen method for moderate relativistic energies. It is shown that
some modified by authors approximate methods for the ELS calculating have the
higher accuracy than the conventional approximate method of
Lijian-Qing-Zhengming. So, in all the cases considered, the accuracy of the
twice modified LQZ method in the ELS calculations is higher than the accuracy
of the usual LQZ method. A thrice modified LQZ method was also suggested in
this work. A number of shortened LQZ methods for the ELS calculating have been
proposed and developed too.",http://arxiv.org/pdf/2309.12558v1
2309.12557v1,cs.CV,Triple-View Knowledge Distillation for Semi-Supervised Semantic Segmentation,2023-09-22 01:02:21+00:00,"To alleviate the expensive human labeling, semi-supervised semantic
segmentation employs a few labeled images and an abundant of unlabeled images
to predict the pixel-level label map with the same size. Previous methods often
adopt co-training using two convolutional networks with the same architecture
but different initialization, which fails to capture the sufficiently diverse
features. This motivates us to use tri-training and develop the triple-view
encoder to utilize the encoders with different architectures to derive diverse
features, and exploit the knowledge distillation skill to learn the
complementary semantics among these encoders. Moreover, existing methods simply
concatenate the features from both encoder and decoder, resulting in redundant
features that require large memory cost. This inspires us to devise a
dual-frequency decoder that selects those important features by projecting the
features from the spatial domain to the frequency domain, where the
dual-frequency channel attention mechanism is introduced to model the feature
importance. Therefore, we propose a Triple-view Knowledge Distillation
framework, termed TriKD, for semi-supervised semantic segmentation, including
the triple-view encoder and the dual-frequency decoder. Extensive experiments
were conducted on two benchmarks, \ie, Pascal VOC 2012 and Cityscapes, whose
results verify the superiority of the proposed method with a good tradeoff
between precision and inference speed.",http://arxiv.org/pdf/2309.12557v1
2309.12554v1,physics.comp-ph,"Assessing r2SCAN meta-GGA functional for structural parameters, cohesive energy, mechanical modulus and thermophysical properties of 3d, 4d and 5d transition metals",2023-09-22 00:55:01+00:00,"The recent development of the accurate and efficient semilocal density
functionals on the third rung of Jacob's ladder of density functional theory
such as the revised regularized strongly constrained and appropriately normed
(r2SCAN) density functional could enable the rapid and highly reliable
prediction of the elasticity and temperature dependence of thermophysical
parameters of refractory elements and their intermetallic compounds using
quasi-harmonic approximation (QHA). Here, we present a comparative evaluation
of the equilibrium cell volumes, cohesive energy, mechanical moduli, and
thermophysical properties (Debye temperature and thermal expansion coefficient)
for 22 transition metals using semilocal density functionals, including local
density approximation (LDA), the Perdew-Burke-Ernzerhof (PBE) and PBEsol
generalized gradient approximations (GGA), and the r2SCAN meta-GGA. PBEsol and
r2SCAN deliver the same level of accuracies for structural, mechanical and
thermophysical properties. Otherwise, PBE and r2SCAN perform better than LDA
and PBEsol for calculating cohesive energies of transition metals. Among the
tested density functionals, r2SCAN provides an overall well-balanced
performance for reliably computing the cell volumes, cohesive energies,
mechanical properties, and thermophysical properties of various 3d, 4d, and 5d
transition metals using QHA. Therefore, we recommend that r2SCAN could be
employed as a workhorse method to evaluate the thermophysical properties of
transition metal compounds and alloys in the high throughput workflows.",http://arxiv.org/pdf/2309.12554v1
2309.12543v1,cs.RO,Real-time Batched Distance Computation for Time-Optimal Safe Path Tracking,2023-09-21 23:58:16+00:00,"In human-robot collaboration, there has been a trade-off relationship between
the speed of collaborative robots and the safety of human workers. In our
previous paper, we introduced a time-optimal path tracking algorithm designed
to maximize speed while ensuring safety for human workers. This algorithm runs
in real-time and provides the safe and fastest control input for every cycle
with respect to ISO standards. However, true optimality has not been achieved
due to inaccurate distance computation resulting from conservative model
simplification. To attain true optimality, we require a method that can compute
distances 1. at many robot configurations to examine along a trajectory 2. in
real-time for online robot control 3. as precisely as possible for optimal
control. In this paper, we propose a batched, fast and precise distance
checking method based on precomputed link-local SDFs. Our method can check
distances for 500 waypoints along a trajectory within less than 1 millisecond
using a GPU at runtime, making it suited for time-critical robotic control.
Additionally, a neural approximation has been proposed to accelerate
preprocessing by a factor of 2. Finally, we experimentally demonstrate that our
method can navigate a 6-DoF robot earlier than a geometric-primitives-based
distance checker in a dynamic and collaborative environment.",http://arxiv.org/pdf/2309.12543v1
2309.12542v1,quant-ph,Spatio-temporal correlations of noise in MOS spin qubits,2023-09-21 23:45:14+00:00,"In quantum computing, characterising the full noise profile of qubits can aid
the efforts towards increasing coherence times and fidelities by creating error
mitigating techniques specific to the type of noise in the system, or by
completely removing the sources of noise. Spin qubits in MOS quantum dots are
exposed to noise originated from the complex glassy behaviour of two-level
fluctuators, leading to non-trivial correlations between qubit properties both
in space and time. With recent engineering progress, large amounts of data are
being collected in typical spin qubit device experiments, and it is beneficiary
to explore data analysis options inspired from fields of research that are
experienced in managing large data sets, examples include astrophysics, finance
and climate science. Here, we propose and demonstrate wavelet-based analysis
techniques to decompose signals into both frequency and time components to gain
a deeper insight into the sources of noise in our systems. We apply the
analysis to a long feedback experiment performed on a state-of-the-art
two-qubit system in a pair of SiMOS quantum dots. The observed correlations
serve to identify common microscopic causes of noise, as well as to elucidate
pathways for multi-qubit operation with a more scalable feedback system.",http://arxiv.org/pdf/2309.12542v1
2309.12530v1,cs.CV,A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance,2023-09-21 23:06:19+00:00,"Domain generalization studies the problem of training a model with samples
from several domains (or distributions) and then testing the model with samples
from a new, unseen domain. In this paper, we propose a novel approach for
domain generalization that leverages recent advances in large vision-language
models, specifically a CLIP teacher model, to train a smaller model that
generalizes to unseen domains. The key technical contribution is a new type of
regularization that requires the student's learned image representations to be
close to the teacher's learned text representations obtained from encoding the
corresponding text descriptions of images. We introduce two designs of the loss
function, absolute and relative distance, which provide specific guidance on
how the training process of the student model should be regularized. We
evaluate our proposed method, dubbed RISE (Regularized Invariance with Semantic
Embeddings), on various benchmark datasets and show that it outperforms several
state-of-the-art domain generalization methods. To our knowledge, our work is
the first to leverage knowledge distillation using a large vision-language
model for domain generalization. By incorporating text-based information, RISE
improves the generalization capability of machine learning models.",http://arxiv.org/pdf/2309.12530v1
2309.12520v1,cond-mat.supr-con,Experimental and theoretical assessment of native oxide in the superconducting TaN,2023-09-21 22:48:34+00:00,"In this manuscript, we show through an experimental-computational proof of
concept the native oxide formation into superconducting TaN films. First, TaN
was synthesized at an ultra-high vacuum system by reactive pulsed laser
deposition and characterized in situ by X-ray photoelectron spectroscopy. The
material was also characterized ex situ by X-ray diffraction, transmission
electron microscopy, and the four-point probe method. It was detected that TaN
contained considerable oxygen impurities (up to 26 %O) even though it was grown
in an ultra-high vacuum chamber. Furthermore, the impurified TaN evidence a
face-centered cubic crystalline structure only and exhibits superconductivity
at 2.99 K. To understand the feasibility of the native oxide in TaN, we study
the effect of incorporating different amounts of O atoms in TaN using ab-initio
calculations. A thermodynamic stability analysis shows that a TaOxN1-x model
increases its stability as oxygen is added, demonstrating that oxygen may
always be present in TaN, even when obtained at ultra-high vacuum conditions.
All analyzed models exhibit metallic behavior. Charge density difference maps
reveal that N and O atoms have a higher charge density redistribution than Ta
atoms. The electron localization function maps and line profiles indicate that
Ta-O and Ta-N bonds are mainly ionic. As expected, stronger ionic behavior is
observed in the Ta-O bonds due to the electronegativity difference between O
and N atoms. Recent evidence points to superconductivity in bulk TaO,
confirming the asseverations of superconductivity in our samples. The results
discussed here highlight the importance of considering native oxide when
reporting superconductivity in TaN films since the TaO regions formed in the
compound may be key to understanding the different critical temperatures
reported in the literature.",http://arxiv.org/pdf/2309.12520v1
2309.12515v1,cs.LO,A Diamond Machine For Strong Evaluation,2023-09-21 22:43:10+00:00,"Abstract machines for strong evaluation of the $\lambda$-calculus enter into
arguments and have a set of transitions for backtracking out of an evaluated
argument. We study a new abstract machine which avoids backtracking by
splitting the run of the machine in smaller jobs, one for argument, and that
jumps directly to the next job once one is finished.
  Usually, machines are also deterministic and implement deterministic
strategies. Here we weaken this aspect and consider a light form of
non-determinism, namely the diamond property, for both the machine and the
strategy. For the machine, this introduces a modular management of jobs,
parametric in a scheduling policy. We then show how to obtain various
strategies, among which leftmost-outermost evaluation.",http://arxiv.org/pdf/2309.12515v1
2309.12513v1,cs.CC,"Mildly Exponential Lower Bounds on Tolerant Testers for Monotonicity, Unateness, and Juntas",2023-09-21 22:37:13+00:00,"We give the first super-polynomial (in fact, mildly exponential) lower bounds
for tolerant testing (equivalently, distance estimation) of monotonicity,
unateness, and juntas with a constant separation between the ""yes"" and ""no""
cases. Specifically, we give
  $\bullet$ A $2^{\Omega(n^{1/4}/\sqrt{\varepsilon})}$-query lower bound for
non-adaptive, two-sided tolerant monotonicity testers and unateness testers
when the ""gap"" parameter $\varepsilon_2-\varepsilon_1$ is equal to
$\varepsilon$, for any $\varepsilon \geq 1/\sqrt{n}$;
  $\bullet$ A $2^{\Omega(k^{1/2})}$-query lower bound for non-adaptive,
two-sided tolerant junta testers when the gap parameter is an absolute
constant.
  In the constant-gap regime no non-trivial prior lower bound was known for
monotonicity, the best prior lower bound known for unateness was
$\tilde{\Omega}(n^{3/2})$ queries, and the best prior lower bound known for
juntas was $\mathrm{poly}(k)$ queries.",http://arxiv.org/pdf/2309.12513v1
2309.12511v1,cond-mat.str-el,MatsubaraFunctions.jl: An equilibrium Green's function library in the Julia programming language,2023-09-21 22:30:59+00:00,"The Matsubara Green's function formalism stands as a powerful technique for
computing the thermodynamic characteristics of interacting quantum
many-particle systems at finite temperatures. In this manuscript, our focus
centers on introducing MatsubaraFunctions.jl, a Julia library that implements
data structures for generalized n-point Green's functions on Matsubara
frequency grids. The package's architecture prioritizes user-friendliness
without compromising the development of efficient solvers for quantum field
theories in equilibrium. Following a comprehensive introduction of the
fundamental types, we delve into a thorough examination of key facets of the
interface. This encompasses avenues for accessing Green's functions, techniques
for extrapolation and interpolation, as well as the incorporation of symmetries
and a variety of parallelization strategies. Examples of increasing complexity
serve to demonstrate the practical utility of the library, supplemented by
discussions on strategies for sidestepping impediments to optimal performance.",http://arxiv.org/pdf/2309.12511v1
2309.12506v1,cs.CV,License Plate Super-Resolution Using Diffusion Models,2023-09-21 22:06:23+00:00,"In surveillance, accurately recognizing license plates is hindered by their
often low quality and small dimensions, compromising recognition precision.
Despite advancements in AI-based image super-resolution, methods like
Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs)
still fall short in enhancing license plate images. This study leverages the
cutting-edge diffusion model, which has consistently outperformed other deep
learning techniques in image restoration. By training this model using a
curated dataset of Saudi license plates, both in low and high resolutions, we
discovered the diffusion model's superior efficacy. The method achieves a
12.55\% and 37.32% improvement in Peak Signal-to-Noise Ratio (PSNR) over SwinIR
and ESRGAN, respectively. Moreover, our method surpasses these techniques in
terms of Structural Similarity Index (SSIM), registering a 4.89% and 17.66%
improvement over SwinIR and ESRGAN, respectively. Furthermore, 92% of human
evaluators preferred our images over those from other algorithms. In essence,
this research presents a pioneering solution for license plate
super-resolution, with tangible potential for surveillance systems.",http://arxiv.org/pdf/2309.12506v1
2309.12496v1,physics.comp-ph,Optical Photon Simulation with Mitsuba3,2023-09-21 21:38:29+00:00,"Optical photon propagation is an embarrassingly parallel operation, well
suited to acceleration on GPU devices. Rendering of images employs similar
techniques -- for this reason, a pipeline to offload optical photon propagation
from Geant4 to the industry-standard open-source renderer Mitsuba3 has been
devised. With the creation of a dedicated plugin for single point multi-source
emission, we find a photon propagation rate of $2\times10^{5}$ photons per
second per CPU thread using LLVM and $1.2\times10^{6}$ photons per second per
GPU using CUDA. This represents a speed-up of 70 on CPU and 400 on GPU over
Geant4 and is competitive with other similar applications. The potential for
further applications is discussed.",http://arxiv.org/pdf/2309.12496v1
2309.12494v1,cs.LG,Evidential uncertainties on rich labels for active learning,2023-09-21 21:26:50+00:00,"Recent research in active learning, and more precisely in uncertainty
sampling, has focused on the decomposition of model uncertainty into reducible
and irreducible uncertainties. In this paper, we propose to simplify the
computational phase and remove the dependence on observations, but more
importantly to take into account the uncertainty already present in the labels,
\emph{i.e.} the uncertainty of the oracles. Two strategies are proposed,
sampling by Klir uncertainty, which addresses the exploration-exploitation
problem, and sampling by evidential epistemic uncertainty, which extends the
reducible uncertainty to the evidential framework, both using the theory of
belief functions.",http://arxiv.org/pdf/2309.12494v1
2309.12492v1,physics.plasm-ph,Quantum Computing Perspective for Electromagnetic Wave Propagation in Cold Magnetized Plasmas,2023-09-21 21:23:19+00:00,"The study of electromagnetic wave propagation in magnetized plasmas is of
paramount importance in various fields, including astrophysics, fusion energy,
and communication systems. In thermonuclear fusion experiments where transient
interaction phenomena between electromagnetic waves and plasma can disrupt the
overall confinement, we have to rely on the modern state of the art,
computational tools to delve into the physics of wave propagation in plasma.
However, even those sophisticated computational methods are facing challenges
in terms of memory resources and speed when they are forced to capture all the
physical processes that occur in wave-plasma interaction. Simultaneously, the
rapidly advancing field of quantum technologies has opened up exciting new
frontiers in the computational studies, by promising a minimization on the
computational strain. In this paper we examine a theoretical quantum computing
re-conceptualization of Maxwell equations inside a cold, inhomogeneous,
magnetized plasma that can lead to quantum simulation of electromagnetic wave
propagation and scattering from inhomogeneities. By constructing a quantum
Schrodinger representation of Maxwell equations in plasma that admit unitary --
energy preserving -- evolution we formulate a unitary product sequence of
operators that can form the basis of either a Qubit Lattice Algorithm (QLA) or
a pure quantum computing implementation. As an illustration of the power of
QLA, a full-wave simulation of wave-packet scattering from different shaped,
non-dispersive dielectrics is presented. QLAs when they are fully unitary, they
can be directly encoded into a quantum computer, further establishing their
versatility and capabilities but more importantly, indicating the impact that
quantum computers will have in the computational studies of wave propagation in
a fusion plasma.",http://arxiv.org/pdf/2309.12492v1
2309.13037v1,cs.RO,"GELLO: A General, Low-Cost, and Intuitive Teleoperation Framework for Robot Manipulators",2023-09-22 17:56:44+00:00,"Imitation learning from human demonstrations is a powerful framework to teach
robots new skills. However, the performance of the learned policies is
bottlenecked by the quality, scale, and variety of the demonstration data. In
this paper, we aim to lower the barrier to collecting large and high-quality
human demonstration data by proposing GELLO, a general framework for building
low-cost and intuitive teleoperation systems for robotic manipulation. Given a
target robot arm, we build a GELLO controller that has the same kinematic
structure as the target arm, leveraging 3D-printed parts and off-the-shelf
motors. GELLO is easy to build and intuitive to use. Through an extensive user
study, we show that GELLO enables more reliable and efficient demonstration
collection compared to commonly used teleoperation devices in the imitation
learning literature such as VR controllers and 3D spacemouses. We further
demonstrate the capabilities of GELLO for performing complex bi-manual and
contact-rich manipulation tasks. To make GELLO accessible to everyone, we have
designed and built GELLO systems for 3 commonly used robotic arms: Franka, UR5,
and xArm. All software and hardware are open-sourced and can be found on our
website: https://wuphilipp.github.io/gello/.",http://arxiv.org/pdf/2309.13037v1
2309.13036v1,quant-ph,Encoding optimization for quantum machine learning demonstrated on a superconducting transmon qutrit,2023-09-22 17:53:16+00:00,"Qutrits, three-level quantum systems, have the advantage of potentially
requiring fewer components than the typically used two-level qubits to
construct equivalent quantum circuits. This work investigates the potential of
qutrit parametric circuits in machine learning classification applications. We
propose and evaluate different data-encoding schemes for qutrits, and find that
the classification accuracy varies significantly depending on the used
encoding. We therefore propose a training method for encoding optimization that
allows to consistently achieve high classification accuracy. Our theoretical
analysis and numerical simulations indicate that the qutrit classifier can
achieve high classification accuracy using fewer components than a comparable
qubit system. We showcase the qutrit classification using the optimized
encoding method on superconducting transmon qutrits, demonstrating the
practicality of the proposed method on noisy hardware. Our work demonstrates
high-precision ternary classification using fewer circuit elements,
establishing qutrit parametric quantum circuits as a viable and efficient tool
for quantum machine learning applications.",http://arxiv.org/pdf/2309.13036v1
2309.13035v1,cs.RO,PyPose v0.6: The Imperative Programming Interface for Robotics,2023-09-22 17:49:58+00:00,"PyPose is an open-source library for robot learning. It combines a
learning-based approach with physics-based optimization, which enables seamless
end-to-end robot learning. It has been used in many tasks due to its
meticulously designed application programming interface (API) and efficient
implementation. From its initial launch in early 2022, PyPose has experienced
significant enhancements, incorporating a wide variety of new features into its
platform. To satisfy the growing demand for understanding and utilizing the
library and reduce the learning curve of new users, we present the fundamental
design principle of the imperative programming interface, and showcase the
flexible usage of diverse functionalities and modules using an extremely simple
Dubins car example. We also demonstrate that the PyPose can be easily used to
navigate a real quadruped robot with a few lines of code.",http://arxiv.org/pdf/2309.13035v1
2309.12997v1,math.PR,Scaling Limits of the Wasserstein information matrix on Gaussian Mixture Models,2023-09-22 16:57:44+00:00,"We consider the Wasserstein metric on the Gaussian mixture models (GMMs),
which is defined as the pullback of the full Wasserstein metric on the space of
smooth probability distributions with finite second moment. It derives a class
of Wasserstein metrics on probability simplices over one-dimensional bounded
homogeneous lattices via a scaling limit of the Wasserstein metric on GMMs.
Specifically, for a sequence of GMMs whose variances tend to zero, we prove
that the limit of the Wasserstein metric exists after certain renormalization.
Generalizations of this metric in general GMMs are established, including
inhomogeneous lattice models whose lattice gaps are not the same, extended GMMs
whose mean parameters of Gaussian components can also change, and the
second-order metric containing high-order information of the scaling limit. We
further study the Wasserstein gradient flows on GMMs for three typical
functionals: potential, internal, and interaction energies. Numerical examples
demonstrate the effectiveness of the proposed GMM models for approximating
Wasserstein gradient flows.",http://arxiv.org/pdf/2309.12997v1
2309.12968v1,cs.CR,Current file Overview PassViz: A Visualisation System for Analysing Leaked Passwords,2023-09-22 16:06:26+00:00,"Passwords remain the most widely used form of user authentication, despite
advancements in other methods. However, their limitations, such as
susceptibility to attacks, especially weak passwords defined by human users,
are well-documented. The existence of weak human-defined passwords has led to
repeated password leaks from websites, many of which are of large scale. While
such password leaks are unfortunate security incidents, they provide security
researchers and practitioners with good opportunities to learn valuable
insights from such leaked passwords, in order to identify ways to improve
password policies and other security controls on passwords. Researchers have
proposed different data visualisation techniques to help analyse leaked
passwords. However, many approaches rely solely on frequency analysis, with
limited exploration of distance-based graphs. This paper reports PassViz, a
novel method that combines the edit distance with the t-SNE (t-distributed
stochastic neighbour embedding) dimensionality reduction algorithm for
visualising and analysing leaked passwords in a 2-D space. We implemented
PassViz as an easy-to-use command-line tool for visualising large-scale
password databases, and also as a graphical user interface (GUI) to support
interactive visual analytics of small password databases. Using the
""000webhost"" leaked database as an example, we show how PassViz can be used to
visually analyse different aspects of leaked passwords and to facilitate the
discovery of previously unknown password patterns. Overall, our approach
empowers researchers and practitioners to gain valuable insights and improve
password security through effective data visualisation and analysis.",http://arxiv.org/pdf/2309.12968v1
2309.12933v1,quant-ph,Learning fermionic correlations by evolving with random translationally invariant Hamiltonians,2023-09-22 15:31:39+00:00,"Schemes of classical shadows have been developed to facilitate the read-out
of digital quantum devices, but similar tools for analog quantum simulators are
scarce and experimentally impractical. In this work, we provide a measurement
scheme for fermionic quantum devices that estimates second and fourth order
correlation functions by means of free fermionic, translationally invariant
evolutions - or quenches - and measurements in the mode occupation number
basis. We precisely characterize what correlation functions can be recovered
and equip the estimates with rigorous bounds on sample complexities, a
particularly important feature in light of the difficulty of getting good
statistics in reasonable experimental platforms, with measurements being slow.
Finally, we demonstrate how our procedure can be approximately implemented with
just nearest-neighbour, translationally invariant hopping quenches, a very
plausible procedure under current experimental requirements, and requiring only
random time-evolution with respect to a single native Hamiltonian. On a
conceptual level, this work brings the idea of classical shadows to the realm
of large scale analog quantum simulators.",http://arxiv.org/pdf/2309.12933v1
2309.12919v1,physics.ins-det,Refining fast simulation using machine learning,2023-09-22 15:17:24+00:00,"At the CMS experiment, a growing reliance on the fast Monte Carlo application
(FastSim) will accompany the high luminosity and detector granularity expected
in Phase 2. The FastSim chain is roughly 10 times faster than the application
based on the GEANT4 detector simulation and full reconstruction referred to as
FullSim. However, this advantage comes at the price of decreased accuracy in
some of the final analysis observables. In this contribution, a machine
learning-based technique to refine those observables is presented. We employ a
regression neural network trained with a sophisticated combination of multiple
loss functions to provide post-hoc corrections to samples produced by the
FastSim chain. The results show considerably improved agreement with the
FullSim output and an improvement in correlations among output observables and
external parameters. This technique is a promising replacement for existing
correction factors, providing higher accuracy and thus contributing to the
wider usage of FastSim.",http://arxiv.org/pdf/2309.12919v1
2309.12918v1,hep-ph,Combining Resonant and Tail-based Anomaly Detection,2023-09-22 15:13:56+00:00,"In many well-motivated models of the electroweak scale, cascade decays of new
particles can result in highly boosted hadronic resonances (e.g. $Z/W/h$). This
can make these models rich and promising targets for recently developed
resonant anomaly detection methods powered by modern machine learning. We
demonstrate this using the state-of-the-art CATHODE method applied to
supersymmetry scenarios with gluino pair production. We show that CATHODE,
despite being model-agnostic, is nevertheless competitive with dedicated
cut-based searches, while simultaneously covering a much wider region of
parameter space. The gluino events also populate the tails of the missing
energy and $H_T$ distributions, making this a novel combination of resonant and
tail-based anomaly detection.",http://arxiv.org/pdf/2309.12918v1
2309.12852v1,math.OC,Ensemble Differential Evolution with Simulation-Based Hybridization and Self-Adaptation for Inventory Management Under Uncertainty,2023-09-22 13:25:58+00:00,"This study proposes an Ensemble Differential Evolution with Simula-tion-Based
Hybridization and Self-Adaptation (EDESH-SA) approach for inven-tory management
(IM) under uncertainty. In this study, DE with multiple runs is combined with a
simulation-based hybridization method that includes a self-adaptive mechanism
that dynamically alters mutation and crossover rates based on the success or
failure of each iteration. Due to its adaptability, the algorithm is able to
handle the complexity and uncertainty present in IM. Utilizing Monte Carlo
Simulation (MCS), the continuous review (CR) inventory strategy is ex-amined
while accounting for stochasticity and various demand scenarios. This
simulation-based approach enables a realistic assessment of the proposed
algo-rithm's applicability in resolving the challenges faced by IM in practical
settings. The empirical findings demonstrate the potential of the proposed
method to im-prove the financial performance of IM and optimize large search
spaces. The study makes use of performance testing with the Ackley function and
Sensitivity Analysis with Perturbations to investigate how changes in variables
affect the objective value. This analysis provides valuable insights into the
behavior and robustness of the algorithm.",http://arxiv.org/pdf/2309.12852v1
2309.12849v1,cs.LG,DeepOPF-U: A Unified Deep Neural Network to Solve AC Optimal Power Flow in Multiple Networks,2023-09-22 13:22:15+00:00,"The traditional machine learning models to solve optimal power flow (OPF) are
mostly trained for a given power network and lack generalizability to today's
power networks with varying topologies and growing plug-and-play distributed
energy resources (DERs). In this paper, we propose DeepOPF-U, which uses one
unified deep neural network (DNN) to solve alternating-current (AC) OPF
problems in different power networks, including a set of power networks that is
successively expanding. Specifically, we design elastic input and output layers
for the vectors of given loads and OPF solutions with varying lengths in
different networks. The proposed method, using a single unified DNN, can deal
with different and growing numbers of buses, lines, loads, and DERs.
Simulations of IEEE 57/118/300-bus test systems and a network growing from 73
to 118 buses verify the improved performance of DeepOPF-U compared to existing
DNN-based solution methods.",http://arxiv.org/pdf/2309.12849v1
2309.12846v1,astro-ph.HE,Estimation of redshift and associated uncertainty of Fermi/LAT extra-galactic sources with Deep Learning,2023-09-22 13:15:59+00:00,"With the advancement of technology, machine learning-based analytical methods
have pervaded nearly every discipline in modern studies. Particularly, a number
of methods have been employed to estimate the redshift of gamma-ray loud active
galactic nuclei (AGN), which are a class of supermassive black hole systems
known for their intense multi-wavelength emissions and violent variability.
Determining the redshifts of AGNs is essential for understanding their
distances, which, in turn, sheds light on our current understanding of the
structure of the nearby universe. However, the task involves a number of
challenges such as the need for meticulous follow-up observations across
multiple wavelengths and astronomical facilities. In this study, we employ a
simple yet effective deep learning model with a single hidden layer having $64$
neurons and a dropout of 0.25 in the hidden layer, on a sample of AGNs with
known redshifts from the latest AGN catalog, 4LAC-DR3, obtained from Fermi-LAT.
We utilized their spectral, spatial, and temporal properties to robustly
predict the redshifts of AGNs as well quantify their associated uncertainties,
by modifying the model using two different variational inference methods. We
achieve a correlation coefficient of 0.784 on the test set from the frequentist
model and 0.777 and 0.778 from both the variants of variational inference, and,
when used to make predictions on the samples with unknown redshifts, we achieve
mean predictions of 0.421, 0.415 and 0.393, with standard deviations of 0.258,
0.246 and 0.207 from the models, respectively.",http://arxiv.org/pdf/2309.12846v1
2309.12841v1,cs.LG,Reward Function Design for Crowd Simulation via Reinforcement Learning,2023-09-22 12:55:30+00:00,"Crowd simulation is important for video-games design, since it enables to
populate virtual worlds with autonomous avatars that navigate in a human-like
manner. Reinforcement learning has shown great potential in simulating virtual
crowds, but the design of the reward function is critical to achieving
effective and efficient results. In this work, we explore the design of reward
functions for reinforcement learning-based crowd simulation. We provide
theoretical insights on the validity of certain reward functions according to
their analytical properties, and evaluate them empirically using a range of
scenarios, using the energy efficiency as the metric. Our experiments show that
directly minimizing the energy usage is a viable strategy as long as it is
paired with an appropriately scaled guiding potential, and enable us to study
the impact of the different reward components on the behavior of the simulated
crowd. Our findings can inform the development of new crowd simulation
techniques, and contribute to the wider study of human-like navigation.",http://arxiv.org/pdf/2309.12841v1
2309.12833v1,stat.ME,Model-based causal feature selection for general response types,2023-09-22 12:42:48+00:00,"Discovering causal relationships from observational data is a fundamental yet
challenging task. In some applications, it may suffice to learn the causal
features of a given response variable, instead of learning the entire
underlying causal structure. Invariant causal prediction (ICP, Peters et al.,
2016) is a method for causal feature selection which requires data from
heterogeneous settings. ICP assumes that the mechanism for generating the
response from its direct causes is the same in all settings and exploits this
invariance to output a subset of the causal features. The framework of ICP has
been extended to general additive noise models and to nonparametric settings
using conditional independence testing. However, nonparametric conditional
independence testing often suffers from low power (or poor type I error
control) and the aforementioned parametric models are not suitable for
applications in which the response is not measured on a continuous scale, but
rather reflects categories or counts. To bridge this gap, we develop ICP in the
context of transformation models (TRAMs), allowing for continuous, categorical,
count-type, and uninformatively censored responses (we show that, in general,
these model classes do not allow for identifiability when there is no exogenous
heterogeneity). We propose TRAM-GCM, a test for invariance of a subset of
covariates, based on the expected conditional covariance between environments
and score residuals which satisfies uniform asymptotic level guarantees. For
the special case of linear shift TRAMs, we propose an additional invariance
test, TRAM-Wald, based on the Wald statistic. We implement both proposed
methods in the open-source R package ""tramicp"" and show in simulations that
under the correct model specification, our approach empirically yields higher
power than nonparametric ICP based on conditional independence testing.",http://arxiv.org/pdf/2309.12833v1
2309.12815v1,cs.LG,Improving Generalization in Game Agents with Data Augmentation in Imitation Learning,2023-09-22 12:08:53+00:00,"Imitation learning is an effective approach for training game-playing agents
and, consequently, for efficient game production. However, generalization - the
ability to perform well in related but unseen scenarios - is an essential
requirement that remains an unsolved challenge for game AI. Generalization is
difficult for imitation learning agents because it requires the algorithm to
take meaningful actions outside of the training distribution. In this paper we
propose a solution to this challenge. Inspired by the success of data
augmentation in supervised learning, we augment the training data so the
distribution of states and actions in the dataset better represents the real
state-action distribution. This study evaluates methods for combining and
applying data augmentations to observations, to improve generalization of
imitation learning agents. It also provides a performance benchmark of these
augmentations across several 3D environments. These results demonstrate that
data augmentation is a promising framework for improving generalization in
imitation learning agents.",http://arxiv.org/pdf/2309.12815v1
2309.12807v1,cs.RO,Teacher-Student Reinforcement Learning for Mapless Navigation using a Planetary Space Rover,2023-09-22 11:39:50+00:00,"We address the challenge of enhancing navigation autonomy for planetary space
rovers using reinforcement learning (RL). The ambition of future space missions
necessitates advanced autonomous navigation capabilities for rovers to meet
mission objectives. RL's potential in robotic autonomy is evident, but its
reliance on simulations poses a challenge. Transferring policies to real-world
scenarios often encounters the ""reality gap"", disrupting the transition from
virtual to physical environments. The reality gap is exacerbated in the context
of mapless navigation on Mars and Moon-like terrains, where unpredictable
terrains and environmental factors play a significant role. Effective
navigation requires a method attuned to these complexities and real-world data
noise. We introduce a novel two-stage RL approach using offline noisy data. Our
approach employs a teacher-student policy learning paradigm, inspired by the
""learning by cheating"" method. The teacher policy is trained in simulation.
Subsequently, the student policy is trained on noisy data, aiming to mimic the
teacher's behaviors while being more robust to real-world uncertainties. Our
policies are transferred to a custom-designed rover for real-world testing.
Comparative analyses between the teacher and student policies reveal that our
approach offers improved behavioral performance, heightened noise resilience,
and more effective sim-to-real transfer.",http://arxiv.org/pdf/2309.12807v1
2309.12789v1,astro-ph.HE,Insights into the properties of GRBs with TeV emission,2023-09-22 11:00:09+00:00,"This study investigates the environments and characteristics of Gamma-Ray
Bursts (GRBs) exhibiting very high energy (VHE) emission. Recent detections of
VHE emission, up to TeV energies, challenge synchrotron-only emission models
and particle acceleration concepts in GRBs. Until now, only a handful of GRBs
have been detected in the VHE range. We compare the number densities of the
circumburst medium of VHE-detected GRBs to check if the environment impacts the
VHE emission. This shows that these GRBs have environments similar to the
larger population of GRBs. We employ machine learning algorithms to create
two-dimensional embeddings of GRB prompt emission light curves from the {\it
Swift}-BAT catalog. VHE-detected GRBs are located across the map, indicating
that VHE emission does not favour any particular cluster. These findings
indicate that VHE-detected GRBs do not show any peculiar characteristics other
than the observational detection of VHE photons. Future detections will
increase the sample size required for a rigorous understanding of the origin of
VHE emission in GRBs.",http://arxiv.org/pdf/2309.12789v1
2309.12742v1,cs.LG,Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation,2023-09-22 09:43:32+00:00,"Domain Adaptation (DA) is always challenged by the spurious correlation
between domain-invariant features (e.g., class identity) and domain-specific
features (e.g., environment) that does not generalize to the target domain.
Unfortunately, even enriched with additional unsupervised target domains,
existing Unsupervised DA (UDA) methods still suffer from it. This is because
the source domain supervision only considers the target domain samples as
auxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the
target domain -- where the valuable de-correlation clues hide -- is
disregarded. We propose to make the U in UDA matter by giving equal status to
the two domains. Specifically, we learn an invariant classifier whose
prediction is simultaneously consistent with the labels in the source domain
and clusters in the target domain, hence the spurious correlation inconsistent
in the target domain is removed. We dub our approach ""Invariant CONsistency
learning"" (ICON). Extensive experiments show that ICON achieves the
state-of-the-art performance on the classic UDA benchmarks: Office-Home and
VisDA-2017, and outperforms all the conventional methods on the challenging
WILDS 2.0 benchmark. Codes are in https://github.com/yue-zhongqi/ICON.",http://arxiv.org/pdf/2309.12742v1
2309.12725v1,astro-ph.SR,Scalable stellar evolution forecasting: Deep learning emulation vs. hierarchical nearest neighbor interpolation,2023-09-22 09:15:06+00:00,"Many astrophysical applications require efficient yet reliable forecasts of
stellar evolution tracks. One example is population synthesis, which generates
forward predictions of models for comparison with observations. The majority of
state-of-the-art population synthesis methods are based on analytic fitting
formulae to stellar evolution tracks that are computationally cheap to sample
statistically over a continuous parameter range. Running detailed stellar
evolution codes, such as MESA, over wide and densely sampled parameter grids is
prohibitively expensive computationally, while stellar-age based linear
interpolation in-between sparsely sampled grid points leads to intolerably
large systematic prediction errors. In this work, we provide two solutions of
automated interpolation methods that find satisfactory trade-off points between
cost-efficiency and accuracy. We construct a timescale-adapted evolutionary
coordinate and use it in a two-step interpolation scheme that traces the
evolution of stars from zero age main sequence all the way to the end of core
helium burning while covering a mass range from ${0.65}$ to $300 \,
\mathrm{M_\odot}$. The feedforward neural network regression model (first
solution) that we train to predict stellar surface variables can make millions
of predictions, sufficiently accurate over the entire parameter space, within
tens of seconds on a 4-core CPU. The hierarchical nearest neighbor
interpolation algorithm (second solution) that we hard-code to the same end
achieves even higher predictive accuracy, the same algorithm remains applicable
to all stellar variables evolved over time, but it is two orders of magnitude
slower. Our methodological framework is demonstrated to work on the MIST data
set. Finally, we discuss prospective applications and provide guidelines how to
generalize our methods to higher dimensional parameter spaces.",http://arxiv.org/pdf/2309.12725v1
2309.12722v1,eess.SY,Direct Learning for Parameter-Varying Feedforward Control: A Neural-Network Approach,2023-09-22 09:12:38+00:00,"The performance of a feedforward controller is primarily determined by the
extent to which it can capture the relevant dynamics of a system. The aim of
this paper is to develop an input-output linear parameter-varying (LPV)
feedforward parameterization and a corresponding data-driven estimation method
in which the dependency of the coefficients on the scheduling signal are
learned by a neural network. The use of a neural network enables the
parameterization to compensate a wide class of constant relative degree LPV
systems. Efficient optimization of the neural-network-based controller is
achieved through a Levenberg-Marquardt approach with analytic gradients and a
pseudolinear approach generalizing Sanathanan-Koerner to the LPV case. The
performance of the developed feedforward learning method is validated in a
simulation study of an LPV system showing excellent performance.",http://arxiv.org/pdf/2309.12722v1
2309.12720v1,cs.CR,Towards a Near-real-time Protocol Tunneling Detector based on Machine Learning Techniques,2023-09-22 09:08:43+00:00,"In the very last years, cybersecurity attacks have increased at an
unprecedented pace, becoming ever more sophisticated and costly. Their impact
has involved both private/public companies and critical infrastructures. At the
same time, due to the COVID-19 pandemic, the security perimeters of many
organizations expanded, causing an increase of the attack surface exploitable
by threat actors through malware and phishing attacks. Given these factors, it
is of primary importance to monitor the security perimeter and the events
occurring in the monitored network, according to a tested security strategy of
detection and response. In this paper, we present a protocol tunneling detector
prototype which inspects, in near real time, a company's network traffic using
machine learning techniques. Indeed, tunneling attacks allow malicious actors
to maximize the time in which their activity remains undetected. The detector
monitors unencrypted network flows and extracts features to detect possible
occurring attacks and anomalies, by combining machine learning and deep
learning. The proposed module can be embedded in any network security
monitoring platform able to provide network flow information along with its
metadata. The detection capabilities of the implemented prototype have been
tested both on benign and malicious datasets. Results show 97.1% overall
accuracy and an F1-score equals to 95.6%.",http://arxiv.org/pdf/2309.12720v1
2309.12694v1,cs.LG,Recurrent Temporal Revision Graph Networks,2023-09-22 08:09:55+00:00,"Temporal graphs offer more accurate modeling of many real-world scenarios
than static graphs. However, neighbor aggregation, a critical building block of
graph networks, for temporal graphs, is currently straightforwardly extended
from that of static graphs. It can be computationally expensive when involving
all historical neighbors during such aggregation. In practice, typically only a
subset of the most recent neighbors are involved. However, such subsampling
leads to incomplete and biased neighbor information. To address this
limitation, we propose a novel framework for temporal neighbor aggregation that
uses the recurrent neural network with node-wise hidden states to integrate
information from all historical neighbors for each node to acquire the complete
neighbor information. We demonstrate the superior theoretical expressiveness of
the proposed framework as well as its state-of-the-art performance in
real-world applications. Notably, it achieves a significant +9.6% improvement
on averaged precision in a real-world Ecommerce dataset over existing methods
on 2-layer models.",http://arxiv.org/pdf/2309.12694v1
2309.12681v1,quant-ph,From Tight Gradient Bounds for Parameterized Quantum Circuits to the Absence of Barren Plateaus in QGANs,2023-09-22 07:38:13+00:00,"Barren plateaus are a central bottleneck in the scalability of variational
quantum algorithms (VQAs), and are known to arise in various ways, from circuit
depth and hardware noise to global observables. However, a caveat of most
existing results is the requirement of t-design circuit assumptions that are
typically not satisfied in practice. In this work, we loosen these assumptions
altogether and derive tight upper and lower bounds on gradient concentration,
for a large class of parameterized quantum circuits and arbitrary observables.
By requiring only a couple of design choices that are constructive and easily
verified, our results can readily be leveraged to rule out barren plateaus for
explicit circuits and mixed observables, namely, observables containing a
non-vanishing local term. This insight has direct implications for hybrid
Quantum Generative Adversarial Networks (qGANs), a generative model that can be
reformulated as a VQA with an observable composed of local and global terms. We
prove that designing the discriminator appropriately leads to 1-local weights
that stay constant in the number of qubits, regardless of discriminator depth.
Combined with our first contribution, this implies that qGANs with shallow
generators can be trained at scale without suffering from barren plateaus --
making them a promising candidate for applications in generative quantum
machine learning. We demonstrate this result by training a qGAN to learn a 2D
mixture of Gaussian distributions with up to 16 qubits, and provide numerical
evidence that global contributions to the gradient, while initially
exponentially small, may kick in substantially over the course of training.",http://arxiv.org/pdf/2309.12681v1
2309.12645v1,cs.IR,KuaiSim: A Comprehensive Simulator for Recommender Systems,2023-09-22 06:29:29+00:00,"Reinforcement Learning (RL)-based recommender systems (RSs) have garnered
considerable attention due to their ability to learn optimal recommendation
policies and maximize long-term user rewards. However, deploying RL models
directly in online environments and generating authentic data through A/B tests
can pose challenges and require substantial resources. Simulators offer an
alternative approach by providing training and evaluation environments for RS
models, reducing reliance on real-world data. Existing simulators have shown
promising results but also have limitations such as simplified user feedback,
lacking consistency with real-world data, the challenge of simulator
evaluation, and difficulties in migration and expansion across RSs. To address
these challenges, we propose KuaiSim, a comprehensive user environment that
provides user feedback with multi-behavior and cross-session responses. The
resulting simulator can support three levels of recommendation problems: the
request level list-wise recommendation task, the whole-session level sequential
recommendation task, and the cross-session level retention optimization task.
For each task, KuaiSim also provides evaluation protocols and baseline
recommendation algorithms that further serve as benchmarks for future research.
We also restructure existing competitive simulators on the KuaiRand Dataset and
compare them against KuaiSim to future assess their performance and behavioral
differences. Furthermore, to showcase KuaiSim's flexibility in accommodating
different datasets, we demonstrate its versatility and robustness when
deploying it on the ML-1m dataset.",http://arxiv.org/pdf/2309.12645v1
2309.12618v1,cs.LG,Zero-Regret Performative Prediction Under Inequality Constraints,2023-09-22 04:54:26+00:00,"Performative prediction is a recently proposed framework where predictions
guide decision-making and hence influence future data distributions. Such
performative phenomena are ubiquitous in various areas, such as transportation,
finance, public policy, and recommendation systems. To date, work on
performative prediction has only focused on unconstrained scenarios, neglecting
the fact that many real-world learning problems are subject to constraints.
This paper bridges this gap by studying performative prediction under
inequality constraints. Unlike most existing work that provides only
performative stable points, we aim to find the optimal solutions. Anticipating
performative gradients is a challenging task, due to the agnostic performative
effect on data distributions. To address this issue, we first develop a robust
primal-dual framework that requires only approximate gradients up to a certain
accuracy, yet delivers the same order of performance as the stochastic
primal-dual algorithm without performativity. Based on this framework, we then
propose an adaptive primal-dual algorithm for location families. Our analysis
demonstrates that the proposed adaptive primal-dual algorithm attains
$\ca{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$
samples, where $T$ is the time horizon. To our best knowledge, this is the
first study and analysis on the optimality of the performative prediction
problem under inequality constraints. Finally, we validate the effectiveness of
our algorithm and theoretical results through numerical simulations.",http://arxiv.org/pdf/2309.12618v1
2309.12600v1,stat.ML,Multiply Robust Federated Estimation of Targeted Average Treatment Effects,2023-09-22 03:15:08+00:00,"Federated or multi-site studies have distinct advantages over single-site
studies, including increased generalizability, the ability to study
underrepresented populations, and the opportunity to study rare exposures and
outcomes. However, these studies are challenging due to the need to preserve
the privacy of each individual's data and the heterogeneity in their covariate
distributions. We propose a novel federated approach to derive valid causal
inferences for a target population using multi-site data. We adjust for
covariate shift and covariate mismatch between sites by developing
multiply-robust and privacy-preserving nuisance function estimation. Our
methodology incorporates transfer learning to estimate ensemble weights to
combine information from source sites. We show that these learned weights are
efficient and optimal under different scenarios. We showcase the finite sample
advantages of our approach in terms of efficiency and robustness compared to
existing approaches.",http://arxiv.org/pdf/2309.12600v1
2309.12548v1,physics.atom-ph,Suppression of Black-body Radiation Induced Zeeman Shifts in the Optical Clocks due to the Fine-structure Intramanifold Resonances,2023-09-22 00:30:06+00:00,"The roles of the fine-structure intramanifold resonances to the Zeeman shifts
caused by the blackbody radiation (BBRz shifts) in the optical clock
transitions are analyzed. The clock frequency measurement in the $^1S_0-^3P_0$
clock transition of the singly charged aluminium ion (Al$^+$) has already been
reached the $10^{-19}$ level at which the BBRz effect can be significant in
determining the uncertainty. In view of this, we probe first the BBRz shift in
this transition rigorously and demonstrate the importance of the contributions
from the intramanifold resonances explicitly. To carry out the analysis, we
determine the dynamic magnetic dipole (M1) polarizabilities of the clock states
over a wide range of angular frequencies by employing two variants of
relativistic many-body methods. This showed the BBRz shift is highly suppressed
due to blue-detuning of the BBR spectrum to the $^3P_0-^3P_1$ fine-structure
intramanifold resonance in Al$^+$ and it fails to follow the usually assumed
static M1 polarizability limit in the estimation of the BBRz shift. The
resonance also leads to a reversal behavior of the temperature dependence and a
cancellation in the shift. After learning this behavior, we extended our
analyses to other optical clocks and found that these shifts are of the order
of micro-hertz leading to fractional shifts in the clock transitions at the
$10^{-20}$ level or below.",http://arxiv.org/pdf/2309.12548v1
2309.12547v1,cs.RO,Real-time Motion Generation and Data Augmentation for Grasping Moving Objects with Dynamic Speed and Position Changes,2023-09-22 00:19:32+00:00,"While deep learning enables real robots to perform complex tasks had been
difficult to implement in the past, the challenge is the enormous amount of
trial-and-error and motion teaching in a real environment. The manipulation of
moving objects, due to their dynamic properties, requires learning a wide range
of factors such as the object's position, movement speed, and grasping timing.
We propose a data augmentation method for enabling a robot to grasp moving
objects with different speeds and grasping timings at low cost. Specifically,
the robot is taught to grasp an object moving at low speed using teleoperation,
and multiple data with different speeds and grasping timings are generated by
down-sampling and padding the robot sensor data in the time-series direction.
By learning multiple sensor data in a time series, the robot can generate
motions while adjusting the grasping timing for unlearned movement speeds and
sudden speed changes. We have shown using a real robot that this data
augmentation method facilitates learning the relationship between object
position and velocity and enables the robot to perform robust grasping motions
for unlearned positions and objects with dynamically changing positions and
velocities.",http://arxiv.org/pdf/2309.12547v1
2309.12510v1,cs.LG,Confidence Calibration for Systems with Cascaded Predictive Modules,2023-09-21 22:12:24+00:00,"Existing conformal prediction algorithms estimate prediction intervals at
target confidence levels to characterize the performance of a regression model
on new test samples. However, considering an autonomous system consisting of
multiple modules, prediction intervals constructed for individual modules fall
short of accommodating uncertainty propagation over different modules and thus
cannot provide reliable predictions on system behavior. We address this
limitation and present novel solutions based on conformal prediction to provide
prediction intervals calibrated for a predictive system consisting of cascaded
modules (e.g., an upstream feature extraction module and a downstream
regression module). Our key idea is to leverage module-level validation data to
characterize the system-level error distribution without direct access to
end-to-end validation data. We provide theoretical justification and empirical
experimental results to demonstrate the effectiveness of proposed solutions. In
comparison to prediction intervals calibrated for individual modules, our
solutions generate improved intervals with more accurate performance guarantees
for system predictions, which are demonstrated on both synthetic systems and
real-world systems performing overlap prediction for indoor navigation using
the Matterport3D dataset.",http://arxiv.org/pdf/2309.12510v1
2309.12508v1,cs.LG,A Diffusion-Model of Joint Interactive Navigation,2023-09-21 22:10:20+00:00,"Simulation of autonomous vehicle systems requires that simulated traffic
participants exhibit diverse and realistic behaviors. The use of prerecorded
real-world traffic scenarios in simulation ensures realism but the rarity of
safety critical events makes large scale collection of driving scenarios
expensive. In this paper, we present DJINN - a diffusion based method of
generating traffic scenarios. Our approach jointly diffuses the trajectories of
all agents, conditioned on a flexible set of state observations from the past,
present, or future. On popular trajectory forecasting datasets, we report state
of the art performance on joint trajectory metrics. In addition, we demonstrate
how DJINN flexibly enables direct test-time sampling from a variety of valuable
conditional distributions including goal-based sampling, behavior-class
sampling, and scenario editing.",http://arxiv.org/pdf/2309.12508v1
2309.12500v1,cs.DS,User-Level Differential Privacy With Few Examples Per User,2023-09-21 21:51:55+00:00,"Previous work on user-level differential privacy (DP) [Ghazi et al. NeurIPS
2021, Bun et al. STOC 2023] obtained generic algorithms that work for various
learning tasks. However, their focus was on the example-rich regime, where the
users have so many examples that each user could themselves solve the problem.
In this work we consider the example-scarce regime, where each user has only a
few examples, and obtain the following results:
  1. For approximate-DP, we give a generic transformation of any item-level DP
algorithm to a user-level DP algorithm. Roughly speaking, the latter gives a
(multiplicative) savings of $O_{\varepsilon,\delta}(\sqrt{m})$ in terms of the
number of users required for achieving the same utility, where $m$ is the
number of examples per user. This algorithm, while recovering most known bounds
for specific problems, also gives new bounds, e.g., for PAC learning.
  2. For pure-DP, we present a simple technique for adapting the exponential
mechanism [McSherry, Talwar FOCS 2007] to the user-level setting. This gives
new bounds for a variety of tasks, such as private PAC learning, hypothesis
selection, and distribution learning. For some of these problems, we show that
our bounds are near-optimal.",http://arxiv.org/pdf/2309.12500v1
2309.12488v1,cs.LG,Sharpness-Aware Minimization and the Edge of Stability,2023-09-21 21:15:51+00:00,"Recent experiments have shown that, often, when training a neural network
with gradient descent (GD) with a step size $\eta$, the operator norm of the
Hessian of the loss grows until it approximately reaches $2/\eta$, after which
it fluctuates around this value.
  The quantity $2/\eta$ has been called the ""edge of stability"" based on
consideration of a local quadratic approximation of the loss. We perform a
similar calculation to arrive at an ""edge of stability"" for Sharpness-Aware
Minimization (SAM), a variant of GD which has been shown to improve its
generalization. Unlike the case for GD, the resulting SAM-edge depends on the
norm of the gradient. Using three deep learning training tasks, we see
empirically that SAM operates on the edge of stability identified by this
analysis.",http://arxiv.org/pdf/2309.12488v1
2309.12487v1,eess.SY,High-Dimensional Controller Tuning through Latent Representations,2023-09-21 21:12:32+00:00,"In this paper, we propose a method to automatically and efficiently tune
high-dimensional vectors of controller parameters. The proposed method first
learns a mapping from the high-dimensional controller parameter space to a
lower dimensional space using a machine learning-based algorithm. This mapping
is then utilized in an actor-critic framework using Bayesian optimization (BO).
The proposed approach is applicable to complex systems (such as quadruped
robots). In addition, the proposed approach also enables efficient
generalization to different control tasks while also reducing the number of
evaluations required while tuning the controller parameters. We evaluate our
method on a legged locomotion application. We show the efficacy of the
algorithm in tuning the high-dimensional controller parameters and also
reducing the number of evaluations required for the tuning. Moreover, it is
shown that the method is successful in generalizing to new tasks and is also
transferable to other robot dynamics.",http://arxiv.org/pdf/2309.12487v1
2309.12484v1,cs.NE,Robust Energy Consumption Prediction with a Missing Value-Resilient Metaheuristic-based Neural Network in Mobile App Development,2023-09-21 21:01:43+00:00,"Energy consumption is a fundamental concern in mobile application
development, bearing substantial significance for both developers and
end-users. Moreover, it is a critical determinant in the consumer's
decision-making process when considering a smartphone purchase. From the
sustainability perspective, it becomes imperative to explore approaches aimed
at mitigating the energy consumption of mobile devices, given the significant
global consequences arising from the extensive utilisation of billions of
smartphones, which imparts a profound environmental impact. Despite the
existence of various energy-efficient programming practices within the Android
platform, the dominant mobile ecosystem, there remains a need for documented
machine learning-based energy prediction algorithms tailored explicitly for
mobile app development. Hence, the main objective of this research is to
propose a novel neural network-based framework, enhanced by a metaheuristic
approach, to achieve robust energy prediction in the context of mobile app
development. The metaheuristic approach here plays a crucial role in not only
identifying suitable learning algorithms and their corresponding parameters but
also determining the optimal number of layers and neurons within each layer. To
the best of our knowledge, prior studies have yet to employ any metaheuristic
algorithm to address all these hyperparameters simultaneously. Moreover, due to
limitations in accessing certain aspects of a mobile phone, there might be
missing data in the data set, and the proposed framework can handle this. In
addition, we conducted an optimal algorithm selection strategy, employing 13
metaheuristic algorithms, to identify the best algorithm based on accuracy and
resistance to missing values. The comprehensive experiments demonstrate that
our proposed approach yields significant outcomes for energy consumption
prediction.",http://arxiv.org/pdf/2309.12484v1
2309.12483v1,cs.CR,A Toolchain for Privacy-Preserving Distributed Aggregation on Edge-Devices,2023-09-21 20:55:29+00:00,"Valuable insights, such as frequently visited environments in the wake of the
COVID-19 pandemic, can oftentimes only be gained by analyzing sensitive data
spread across edge-devices like smartphones. To facilitate such an analysis, we
present a toolchain for a distributed, privacy-preserving aggregation of local
data by taking the limited resources of edge-devices into account. The
distributed aggregation is based on secure summation and simultaneously
satisfies the notion of differential privacy. In this way, other parties can
neither learn the sensitive data of single clients nor a single client's
influence on the final result. We perform an evaluation of the power
consumption, the running time and the bandwidth overhead on real as well as
simulated devices and demonstrate the flexibility of our toolchain by
presenting an extension of the summation of histograms to distributed
clustering.",http://arxiv.org/pdf/2309.12483v1
2309.12476v1,eess.SY,Differentially Private Reward Functions for Multi-Agent Markov Decision Processes,2023-09-21 20:48:17+00:00,"Reward functions encode desired behavior in multi-agent Markov decision
processes, but onlookers may learn reward functions by observing agents, which
can reveal sensitive information. Therefore, in this paper we introduce and
compare two methods for privatizing reward functions in policy synthesis for
multi-agent Markov decision processes. Reward functions are privatized using
differential privacy, a statistical framework for protecting sensitive data.
Both methods we develop rely on the Gaussian mechanism, which is a method of
randomization we use to perturb (i) each agent's individual reward function or
(ii) the joint reward function shared by all agents. We prove that both of
these methods are differentially private and compare the abilities of each to
provide accurate reward values for policy synthesis. We then develop an
algorithm for the numerical computation of the performance loss due to privacy
on a case-by-case basis. We also exactly compute the computational complexity
of this algorithm in terms of system parameters and show that it is inherently
tractable. Numerical simulations are performed on a gridworld example and in
waypoint guidance of an autonomous vehicle, and both examples show that privacy
induces only negligible performance losses in practice.",http://arxiv.org/pdf/2309.12476v1
2309.12463v1,cs.CV,Impact of architecture on robustness and interpretability of multispectral deep neural networks,2023-09-21 20:11:01+00:00,"Including information from additional spectral bands (e.g., near-infrared)
can improve deep learning model performance for many vision-oriented tasks.
There are many possible ways to incorporate this additional information into a
deep learning model, but the optimal fusion strategy has not yet been
determined and can vary between applications. At one extreme, known as ""early
fusion,"" additional bands are stacked as extra channels to obtain an input
image with more than three channels. At the other extreme, known as ""late
fusion,"" RGB and non-RGB bands are passed through separate branches of a deep
learning model and merged immediately before a final classification or
segmentation layer. In this work, we characterize the performance of a suite of
multispectral deep learning models with different fusion approaches, quantify
their relative reliance on different input bands and evaluate their robustness
to naturalistic image corruptions affecting one or more input channels.",http://arxiv.org/pdf/2309.12463v1
2309.12458v1,cs.LG,A Theory of Multimodal Learning,2023-09-21 20:05:49+00:00,"Human perception of the empirical world involves recognizing the diverse
appearances, or 'modalities', of underlying objects. Despite the longstanding
consideration of this perspective in philosophy and cognitive science, the
study of multimodality remains relatively under-explored within the field of
machine learning. Nevertheless, current studies of multimodal machine learning
are limited to empirical practices, lacking theoretical foundations beyond
heuristic arguments. An intriguing finding from the practice of multimodal
learning is that a model trained on multiple modalities can outperform a
finely-tuned unimodal model, even on unimodal tasks. This paper provides a
theoretical framework that explains this phenomenon, by studying generalization
properties of multimodal learning algorithms. We demonstrate that multimodal
learning allows for a superior generalization bound compared to unimodal
learning, up to a factor of $O(\sqrt{n})$, where $n$ represents the sample
size. Such advantage occurs when both connection and heterogeneity exist
between the modalities.",http://arxiv.org/pdf/2309.12458v1
2309.12450v1,stat.ML,A Convex Framework for Confounding Robust Inference,2023-09-21 19:45:37+00:00,"We study policy evaluation of offline contextual bandits subject to
unobserved confounders. Sensitivity analysis methods are commonly used to
estimate the policy value under the worst-case confounding over a given
uncertainty set. However, existing work often resorts to some coarse relaxation
of the uncertainty set for the sake of tractability, leading to overly
conservative estimation of the policy value. In this paper, we propose a
general estimator that provides a sharp lower bound of the policy value using
convex programming. The generality of our estimator enables various extensions
such as sensitivity analysis with f-divergence, model selection with cross
validation and information criterion, and robust policy learning with the sharp
lower bound. Furthermore, our estimation method can be reformulated as an
empirical risk minimization problem thanks to the strong duality, which enables
us to provide strong theoretical guarantees of the proposed estimator using
techniques of the M-estimation.",http://arxiv.org/pdf/2309.12450v1
2309.12449v1,cs.SE,Dynamic Prediction of Delays in Software Projects Using Delay Patterns and Bayesian Modeling,2023-09-21 19:45:35+00:00,"Modern agile software projects are subject to constant change, making it
essential to re-asses overall delay risk throughout the project life cycle.
Existing effort estimation models are static and not able to incorporate
changes occurring during project execution. In this paper, we propose a dynamic
model for continuously predicting overall delay using delay patterns and
Bayesian modeling. The model incorporates the context of the project phase and
learns from changes in team performance over time. We apply the approach to
real-world data from 4,040 epics and 270 teams at ING. An empirical evaluation
of our approach and comparison to the state-of-the-art demonstrate significant
improvements in predictive accuracy. The dynamic model consistently outperforms
static approaches and the state-of-the-art, even during early project phases.",http://arxiv.org/pdf/2309.12449v1
2309.12443v1,cs.CL,Active Learning for Multilingual Fingerspelling Corpora,2023-09-21 19:36:22+00:00,"We apply active learning to help with data scarcity problems in sign
languages. In particular, we perform a novel analysis of the effect of
pre-training. Since many sign languages are linguistic descendants of French
sign language, they share hand configurations, which pre-training can hopefully
exploit. We test this hypothesis on American, Chinese, German, and Irish
fingerspelling corpora. We do observe a benefit from pre-training, but this may
be due to visual rather than linguistic similarities",http://arxiv.org/pdf/2309.12443v1
2309.12439v1,cs.SE,"Process Improvement Archaeology: What Led Us Here, and What's Next?",2023-09-21 19:14:38+00:00,"While in every organization corporate culture and history change over time,
intentional efforts to identify performance problems are of particular interest
when trying to understand the current state of an organization. The results of
past improvement initiatives can shed light on the evolution of an organization
and represent, with the advantage of perfect hindsight, a learning opportunity
for future process improvements. The opportunity to test this premise occurred
in an applied research collaboration with the Swedish Transport Administration,
the government agency responsible for the planning, implementation, and
maintenance of long-term rail, road, shipping, and aviation infrastructure in
Sweden. This article is part of a theme issue on Process Improvement.",http://arxiv.org/pdf/2309.12439v1
2309.12437v1,cs.ET,Implementation of digital MemComputing using standard electronic components,2023-09-21 19:11:34+00:00,"Digital MemComputing machines (DMMs), which employ nonlinear dynamical
systems with memory (time non-locality), have proven to be a robust and
scalable unconventional computing approach for solving a wide variety of
combinatorial optimization problems. However, most of the research so far has
focused on the numerical simulations of the equations of motion of DMMs. This
inevitably subjects time to discretization, which brings its own (numerical)
issues that would be absent in actual physical systems operating in continuous
time. Although hardware realizations of DMMs have been previously suggested,
their implementation would require materials and devices that are not so easy
to integrate with traditional electronics. In this study, we propose a novel
hardware design for DMMs that leverages only conventional electronic
components. Our findings suggest that this design offers a marked improvement
in speed compared to existing realizations of these machines, without requiring
special materials or novel device concepts. Moreover, the absence of numerical
noise promises enhanced stability over extended periods of the machines'
operation, paving the way for addressing even more complex problems.",http://arxiv.org/pdf/2309.12437v1
2309.12429v1,cs.CV,"DIOR: Dataset for Indoor-Outdoor Reidentification -- Long Range 3D/2D Skeleton Gait Collection Pipeline, Semi-Automated Gait Keypoint Labeling and Baseline Evaluation Methods",2023-09-21 18:51:00+00:00,"In recent times, there is an increased interest in the identification and
re-identification of people at long distances, such as from rooftop cameras,
UAV cameras, street cams, and others. Such recognition needs to go beyond face
and use whole-body markers such as gait. However, datasets to train and test
such recognition algorithms are not widely prevalent, and fewer are labeled.
This paper introduces DIOR -- a framework for data collection, semi-automated
annotation, and also provides a dataset with 14 subjects and 1.649 million RGB
frames with 3D/2D skeleton gait labels, including 200 thousands frames from a
long range camera. Our approach leverages advanced 3D computer vision
techniques to attain pixel-level accuracy in indoor settings with motion
capture systems. Additionally, for outdoor long-range settings, we remove the
dependency on motion capture systems and adopt a low-cost, hybrid 3D computer
vision and learning pipeline with only 4 low-cost RGB cameras, successfully
achieving precise skeleton labeling on far-away subjects, even when their
height is limited to a mere 20-25 pixels within an RGB frame. On publication,
we will make our pipeline open for others to use.",http://arxiv.org/pdf/2309.12429v1
2309.12424v1,cs.CV,DualToken-ViT: Position-aware Efficient Vision Transformer with Dual Token Fusion,2023-09-21 18:46:32+00:00,"Self-attention-based vision transformers (ViTs) have emerged as a highly
competitive architecture in computer vision. Unlike convolutional neural
networks (CNNs), ViTs are capable of global information sharing. With the
development of various structures of ViTs, ViTs are increasingly advantageous
for many vision tasks. However, the quadratic complexity of self-attention
renders ViTs computationally intensive, and their lack of inductive biases of
locality and translation equivariance demands larger model sizes compared to
CNNs to effectively learn visual features. In this paper, we propose a
light-weight and efficient vision transformer model called DualToken-ViT that
leverages the advantages of CNNs and ViTs. DualToken-ViT effectively fuses the
token with local information obtained by convolution-based structure and the
token with global information obtained by self-attention-based structure to
achieve an efficient attention structure. In addition, we use position-aware
global tokens throughout all stages to enrich the global information, which
further strengthening the effect of DualToken-ViT. Position-aware global tokens
also contain the position information of the image, which makes our model
better for vision tasks. We conducted extensive experiments on image
classification, object detection and semantic segmentation tasks to demonstrate
the effectiveness of DualToken-ViT. On the ImageNet-1K dataset, our models of
different scales achieve accuracies of 75.4% and 79.4% with only 0.5G and 1.0G
FLOPs, respectively, and our model with 1.0G FLOPs outperforms LightViT-T using
global tokens by 0.7%.",http://arxiv.org/pdf/2309.12424v1
