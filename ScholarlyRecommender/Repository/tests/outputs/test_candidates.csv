Id,Category,Title,Published,Abstract,URL
2309.14341v1,cs.RO,Extreme Parkour with Legged Robots,2023-09-25 17:59:55+00:00,"Humans can perform parkour by traversing obstacles in a highly dynamic
fashion requiring precise eye-muscle coordination and movement. Getting robots
to do the same task requires overcoming similar challenges. Classically, this
is done by independently engineering perception, actuation, and control systems
to very low tolerances. This restricts them to tightly controlled settings such
as a predetermined obstacle course in labs. In contrast, humans are able to
learn parkour through practice without significantly changing their underlying
biology. In this paper, we take a similar approach to developing robot parkour
on a small low-cost robot with imprecise actuation and a single front-facing
depth camera for perception which is low-frequency, jittery, and prone to
artifacts. We show how a single neural net policy operating directly from a
camera image, trained in simulation with large-scale RL, can overcome imprecise
sensing and actuation to output highly precise control behavior end-to-end. We
show our robot can perform a high jump on obstacles 2x its height, long jump
across gaps 2x its length, do a handstand and run across tilted ramps, and
generalize to novel obstacle courses with different physical properties.
Parkour videos at https://extreme-parkour.github.io/",http://arxiv.org/pdf/2309.14341v1
2309.14335v1,cs.CV,UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation,2023-09-25 17:58:46+00:00,"Human generation has achieved significant progress. Nonetheless, existing
methods still struggle to synthesize specific regions such as faces and hands.
We argue that the main reason is rooted in the training data. A holistic human
dataset inevitably has insufficient and low-resolution information on local
parts. Therefore, we propose to use multi-source datasets with various
resolution images to jointly learn a high-resolution human generative model.
However, multi-source data inherently a) contains different parts that do not
spatially align into a coherent human, and b) comes with different scales. To
tackle these challenges, we propose an end-to-end framework, UnitedHuman, that
empowers continuous GAN with the ability to effectively utilize multi-source
data for high-resolution human generation. Specifically, 1) we design a
Multi-Source Spatial Transformer that spatially aligns multi-source images to
full-body space with a human parametric model. 2) Next, a continuous GAN is
proposed with global-structural guidance and CutMix consistency. Patches from
different datasets are then sampled and transformed to supervise the training
of this scale-invariant generative model. Extensive experiments demonstrate
that our model jointly learned from multi-source data achieves superior quality
than those learned from a holistic dataset.",http://arxiv.org/pdf/2309.14335v1
2309.14331v1,cs.LG,LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference,2023-09-25 17:56:54+00:00,"The growth of Graph Convolution Network (GCN) model sizes has revolutionized
numerous applications, surpassing human performance in areas such as personal
healthcare and financial systems. The deployment of GCNs in the cloud raises
privacy concerns due to potential adversarial attacks on client data. To
address security concerns, Privacy-Preserving Machine Learning (PPML) using
Homomorphic Encryption (HE) secures sensitive client data. However, it
introduces substantial computational overhead in practical applications. To
tackle those challenges, we present LinGCN, a framework designed to reduce
multiplication depth and optimize the performance of HE based GCN inference.
LinGCN is structured around three key elements: (1) A differentiable structural
linearization algorithm, complemented by a parameterized discrete indicator
function, co-trained with model weights to meet the optimization goal. This
strategy promotes fine-grained node-level non-linear location selection,
resulting in a model with minimized multiplication depth. (2) A compact
node-wise polynomial replacement policy with a second-order trainable
activation function, steered towards superior convergence by a two-level
distillation approach from an all-ReLU based teacher model. (3) an enhanced HE
solution that enables finer-grained operator fusion for node-wise activation
functions, further reducing multiplication level consumption in HE-based
inference. Our experiments on the NTU-XVIEW skeleton joint dataset reveal that
LinGCN excels in latency, accuracy, and scalability for homomorphically
encrypted inference, outperforming solutions such as CryptoGCN. Remarkably,
LinGCN achieves a 14.2x latency speedup relative to CryptoGCN, while preserving
an inference accuracy of 75% and notably reducing multiplication depth.",http://arxiv.org/pdf/2309.14331v1
2309.14316v1,cs.CL,"Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",2023-09-25 17:37:20+00:00,"Large language models can store extensive world knowledge, often extractable
through question-answering (e.g., ""What is Abraham Lincoln's birthday?"").
However, it's unclear whether the model answers questions based on exposure to
exact/similar questions during training, or if it genuinely extracts knowledge
from the source (e.g., Wikipedia biographies).
  In this paper, we conduct an in-depth study of this problem using a
controlled set of semi-synthetic biography data. We uncover a relationship
between the model's knowledge extraction ability and different diversity
measures of the training data. We conduct (nearly) linear probing, revealing a
strong correlation between this relationship and whether the model (nearly)
linearly encodes the knowledge attributes at the hidden embedding of the entity
names, or across the embeddings of other tokens in the training text.",http://arxiv.org/pdf/2309.14316v1
2309.14309v1,cs.CV,Multiple Different Explanations for Image Classifiers,2023-09-25 17:28:28+00:00,"Existing explanation tools for image classifiers usually give only one single
explanation for an image. For many images, however, both humans and image
classifiers accept more than one explanation for the image label. Thus,
restricting the number of explanations to just one severely limits the insight
into the behavior of the classifier. In this paper, we describe an algorithm
and a tool, REX, for computing multiple explanations of the output of a
black-box image classifier for a given image. Our algorithm uses a principled
approach based on causal theory. We analyse its theoretical complexity and
provide experimental results showing that REX finds multiple explanations on 7
times more images than the previous work on the ImageNet-mini benchmark.",http://arxiv.org/pdf/2309.14309v1
2309.14304v1,cs.CV,Overview of Class Activation Maps for Visualization Explainability,2023-09-25 17:20:51+00:00,"Recent research in deep learning methodology has led to a variety of complex
modelling techniques in computer vision (CV) that reach or even outperform
human performance. Although these black-box deep learning models have obtained
astounding results, they are limited in their interpretability and transparency
which are critical to take learning machines to the next step to include them
in sensitive decision-support systems involving human supervision. Hence, the
development of explainable techniques for computer vision (XCV) has recently
attracted increasing attention. In the realm of XCV, Class Activation Maps
(CAMs) have become widely recognized and utilized for enhancing
interpretability and insights into the decision-making process of deep learning
models. This work presents a comprehensive overview of the evolution of Class
Activation Map methods over time. It also explores the metrics used for
evaluating CAMs and introduces auxiliary techniques to improve the saliency of
these methods. The overview concludes by proposing potential avenues for future
research in this evolving field.",http://arxiv.org/pdf/2309.14304v1
2309.14293v1,cs.CV,NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields,2023-09-25 17:04:30+00:00,"Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their prohibitively high computational complexity limits deployability,
especially on resource-constrained platforms. To enable practical usage of
NeRFs, quality tuning is essential to reduce computational complexity, akin to
adjustable graphics settings in video games. However while existing solutions
strive for efficiency, they use one-size-fits-all architectures regardless of
scene complexity, although the same architecture may be unnecessarily large for
simple scenes but insufficient for complex ones. Thus as NeRFs become more
widely used for 3D visualization, there is a need to dynamically optimize the
neural network component of NeRFs to achieve a balance between computational
complexity and specific targets for synthesis quality. Addressing this gap, we
introduce NAS-NeRF: a generative neural architecture search strategy uniquely
tailored to generate NeRF architectures on a per-scene basis by optimizing the
trade-off between complexity and performance, while adhering to constraints on
computational budget and minimum synthesis quality. Our experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3\% average SSIM drop. The source code for our
work is also made publicly available at
https://saeejithnair.github.io/NAS-NeRF.",http://arxiv.org/pdf/2309.14293v1
2309.14288v1,math.AP,Comparison of One- Two- and Three- Dimensional CNN models for Drawing-Test-Based Diagnostics of the Parkinson's Disease,2023-09-25 16:52:57+00:00,"Subject: In this article, convolutional networks of one, two, and three
dimensions are compared with respect to their ability to distinguish between
the drawing tests produced by Parkinson's disease patients and healthy control
subjects.
  Motivation: The application of deep learning techniques for the analysis of
drawing tests to support the diagnosis of Parkinson's disease has become a
growing trend in the area of Artificial Intelligence.
  Method: The dynamic features of the handwriting signal are embedded in the
static test data to generate one-dimensional time series, two-dimensional RGB
images and three-dimensional voxelized point clouds, and then one-, two-, and
three-dimensional CNN can be used to automatically extract features for
effective diagnosis.
  Novelty: While there are many results that describe the application of
two-dimensional convolutional models to the problem, to the best knowledge of
the authors, there are no results based on the application of three-dimensional
models and very few using one-dimensional models.
  Main result: The accuracy of the one-, two- and three-dimensional CNN models
was 62.50%, 77.78% and 83.34% in the DraWritePD dataset (acquired by the
authors) and 73.33%, 80.00% and 86.67% in the PaHaW dataset (well known from
the literature), respectively. For these two data sets, the proposed
three-dimensional convolutional classification method exhibits the best
diagnostic performance.",http://arxiv.org/pdf/2309.14288v1
2309.14280v1,eess.SP,Joint RIS Phase Profile Design and Power Allocation for Parameter Estimation in Presence of Eavesdropping,2023-09-25 16:42:27+00:00,"We consider secure transmission of a deterministic complex-valued parameter
vector from a transmitter to an intended receiver in the presence of an
eavesdropper in a reconfigurable intelligent surface (RIS)-integrated
environment. We aim to jointly optimize the RIS phase profile and the power
allocation matrix at the transmitter to enhance the estimation accuracy at the
intended receiver while limiting that at the eavesdropper. We utilize the trace
of the Fisher information matrix (FIM), equivalently, the average Fisher
information, as the estimation accuracy metric, and obtain its closed form
expression for the intended receiver and the eavesdropper. Accordingly, the
joint RIS phase profile and power allocation problem is formulated, and it is
solved via alternating optimization. When the power allocation matrix is fixed
during alternating optimization, the optimal RIS phase profile design problem
is formulated as a non-convex problem and it is solved via semidefinite
relaxation and rank reduction. When the RIS phase profile is fixed, a linear
programming formulation is obtained for optimal power allocation. Via
simulations, the effects of RIS phase design and power allocation are
illustrated individually and jointly. Moreover, extensions are provided by
considering the presence of line of sight paths in the environment and the
availability of RIS elements with adjustable magnitudes.",http://arxiv.org/pdf/2309.14280v1
2309.14272v1,cs.RO,Perception-and-Energy-aware Motion Planning for UAV using Learning-based Model under Heteroscedastic Uncertainty,2023-09-25 16:34:54+00:00,"Global navigation satellite systems (GNSS) denied environments/conditions
require unmanned aerial vehicles (UAVs) to energy-efficiently and reliably fly.
To this end, this study presents perception-and-energy-aware motion planning
for UAVs in GNSS-denied environments. The proposed planner solves the
trajectory planning problem by optimizing a cost function consisting of two
indices: the total energy consumption of a UAV and the perception quality of
light detection and ranging (LiDAR) sensor mounted on the UAV. Before online
navigation, a high-fidelity simulator acquires a flight dataset to learn energy
consumption for the UAV and heteroscedastic uncertainty associated with LiDAR
measurements, both as functions of the horizontal velocity of the UAV. The
learned models enable the online planner to estimate energy consumption and
perception quality, reducing UAV battery usage and localization errors.
Simulation experiments in a photorealistic environment confirm that the
proposed planner can address the trade-off between energy efficiency and
perception quality under heteroscedastic uncertainty. The open-source code is
released at https://gitlab.com/ReI08/perception-energy-planner.",http://arxiv.org/pdf/2309.14272v1
2309.14269v1,cs.CV,Unsupervised correspondence with combined geometric learning and imaging for radiotherapy applications,2023-09-25 16:29:18+00:00,"The aim of this study was to develop a model to accurately identify
corresponding points between organ segmentations of different patients for
radiotherapy applications. A model for simultaneous correspondence and
interpolation estimation in 3D shapes was trained with head and neck organ
segmentations from planning CT scans. We then extended the original model to
incorporate imaging information using two approaches: 1) extracting features
directly from image patches, and 2) including the mean square error between
patches as part of the loss function. The correspondence and interpolation
performance were evaluated using the geodesic error, chamfer distance and
conformal distortion metrics, as well as distances between anatomical
landmarks. Each of the models produced significantly better correspondences
than the baseline non-rigid registration approach. The original model performed
similarly to the model with direct inclusion of image features. The best
performing model configuration incorporated imaging information as part of the
loss function which produced more anatomically plausible correspondences. We
will use the best performing model to identify corresponding anatomical points
on organs to improve spatial normalisation, an important step in outcome
modelling, or as an initialisation for anatomically informed registrations. All
our code is publicly available at
https://github.com/rrr-uom-projects/Unsup-RT-Corr-Net",http://arxiv.org/pdf/2309.14269v1
2309.14258v1,cs.CL,"OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding",2023-09-25 16:15:09+00:00,"Event understanding aims at understanding the content and relationship of
events within texts, which covers multiple complicated information extraction
tasks: event detection, event argument extraction, and event relation
extraction. To facilitate related research and application, we present an event
understanding toolkit OmniEvent, which features three desiderata: (1)
Comprehensive. OmniEvent supports mainstream modeling paradigms of all the
event understanding tasks and the processing of 15 widely-used English and
Chinese datasets. (2) Fair. OmniEvent carefully handles the inconspicuous
evaluation pitfalls reported in Peng et al. (2023), which ensures fair
comparisons between different models. (3) Easy-to-use. OmniEvent is designed to
be easily used by users with varying needs. We provide off-the-shelf models
that can be directly deployed as web services. The modular framework also
enables users to easily implement and evaluate new event understanding models
with OmniEvent. The toolkit (https://github.com/THU-KEG/OmniEvent) is publicly
released along with the demonstration website and video
(https://omnievent.xlore.cn/).",http://arxiv.org/pdf/2309.14258v1
2309.14256v1,stat.ME,A Weighted Prognostic Covariate Adjustment Method for Efficient and Powerful Treatment Effect Inferences in Randomized Controlled Trials,2023-09-25 16:14:13+00:00,"A crucial task for a randomized controlled trial (RCT) is to specify a
statistical method that can yield an efficient estimator and powerful test for
the treatment effect. A novel and effective strategy to obtain efficient and
powerful treatment effect inferences is to incorporate predictions from
generative artificial intelligence (AI) algorithms into covariate adjustment
for the regression analysis of a RCT. Training a generative AI algorithm on
historical control data enables one to construct a digital twin generator (DTG)
for RCT participants, which utilizes a participant's baseline covariates to
generate a probability distribution for their potential control outcome.
Summaries of the probability distribution from the DTG are highly predictive of
the trial outcome, and adjusting for these features via regression can thus
improve the quality of treatment effect inferences, while satisfying regulatory
guidelines on statistical analyses, for a RCT. However, a critical assumption
in this strategy is homoskedasticity, or constant variance of the outcome
conditional on the covariates. In the case of heteroskedasticity, existing
covariate adjustment methods yield inefficient estimators and underpowered
tests. We propose to address heteroskedasticity via a weighted prognostic
covariate adjustment methodology (Weighted PROCOVA) that adjusts for both the
mean and variance of the regression model using information obtained from the
DTG. We prove that our method yields unbiased treatment effect estimators, and
demonstrate via comprehensive simulation studies and case studies from
Alzheimer's disease that it can reduce the variance of the treatment effect
estimator, maintain the Type I error rate, and increase the power of the test
for the treatment effect from 80% to 85%~90% when the variances from the DTG
can explain 5%~10% of the variation in the RCT participants' outcomes.",http://arxiv.org/pdf/2309.14256v1
2309.14250v1,stat.AP,Prediction Model For Wordle Game Results With High Robustness,2023-09-25 16:10:35+00:00,"In this study, we delve into the dynamics of Wordle using data analysis and
machine learning. Our analysis initially focused on the correlation between the
date and the number of submitted results. Due to initial popularity bias, we
modeled stable data using an ARIMAX model with coefficient values of 9, 0, 2,
and weekdays/weekends as the exogenous variable. We found no significant
relationship between word attributes and hard mode results.
  To predict word difficulty, we employed a Backpropagation Neural Network,
overcoming overfitting via feature engineering. We also used K-means
clustering, optimized at five clusters, to categorize word difficulty
numerically. Our findings indicate that on March 1st, 2023, around 12,884
results will be submitted and the word ""eerie"" averages 4.8 attempts, falling
into the hardest difficulty cluster.
  We further examined the percentage of loyal players and their propensity to
undertake daily challenges. Our models underwent rigorous sensitivity analyses,
including ADF, ACF, PACF tests, and cross-validation, confirming their
robustness. Overall, our study provides a predictive framework for Wordle
gameplay based on date or a given five-letter word. Results have been
summarized and submitted to the Puzzle Editor of the New York Times.",http://arxiv.org/pdf/2309.14250v1
2309.14247v1,cs.NI,Rethinking Internet Communication Through LLMs: How Close Are We?,2023-09-25 16:07:07+00:00,"In this paper, we rethink the way that communication among users over the
Internet, one of the fundamental outcomes of the Internet evolution, takes
place. Instead of users communicating directly over the Internet, we explore an
architecture that enables users to communicate with (query) Large Language
Models (LLMs) that capture the cognition of users on the other end of the
communication channel. We present an architecture to achieve such LLM-based
communication and we perform a reality check to assess how close we are today
to realizing such a communication architecture from a technical point of view.
Finally, we discuss several research challenges and identify interesting
directions for future research.",http://arxiv.org/pdf/2309.14247v1
2309.14243v1,cs.LG,Enhancing data efficiency in reinforcement learning: a novel imagination mechanism based on mesh information propagation,2023-09-25 16:03:08+00:00,"Reinforcement learning (RL) algorithms face the challenge of limited data
efficiency, particularly when dealing with high-dimensional state spaces and
large-scale problems. Most RL methods often rely solely on state transition
information within the same episode when updating the agent's Critic, which can
lead to low data efficiency and sub-optimal training time consumption. Inspired
by human-like analogical reasoning abilities, we introduce a novel mesh
information propagation mechanism, termed the 'Imagination Mechanism (IM)',
designed to significantly enhance the data efficiency of RL algorithms.
Specifically, IM enables information generated by a single sample to be
effectively broadcasted to different states, instead of simply transmitting in
the same episode and it allows the model to better understand the
interdependencies between states and learn scarce sample information more
efficiently. To promote versatility, we extend the imagination mechanism to
function as a plug-and-play module that can be seamlessly and fluidly
integrated into other widely adopted RL models. Our experiments demonstrate
that Imagination mechanism consistently boosts four mainstream SOTA
RL-algorithms, such as SAC, PPO, DDPG, and DQN, by a considerable margin,
ultimately leading to superior performance than before across various tasks.
For access to our code and data, please visit
https://github.com/Zero-coder/FECAM.",http://arxiv.org/pdf/2309.14243v1
2309.14235v1,cs.LG,Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving,2023-09-25 15:47:07+00:00,"The deployment of autonomous vehicles (AVs) has faced hurdles due to the
dominance of rare but critical corner cases within the long-tail distribution
of driving scenarios, which negatively affects their overall performance. To
address this challenge, adversarial generation methods have emerged as a class
of efficient approaches to synthesize safety-critical scenarios for AV testing.
However, these generated scenarios are often underutilized for AV training,
resulting in the potential for continual AV policy improvement remaining
untapped, along with a deficiency in the closed-loop design needed to achieve
it. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately
characterize the hierarchical nature of vehicle interaction dynamics,
facilitating iterative improvement by engaging background vehicles (BVs) and AV
in a sequential game-like interaction paradigm. With AV acting as the leader
and BVs as followers, this leader-follower modeling ensures that AV would
consistently refine its policy, always taking into account the additional
information that BVs play the best response to challenge AV. Extensive
experiments have shown that our algorithm exhibits superior performance
compared to several baselines especially in higher dimensional scenarios,
leading to substantial advancements in AV capabilities while continually
generating progressively challenging scenarios.",http://arxiv.org/pdf/2309.14235v1
2309.14231v1,math.OC,Combined sizing and layout optimization of truss structures via update Monte Carlo tree search (UMCTS) algorithm,2023-09-25 15:42:52+00:00,"The main concern of this study is to find the optimal design of truss
structures considering sizing and layout variables simultaneously. As compared
to purely sizing optimization problems, this problem is more challenging since
the two types of variables involved are fundamentally different in nature. In
this paper, a reinforcement learning method combining the update process and
Monte Carlo tree search called the update Monte Carlo tree search (UMCTS) for
sizing optimization problems is applied to solve combined sizing and layout
optimization for truss structures. This study proposes a novel update process
for nodal coordinates with two features. (1) The allowed range of each
coordinate varies in each round. (2) Accelerators for the number of entries in
the allowed range and iteration numbers are introduced to reduce the
computation time. Furthermore, nodal coordinates and member areas are
determined at the same time with only one search tree in each round. The
validation and efficiency of the UMCTS are tested on benchmark problems of
planar and spatial trusses with discrete sizing variables and continuous layout
variables. It is shown that the CPU time of the UMCTS is two times faster than
the branch and bound method. The numerical results demonstrate that the
proposed method stably achieves a better solution than other traditional
methods.",http://arxiv.org/pdf/2309.14231v1
2309.14221v1,cs.LG,Accelerating Machine Learning Algorithms with Adaptive Sampling,2023-09-25 15:25:59+00:00,"The era of huge data necessitates highly efficient machine learning
algorithms. Many common machine learning algorithms, however, rely on
computationally intensive subroutines that are prohibitively expensive on large
datasets. Oftentimes, existing techniques subsample the data or use other
methods to improve computational efficiency, at the expense of incurring some
approximation error. This thesis demonstrates that it is often sufficient,
instead, to substitute computationally intensive subroutines with a special
kind of randomized counterparts that results in almost no degradation in
quality.",http://arxiv.org/pdf/2309.14221v1
2309.14216v1,cs.LG,MemDA: Forecasting Urban Time Series with Memory-based Drift Adaptation,2023-09-25 15:22:28+00:00,"Urban time series data forecasting featuring significant contributions to
sustainable development is widely studied as an essential task of the smart
city. However, with the dramatic and rapid changes in the world environment,
the assumption that data obey Independent Identically Distribution is
undermined by the subsequent changes in data distribution, known as concept
drift, leading to weak replicability and transferability of the model over
unseen data. To address the issue, previous approaches typically retrain the
model, forcing it to fit the most recent observed data. However, retraining is
problematic in that it leads to model lag, consumption of resources, and model
re-invalidation, causing the drift problem to be not well solved in realistic
scenarios. In this study, we propose a new urban time series prediction model
for the concept drift problem, which encodes the drift by considering the
periodicity in the data and makes on-the-fly adjustments to the model based on
the drift using a meta-dynamic network. Experiments on real-world datasets show
that our design significantly outperforms state-of-the-art methods and can be
well generalized to existing prediction backbones by reducing their sensitivity
to distribution changes.",http://arxiv.org/pdf/2309.14216v1
2309.14213v1,cs.CE,"Autonomous Vehicles an overview on system, cyber security, risks, issues, and a way forward",2023-09-25 15:19:09+00:00,"This chapter explores the complex realm of autonomous cars, analyzing their
fundamental components and operational characteristics. The initial phase of
the discussion is elucidating the internal mechanics of these automobiles,
encompassing the crucial involvement of sensors, artificial intelligence (AI)
identification systems, control mechanisms, and their integration with
cloud-based servers within the framework of the Internet of Things (IoT). It
delves into practical implementations of autonomous cars, emphasizing their
utilization in forecasting traffic patterns and transforming the dynamics of
transportation. The text also explores the topic of Robotic Process Automation
(RPA), illustrating the impact of autonomous cars on different businesses
through the automation of tasks. The primary focus of this investigation lies
in the realm of cybersecurity, specifically in the context of autonomous
vehicles. A comprehensive analysis will be conducted to explore various risk
management solutions aimed at protecting these vehicles from potential threats
including ethical, environmental, legal, professional, and social dimensions,
offering a comprehensive perspective on their societal implications. A
strategic plan for addressing the challenges and proposing strategies for
effectively traversing the complex terrain of autonomous car systems,
cybersecurity, hazards, and other concerns are some resources for acquiring an
understanding of the intricate realm of autonomous cars and their ramifications
in contemporary society, supported by a comprehensive compilation of resources
for additional investigation.
  Keywords: RPA, Cyber Security, AV, Risk, Smart Cars",http://arxiv.org/pdf/2309.14213v1
2309.14209v1,cs.LG,Continual Driving Policy Optimization with Closed-Loop Individualized Curricula,2023-09-25 15:14:54+00:00,"The safety of autonomous vehicles (AV) has been a long-standing top concern,
stemming from the absence of rare and safety-critical scenarios in the
long-tail naturalistic driving distribution. To tackle this challenge, a surge
of research in scenario-based autonomous driving has emerged, with a focus on
generating high-risk driving scenarios and applying them to conduct
safety-critical testing of AV models. However, limited work has been explored
on the reuse of these extensive scenarios to iteratively improve AV models.
Moreover, it remains intractable and challenging to filter through gigantic
scenario libraries collected from other AV models with distinct behaviors,
attempting to extract transferable information for current AV improvement.
Therefore, we develop a continual driving policy optimization framework
featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into
a set of standardized sub-modules for flexible implementation choices: AV
Evaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a
collision prediction task, where it estimates the chance of AV failures in
these scenarios at each iteration. Subsequently, by re-sampling from historical
scenarios based on these failure probabilities, CLIC tailors individualized
curricula for downstream training, aligning them with the evaluated capability
of AV. Accordingly, CLIC not only maximizes the utilization of the vast
pre-collected scenario library for closed-loop driving policy optimization but
also facilitates AV improvement by individualizing its training with more
challenging cases out of those poorly organized scenarios. Experimental results
clearly indicate that CLIC surpasses other curriculum-based training
strategies, showing substantial improvement in managing risky scenarios, while
still maintaining proficiency in handling simpler cases.",http://arxiv.org/pdf/2309.14209v1
2309.14208v1,cs.CY,Framework based on complex networks to model and mine patient pathways,2023-09-25 15:11:52+00:00,"The automatic discovery of a model to represent the history of encounters of
a group of patients with the healthcare system -- the so-called ``pathway of
patients'' -- is a new field of research that supports clinical and
organisational decisions to improve the quality and efficiency of the treatment
provided. The pathways of patients with chronic conditions tend to vary
significantly from one person to another, have repetitive tasks, and demand the
analysis of multiple perspectives (interventions, diagnoses, medical
specialities, among others) influencing the results. Therefore, modelling and
mining those pathways is still a challenging task. In this work, we propose a
framework comprising: (i) a pathway model based on a multi-aspect graph, (ii) a
novel dissimilarity measurement to compare pathways taking the elapsed time
into account, and (iii) a mining method based on traditional centrality
measures to discover the most relevant steps of the pathways. We evaluated the
framework using the study cases of pregnancy and diabetes, which revealed its
usefulness in finding clusters of similar pathways, representing them in an
easy-to-interpret way, and highlighting the most significant patterns according
to multiple perspectives.",http://arxiv.org/pdf/2309.14208v1
2309.14183v2,cs.CV,Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition,2023-09-25 14:46:01+00:00,"The development of foundation vision models has pushed the general visual
recognition to a high level, but cannot well address the fine-grained
recognition in specialized domain such as invasive species classification.
Identifying and managing invasive species has strong social and ecological
value. Currently, most invasive species datasets are limited in scale and cover
a narrow range of species, which restricts the development of deep-learning
based invasion biometrics systems. To fill the gap of this area, we introduced
Species196, a large-scale semi-supervised dataset of 196-category invasive
species. It collects over 19K images with expert-level accurate annotations
Species196-L, and 1.2M unlabeled images of invasive species Species196-U. The
dataset provides four experimental settings for benchmarking the existing
models and algorithms, namely, supervised learning, semi-supervised learning,
self-supervised pretraining and zero-shot inference ability of large
multi-modal models. To facilitate future research on these four learning
paradigms, we conduct an empirical study of the representative methods on the
introduced dataset. The dataset is publicly available at
https://species-dataset.github.io/.",http://arxiv.org/pdf/2309.14183v2
2309.14181v1,cs.CV,Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision,2023-09-25 14:43:43+00:00,"The rapid evolution of Multi-modality Large Language Models (MLLMs) has
catalyzed a shift in computer vision from specialized models to general-purpose
foundation models. Nevertheless, there is still an inadequacy in assessing the
abilities of MLLMs on low-level visual perception and understanding. To address
this gap, we present Q-Bench, a holistic benchmark crafted to systematically
evaluate potential abilities of MLLMs on three realms: low-level visual
perception, low-level visual description, and overall visual quality
assessment. a) To evaluate the low-level perception ability, we construct the
LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped
with a human-asked question focusing on its low-level attributes. We then
measure the correctness of MLLMs on answering these questions. b) To examine
the description ability of MLLMs on low-level information, we propose the
LLDescribe dataset consisting of long expert-labelled golden low-level text
descriptions on 499 images, and a GPT-involved comparison pipeline between
outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we
further measure their visual quality assessment ability to align with human
opinion scores. Specifically, we design a softmax-based strategy that enables
MLLMs to predict quantifiable quality scores, and evaluate them on various
existing image quality assessment (IQA) datasets. Our evaluation across the
three abilities confirms that MLLMs possess fundamental low-level visual
skills. However, these skills are still unstable and relatively imprecise,
indicating the need for specific enhancements on MLLMs towards these abilities.
We hope that our benchmark can encourage the research community to delve deeper
to discover and enhance these untapped potentials of MLLMs.",http://arxiv.org/pdf/2309.14181v1
2309.14177v1,physics.optics,\textit{Anoplophora graafi} Longhorn Beetle Coloration is due to Disordered Diamond-like Packed Spheres,2023-09-25 14:40:51+00:00,"While artificially photonic materials are typically highly ordered, photonic
structures in many species of birds and insects do not possess a long-range
order. Studying their order-disorder interplay sheds light on the origin of the
photonic band gap. Here, we investigated the scale morphology of the
\textit{Anoplophora graafi} longhorn beetle. Combining small-angle X-ray
scattering and slice-and-view FIB-SEM tomography with molecular dynamics and
optical simulations, we characterised the chitin sphere assemblies within blue
and green \textit{A.\ graafi} scales. The low volume fraction of spheres and
the number of their nearest neighbours are incompatible with any known
close-packed sphere morphology. A short-range diamond lattice with long-range
disorder best describes the sphere assembly, which will inspire the development
of new colloid-based photonic materials.",http://arxiv.org/pdf/2309.14177v1
2309.14162v1,cs.CV,Data Upcycling Knowledge Distillation for Image Super-Resolution,2023-09-25 14:13:26+00:00,"Knowledge distillation (KD) emerges as a challenging yet promising technique
for compressing deep learning models, characterized by the transmission of
extensive learning representations from proficient and computationally
intensive teacher models to compact student models. However, only a handful of
studies have endeavored to compress the models for single image
super-resolution (SISR) through KD, with their effects on student model
enhancement remaining marginal. In this paper, we put forth an approach from
the perspective of efficient data utilization, namely, the Data Upcycling
Knowledge Distillation (DUKD) which facilitates the student model by the prior
knowledge teacher provided via upcycled in-domain data derived from their
inputs. This upcycling process is realized through two efficient image zooming
operations and invertible data augmentations which introduce the label
consistency regularization to the field of KD for SISR and substantially boosts
student model's generalization. The DUKD, due to its versatility, can be
applied across a broad spectrum of teacher-student architectures. Comprehensive
experiments across diverse benchmarks demonstrate that our proposed DUKD method
significantly outperforms previous art, exemplified by an increase of up to
0.5dB in PSNR over baselines methods, and a 67% parameters reduced RCAN model's
performance remaining on par with that of the RCAN teacher model.",http://arxiv.org/pdf/2309.14162v1
2309.14153v1,cs.ET,An optimized quantum minimum searching algorithm with sure-success probability and its experiment simulation with Cirq,2023-09-25 14:07:27+00:00,"Finding a minimum is an essential part of mathematical models, and it plays
an important role in some optimization problems. Durr and Hoyer proposed a
quantum searching algorithm (DHA), with a certain probability of success, to
achieve quadratic speed than classical ones. In this paper, we propose an
optimized quantum minimum searching algorithm with sure-success probability,
which utilizes Grover-Long searching to implement the optimal exact searching,
and the dynamic strategy to reduce the iterations of our algorithm. Besides, we
optimize the oracle circuit to reduce the number of gates by the simplified
rules. The performance evaluation including the theoretical success rate and
computational complexity shows that our algorithm has higher accuracy and
efficiency than DHA algorithm. Finally, a simulation experiment based on Cirq
is performed to verify its feasibility.",http://arxiv.org/pdf/2309.14153v1
2309.14148v1,cs.DC,SPIRT: A Fault-Tolerant and Reliable Peer-to-Peer Serverless ML Training Architecture,2023-09-25 14:01:35+00:00,"The advent of serverless computing has ushered in notable advancements in
distributed machine learning, particularly within parameter server-based
architectures. Yet, the integration of serverless features within peer-to-peer
(P2P) distributed networks remains largely uncharted. In this paper, we
introduce SPIRT, a fault-tolerant, reliable, and secure serverless P2P ML
training architecture. designed to bridge this existing gap.
  Capitalizing on the inherent robustness and reliability innate to P2P
systems, SPIRT employs RedisAI for in-database operations, leading to an 82\%
reduction in the time required for model updates and gradient averaging across
a variety of models and batch sizes. This architecture showcases resilience
against peer failures and adeptly manages the integration of new peers, thereby
highlighting its fault-tolerant characteristics and scalability. Furthermore,
SPIRT ensures secure communication between peers, enhancing the reliability of
distributed machine learning tasks. Even in the face of Byzantine attacks, the
system's robust aggregation algorithms maintain high levels of accuracy. These
findings illuminate the promising potential of serverless architectures in P2P
distributed machine learning, offering a significant stride towards the
development of more efficient, scalable, and resilient applications.",http://arxiv.org/pdf/2309.14148v1
2309.14139v1,cs.DC,Exploring the Impact of Serverless Computing on Peer To Peer Training Machine Learning,2023-09-25 13:51:07+00:00,"The increasing demand for computational power in big data and machine
learning has driven the development of distributed training methodologies.
Among these, peer-to-peer (P2P) networks provide advantages such as enhanced
scalability and fault tolerance. However, they also encounter challenges
related to resource consumption, costs, and communication overhead as the
number of participating peers grows. In this paper, we introduce a novel
architecture that combines serverless computing with P2P networks for
distributed training and present a method for efficient parallel gradient
computation under resource constraints.
  Our findings show a significant enhancement in gradient computation time,
with up to a 97.34\% improvement compared to conventional P2P distributed
training methods. As for costs, our examination confirmed that the serverless
architecture could incur higher expenses, reaching up to 5.4 times more than
instance-based architectures. It is essential to consider that these higher
costs are associated with marked improvements in computation time, particularly
under resource-constrained scenarios. Despite the cost-time trade-off, the
serverless approach still holds promise due to its pay-as-you-go model.
Utilizing dynamic resource allocation, it enables faster training times and
optimized resource utilization, making it a promising candidate for a wide
range of machine learning applications.",http://arxiv.org/pdf/2309.14139v1
2309.14134v1,cs.LG,One-Class Classification for Intrusion Detection on Vehicular Networks,2023-09-25 13:42:22+00:00,"Controller Area Network bus systems within vehicular networks are not
equipped with the tools necessary to ward off and protect themselves from
modern cyber-security threats. Work has been done on using machine learning
methods to detect and report these attacks, but common methods are not robust
towards unknown attacks. These methods usually rely on there being a sufficient
representation of attack data, which may not be available due to there either
not being enough data present to adequately represent its distribution or the
distribution itself is too diverse in nature for there to be a sufficient
representation of it. With the use of one-class classification methods, this
issue can be mitigated as only normal data is required to train a model for the
detection of anomalous instances. Research has been done on the efficacy of
these methods, most notably One-Class Support Vector Machine and Support Vector
Data Description, but many new extensions of these works have been proposed and
have yet to be tested for injection attacks in vehicular networks. In this
paper, we investigate the performance of various state-of-the-art one-class
classification methods for detecting injection attacks on Controller Area
Network bus traffic. We investigate the effectiveness of these techniques on
attacks launched on Controller Area Network buses from two different vehicles
during normal operation and while being attacked. We observe that the Subspace
Support Vector Data Description method outperformed all other tested methods
with a Gmean of about 85%.",http://arxiv.org/pdf/2309.14134v1
2309.14123v1,eess.SY,Harnessing Supervised Learning for Adaptive Beamforming in Multibeam Satellite Systems,2023-09-25 13:23:22+00:00,"In today's ever-connected world, the demand for fast and widespread
connectivity is insatiable, making multibeam satellite systems an indispensable
pillar of modern telecommunications infrastructure. However, the evolving
communication landscape necessitates a high degree of adaptability. This
adaptability is particularly crucial for beamforming, as it enables the
adjustment of peak throughput and beamwidth to meet fluctuating traffic demands
by varying the beamwidth, side lobe level (SLL), and effective isotropic
radiated power (EIRP). This paper introduces an innovative approach rooted in
supervised learning to efficiently derive the requisite beamforming matrix,
aligning it with system requirements. Significantly reducing computation time,
this method is uniquely tailored for real-time adaptation, enhancing the
agility and responsiveness of satellite multibeam systems. Exploiting the power
of supervised learning, this research enables multibeam satellites to respond
quickly and intelligently to changing communication needs, ultimately ensuring
uninterrupted and optimized connectivity in a dynamic world.",http://arxiv.org/pdf/2309.14123v1
2309.14117v1,cs.CV,Small Objects Matters in Weakly-supervised Semantic Segmentation,2023-09-25 13:15:57+00:00,"Weakly-supervised semantic segmentation (WSSS) performs pixel-wise
classification given only image-level labels for training. Despite the
difficulty of this task, the research community has achieved promising results
over the last five years. Still, current WSSS literature misses the detailed
sense of how well the methods perform on different sizes of objects. Thus we
propose a novel evaluation metric to provide a comprehensive assessment across
different object sizes and collect a size-balanced evaluation set to complement
PASCAL VOC. With these two gadgets, we reveal that the existing WSSS methods
struggle in capturing small objects. Furthermore, we propose a size-balanced
cross-entropy loss coupled with a proper training strategy. It generally
improves existing WSSS methods as validated upon ten baselines on three
different datasets.",http://arxiv.org/pdf/2309.14117v1
2309.14112v1,cs.AI,Semi-Abstract Value-Based Argumentation Framework,2023-09-25 13:10:56+00:00,"In his seminal paper, Phan Minh Dung (1995) proposed abstract argumentation
framework, which models argumentation using directed graphs where structureless
arguments are the nodes and attacks among the arguments are the edges. In the
following years, many extensions of this framework were introduced. These
extensions typically add a certain form of structure to the arguments. This
thesis showcases two such extensions -- value-based argumentation framework by
Trevor Bench-Capon (2002) and semi-abstract argumentation framework by Esther
Anna Corsi and Christian Ferm\""uller (2017). The former introduces a mapping
function that links individual arguments to a set of ordered values, enabling a
distinction between objectively and subjectively acceptable arguments. The
latter links claims of individual arguments to propositional formulae and then
applies newly-introduced attack principles in order to make implicit attacks
explicit and to enable a definition of a consequence relation that relies on
neither the truth values nor the interpretations in the usual sense.
  The contribution of this thesis is two-fold. Firstly, the new semi-abstract
value-based argumentation framework is introduced. This framework maps
propositional formulae associated with individual arguments to a set of ordered
values. Secondly, a complex moral dilemma is formulated using the original and
the value-based argumentation frameworks showcasing the expressivity of these
formalisms.",http://arxiv.org/pdf/2309.14112v1
2309.14092v1,cs.DB,From OCEL to DOCEL -- Datasets and Automated Transformation,2023-09-25 12:31:50+00:00,"Object-centric event data represent processes from the point of view of all
the involved object types. This perspective has gained interest in recent years
as it supports the analysis of processes that previously could not be
adequately captured, due to the lack of a clear case notion as well as an
increasing amount of output data that needs to be stored. Although publicly
available event logs are crucial artifacts for researchers to develop and
evaluate novel process mining techniques, the currently available
object-centric event logs have limitations in this regard. Specifically, they
mainly focus on control-flow and rarely contain objects with attributes that
change over time, even though this is not realistic, as the attribute values of
objects can be altered during their lifecycle. This paper addresses this gap by
providing two means of establishing object-centric datasets with dynamically
evolving attributes. First, we provide event log generators, which allow
researchers to generate customized, artificial logs with dynamic attributes in
the recently proposed DOCEL format. Second, we propose and evaluate an
algorithm to convert OCEL logs into DOCEL logs, which involves the detection of
event attributes that capture evolving object information and the creation of
dynamic attributes from these. Through these contributions, this paper supports
the advancement of object-centric process analysis by providing researchers
with new means to obtain relevant data to use during the development of new
techniques.",http://arxiv.org/pdf/2309.14092v1
2309.14087v1,eess.SP,Adaptive Three Layer Hybrid Reconfigurable Intelligent Surface for 6G Wireless Communication: Trade-offs and Performance,2023-09-25 12:30:03+00:00,"A potential candidate technology for the development of future 6G networks
has been recognized as Reconfigurable Intelligent Surface (RIS). However, due
to the variation in radio link quality, traditional passive RISs only
accomplish a minimal signal gain in situations with strong direct links between
user equipment (UE) and base station (BS). In order to get over this
fundamental restriction of smaller gain, the idea of active RISs might be a
suitable solution. In contrast to current passive RIS, which simply reflects
and directs signals without any additional amplification, active RISs have the
ability to enhance reflected signals by the incorporation of amplifiers inside
its elements. However, with additional amplifiers, apart from the relatively
complex attributes of RIS-assisted arrangements, the additional energy
consumption of such technologies is often disregarded. So, there might be a
tradeoff between the additional energy consumption for the RIS technologies and
the overall gain acquired by deploying this potential advancement. The
objective of this work is to provide a primary idea of a three-layer hybrid
RIS-assisted configuration that is responsive to both active and passive RIS,
as well as an additional dormant or inactive state. The single RIS structure
should be capable of adjusting its overall configuration in response to
fluctuations in transmit power and radio link quality. Furthermore, our
fabricated passive RIS-assisted structure verifies a portion of the proposed
idea, with simulations highlighting its advantages over standalone passive or
active RIS-assisted technologies.",http://arxiv.org/pdf/2309.14087v1
2309.14084v1,cs.CL,"Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges",2023-09-25 12:23:37+00:00,"In the domain of Natural Language Processing (NLP), Named Entity Recognition
(NER) stands out as a pivotal mechanism for extracting structured insights from
unstructured text. This manuscript offers an exhaustive exploration into the
evolving landscape of NER methodologies, blending foundational principles with
contemporary AI advancements. Beginning with the rudimentary concepts of NER,
the study spans a spectrum of techniques from traditional rule-based strategies
to the contemporary marvels of transformer architectures, particularly
highlighting integrations such as BERT with LSTM and CNN. The narrative
accentuates domain-specific NER models, tailored for intricate areas like
finance, legal, and healthcare, emphasizing their specialized adaptability.
Additionally, the research delves into cutting-edge paradigms including
reinforcement learning, innovative constructs like E-NER, and the interplay of
Optical Character Recognition (OCR) in augmenting NER capabilities. Grounding
its insights in practical realms, the paper sheds light on the indispensable
role of NER in sectors like finance and biomedicine, addressing the unique
challenges they present. The conclusion outlines open challenges and avenues,
marking this work as a comprehensive guide for those delving into NER research
and applications.",http://arxiv.org/pdf/2309.14084v1
2309.14078v1,cs.LG,ODE-based Recurrent Model-free Reinforcement Learning for POMDPs,2023-09-25 12:13:56+00:00,"Neural ordinary differential equations (ODEs) are widely recognized as the
standard for modeling physical mechanisms, which help to perform approximate
inference in unknown physical or biological environments. In partially
observable (PO) environments, how to infer unseen information from raw
observations puzzled the agents. By using a recurrent policy with a compact
context, context-based reinforcement learning provides a flexible way to
extract unobservable information from historical transitions. To help the agent
extract more dynamics-related information, we present a novel ODE-based
recurrent model combines with model-free reinforcement learning (RL) framework
to solve partially observable Markov decision processes (POMDPs). We
experimentally demonstrate the efficacy of our methods across various PO
continuous control and meta-RL tasks. Furthermore, our experiments illustrate
that our method is robust against irregular observations, owing to the ability
of ODEs to model irregularly-sampled time series.",http://arxiv.org/pdf/2309.14078v1
2309.14073v1,stat.ML,Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach,2023-09-25 12:07:00+00:00,"We propose a graphical structure for structural equation models that is
stable under marginalization under linearity and Gaussianity assumptions. We
show that computing the maximum likelihood estimation of this model is
equivalent to training a neural network. We implement a GPU-based algorithm
that computes the maximum likelihood estimation of these models.",http://arxiv.org/pdf/2309.14073v1
2309.14065v2,cs.CV,AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation,2023-09-25 11:57:16+00:00,"In the realm of robotic intelligence, achieving efficient and precise RGB-D
semantic segmentation is a key cornerstone. State-of-the-art multimodal
semantic segmentation methods, primarily rooted in symmetrical skeleton
networks, find it challenging to harmonize computational efficiency and
precision. In this work, we propose AsymFormer, a novel network for real-time
RGB-D semantic segmentation, which targets the minimization of superfluous
parameters by optimizing the distribution of computational resources and
introduces an asymmetrical backbone to allow for the effective fusion of
multimodal features. Furthermore, we explore techniques to bolster network
accuracy by redefining feature selection and extracting multi-modal
self-similarity features without a substantial increase in the parameter count,
thereby ensuring real-time execution on robotic platforms. Additionally, a
Local Attention-Guided Feature Selection (LAFS) module is used to selectively
fuse features from different modalities by leveraging their dependencies.
Subsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding
(CMA) module is introduced to further extract cross-modal representations. This
method is evaluated on NYUv2 and SUNRGBD datasets, with AsymFormer
demonstrating competitive results with 52.0% mIoU on NYUv2 and 49.1% mIoU on
SUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS and after
implementing mixed precision quantization, it attains an impressive inference
speed of 79 FPS on RTX3090. This significantly outperforms existing multi-modal
methods, thereby demonstrating that AsymFormer can strike a balance between
high accuracy and efficiency for RGB-D semantic segmentation.",http://arxiv.org/pdf/2309.14065v2
2309.14054v1,cs.LG,Adapt then Unlearn: Exploiting Parameter Space Semantics for Unlearning in Generative Adversarial Networks,2023-09-25 11:36:20+00:00,"The increased attention to regulating the outputs of deep generative models,
driven by growing concerns about privacy and regulatory compliance, has
highlighted the need for effective control over these models. This necessity
arises from instances where generative models produce outputs containing
undesirable, offensive, or potentially harmful content. To tackle this
challenge, the concept of machine unlearning has emerged, aiming to forget
specific learned information or to erase the influence of undesired data
subsets from a trained model. The objective of this work is to prevent the
generation of outputs containing undesired features from a pre-trained GAN
where the underlying training data set is inaccessible. Our approach is
inspired by a crucial observation: the parameter space of GANs exhibits
meaningful directions that can be leveraged to suppress specific undesired
features. However, such directions usually result in the degradation of the
quality of generated samples. Our proposed method, known as
'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also
maintaining the quality of generated samples. This method unfolds in two
stages: in the initial stage, we adapt the pre-trained GAN using negative
samples provided by the user, while in the subsequent stage, we focus on
unlearning the undesired feature. During the latter phase, we train the
pre-trained GAN using positive samples, incorporating a repulsion regularizer.
This regularizer encourages the model's parameters to be away from the
parameters associated with the adapted model from the first stage while also
maintaining the quality of generated samples. To the best of our knowledge, our
approach stands as first method addressing unlearning in GANs. We validate the
effectiveness of our method through comprehensive experiments.",http://arxiv.org/pdf/2309.14054v1
2309.14053v1,cs.LG,Revisiting LARS for Large Batch Training Generalization of Neural Networks,2023-09-25 11:35:10+00:00,"LARS and LAMB have emerged as prominent techniques in Large Batch Learning
(LBL), ensuring the stability of AI training. One of the primary challenges in
LBL is convergence stability, where the AI agent usually gets trapped into the
sharp minimizer. Addressing this challenge, a relatively recent technique,
known as warm-up, has been employed. However, warm-up lacks a strong
theoretical foundation, leaving the door open for further exploration of more
efficacious algorithms. In light of this situation, we conduct empirical
experiments to analyze the behaviors of the two most popular optimizers in the
LARS family: LARS and LAMB, with and without a warm-up strategy. Our analyses
give us a comprehension of the novel LARS, LAMB, and the necessity of a warm-up
technique in LBL. Building upon these insights, we propose a novel algorithm
called Time Varying LARS (TVLARS), which facilitates robust training in the
initial phase without the need for warm-up. Experimental evaluation
demonstrates that TVLARS achieves competitive results with LARS and LAMB when
warm-up is utilized while surpassing their performance without the warm-up
technique.",http://arxiv.org/pdf/2309.14053v1
2309.14042v1,cond-mat.mes-hall,Making topologically trivial non-Hermitian systems non-trivial via gauge fields,2023-09-25 11:19:15+00:00,"Non-Hermiticity significantly enriches the concepts of symmetry and topology
in physics. Particularly, non-Hermiticity gives rise to the ramified
symmetries, where the non-Hermitian Hamiltonian $H$ is transformed to
$H^\dagger$. For time-reversal ($T$) and sublattice symmetries, there are six
ramified symmetry classes leading to novel topological classifications with
various non-Hermitian skin effects. As artificial crystals are the main
experimental platforms for non-Hermitian physics, there exists the symmetry
barrier for realizing topological physics in the six ramified symmetry classes:
While artificial crystals are in spinless classes with $T^2=1$, nontrivial
classifications dominantly appear in spinful classes with $T^2=-1$. Here, we
present a general mechanism to cross the symmetry barrier. With an internal
parity symmetry $P$, the square of the combination $\tilde{T}=PT$ can be
modified by appropriate gauge fluxes. Using the general mechanism, we
systematically construct spinless models for all non-Hermitian spinful
topological phases in one and two dimensions, which are experimentally
realizable. Our work suggests that gauge structures may significantly enrich
non-Hermitian physics at the fundamental level.",http://arxiv.org/pdf/2309.14042v1
2309.14037v1,cs.NE,An automatic selection of optimal recurrent neural network architecture for processes dynamics modelling purposes,2023-09-25 11:06:35+00:00,"A problem related to the development of algorithms designed to find the
structure of artificial neural network used for behavioural (black-box)
modelling of selected dynamic processes has been addressed in this paper. The
research has included four original proposals of algorithms dedicated to neural
network architecture search. Algorithms have been based on well-known
optimisation techniques such as evolutionary algorithms and gradient descent
methods. In the presented research an artificial neural network of recurrent
type has been used, whose architecture has been selected in an optimised way
based on the above-mentioned algorithms. The optimality has been understood as
achieving a trade-off between the size of the neural network and its accuracy
in capturing the response of the mathematical model under which it has been
learnt. During the optimisation, original specialised evolutionary operators
have been proposed. The research involved an extended validation study based on
data generated from a mathematical model of the fast processes occurring in a
pressurised water nuclear reactor.",http://arxiv.org/pdf/2309.14037v1
2309.14032v1,cs.NE,DeepACO: Neural-enhanced Ant Systems for Combinatorial Optimization,2023-09-25 10:56:38+00:00,"Ant Colony Optimization (ACO) is a meta-heuristic algorithm that has been
successfully applied to various Combinatorial Optimization Problems (COPs).
Traditionally, customizing ACO for a specific problem requires the expert
design of knowledge-driven heuristics. In this paper, we propose DeepACO, a
generic framework that leverages deep reinforcement learning to automate
heuristic designs. DeepACO serves to strengthen the heuristic measures of
existing ACO algorithms and dispense with laborious manual design in future ACO
applications. As a neural-enhanced meta-heuristic, DeepACO consistently
outperforms its ACO counterparts on eight COPs using a single neural model and
a single set of hyperparameters. As a Neural Combinatorial Optimization method,
DeepACO performs better than or on par with problem-specific methods on
canonical routing problems. Our code is publicly available at
https://github.com/henry-yeh/DeepACO.",http://arxiv.org/pdf/2309.14032v1
2309.14029v1,cs.LG,Diffeomorphic Transformations for Time Series Analysis: An Efficient Approach to Nonlinear Warping,2023-09-25 10:51:47+00:00,"The proliferation and ubiquity of temporal data across many disciplines has
sparked interest for similarity, classification and clustering methods
specifically designed to handle time series data. A core issue when dealing
with time series is determining their pairwise similarity, i.e., the degree to
which a given time series resembles another. Traditional distance measures such
as the Euclidean are not well-suited due to the time-dependent nature of the
data. Elastic metrics such as dynamic time warping (DTW) offer a promising
approach, but are limited by their computational complexity,
non-differentiability and sensitivity to noise and outliers. This thesis
proposes novel elastic alignment methods that use parametric \& diffeomorphic
warping transformations as a means of overcoming the shortcomings of DTW-based
metrics. The proposed method is differentiable \& invertible, well-suited for
deep learning architectures, robust to noise and outliers, computationally
efficient, and is expressive and flexible enough to capture complex patterns.
Furthermore, a closed-form solution was developed for the gradient of these
diffeomorphic transformations, which allows an efficient search in the
parameter space, leading to better solutions at convergence. Leveraging the
benefits of these closed-form diffeomorphic transformations, this thesis
proposes a suite of advancements that include: (a) an enhanced temporal
transformer network for time series alignment and averaging, (b) a
deep-learning based time series classification model to simultaneously align
and classify signals with high accuracy, (c) an incremental time series
clustering algorithm that is warping-invariant, scalable and can operate under
limited computational and time resources, and finally, (d) a normalizing flow
model that enhances the flexibility of affine transformations in coupling and
autoregressive layers.",http://arxiv.org/pdf/2309.14029v1
2309.14025v1,physics.flu-dyn,Optimum control strategies for maximum thrust production in underwater undulatory swimming,2023-09-25 10:42:58+00:00,"Fish, cetaceans and many other aquatic vertebrates undulate their bodies to
propel themselves through water. Numerous studies on natural, artificial or
analogous swimmers are dedicated to revealing the links between the kinematics
of body oscillation and the production of thrust for swimming. One of the most
open and difficult questions concerns the best kinematics to maximize this
later quantity for given constraints and how a system strategizes and adjusts
its internal parameters to reach this maximum. To address this challenge, we
exploit a biomimetic robotic swimmer to determine the control signal that
produces the highest thrust. Using machine learning techniques and intuitive
models, we find that this optimal control consists of a square wave function,
whose frequency is fixed by the interplay between the internal dynamics of the
swimmer and the fluid-structure interaction with the surrounding fluid. We then
propose a simple implementation for autonomous robotic swimmers that requires
no prior knowledge of systems or equations. This application to aquatic
locomotion is validated by 2D numerical simulations.",http://arxiv.org/pdf/2309.14025v1
2309.14021v1,cs.CL,LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression,2023-09-25 10:35:17+00:00,"Low Rank Decomposition of matrix - splitting a large matrix into a product of
two smaller matrix offers a means for compression that reduces the parameters
of a model without sparsification, and hence delivering more speedup on modern
hardware. Moreover, unlike quantization, the compressed linear layers remain
fully differentiable and all the parameters trainable, while being able to
leverage the existing highly efficient kernels over floating point matrices. We
study the potential to compress Large Language Models (LLMs) for monolingual
Code generation via Low Rank Decomposition (LoRD) and observe that ranks for
the linear layers in these models can be reduced by upto 39.58% with less than
1% increase in perplexity. We then use Low Rank Decomposition (LoRD) to
compress StarCoder 16B to 13.2B parameter with no drop and to 12.3B with
minimal drop in HumanEval Pass@1 score, in less than 10 minutes on a single
A100. The compressed models speeds up inference by up to 22.35% with just a
single line of change in code over huggingface's implementation with pytorch
backend. Low Rank Decomposition (LoRD) models remain compatible with state of
the art near-lossless quantization method such as SpQR, which allows leveraging
further compression gains of quantization. Lastly, QLoRA over Low Rank
Decomposition (LoRD) model further reduces memory requirements by as much as
21.2% over vanilla QLoRA while offering similar gains from parameter efficient
fine tuning. Our work shows Low Rank Decomposition (LoRD) as a promising new
paradigm for LLM compression.",http://arxiv.org/pdf/2309.14021v1
2309.14008v1,eess.SP,Carrier Aggregation Enabled Integrated Sensing and Communication Signal Design and Processing,2023-09-25 10:20:13+00:00,"The future mobile communication systems will support intelligent applications
such as Internet of Vehicles (IoV) and Extended Reality (XR). Integrated
Sensing and Communication (ISAC) is regarded as one of the key technologies
satisfying the high data rate communication and highly accurate sensing for
these intelligent applications in future mobile communication systems. With the
explosive growth of wireless devices and services, the shortage of spectrum
resources leads to the fragmentation of available frequency bands for ISAC
systems, which degrades sensing performance. Facing the above challenges, this
paper proposes a Carrier Aggregation (CA)-based ISAC signal aggregating high
and low-frequency bands to improve the sensing performance, where the CA-based
ISAC signal can use four different aggregated pilot structures for sensing.
Then, an ISAC signal processing algorithm with Compressed Sensing (CS) is
proposed and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) is
used to solve the reconfiguration convex optimization problem. Finally, the
Cram'er-Rao Lower Bounds (CRLBs) are derived for the CA-based ISAC signal.
Simulation results show that CA efficiently improves the accuracy of range and
velocity estimation.",http://arxiv.org/pdf/2309.14008v1
2309.13979v1,cs.OH,"Morphological Computing as Logic Underlying Cognition in Human, Animal, and Intelligent Machine",2023-09-25 09:31:25+00:00,"This work examines the interconnections between logic, epistemology, and
sciences within the Naturalist tradition. It presents a scheme that connects
logic, mathematics, physics, chemistry, biology, and cognition, emphasizing
scale-invariant, self-organizing dynamics across organizational tiers of
nature. The inherent logic of agency exists in natural processes at various
levels, under information exchanges. It applies to humans, animals, and
artifactual agents. The common human-centric, natural language-based logic is
an example of complex logic evolved by living organisms that already appears in
the simplest form at the level of basal cognition of unicellular organisms.
Thus, cognitive logic stems from the evolution of physical, chemical, and
biological logic. In a computing nature framework with a self-organizing
agency, innovative computational frameworks grounded in
morphological/physical/natural computation can be used to explain the genesis
of human-centered logic through the steps of naturalized logical processes at
lower levels of organization. The Extended Evolutionary Synthesis of living
agents is essential for understanding the emergence of human-level logic and
the relationship between logic and information processing/computational
epistemology. We conclude that more research is needed to elucidate the details
of the mechanisms linking natural phenomena with the logic of agency in nature.",http://arxiv.org/pdf/2309.13979v1
2309.13972v1,cs.SD,Audio classification with Dilated Convolution with Learnable Spacings,2023-09-25 09:09:54+00:00,"Dilated convolution with learnable spacings (DCLS) is a recent convolution
method in which the positions of the kernel elements are learned throughout
training by backpropagation. Its interest has recently been demonstrated in
computer vision (ImageNet classification and downstream tasks). Here we show
that DCLS is also useful for audio tagging using the AudioSet classification
benchmark. We took two state-of-the-art convolutional architectures using
depthwise separable convolutions (DSC), ConvNeXt and ConvFormer, and a hybrid
one using attention in addition, FastViT, and drop-in replaced all the DSC
layers by DCLS ones. This significantly improved the mean average precision
(mAP) with the three architectures without increasing the number of parameters
and with only a low cost on the throughput. The method code is based on PyTorch
and is available at https://github.com/K-H-Ismail/DCLS-Audio",http://arxiv.org/pdf/2309.13972v1
2309.13970v1,cs.HC,A Cyberpunk 2077 perspective on the prediction and understanding of future technology,2023-09-25 09:08:30+00:00,"Science fiction and video games have long served as valuable tools for
envisioning and inspiring future technological advancements. This position
paper investigates the potential of Cyberpunk 2077, a popular science fiction
video game, to shed light on the future of technology, particularly in the
areas of artificial intelligence, edge computing, augmented humans, and
biotechnology. By analyzing the game's portrayal of these technologies and
their implications, we aim to understand the possibilities and challenges that
lie ahead. We discuss key themes such as neurolink and brain-computer
interfaces, multimodal recording systems, virtual and simulated reality,
digital representation of the physical world, augmented and AI-based home
appliances, smart clothing, and autonomous vehicles. The paper highlights the
importance of designing technologies that can coexist with existing preferences
and systems, considering the uneven adoption of new technologies. Through this
exploration, we emphasize the potential of science fiction and video games like
Cyberpunk 2077 as tools for guiding future technological advancements and
shaping public perception of emerging innovations.",http://arxiv.org/pdf/2309.13970v1
2309.13965v1,cs.HC,May I Ask a Follow-up Question? Understanding the Benefits of Conversations in Neural Network Explainability,2023-09-25 09:00:38+00:00,"Research in explainable AI (XAI) aims to provide insights into the
decision-making process of opaque AI models. To date, most XAI methods offer
one-off and static explanations, which cannot cater to the diverse backgrounds
and understanding levels of users. With this paper, we investigate if free-form
conversations can enhance users' comprehension of static explanations, improve
acceptance and trust in the explanation methods, and facilitate human-AI
collaboration. Participants are presented with static explanations, followed by
a conversation with a human expert regarding the explanations. We measure the
effect of the conversation on participants' ability to choose, from three
machine learning models, the most accurate one based on explanations and their
self-reported comprehension, acceptance, and trust. Empirical results show that
conversations significantly improve comprehension, acceptance, trust, and
collaboration. Our findings highlight the importance of customized model
explanations in the format of free-form conversations and provide insights for
the future design of conversational explanations.",http://arxiv.org/pdf/2309.13965v1
2309.13960v1,cs.LG,Newton Method-based Subspace Support Vector Data Description,2023-09-25 08:49:41+00:00,"In this paper, we present an adaptation of Newton's method for the
optimization of Subspace Support Vector Data Description (S-SVDD). The
objective of S-SVDD is to map the original data to a subspace optimized for
one-class classification, and the iterative optimization process of data
mapping and description in S-SVDD relies on gradient descent. However, gradient
descent only utilizes first-order information, which may lead to suboptimal
results. To address this limitation, we leverage Newton's method to enhance
data mapping and data description for an improved optimization of subspace
learning-based one-class classification. By incorporating this auxiliary
information, Newton's method offers a more efficient strategy for subspace
learning in one-class classification as compared to gradient-based
optimization. The paper discusses the limitations of gradient descent and the
advantages of using Newton's method in subspace learning for one-class
classification tasks. We provide both linear and nonlinear formulations of
Newton's method-based optimization for S-SVDD. In our experiments, we explored
both the minimization and maximization strategies of the objective. The results
demonstrate that the proposed optimization strategy outperforms the
gradient-based S-SVDD in most cases.",http://arxiv.org/pdf/2309.13960v1
2309.13952v1,cs.CV,VidChapters-7M: Video Chapters at Scale,2023-09-25 08:38:11+00:00,"Segmenting long videos into chapters enables users to quickly navigate to the
information of their interest. This important topic has been understudied due
to the lack of publicly released datasets. To address this issue, we present
VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters
in total. VidChapters-7M is automatically created from videos online in a
scalable manner by scraping user-annotated chapters and hence without any
additional manual annotation. We introduce the following three tasks based on
this data. First, the video chapter generation task consists of temporally
segmenting the video and generating a chapter title for each segment. To
further dissect the problem, we also define two variants of this task: video
chapter generation given ground-truth boundaries, which requires generating a
chapter title given an annotated video segment, and video chapter grounding,
which requires temporally localizing a chapter given its annotated title. We
benchmark both simple baselines and state-of-the-art video-language models for
these three tasks. We also show that pretraining on VidChapters-7M transfers
well to dense video captioning tasks in both zero-shot and finetuning settings,
largely improving the state of the art on the YouCook2 and ViTT benchmarks.
Finally, our experiments reveal that downstream performance scales well with
the size of the pretraining dataset. Our dataset, code, and models are publicly
available at https://antoyang.github.io/vidchapters.html.",http://arxiv.org/pdf/2309.13952v1
2309.13948v1,cs.RO,Co-Design Optimisation of Morphing Topology and Control of Winged Drones,2023-09-25 08:27:35+00:00,"The design and control of winged aircraft and drones is an iterative process
aimed at identifying a compromise of mission-specific costs and constraints.
When agility is required, shape-shifting (morphing) drones represent an
efficient solution. However, morphing drones require the addition of actuated
joints that increase the topology and control coupling, making the design
process more complex. We propose a co-design optimisation method that assists
the engineers by proposing a morphing drone's conceptual design that includes
topology, actuation, morphing strategy, and controller parameters. The method
consists of applying multi-objective constraint-based optimisation to a
multi-body winged drone with trajectory optimisation to solve the motion
intelligence problem under diverse flight mission requirements. We show that
co-designed morphing drones outperform fixed-winged drones in terms of energy
efficiency and agility, suggesting that the proposed co-design method could be
a useful addition to the aircraft engineering toolbox.",http://arxiv.org/pdf/2309.13948v1
2309.13939v1,cs.AI,"The Time Traveler's Guide to Semantic Web Research: Analyzing Fictitious Research Themes in the ESWC ""Next 20 Years"" Track",2023-09-25 08:20:06+00:00,"What will Semantic Web research focus on in 20 years from now? We asked this
question to the community and collected their visions in the ""Next 20 years""
track of ESWC 2023. We challenged the participants to submit ""future"" research
papers, as if they were submitting to the 2043 edition of the conference. The
submissions - entirely fictitious - were expected to be full scientific papers,
with research questions, state of the art references, experimental results and
future work, with the goal to get an idea of the research agenda for the late
2040s and early 2050s. We received ten submissions, eight of which were
accepted for presentation at the conference, that mixed serious ideas of
potential future research themes and discussion topics with some fun and irony.
  In this paper, we intend to provide a survey of those ""science fiction""
papers, considering the emerging research themes and topics, analysing the
research methods applied by the authors in these very special submissions, and
investigating also the most fictitious parts (e.g., neologisms, fabricated
references). Our goal is twofold: on the one hand, we investigate what this
special track tells us about the Semantic Web community and, on the other hand,
we aim at getting some insights on future research practices and directions.",http://arxiv.org/pdf/2309.13939v1
2309.13937v1,cs.RO,SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems,2023-09-25 08:13:49+00:00,"Pick-and-place is one of the fundamental tasks in robotics research. However,
the attention has been mostly focused on the ``pick'' task, leaving the
``place'' task relatively unexplored. In this paper, we address the problem of
placing objects in the context of a teleoperation framework. Particularly, we
focus on two aspects of the place task: stability robustness and contextual
reasonableness of object placements. Our proposed method combines
simulation-driven physical stability verification via real-to-sim and the
semantic reasoning capability of large language models. In other words, given
place context information (e.g., user preferences, object to place, and current
scene information), our proposed method outputs a probability distribution over
the possible placement candidates, considering the robustness and
reasonableness of the place task. Our proposed method is extensively evaluated
in two simulation and one real world environments and we show that our method
can greatly increase the physical plausibility of the placement as well as
contextual soundness while considering user preferences.",http://arxiv.org/pdf/2309.13937v1
2309.13933v1,cs.CY,Fairness and Bias in Algorithmic Hiring,2023-09-25 08:04:18+00:00,"Employers are adopting algorithmic hiring technology throughout the
recruitment pipeline. Algorithmic fairness is especially applicable in this
domain due to its high stakes and structural inequalities. Unfortunately, most
work in this space provides partial treatment, often constrained by two
competing narratives, optimistically focused on replacing biased recruiter
decisions or pessimistically pointing to the automation of discrimination.
Whether, and more importantly what types of, algorithmic hiring can be less
biased and more beneficial to society than low-tech alternatives currently
remains unanswered, to the detriment of trustworthiness. This multidisciplinary
survey caters to practitioners and researchers with a balanced and integrated
coverage of systems, biases, measures, mitigation strategies, datasets, and
legal aspects of algorithmic hiring and fairness. Our work supports a
contextualized understanding and governance of this technology by highlighting
current opportunities and limitations, providing recommendations for future
work to ensure shared benefits for all stakeholders.",http://arxiv.org/pdf/2309.13933v1
2309.13926v2,cs.LG,Pseudo Label Selection is a Decision Problem,2023-09-25 07:48:02+00:00,"Pseudo-Labeling is a simple and effective approach to semi-supervised
learning. It requires criteria that guide the selection of pseudo-labeled data.
The latter have been shown to crucially affect pseudo-labeling's generalization
performance. Several such criteria exist and were proven to work reasonably
well in practice. However, their performance often depends on the initial model
fit on labeled data. Early overfitting can be propagated to the final model by
choosing instances with overconfident but wrong predictions, often called
confirmation bias. In two recent works, we demonstrate that pseudo-label
selection (PLS) can be naturally embedded into decision theory. This paves the
way for BPLS, a Bayesian framework for PLS that mitigates the issue of
confirmation bias. At its heart is a novel selection criterion: an analytical
approximation of the posterior predictive of pseudo-samples and labeled data.
We derive this selection criterion by proving Bayes-optimality of this ""pseudo
posterior predictive"". We empirically assess BPLS for generalized linear,
non-parametric generalized additive models and Bayesian neural networks on
simulated and real-world data. When faced with data prone to overfitting and
thus a high chance of confirmation bias, BPLS outperforms traditional PLS
methods. The decision-theoretic embedding further allows us to render PLS more
robust towards the involved modeling assumptions. To achieve this goal, we
introduce a multi-objective utility function. We demonstrate that the latter
can be constructed to account for different sources of uncertainty and explore
three examples: model selection, accumulation of errors and covariate shift.",http://arxiv.org/pdf/2309.13926v2
2309.13925v1,cs.CV,UCF-Crime Annotation: A Benchmark for Surveillance Video-and-Language Understanding,2023-09-25 07:46:56+00:00,"Surveillance videos are an essential component of daily life with various
critical applications, particularly in public security. However, current
surveillance video tasks mainly focus on classifying and localizing anomalous
events. Existing methods are limited to detecting and classifying the
predefined events with unsatisfactory generalization ability and semantic
understanding, although they have obtained considerable performance. To address
this issue, we propose constructing the first multimodal surveillance video
dataset by manually annotating the real-world surveillance dataset UCF-Crime
with fine-grained event content and timing. Our newly annotated dataset, UCA
(UCF-Crime Annotation), provides a novel benchmark for multimodal surveillance
video analysis. It not only describes events in detailed descriptions but also
provides precise temporal grounding of the events in 0.1-second intervals. UCA
contains 20,822 sentences, with an average length of 23 words, and its
annotated videos are as long as 102 hours. Furthermore, we benchmark the
state-of-the-art models of multiple multimodal tasks on this newly created
dataset, including temporal sentence grounding in videos, video captioning, and
dense video captioning. Through our experiments, we found that mainstream
models used in previously publicly available datasets perform poorly on
multimodal surveillance video scenarios, which highlights the necessity of
constructing this dataset. The link to our dataset and code is provided at:
https://github.com/Xuange923/UCA-dataset.",http://arxiv.org/pdf/2309.13925v1
2309.13908v1,cs.RO,A comparison of controller architectures and learning mechanisms for arbitrary robot morphologies,2023-09-25 07:11:43+00:00,"The main question this paper addresses is: What combination of a robot
controller and a learning method should be used, if the morphology of the
learning robot is not known in advance? Our interest is rooted in the context
of morphologically evolving modular robots, but the question is also relevant
in general, for system designers interested in widely applicable solutions. We
perform an experimental comparison of three controller-and-learner
combinations: one approach where controllers are based on modelling animal
locomotion (Central Pattern Generators, CPG) and the learner is an evolutionary
algorithm, a completely different method using Reinforcement Learning (RL) with
a neural network controller architecture, and a combination `in-between' where
controllers are neural networks and the learner is an evolutionary algorithm.
We apply these three combinations to a test suite of modular robots and compare
their efficacy, efficiency, and robustness. Surprisingly, the usual CPG-based
and RL-based options are outperformed by the in-between combination that is
more robust and efficient than the other two setups.",http://arxiv.org/pdf/2309.13908v1
2309.13902v1,eess.SP,NoncovANM: Gridless DOA Estimation for LPDF System,2023-09-25 06:51:05+00:00,"Direction of arrival (DOA) estimation is an important research in the area of
array signal processing, and has been studied for decades. High resolution DOA
estimation requires large array aperture, which leads to the increase of
hardware cost. Besides, high accuracy DOA estimation methods usually have high
computational complexity. In this paper, the problem of decreasing the hardware
cost and algorithm complexity is addressed. First, considering the ability of
flexible controlling the electromagnetic waves and low-cost, an intelligent
reconfigurable surface (IRS)-aided low-cost passive direction finding (LPDF)
system is developed, where only one fully functional receiving channel is
adopted. Then, the sparsity of targets direction in the spatial domain is
exploited by formulating an atomic norm minimization (ANM) problem to estimate
the DOA. Traditionally, solving ANM problem is complex and cannot be realized
efficiently. Hence, a novel nonconvex-based ANM (NC-ANM) method is proposed by
gradient threshold iteration, where a perturbation is introduced to avoid
falling into saddle points. The theoretical analysis for the convergence of the
NC-ANM method is also given. Moreover, the corresponding Cram\'er-Rao lower
bound (CRLB) in the LPDF system is derived, and taken as the referred bound of
the DOA estimation. Simulation results show that the proposed method
outperforms the compared methods in the DOA estimation with lower computational
complexity in the LPDF system.",http://arxiv.org/pdf/2309.13902v1
2309.13893v1,cs.RO,Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments,2023-09-25 06:16:09+00:00,"Navigating complex and dynamic environments requires autonomous vehicles
(AVs) to reason about both visible and occluded regions. This involves
predicting the future motion of observed agents, inferring occluded ones, and
modeling their interactions based on vectorized scene representations of the
partially observable environment. However, prior work on occlusion inference
and trajectory prediction have developed in isolation, with the former based on
simplified rasterized methods and the latter assuming full environment
observability. We introduce the Scene Informer, a unified approach for
predicting both observed agent trajectories and inferring occlusions in a
partially observable setting. It uses a transformer to aggregate various input
modalities and facilitate selective queries on occlusions that might intersect
with the AV's planned path. The framework estimates occupancy probabilities and
likely trajectories for occlusions, as well as forecast motion for observed
agents. We explore common observability assumptions in both domains and their
performance impact. Our approach outperforms existing methods in both occupancy
prediction and trajectory prediction in partially observable setting on the
Waymo Open Motion Dataset.",http://arxiv.org/pdf/2309.13893v1
2309.13885v1,cs.LG,TouchUp-G: Improving Feature Representation through Graph-Centric Finetuning,2023-09-25 05:44:40+00:00,"How can we enhance the node features acquired from Pretrained Models (PMs) to
better suit downstream graph learning tasks? Graph Neural Networks (GNNs) have
become the state-of-the-art approach for many high-impact, real-world graph
applications. For feature-rich graphs, a prevalent practice involves utilizing
a PM directly to generate features, without incorporating any domain adaptation
techniques. Nevertheless, this practice is suboptimal because the node features
extracted from PM are graph-agnostic and prevent GNNs from fully utilizing the
potential correlations between the graph structure and node features, leading
to a decline in GNNs performance. In this work, we seek to improve the node
features obtained from a PM for downstream graph tasks and introduce TOUCHUP-G,
which has several advantages. It is (a) General: applicable to any downstream
graph task, including link prediction which is often employed in recommender
systems; (b) Multi-modal: able to improve raw features of any modality (e.g.
images, texts, audio); (c) Principled: it is closely related to a novel metric,
feature homophily, which we propose to quantify the potential correlations
between the graph structure and node features and we show that TOUCHUP-G can
effectively shrink the discrepancy between the graph structure and node
features; (d) Effective: achieving state-of-the-art results on four real-world
datasets spanning different tasks and modalities.",http://arxiv.org/pdf/2309.13885v1
2309.13860v1,cs.CL,Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning,2023-09-25 04:07:34+00:00,"Recent years have witnessed significant advancements in self-supervised
learning (SSL) methods for speech-processing tasks. Various speech-based SSL
models have been developed and present promising performance on a range of
downstream tasks including speech recognition. However, existing speech-based
SSL models face a common dilemma in terms of computational cost, which might
hinder their potential application and in-depth academic research. To address
this issue, we first analyze the computational cost of different modules during
HuBERT pre-training and then introduce a stack of efficiency optimizations,
which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be
trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without
performance degradation, resulting in a 5.2x speedup, compared to the original
implementation. Moreover, we explore two well-studied techniques in the
Fast-HuBERT and demonstrate consistent improvements as reported in previous
work.",http://arxiv.org/pdf/2309.13860v1
2309.13856v1,eess.SP,DNN-DANM: A High-Accuracy Two-Dimensional DOA Estimation Method Using Practical RIS,2023-09-25 03:47:41+00:00,"Reconfigurable intelligent surface (RIS) or intelligent reflecting surface
(IRS) has been an attractive technology for future wireless communication and
sensing systems. However, in the practical RIS, the mutual coupling effect
among RIS elements, the reflection phase shift, and amplitude errors will
degrade the RIS performance significantly. This paper investigates the
two-dimensional direction-of-arrival (DOA) estimation problem in the scenario
using a practical RIS. After formulating the system model with the mutual
coupling effect and the reflection phase/amplitude errors of the RIS, a novel
DNNDANM method is proposed for the DOA estimation by combining the deep neural
network (DNN) and the decoupling atomic norm minimization (DANM). The DNN step
reconstructs the received signal from the one with RIS impairments, and the
DANM step exploits the signal sparsity in the two-dimensional spatial domain.
Additionally, a semi-definite programming (SDP) method with low computational
complexity is proposed to solve the atomic minimization problem. Finally, both
simulation and prototype are carried out to show estimation performance, and
the proposed method outperforms the existing methods in the two-dimensional DOA
estimation with low complexity in the scenario with practical RIS.",http://arxiv.org/pdf/2309.13856v1
2309.13851v1,cs.CV,DISeR: Designing Imaging Systems with Reinforcement Learning,2023-09-25 03:35:51+00:00,"Imaging systems consist of cameras to encode visual information about the
world and perception models to interpret this encoding. Cameras contain (1)
illumination sources, (2) optical elements, and (3) sensors, while perception
models use (4) algorithms. Directly searching over all combinations of these
four building blocks to design an imaging system is challenging due to the size
of the search space. Moreover, cameras and perception models are often designed
independently, leading to sub-optimal task performance. In this paper, we
formulate these four building blocks of imaging systems as a context-free
grammar (CFG), which can be automatically searched over with a learned camera
designer to jointly optimize the imaging system with task-specific perception
models. By transforming the CFG to a state-action space, we then show how the
camera designer can be implemented with reinforcement learning to intelligently
search over the combinatorial space of possible imaging system configurations.
We demonstrate our approach on two tasks, depth estimation and camera rig
design for autonomous vehicles, showing that our method yields rigs that
outperform industry-wide standards. We believe that our proposed approach is an
important step towards automating imaging system design.",http://arxiv.org/pdf/2309.13851v1
2309.13834v1,cs.AI,Prior Bilinear Based Models for Knowledge Graph Completion,2023-09-25 02:44:33+00:00,"Bilinear based models are powerful and widely used approaches for Knowledge
Graphs Completion (KGC). Although bilinear based models have achieved
significant advances, these studies mainly concentrate on posterior properties
(based on evidence, e.g. symmetry pattern) while neglecting the prior
properties. In this paper, we find a prior property named ""the law of identity""
that cannot be captured by bilinear based models, which hinders them from
comprehensively modeling the characteristics of KGs. To address this issue, we
introduce a solution called Unit Ball Bilinear Model (UniBi). This model not
only achieves theoretical superiority but also offers enhanced interpretability
and performance by minimizing ineffective learning through minimal constraints.
Experiments demonstrate that UniBi models the prior property and verify its
interpretability and performance.",http://arxiv.org/pdf/2309.13834v1
2309.13833v1,cs.CV,Dual Feature Augmentation Network for Generalized Zero-shot Learning,2023-09-25 02:37:52+00:00,"Zero-shot learning (ZSL) aims to infer novel classes without training samples
by transferring knowledge from seen classes. Existing embedding-based
approaches for ZSL typically employ attention mechanisms to locate attributes
on an image. However, these methods often ignore the complex entanglement among
different attributes' visual features in the embedding space. Additionally,
these methods employ a direct attribute prediction scheme for classification,
which does not account for the diversity of attributes in images of the same
category. To address these issues, we propose a novel Dual Feature Augmentation
Network (DFAN), which comprises two feature augmentation modules, one for
visual features and the other for semantic features. The visual feature
augmentation module explicitly learns attribute features and employs cosine
distance to separate them, thus enhancing attribute representation. In the
semantic feature augmentation module, we propose a bias learner to capture the
offset that bridges the gap between actual and predicted attribute values from
a dataset's perspective. Furthermore, we introduce two predictors to reconcile
the conflicts between local and global features. Experimental results on three
benchmarks demonstrate the marked advancement of our method compared to
state-of-the-art approaches. Our code is available at
https://github.com/Sion1/DFAN.",http://arxiv.org/pdf/2309.13833v1
2309.13803v1,cs.CR,Privacy-preserving Linear Computations in Spiking Neural P Systems,2023-09-25 01:16:18+00:00,"Spiking Neural P systems are a class of membrane computing models inspired
directly by biological neurons. Besides the theoretical progress made in this
new computational model, there are also numerous applications of P systems in
fields like formal verification, artificial intelligence, or cryptography.
Motivated by all the use cases of SN P systems, in this paper, we present a new
privacy-preserving protocol that enables a client to compute a linear function
using an SN P system hosted on a remote server. Our protocol allows the client
to use the server to evaluate functions of the form t_1k + t_2 without
revealing t_1, t_2 or k and without the server knowing the result. We also
present an SN P system to implement any linear function over natural numbers
and some security considerations of our protocol in the honest-but-curious
security model.",http://arxiv.org/pdf/2309.13803v1
2309.13788v1,cs.CL,Can LLM-Generated Misinformation Be Detected?,2023-09-25 00:45:07+00:00,"The advent of Large Language Models (LLMs) has made a transformative impact.
However, the potential that LLMs such as ChatGPT can be exploited to generate
misinformation has posed a serious concern to online safety and public trust. A
fundamental research question is: will LLM-generated misinformation cause more
harm than human-written misinformation? We propose to tackle this question from
the perspective of detection difficulty. We first build a taxonomy of
LLM-generated misinformation. Then we categorize and validate the potential
real-world methods for generating misinformation with LLMs. Then, through
extensive empirical investigation, we discover that LLM-generated
misinformation can be harder to detect for humans and detectors compared to
human-written misinformation with the same semantics, which suggests it can
have more deceptive styles and potentially cause more harm. We also discuss the
implications of our discovery on combating misinformation in the age of LLMs
and the countermeasures.",http://arxiv.org/pdf/2309.13788v1
2309.13782v1,cs.LG,On the Computational Benefit of Multimodal Learning,2023-09-25 00:20:50+00:00,"Human perception inherently operates in a multimodal manner. Similarly, as
machines interpret the empirical world, their learning processes ought to be
multimodal. The recent, remarkable successes in empirical multimodal learning
underscore the significance of understanding this paradigm. Yet, a solid
theoretical foundation for multimodal learning has eluded the field for some
time. While a recent study by Lu (2023) has shown the superior sample
complexity of multimodal learning compared to its unimodal counterpart, another
basic question remains: does multimodal learning also offer computational
advantages over unimodal learning? This work initiates a study on the
computational benefit of multimodal learning. We demonstrate that, under
certain conditions, multimodal learning can outpace unimodal learning
exponentially in terms of computation. Specifically, we present a learning task
that is NP-hard for unimodal learning but is solvable in polynomial time by a
multimodal algorithm. Our construction is based on a novel modification to the
intersection of two half-spaces problem.",http://arxiv.org/pdf/2309.13782v1
2309.13781v2,cs.LG,Explainable Machine Learning for ICU Readmission Prediction,2023-09-25 00:16:43+00:00,"The intensive care unit (ICU) comprises a complex hospital environment, where
decisions made by clinicians have a high level of risk for the patients' lives.
A comprehensive care pathway must then be followed to reduce p complications.
Uncertain, competing and unplanned aspects within this environment increase the
difficulty in uniformly implementing the care pathway. Readmission contributes
to this pathway's difficulty, occurring when patients are admitted again to the
ICU in a short timeframe, resulting in high mortality rates and high resource
utilisation. Several works have tried to predict readmission through patients'
medical information. Although they have some level of success while predicting
readmission, those works do not properly assess, characterise and understand
readmission prediction. This work proposes a standardised and explainable
machine learning pipeline to model patient readmission on a multicentric
database (i.e., the eICU cohort with 166,355 patients, 200,859 admissions and
6,021 readmissions) while validating it on monocentric (i.e., the MIMIC IV
cohort with 382,278 patients, 523,740 admissions and 5,984 readmissions) and
multicentric settings. Our machine learning pipeline achieved predictive
performance in terms of the area of the receiver operating characteristic curve
(AUC) up to 0.7 with a Random Forest classification model, yielding an overall
good calibration and consistency on validation sets. From explanations provided
by the constructed models, we could also derive a set of insightful
conclusions, primarily on variables related to vital signs and blood tests
(e.g., albumin, blood urea nitrogen and hemoglobin levels), demographics (e.g.,
age, and admission height and weight), and ICU-associated variables (e.g., unit
type). These insights provide an invaluable source of information during
clinicians' decision-making while discharging ICU patients.",http://arxiv.org/pdf/2309.13781v2
2309.13773v1,cs.LG,GHN-QAT: Training Graph Hypernetworks to Predict Quantization-Robust Parameters of Unseen Limited Precision Neural Networks,2023-09-24 23:01:00+00:00,"Graph Hypernetworks (GHN) can predict the parameters of varying unseen CNN
architectures with surprisingly good accuracy at a fraction of the cost of
iterative optimization. Following these successes, preliminary research has
explored the use of GHNs to predict quantization-robust parameters for 8-bit
and 4-bit quantized CNNs. However, this early work leveraged full-precision
float32 training and only quantized for testing. We explore the impact of
quantization-aware training and/or other quantization-based training strategies
on quantized robustness and performance of GHN predicted parameters for
low-precision CNNs. We show that quantization-aware training can significantly
improve quantized accuracy for GHN predicted parameters of 4-bit quantized CNNs
and even lead to greater-than-random accuracy for 2-bit quantized CNNs. These
promising results open the door for future explorations such as investigating
the use of GHN predicted parameters as initialization for further quantized
training of individual CNNs, further exploration of ""extreme bitwidth""
quantization, and mixed precision quantization schemes.",http://arxiv.org/pdf/2309.13773v1
2309.13746v1,cs.RO,Deep Learning-Based Connector Detection for Robotized Assembly of Automotive Wire Harnesses,2023-09-24 20:28:35+00:00,"The shift towards electrification and autonomous driving in the automotive
industry results in more and more automotive wire harnesses being installed in
modern automobiles, which stresses the great significance of guaranteeing the
quality of automotive wire harness assembly. The mating of connectors is
essential in the final assembly of automotive wire harnesses due to the
importance of connectors on wire harness connection and signal transmission.
However, the current manual operation of mating connectors leads to severe
problems regarding assembly quality and ergonomics, where the robotized
assembly has been considered, and different vision-based solutions have been
proposed to facilitate a better perception of the robot control system on
connectors. Nonetheless, there has been a lack of deep learning-based solutions
for detecting automotive wire harness connectors in previous literature. This
paper presents a deep learning-based connector detection for robotized
automotive wire harness assembly. A dataset of twenty automotive wire harness
connectors was created to train and evaluate a two-stage and a one-stage object
detection model, respectively. The experiment results indicate the
effectiveness of deep learning-based connector detection for automotive wire
harness assembly but are limited by the design of the exteriors of connectors.",http://arxiv.org/pdf/2309.13746v1
2309.13745v1,cs.RO,Computer Vision Technology for Robotized Wire Harness Assembly,2023-09-24 20:28:19+00:00,"Wire harnesses are essential hardware for electronic systems in modern
automotive vehicles. With a shift in the automotive industry towards
electrification and autonomous driving, more and more automotive electronics
are responsible for energy transmission and safety-critical functions such as
maneuvering, driver assistance, and safety system. This paradigm shift places
more demand on automotive wiring harnesses from the safety perspective and
stresses the greater importance of high-quality wire harness assembly in
vehicles. However, most of the current operations of wire harness assembly are
still performed manually by skilled workers, and some of the manual processes
are problematic from different perspectives, such as quality control and
ergonomics. There is also a persistent demand in the industry to increase
competitiveness and gain market share. Hence, assuring assembly quality while
improving ergonomics and optimizing labor costs is desired. Robotized assembly,
accomplished by robots or in human-robot collaboration, is a key enabler for
fulfilling the increasingly demanding quality and safety as it enables more
replicable, transparent, and comprehensible processes than completely manual
operations. However, robotized assembly of wire harnesses is challenging in
real environments due to the flexibility of the deformable objects, though many
preliminary automation solutions have been proposed under simplified industrial
configurations. Previous research efforts have proposed the use of computer
vision technology to facilitate robotized automation of wire harness assembly,
enabling the robots to better perceive and manipulate the flexible wire
harness. This article presents an overview on computer vision technology
proposed for robotized wire harness assembly and derives research gaps that
require further study to facilitate a more practical robotized assembly of wire
harness.",http://arxiv.org/pdf/2309.13745v1
2309.13744v1,cs.CV,A Systematic Literature Review of Computer Vision Applications in Robotized Wire Harness Assembly,2023-09-24 20:28:01+00:00,"This article presents a systematic literature review on computer vision
applications that have been proposed for robotized wire harness assembly,
derives challenges from existing studies, and identifies opportunities for
future research to promote a more practical robotized assembly of wire
harnesses.",http://arxiv.org/pdf/2309.13744v1
2309.13734v1,cs.CL,Use of Large Language Models for Stance Classification,2023-09-24 19:36:17+00:00,"Stance detection, the task of predicting an author's viewpoint towards a
subject of interest, has long been a focal point of research. Current stance
detection methods predominantly rely on manual annotation of sentences,
followed by training a supervised machine learning model. This manual
annotation process, however, imposes limitations on the model's ability to
fully comprehend the stances in the sentence and hampers its potential to
generalize across different contexts. In this study, we investigate the use of
Large Language Models (LLMs) for the task of stance classification, with an
absolute minimum use of human labels. We scrutinize four distinct types of
prompting schemes combined with LLMs, comparing their accuracies with manual
stance determination. Our study reveals that while LLMs can match or sometimes
even exceed the benchmark results in each dataset, their overall accuracy is
not definitively better than what can be produced by supervised models. This
suggests potential areas for improvement in the stance classification for LLMs.
The application of LLMs, however, opens up promising avenues for unsupervised
stance detection, thereby curtailing the need for manual collection and
annotation of stances. This not only streamlines the process but also paves the
way for expanding stance detection capabilities across languages. Through this
paper, we shed light on the stance classification abilities of LLMs, thereby
contributing valuable insights that can guide future advancements in this
domain.",http://arxiv.org/pdf/2309.13734v1
2309.13731v1,cs.CL,Arabic Sentiment Analysis with Noisy Deep Explainable Model,2023-09-24 19:26:53+00:00,"Sentiment Analysis (SA) is an indispensable task for many real-world
applications. Compared to limited resourced languages (i.e., Arabic, Bengali),
most of the research on SA are conducted for high resourced languages (i.e.,
English, Chinese). Moreover, the reasons behind any prediction of the Arabic
sentiment analysis methods exploiting advanced artificial intelligence
(AI)-based approaches are like black-box - quite difficult to understand. This
paper proposes an explainable sentiment classification framework for the Arabic
language by introducing a noise layer on Bi-Directional Long Short-Term Memory
(BiLSTM) and Convolutional Neural Networks (CNN)-BiLSTM models that overcome
over-fitting problem. The proposed framework can explain specific predictions
by training a local surrogate explainable model to understand why a particular
sentiment (positive or negative) is being predicted. We carried out experiments
on public benchmark Arabic SA datasets. The results concluded that adding noise
layers improves the performance in sentiment analysis for the Arabic language
by reducing overfitting and our method outperformed some known state-of-the-art
methods. In addition, the introduced explainability with noise layer could make
the model more transparent and accountable and hence help adopting AI-enabled
system in practice.",http://arxiv.org/pdf/2309.13731v1
2309.13707v1,cs.RO,ORLA*: Mobile Manipulator-Based Object Rearrangement with Lazy A*,2023-09-24 17:40:19+00:00,"Effectively performing object rearrangement is an essential skill for mobile
manipulators, e.g., setting up a dinner table or organizing a desk. A key
challenge in such problems is deciding an appropriate manipulation order for
objects to effectively untangle dependencies between objects while considering
the necessary motions for realizing the manipulations (e.g., pick and place).
To our knowledge, computing time-optimal multi-object rearrangement solutions
for mobile manipulators remains a largely untapped research direction. In this
research, we propose ORLA*, which leverages delayed (lazy) evaluation in
searching for a high-quality object pick and place sequence that considers both
end-effector and mobile robot base travel. ORLA* also supports multi-layered
rearrangement tasks considering pile stability using machine learning.
Employing an optimal solver for finding temporary locations for displacing
objects, ORLA* can achieve global optimality. Through extensive simulation and
ablation study, we confirm the effectiveness of ORLA* delivering quality
solutions for challenging rearrangement instances. Supplementary materials are
available at: https://gaokai15.github.io/ORLA-Star/",http://arxiv.org/pdf/2309.13707v1
2309.13705v1,cs.LG,A Neural-Guided Dynamic Symbolic Network for Exploring Mathematical Expressions from Data,2023-09-24 17:37:45+00:00,"Symbolic regression (SR) is a powerful technique for discovering the
underlying mathematical expressions from observed data. Inspired by the success
of deep learning, recent efforts have focused on two categories for SR methods.
One is using a neural network or genetic programming to search the expression
tree directly. Although this has shown promising results, the large search
space poses difficulties in learning constant factors and processing
high-dimensional problems. Another approach is leveraging a transformer-based
model training on synthetic data and offers advantages in inference speed.
However, this method is limited to fixed small numbers of dimensions and may
encounter inference problems when given data is out-of-distribution compared to
the synthetic data. In this work, we propose DySymNet, a novel neural-guided
Dynamic Symbolic Network for SR. Instead of searching for expressions within a
large search space, we explore DySymNet with various structures and optimize
them to identify expressions that better-fitting the data. With a topology
structure like neural networks, DySymNet not only tackles the challenge of
high-dimensional problems but also proves effective in optimizing constants.
Based on extensive numerical experiments using low-dimensional public standard
benchmarks and the well-known SRBench with more variables, our method achieves
state-of-the-art performance in terms of fitting accuracy and robustness to
noise.",http://arxiv.org/pdf/2309.13705v1
2309.13702v1,cs.CL,Skill Check: Some Considerations on the Evaluation of Gamemastering Models for Role-playing Games,2023-09-24 17:19:36+00:00,"In role-playing games a Game Master (GM) is the player in charge of the game,
who must design the challenges the players face and narrate the outcomes of
their actions. In this work we discuss some challenges to model GMs from an
Interactive Storytelling and Natural Language Processing perspective. Following
those challenges we propose three test categories to evaluate such dialogue
systems, and we use them to test ChatGPT, Bard and OpenAssistant as
out-of-the-box GMs.",http://arxiv.org/pdf/2309.13702v1
2309.13701v1,cs.CL,ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning,2023-09-24 17:15:58+00:00,"From grading papers to summarizing medical documents, large language models
(LLMs) are evermore used for evaluation of text generated by humans and AI
alike. However, despite their extensive utility, LLMs exhibit distinct failure
modes, necessitating a thorough audit and improvement of their text evaluation
capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large
Language Models Understanding and Reasoning Errors. ALLURE involves comparing
LLM-generated evaluations with annotated data, and iteratively incorporating
instances of significant deviation into the evaluator, which leverages
in-context learning (ICL) to enhance and improve robust evaluation of text by
LLMs. Through this iterative process, we aim to refine the performance of the
evaluator LLM, ultimately reducing the reliance on human annotators in the
evaluation process. We anticipate ALLURE to serve diverse applications of LLMs
in various domains related to evaluation of textual data and productivity in
these fields.",http://arxiv.org/pdf/2309.13701v1
2309.13696v1,q-fin.PM,Performance Evaluation of Equal-Weight Portfolio and Optimum Risk Portfolio on Indian Stocks,2023-09-24 17:06:58+00:00,"Designing an optimum portfolio for allocating suitable weights to its
constituent assets so that the return and risk associated with the portfolio
are optimized is a computationally hard problem. The seminal work of Markowitz
that attempted to solve the problem by estimating the future returns of the
stocks is found to perform sub-optimally on real-world stock market data. This
is because the estimation task becomes extremely challenging due to the
stochastic and volatile nature of stock prices. This work illustrates three
approaches to portfolio design minimizing the risk, optimizing the risk, and
assigning equal weights to the stocks of a portfolio. Thirteen critical sectors
listed on the National Stock Exchange (NSE) of India are first chosen. Three
portfolios are designed following the above approaches choosing the top ten
stocks from each sector based on their free-float market capitalization. The
portfolios are designed using the historical prices of the stocks from Jan 1,
2017, to Dec 31, 2022. The portfolios are evaluated on the stock price data
from Jan 1, 2022, to Dec 31, 2022. The performances of the portfolios are
compared, and the portfolio yielding the higher return for each sector is
identified.",http://arxiv.org/pdf/2309.13696v1
2309.13672v1,cs.CV,Deep Reinforcement Learning for Image-to-Image Translation,2023-09-24 15:40:40+00:00,"Most existing Image-to-Image Translation (I2IT) methods generate images in a
single run of a deep learning (DL) model. However, designing such a single-step
model is always challenging, requiring a huge number of parameters and easily
falling into bad global minimums and overfitting. In this work, we reformulate
I2IT as a step-wise decision-making problem via deep reinforcement learning
(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The
key feature in the RL-I2IT framework is to decompose a monolithic learning
process into small steps with a lightweight model to progressively transform a
source image successively to a target image. Considering that it is challenging
to handle high dimensional continuous state and action spaces in the
conventional RL framework, we introduce meta policy with a new concept Plan to
the standard Actor-Critic model, which is of a lower dimension than the
original image and can facilitate the actor to generate a tractable high
dimensional action. In the RL-I2IT framework, we also employ a task-specific
auxiliary learning strategy to stabilize the training process and improve the
performance of the corresponding task. Experiments on several I2IT tasks
demonstrate the effectiveness and robustness of the proposed method when facing
high-dimensional continuous action space problems.",http://arxiv.org/pdf/2309.13672v1
2309.13664v1,eess.AS,VoiceLDM: Text-to-Speech with Environmental Context,2023-09-24 15:20:59+00:00,"This paper presents VoiceLDM, a model designed to produce audio that
accurately follows two distinct natural language text prompts: the description
prompt and the content prompt. The former provides information about the
overall environmental context of the audio, while the latter conveys the
linguistic content. To achieve this, we adopt a text-to-audio (TTA) model based
on latent diffusion models and extend its functionality to incorporate an
additional content prompt as a conditional input. By utilizing pretrained
contrastive language-audio pretraining (CLAP) and Whisper, VoiceLDM is trained
on large amounts of real-world audio without manual annotations or
transcriptions. Additionally, we employ dual classifier-free guidance to
further enhance the controllability of VoiceLDM. Experimental results
demonstrate that VoiceLDM is capable of generating plausible audio that aligns
well with both input conditions, even surpassing the speech intelligibility of
the ground truth audio on the AudioCaps test set. Furthermore, we explore the
text-to-speech (TTS) and zero-shot text-to-audio capabilities of VoiceLDM and
show that it achieves competitive results. Demos and code are available at
https://voiceldm.github.io.",http://arxiv.org/pdf/2309.13664v1
2309.13638v1,cs.CL,Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve,2023-09-24 13:35:28+00:00,"The widespread adoption of large language models (LLMs) makes it important to
recognize their strengths and limitations. We argue that in order to develop a
holistic understanding of these systems we need to consider the problem that
they were trained to solve: next-word prediction over Internet text. By
recognizing the pressures that this task exerts we can make predictions about
the strategies that LLMs will adopt, allowing us to reason about when they will
succeed or fail. This approach - which we call the teleological approach -
leads us to identify three factors that we hypothesize will influence LLM
accuracy: the probability of the task to be performed, the probability of the
target output, and the probability of the provided input. We predict that LLMs
will achieve higher accuracy when these probabilities are high than when they
are low - even in deterministic settings where probability should not matter.
To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven
tasks, and we find robust evidence that LLMs are influenced by probability in
the ways that we have hypothesized. In many cases, the experiments reveal
surprising failure modes. For instance, GPT-4's accuracy at decoding a simple
cipher is 51% when the output is a high-probability word sequence but only 13%
when it is low-probability. These results show that AI practitioners should be
careful about using LLMs in low-probability situations. More broadly, we
conclude that we should not evaluate LLMs as if they are humans but should
instead treat them as a distinct type of system - one that has been shaped by
its own particular set of pressures.",http://arxiv.org/pdf/2309.13638v1
2309.13636v1,cs.LG,Development of an intelligent system for the detection of corona virus using artificial neural network,2023-09-24 13:30:50+00:00,"This paper presents the development of an intelligent system for the
detection of coronavirus using artificial neural network. This was done after
series of literature review which indicated that high fever accounts for 87.9%
of the COVID-19 symptoms. 683 temperature data of COVID-19 patients at >= 38C^o
were collected from Colliery hospital Enugu, Nigeria and used to train an
artificial neural network detective model for the detection of COVID-19. The
reference model generated was used converted into Verilog codes using Hardware
Description Language (HDL) and then burn into a Field Programming Gate Array
(FPGA) controller using FPGA tool in Matlab. The performance of the model when
evaluated using confusion matrix, regression and means square error (MSE)
showed that the regression value is 0.967; the accuracy is 97% and then MSE is
0.00100Mu. These results all implied that the new detection system for is
reliable and very effective for the detection of COVID-19.",http://arxiv.org/pdf/2309.13636v1
2309.13635v1,cs.RO,PanopticNDT: Efficient and Robust Panoptic Mapping,2023-09-24 13:21:33+00:00,"As the application scenarios of mobile robots are getting more complex and
challenging, scene understanding becomes increasingly crucial. A mobile robot
that is supposed to operate autonomously in indoor environments must have
precise knowledge about what objects are present, where they are, what their
spatial extent is, and how they can be reached; i.e., information about free
space is also crucial. Panoptic mapping is a powerful instrument providing such
information. However, building 3D panoptic maps with high spatial resolution is
challenging on mobile robots, given their limited computing capabilities. In
this paper, we propose PanopticNDT - an efficient and robust panoptic mapping
approach based on occupancy normal distribution transform (NDT) mapping. We
evaluate our approach on the publicly available datasets Hypersim and
ScanNetV2. The results reveal that our approach can represent panoptic
information at a higher level of detail than other state-of-the-art approaches
while enabling real-time panoptic mapping on mobile robots. Finally, we prove
the real-world applicability of PanopticNDT with qualitative results in a
domestic application.",http://arxiv.org/pdf/2309.13635v1
2309.13633v1,cs.HC,EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria,2023-09-24 13:19:38+00:00,"By simply composing prompts, developers can prototype novel generative
applications with Large Language Models (LLMs). To refine prototypes into
products, however, developers must iteratively revise prompts by evaluating
outputs to diagnose weaknesses. Formative interviews (N=8) revealed that
developers invest significant effort in manually evaluating outputs as they
assess context-specific and subjective criteria. We present EvalLM, an
interactive system for iteratively refining prompts by evaluating multiple
outputs on user-defined criteria. By describing criteria in natural language,
users can employ the system's LLM-based evaluator to get an overview of where
prompts excel or fail, and improve these based on the evaluator's feedback. A
comparative study (N=12) showed that EvalLM, when compared to manual
evaluation, helped participants compose more diverse criteria, examine twice as
many outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond
prompts, our work can be extended to augment model evaluation and alignment in
specific application contexts.",http://arxiv.org/pdf/2309.13633v1
2309.13629v1,astro-ph.IM,Periodic Variable Star Classification with Deep Learning: Handling Data Imbalance in an Ensemble Augmentation Way,2023-09-24 13:08:32+00:00,"Time-domain astronomy is progressing rapidly with the ongoing and upcoming
large-scale photometric sky surveys led by the Vera C. Rubin Observatory
project (LSST). Billions of variable sources call for better automatic
classification algorithms for light curves. Among them, periodic variable stars
are frequently studied. Different categories of periodic variable stars have a
high degree of class imbalance and pose a challenge to algorithms including
deep learning methods. We design two kinds of architectures of neural networks
for the classification of periodic variable stars in the Catalina Survey's Data
Release 2: a multi-input recurrent neural network (RNN) and a compound network
combing the RNN and the convolutional neural network (CNN). To deal with class
imbalance, we apply Gaussian Process to generate synthetic light curves with
artificial uncertainties for data augmentation. For better performance, we
organize the augmentation and training process in a ""bagging-like"" ensemble
learning scheme. The experimental results show that the better approach is the
compound network combing RNN and CNN, which reaches the best result of 86.2% on
the overall balanced accuracy and 0.75 on the macro F1 score. We develop the
ensemble augmentation method to solve the data imbalance when classifying
variable stars and prove the effectiveness of combining different
representations of light curves in a single model. The proposed methods would
help build better classification algorithms of periodic time series data for
future sky surveys (e.g., LSST).",http://arxiv.org/pdf/2309.13629v1
2309.13625v1,cs.CV,GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph,2023-09-24 12:56:40+00:00,"Adapter-style efficient transfer learning (ETL) has shown excellent
performance in the tuning of vision-language models (VLMs) under the low-data
regime, where only a few additional parameters are introduced to excavate the
task-specific knowledge based on the general and powerful representation of
VLMs. However, most adapter-style works face two limitations: (i) modeling
task-specific knowledge with a single modality only; and (ii) overlooking the
exploitation of the inter-class relationships in downstream tasks, thereby
leading to sub-optimal solutions. To mitigate that, we propose an effective
adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual
adapter by explicitly modeling the dual-modality structure knowledge (i.e., the
correlation of different semantics/classes in textual and visual modalities)
with a dual knowledge graph. In particular, the dual knowledge graph is
established with two sub-graphs, i.e., a textual knowledge sub-graph, and a
visual knowledge sub-graph, where the nodes and edges represent the
semantics/classes and their correlations in two modalities, respectively. This
enables the textual feature of each prompt to leverage the task-specific
structure knowledge from both textual and visual modalities, yielding a more
effective classifier for downstream tasks. Extensive experimental results on 11
benchmark datasets reveal that our GraphAdapter significantly outperforms
previous adapter-based methods. The code will be released at
https://github.com/lixinustc/GraphAdapter",http://arxiv.org/pdf/2309.13625v1
2309.13620v1,cs.CV,PRIS: Practical robust invertible network for image steganography,2023-09-24 12:29:13+00:00,"Image steganography is a technique of hiding secret information inside
another image, so that the secret is not visible to human eyes and can be
recovered when needed. Most of the existing image steganography methods have
low hiding robustness when the container images affected by distortion. Such as
Gaussian noise and lossy compression. This paper proposed PRIS to improve the
robustness of image steganography, it based on invertible neural networks, and
put two enhance modules before and after the extraction process with a 3-step
training strategy. Moreover, rounding error is considered which is always
ignored by existing methods, but actually it is unavoidable in practical. A
gradient approximation function (GAF) is also proposed to overcome the
undifferentiable issue of rounding distortion. Experimental results show that
our PRIS outperforms the state-of-the-art robust image steganography method in
both robustness and practicability. Codes are available at
https://github.com/yanghangAI/PRIS, demonstration of our model in practical at
http://yanghang.site/hide/.",http://arxiv.org/pdf/2309.13620v1
2309.13614v1,cs.RO,Boosting Offline Reinforcement Learning for Autonomous Driving with Hierarchical Latent Skills,2023-09-24 11:51:17+00:00,"Learning-based vehicle planning is receiving increasing attention with the
emergence of diverse driving simulators and large-scale driving datasets. While
offline reinforcement learning (RL) is well suited for these safety-critical
tasks, it still struggles to plan over extended periods. In this work, we
present a skill-based framework that enhances offline RL to overcome the
long-horizon vehicle planning challenge. Specifically, we design a variational
autoencoder (VAE) to learn skills from offline demonstrations. To mitigate
posterior collapse of common VAEs, we introduce a two-branch sequence encoder
to capture both discrete options and continuous variations of the complex
driving skills. The final policy treats learned skills as actions and can be
trained by any off-the-shelf offline RL algorithms. This facilitates a shift in
focus from per-step actions to temporally extended skills, thereby enabling
long-term reasoning into the future. Extensive results on CARLA prove that our
model consistently outperforms strong baselines at both training and new
scenarios. Additional visualizations and experiments demonstrate the
interpretability and transferability of extracted skills.",http://arxiv.org/pdf/2309.13614v1
2309.13612v1,cs.CR,Digital Twins and the Future of their Use Enabling Shift Left and Shift Right Cybersecurity Operations,2023-09-24 11:20:58+00:00,"Digital Twins (DTs), optimize operations and monitor performance in Smart
Critical Systems (SCS) domains like smart grids and manufacturing. DT-based
cybersecurity solutions are in their infancy, lacking a unified strategy to
overcome challenges spanning next three to five decades. These challenges
include reliable data accessibility from Cyber-Physical Systems (CPS),
operating in unpredictable environments. Reliable data sources are pivotal for
intelligent cybersecurity operations aided with underlying modeling
capabilities across the SCS lifecycle, necessitating a DT. To address these
challenges, we propose Security Digital Twins (SDTs) collecting realtime data
from CPS, requiring the Shift Left and Shift Right (SLSR) design paradigm for
SDT to implement both design time and runtime cybersecurity operations.
Incorporating virtual CPS components (VC) in Cloud/Edge, data fusion to SDT
models is enabled with high reliability, providing threat insights and
enhancing cyber resilience. VC-enabled SDT ensures accurate data feeds for
security monitoring for both design and runtime. This design paradigm shift
propagates innovative SDT modeling and analytics for securing future critical
systems. This vision paper outlines intelligent SDT design through innovative
techniques, exploring hybrid intelligence with data-driven and rule-based
semantic SDT models. Various operational use cases are discussed for securing
smart critical systems through underlying modeling and analytics capabilities.",http://arxiv.org/pdf/2309.13612v1
2309.13607v1,cs.CV,MM-NeRF: Multimodal-Guided 3D Multi-Style Transfer of Neural Radiance Field,2023-09-24 11:04:50+00:00,"3D style transfer aims to render stylized novel views of 3D scenes with the
specified style, which requires high-quality rendering and keeping multi-view
consistency. Benefiting from the ability of 3D representation from Neural
Radiance Field (NeRF), existing methods learn the stylized NeRF by giving a
reference style from an image. However, they suffer the challenges of
high-quality stylization with texture details for multi-style transfer and
stylization with multimodal guidance. In this paper, we reveal that the same
objects in 3D scenes show various states (color tone, details, etc.) from
different views after stylization since previous methods optimized by
single-view image-based style loss functions, leading NeRF to tend to smooth
texture details, further resulting in low-quality rendering. To tackle these
problems, we propose a novel Multimodal-guided 3D Multi-style transfer of NeRF,
termed MM-NeRF, which achieves high-quality 3D multi-style rendering with
texture details and can be driven by multimodal-style guidance. First, MM-NeRF
adopts a unified framework to project multimodal guidance into CLIP space and
extracts multimodal style features to guide the multi-style stylization. To
relieve the problem of lacking details, we propose a novel Multi-Head Learning
Scheme (MLS), in which each style head predicts the parameters of the color
head of NeRF. MLS decomposes the learning difficulty caused by the
inconsistency of multi-style transfer and improves the quality of stylization.
In addition, the MLS can generalize pre-trained MM-NeRF to any new styles by
adding heads with small training costs (a few minutes). Extensive experiments
on three real-world 3D scene datasets show that MM-NeRF achieves high-quality
3D multi-style stylization with multimodal guidance, keeps multi-view
consistency, and keeps semantic consistency of multimodal style guidance. Codes
will be released later.",http://arxiv.org/pdf/2309.13607v1
2309.13604v1,cs.CV,Distribution-Aware Continual Test Time Adaptation for Semantic Segmentation,2023-09-24 10:48:20+00:00,"Since autonomous driving systems usually face dynamic and ever-changing
environments, continual test-time adaptation (CTTA) has been proposed as a
strategy for transferring deployed models to continually changing target
domains. However, the pursuit of long-term adaptation often introduces
catastrophic forgetting and error accumulation problems, which impede the
practical implementation of CTTA in the real world. Recently, existing CTTA
methods mainly focus on utilizing a majority of parameters to fit target domain
knowledge through self-training. Unfortunately, these approaches often amplify
the challenge of error accumulation due to noisy pseudo-labels, and pose
practical limitations stemming from the heavy computational costs associated
with entire model updates. In this paper, we propose a distribution-aware
tuning (DAT) method to make the semantic segmentation CTTA efficient and
practical in real-world applications. DAT adaptively selects and updates two
small groups of trainable parameters based on data distribution during the
continual adaptation process, including domain-specific parameters (DSP) and
task-relevant parameters (TRP). Specifically, DSP exhibits sensitivity to
outputs with substantial distribution shifts, effectively mitigating the
problem of error accumulation. In contrast, TRP are allocated to positions that
are responsive to outputs with minor distribution shifts, which are fine-tuned
to avoid the catastrophic forgetting problem. In addition, since CTTA is a
temporal task, we introduce the Parameter Accumulation Update (PAU) strategy to
collect the updated DSP and TRP in target domain sequences. We conduct
extensive experiments on two widely-used semantic segmentation CTTA benchmarks,
achieving promising performance compared to previous state-of-the-art methods.",http://arxiv.org/pdf/2309.13604v1
2309.13599v1,cs.LG,From Cluster Assumption to Graph Convolution: Graph-based Semi-Supervised Learning Revisited,2023-09-24 10:10:21+00:00,"Graph-based semi-supervised learning (GSSL) has long been a hot research
topic. Traditional methods are generally shallow learners, based on the cluster
assumption. Recently, graph convolutional networks (GCNs) have become the
predominant techniques for their promising performance. In this paper, we
theoretically discuss the relationship between these two types of methods in a
unified optimization framework. One of the most intriguing findings is that,
unlike traditional ones, typical GCNs may not jointly consider the graph
structure and label information at each layer. Motivated by this, we further
propose three simple but powerful graph convolution methods. The first is a
supervised method OGC which guides the graph convolution process with labels.
The others are two unsupervised methods: GGC and its multi-scale version GGCM,
both aiming to preserve the graph structure information during the convolution
process. Finally, we conduct extensive experiments to show the effectiveness of
our methods.",http://arxiv.org/pdf/2309.13599v1
2309.13579v1,cs.CR,Seeing Is Not Always Believing: Invisible Collision Attack and Defence on Pre-Trained Models,2023-09-24 08:34:35+00:00,"Large-scale pre-trained models (PTMs) such as BERT and GPT have achieved
great success in diverse fields. The typical paradigm is to pre-train a big
deep learning model on large-scale data sets, and then fine-tune the model on
small task-specific data sets for downstream tasks. Although PTMs have rapidly
progressed with wide real-world applications, they also pose significant risks
of potential attacks. Existing backdoor attacks or data poisoning methods often
build up the assumption that the attacker invades the computers of victims or
accesses the target data, which is challenging in real-world scenarios. In this
paper, we propose a novel framework for an invisible attack on PTMs with
enhanced MD5 collision. The key idea is to generate two equal-size models with
the same MD5 checksum by leveraging the MD5 chosen-prefix collision.
Afterwards, the two ``same"" models will be deployed on public websites to
induce victims to download the poisoned model. Unlike conventional attacks on
deep learning models, this new attack is flexible, covert, and
model-independent. Additionally, we propose a simple defensive strategy for
recognizing the MD5 chosen-prefix collision and provide a theoretical
justification for its feasibility. We extensively validate the effectiveness
and stealthiness of our proposed attack and defensive method on different
models and data sets.",http://arxiv.org/pdf/2309.13579v1
2309.13575v2,cs.LG,Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization,2023-09-24 08:04:28+00:00,"Weight-sharing quantization has emerged as a technique to reduce energy
expenditure during inference in large neural networks by constraining their
weights to a limited set of values. However, existing methods for
weight-sharing quantization often make assumptions about the treatment of
weights based on value alone that neglect the unique role weight position
plays. This paper proposes a probabilistic framework based on Bayesian neural
networks (BNNs) and a variational relaxation to identify which weights can be
moved to which cluster centre and to what degree based on their individual
position-specific learned uncertainty distributions. We introduce a new
initialisation setting and a regularisation term which allow for the training
of BNNs under complex dataset-model combinations. By leveraging the flexibility
of weight values captured through a probability distribution, we enhance noise
resilience and downstream compressibility. Our iterative clustering procedure
demonstrates superior compressibility and higher accuracy compared to
state-of-the-art methods on both ResNet models and the more complex
transformer-based architectures. In particular, our method outperforms the
state-of-the-art quantization method top-1 accuracy by 1.6% on ImageNet using
DeiT-Tiny, with its 5 million+ weights now represented by only 296 unique
values.",http://arxiv.org/pdf/2309.13575v2
2309.13562v1,cs.CL,Keeping in Time: Adding Temporal Context to Sentiment Analysis Models,2023-09-24 06:38:21+00:00,"This paper presents a state-of-the-art solution to the LongEval CLEF 2023 Lab
Task 2: LongEval-Classification. The goal of this task is to improve and
preserve the performance of sentiment analysis models across shorter and longer
time periods. Our framework feeds date-prefixed textual inputs to a pre-trained
language model, where the timestamp is included in the text. We show
date-prefixed samples better conditions model outputs on the temporal context
of the respective texts. Moreover, we further boost performance by performing
self-labeling on unlabeled data to train a student model. We augment the
self-labeling process using a novel augmentation strategy leveraging the
date-prefixed formatting of our samples. We demonstrate concrete performance
gains on the LongEval-Classification evaluation set over non-augmented
self-labeling. Our framework achieves a 2nd place ranking with an overall score
of 0.6923 and reports the best Relative Performance Drop (RPD) of -0.0656 over
the short evaluation set.",http://arxiv.org/pdf/2309.13562v1
2309.13561v1,cs.CL,Cordyceps@LT-EDI: Patching Language-Specific Homophobia/Transphobia Classifiers with a Multilingual Understanding,2023-09-24 06:37:54+00:00,"Detecting transphobia, homophobia, and various other forms of hate speech is
difficult. Signals can vary depending on factors such as language, culture,
geographical region, and the particular online platform. Here, we present a
joint multilingual (M-L) and language-specific (L-S) approach to homophobia and
transphobic hate speech detection (HSD). M-L models are needed to catch words,
phrases, and concepts that are less common or missing in a particular language
and subsequently overlooked by L-S models. Nonetheless, L-S models are better
situated to understand the cultural and linguistic context of the users who
typically write in a particular language. Here we construct a simple and
successful way to merge the M-L and L-S approaches through simple weight
interpolation in such a way that is interpretable and data-driven. We
demonstrate our system on task A of the 'Shared Task on Homophobia/Transphobia
Detection in social media comments' dataset for homophobia and transphobic HSD.
Our system achieves the best results in three of five languages and achieves a
0.997 macro average F1-score on Malayalam texts.",http://arxiv.org/pdf/2309.13561v1
2309.13550v1,cs.CV,Decoding Radiologists Intense Focus for Accurate CXR Diagnoses: A Controllable and Interpretable AI System,2023-09-24 04:48:44+00:00,"In the field of chest X-ray (CXR) diagnosis, existing works often focus
solely on determining where a radiologist looks, typically through tasks such
as detection, segmentation, or classification. However, these approaches are
often designed as black-box models, lacking interpretability. In this paper, we
introduce a novel and unified controllable interpretable pipeline for decoding
the intense focus of radiologists in CXR diagnosis. Our approach addresses
three key questions: where a radiologist looks, how long they focus on specific
areas, and what findings they diagnose. By capturing the intensity of the
radiologist's gaze, we provide a unified solution that offers insights into the
cognitive process underlying radiological interpretation. Unlike current
methods that rely on black-box machine learning models, which can be prone to
extracting erroneous information from the entire input image during the
diagnosis process, we tackle this issue by effectively masking out irrelevant
information. Our approach leverages a vision-language model, allowing for
precise control over the interpretation process while ensuring the exclusion of
irrelevant features. To train our model, we utilize an eye gaze dataset to
extract anatomical gaze information and generate ground truth heatmaps. Through
extensive experimentation, we demonstrate the efficacy of our method. We
showcase that the attention heatmaps, designed to mimic radiologists' focus,
encode sufficient and relevant information, enabling accurate classification
tasks using only a portion of CXR.",http://arxiv.org/pdf/2309.13550v1
2309.13544v1,cs.IR,Related Rhythms: Recommendation System To Discover Music You May Like,2023-09-24 04:18:40+00:00,"Machine Learning models are being utilized extensively to drive recommender
systems, which is a widely explored topic today. This is especially true of the
music industry, where we are witnessing a surge in growth. Besides a large
chunk of active users, these systems are fueled by massive amounts of data.
These large-scale systems yield applications that aim to provide a better user
experience and to keep customers actively engaged. In this paper, a distributed
Machine Learning (ML) pipeline is delineated, which is capable of taking a
subset of songs as input and producing a new subset of songs identified as
being similar to the inputted subset. The publicly accessible Million Songs
Dataset (MSD) enables researchers to develop and explore reasonably efficient
systems for audio track analysis and recommendations, without having to access
a commercialized music platform. The objective of the proposed application is
to leverage an ML system trained to optimally recommend songs that a user might
like.",http://arxiv.org/pdf/2309.13544v1
2309.13542v1,cs.IT,Integrated Sensing and Communications for IoT: Synergies with Key 6G Technology Enablers,2023-09-24 03:59:08+00:00,"The Internet of Things (IoT) and wireless generations have been evolving
simultaneously for the past few decades. Built upon wireless communication and
sensing technologies, IoT networks are usually evaluated based on metrics that
measure the device ability to sense information and effectively share it with
the network, which makes Integrated Sensing and Communication (ISAC) a pivotal
candidate for the sixth-generation (6G) IoT standards. This paper reveals
several innovative aspects of ISAC from an IoT perspective in 6G, empowering
various modern IoT use cases and key technology enablers. Moreover, we address
the challenges and future potential of ISAC-enabled IoT, including synergies
with Reconfigurable Intelligent Surfaces (RIS), Artificial Intelligence (AI),
and key updates of ISAC-IoT in 6G standardization. Furthermore, several
evolutionary concepts are introduced to open future research in 6G ISAC-IoT,
including the interplay with Non-Terrestrial Networks (NTN) and Orthogonal
Time-Frequency Space (OTFS) modulation.",http://arxiv.org/pdf/2309.13542v1
2309.13537v1,eess.AS,Speech enhancement with frequency domain auto-regressive modeling,2023-09-24 03:25:51+00:00,"Speech applications in far-field real world settings often deal with signals
that are corrupted by reverberation. The task of dereverberation constitutes an
important step to improve the audible quality and to reduce the error rates in
applications like automatic speech recognition (ASR). We propose a unified
framework of speech dereverberation for improving the speech quality and the
ASR performance using the approach of envelope-carrier decomposition provided
by an autoregressive (AR) model. The AR model is applied in the frequency
domain of the sub-band speech signals to separate the envelope and carrier
parts. A novel neural architecture based on dual path long short term memory
(DPLSTM) model is proposed, which jointly enhances the sub-band envelope and
carrier components. The dereverberated envelope-carrier signals are modulated
and the sub-band signals are synthesized to reconstruct the audio signal back.
The DPLSTM model for dereverberation of envelope and carrier components also
allows the joint learning of the network weights for the down stream ASR task.
In the ASR tasks on the REVERB challenge dataset as well as on the VOiCES
dataset, we illustrate that the joint learning of speech dereverberation
network and the E2E ASR model yields significant performance improvements over
the baseline ASR system trained on log-mel spectrogram as well as other
benchmarks for dereverberation (average relative improvements of 10-24% over
the baseline system). The speech quality improvements, evaluated using
subjective listening tests, further highlight the improved quality of the
reconstructed audio.",http://arxiv.org/pdf/2309.13537v1
2309.13532v1,cs.RO,Anisotropic body compliance facilitates robotic sidewinding in complex environments,2023-09-24 02:59:42+00:00,"Sidewinding, a locomotion strategy characterized by the coordination of
lateral and vertical body undulations, is frequently observed in rattlesnakes
and has been successfully reconstructed by limbless robotic systems for
effective movement across diverse terrestrial terrains. However, the
integration of compliant mechanisms into sidewinding limbless robots remains
less explored, posing challenges for navigation in complex, rheologically
diverse environments. Inspired by a notable control simplification via
mechanical intelligence in lateral undulation, which offloads feedback control
to passive body mechanics and interactions with the environment, we present an
innovative design of a mechanically intelligent limbless robot for sidewinding.
This robot features a decentralized bilateral cable actuation system that
resembles organismal muscle actuation mechanisms. We develop a feedforward
controller that incorporates programmable body compliance into the sidewinding
gait template. Our experimental results highlight the emergence of mechanical
intelligence when the robot is equipped with an appropriate level of body
compliance. This allows the robot to 1) locomote more energetically
efficiently, as evidenced by a reduced cost of transport, and 2) navigate
through terrain heterogeneities, all achieved in an open-loop manner, without
the need for environmental awareness.",http://arxiv.org/pdf/2309.13532v1
2309.13528v1,cs.LG,Iterative Reachability Estimation for Safe Reinforcement Learning,2023-09-24 02:36:42+00:00,"Ensuring safety is important for the practical deployment of reinforcement
learning (RL). Various challenges must be addressed, such as handling
stochasticity in the environments, providing rigorous guarantees of persistent
state-wise safety satisfaction, and avoiding overly conservative behaviors that
sacrifice performance. We propose a new framework, Reachability Estimation for
Safe Policy Optimization (RESPO), for safety-constrained RL in general
stochastic settings. In the feasible set where there exist violation-free
policies, we optimize for rewards while maintaining persistent safety. Outside
this feasible set, our optimization produces the safest behavior by
guaranteeing entrance into the feasible set whenever possible with the least
cumulative discounted violations. We introduce a class of algorithms using our
novel reachability estimation function to optimize in our proposed framework
and in similar frameworks such as those concurrently handling multiple hard and
soft constraints. We theoretically establish that our algorithms almost surely
converge to locally optimal policies of our safe optimization framework. We
evaluate the proposed methods on a diverse suite of safe RL environments from
Safety Gym, PyBullet, and MuJoCo, and show the benefits in improving both
reward performance and safety compared with state-of-the-art baselines.",http://arxiv.org/pdf/2309.13528v1
2309.13524v2,cs.CV,Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction,2023-09-24 02:10:25+00:00,"Reconstructing 3D clothed human avatars from single images is a challenging
task, especially when encountering complex poses and loose clothing. Current
methods exhibit limitations in performance, largely attributable to their
dependence on insufficient 2D image features and inconsistent query methods.
Owing to this, we present the Global-correlated 3D-decoupling Transformer for
clothed Avatar reconstruction (GTA), a novel transformer-based architecture
that reconstructs clothed human avatars from monocular images. Our approach
leverages transformer architectures by utilizing a Vision Transformer model as
an encoder for capturing global-correlated image features. Subsequently, our
innovative 3D-decoupling decoder employs cross-attention to decouple tri-plane
features, using learnable embeddings as queries for cross-plane generation. To
effectively enhance feature fusion with the tri-plane 3D feature and human body
prior, we propose a hybrid prior fusion strategy combining spatial and
prior-enhanced queries, leveraging the benefits of spatial localization and
human body prior knowledge. Comprehensive experiments on CAPE and THuman2.0
datasets illustrate that our method outperforms state-of-the-art approaches in
both geometry and texture reconstruction, exhibiting high robustness to
challenging poses and loose clothing, and producing higher-resolution textures.
Codes will be available at https://github.com/River-Zhang/GTA.",http://arxiv.org/pdf/2309.13524v2
2309.13512v1,cs.CV,Object Classification Model Using Ensemble Learning with Gray-Level Co-Occurrence Matrix and Histogram Extraction,2023-09-24 00:20:16+00:00,"In the field of object classification, identification based on object
variations is a challenge in itself. Variations include shape, size, color, and
texture, these can cause problems in recognizing and distinguishing objects
accurately. The purpose of this research is to develop a classification method
so that objects can be accurately identified. The proposed classification model
uses Voting and Combined Classifier, with Random Forest, K-NN, Decision Tree,
SVM, and Naive Bayes classification methods. The test results show that the
voting method and Combined Classifier obtain quite good results with each of
them, ensemble voting with an accuracy value of 92.4%, 78.6% precision, 95.2%
recall, and 86.1% F1-score. While the combined classifier with an accuracy
value of 99.3%, a precision of 97.6%, a recall of 100%, and a 98.8% F1-score.
Based on the test results, it can be concluded that the use of the Combined
Classifier and voting methods is proven to increase the accuracy value. The
contribution of this research increases the effectiveness of the Ensemble
Learning method, especially the voting ensemble method and the Combined
Classifier in increasing the accuracy of object classification in image
processing.",http://arxiv.org/pdf/2309.13512v1
2309.13508v1,cs.LG,Guided Cooperation in Hierarchical Reinforcement Learning via Model-based Rollout,2023-09-24 00:13:16+00:00,"Goal-conditioned hierarchical reinforcement learning (HRL) presents a
promising approach for enabling effective exploration in complex long-horizon
reinforcement learning (RL) tasks via temporal abstraction. Yet, most
goal-conditioned HRL algorithms focused on the subgoal discovery, regardless of
inter-level coupling. In essence, for hierarchical systems, the increased
inter-level communication and coordination can induce more stable and robust
policy improvement. Here, we present a goal-conditioned HRL framework with
Guided Cooperation via Model-based Rollout (GCMR), which estimates forward
dynamics to promote inter-level cooperation. The GCMR alleviates the
state-transition error within off-policy correction through a model-based
rollout, further improving the sample efficiency. Meanwhile, to avoid being
disrupted by these corrected but possibly unseen or faraway goals, lower-level
Q-function gradients are constrained using a gradient penalty with a
model-inferred upper bound, leading to a more stable behavioral policy.
Besides, we propose a one-step rollout-based planning to further facilitate
inter-level cooperation, where the higher-level Q-function is used to guide the
lower-level policy by estimating the value of future states so that global task
information is transmitted downwards to avoid local pitfalls. Experimental
results demonstrate that incorporating the proposed GCMR framework with ACLG, a
disentangled variant of HIGL, yields more stable and robust policy improvement
than baselines and substantially outperforms previous state-of-the-art (SOTA)
HRL algorithms in both hard-exploration problems and robotic control.",http://arxiv.org/pdf/2309.13508v1
2309.13500v1,cs.LG,Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy,2023-09-23 23:37:55+00:00,"As an emerging education strategy, learnersourcing offers the potential for
personalized learning content creation, but also grapples with the challenge of
predicting student performance due to inherent noise in student-generated data.
While graph-based methods excel in capturing dense learner-question
interactions, they falter in cold start scenarios, characterized by limited
interactions, as seen when questions lack substantial learner responses. In
response, we introduce an innovative strategy that synergizes the potential of
integrating Signed Graph Neural Networks (SGNNs) and Large Language Model (LLM)
embeddings. Our methodology employs a signed bipartite graph to comprehensively
model student answers, complemented by a contrastive learning framework that
enhances noise resilience. Furthermore, LLM's contribution lies in generating
foundational question embeddings, proving especially advantageous in addressing
cold start scenarios characterized by limited graph data interactions.
Validation across five real-world datasets sourced from the PeerWise platform
underscores our approach's effectiveness. Our method outperforms baselines,
showcasing enhanced predictive accuracy and robustness.",http://arxiv.org/pdf/2309.13500v1
2309.13498v1,cs.DC,Consensus Algorithms of Distributed Ledger Technology -- A Comprehensive Analysis,2023-09-23 23:32:11+00:00,"The most essential component of every Distributed Ledger Technology (DLT) is
the Consensus Algorithm (CA), which enables users to reach a consensus in a
decentralized and distributed manner. Numerous CA exist, but their viability
for particular applications varies, making their trade-offs a crucial factor to
consider when implementing DLT in a specific field. This article provided a
comprehensive analysis of the various consensus algorithms used in distributed
ledger technologies (DLT) and blockchain networks. We cover an extensive array
of thirty consensus algorithms. Eleven attributes including hardware
requirements, pre-trust level, tolerance level, and more, were used to generate
a series of comparison tables evaluating these consensus algorithms. In
addition, we discuss DLT classifications, the categories of certain consensus
algorithms, and provide examples of authentication-focused and
data-storage-focused DLTs. In addition, we analyze the pros and cons of
particular consensus algorithms, such as Nominated Proof of Stake (NPoS),
Bonded Proof of Stake (BPoS), and Avalanche. In conclusion, we discuss the
applicability of these consensus algorithms to various Cyber Physical System
(CPS) use cases, including supply chain management, intelligent transportation
systems, and smart healthcare.",http://arxiv.org/pdf/2309.13498v1
2309.13483v1,stat.ML,"Enhancing Prediction and Analysis of UK Road Traffic Accident Severity Using AI: Integration of Machine Learning, Econometric Techniques, and Time Series Forecasting in Public Health Research",2023-09-23 21:46:43+00:00,"This research investigates road traffic accident severity in the UK, using a
combination of machine learning, econometric, and statistical methods on
historical data. We employed various techniques, including correlation
analysis, regression models, GMM for error term issues, and time-series
forecasting with VAR and ARIMA models. Our approach outperforms naive
forecasting with an MASE of 0.800 and ME of -73.80. We also built a random
forest classifier with 73% precision, 78% recall, and a 73% F1-score.
Optimizing with H2O AutoML led to an XGBoost model with an RMSE of 0.176 and
MAE of 0.087. Factor Analysis identified key variables, and we used SHAP for
Explainable AI, highlighting influential factors like Driver_Home_Area_Type and
Road_Type. Our study enhances understanding of accident severity and offers
insights for evidence-based road safety policies.",http://arxiv.org/pdf/2309.13483v1
2309.13467v1,cs.CR,SUDS: Sanitizing Universal and Dependent Steganography,2023-09-23 19:39:44+00:00,"Steganography, or hiding messages in plain sight, is a form of information
hiding that is most commonly used for covert communication. As modern
steganographic mediums include images, text, audio, and video, this
communication method is being increasingly used by bad actors to propagate
malware, exfiltrate data, and discreetly communicate. Current protection
mechanisms rely upon steganalysis, or the detection of steganography, but these
approaches are dependent upon prior knowledge, such as steganographic
signatures from publicly available tools and statistical knowledge about known
hiding methods. These dependencies render steganalysis useless against new or
unique hiding methods, which are becoming increasingly common with the
application of deep learning models. To mitigate the shortcomings of
steganalysis, this work focuses on a deep learning sanitization technique
called SUDS that is not reliant upon knowledge of steganographic hiding
techniques and is able to sanitize universal and dependent steganography. SUDS
is tested using least significant bit method (LSB), dependent deep hiding
(DDH), and universal deep hiding (UDH). We demonstrate the capabilities and
limitations of SUDS by answering five research questions, including baseline
comparisons and an ablation study. Additionally, we apply SUDS to a real-world
scenario, where it is able to increase the resistance of a poisoned classifier
against attacks by 1375%.",http://arxiv.org/pdf/2309.13467v1
2309.13464v1,cs.HC,Personalised and Adjustable Interval Type-2 Fuzzy-Based PPG Quality Assessment for the Edge,2023-09-23 19:35:00+00:00,"Most of today's wearable technology provides seamless cardiac activity
monitoring. Specifically, the vast majority employ Photoplethysmography (PPG)
sensors to acquire blood volume pulse information, which is further analysed to
extract useful and physiologically related features. Nevertheless, PPG-based
signal reliability presents different challenges that strongly affect such data
processing. This is mainly related to the fact of PPG morphological wave
distortion due to motion artefacts, which can lead to erroneous interpretation
of the extracted cardiac-related features. On this basis, in this paper, we
propose a novel personalised and adjustable Interval Type-2 Fuzzy Logic System
(IT2FLS) for assessing the quality of PPG signals. The proposed system employs
a personalised approach to adapt the IT2FLS parameters to the unique
characteristics of each individual's PPG signals.Additionally, the system
provides adjustable levels of personalisation, allowing healthcare providers to
adjust the system to meet specific requirements for different applications. The
proposed system obtained up to 93.72\% for average accuracy during validation.
The presented system has the potential to enable ultra-low complexity and
real-time PPG quality assessment, improving the accuracy and reliability of
PPG-based health monitoring systems at the edge.",http://arxiv.org/pdf/2309.13464v1
2309.13459v1,stat.ML,A Model-Agnostic Graph Neural Network for Integrating Local and Global Information,2023-09-23 19:07:03+00:00,"Graph Neural Networks (GNNs) have achieved promising performance in a variety
of graph-focused tasks. Despite their success, existing GNNs suffer from two
significant limitations: a lack of interpretability in results due to their
black-box nature, and an inability to learn representations of varying orders.
To tackle these issues, we propose a novel Model-agnostic Graph Neural Network
(MaGNet) framework, which is able to sequentially integrate information of
various orders, extract knowledge from high-order neighbors, and provide
meaningful and interpretable results by identifying influential compact graph
structures. In particular, MaGNet consists of two components: an estimation
model for the latent representation of complex relationships under graph
topology, and an interpretation model that identifies influential nodes, edges,
and important node features. Theoretically, we establish the generalization
error bound for MaGNet via empirical Rademacher complexity, and showcase its
power to represent layer-wise neighborhood mixing. We conduct comprehensive
numerical studies using simulated data to demonstrate the superior performance
of MaGNet in comparison to several state-of-the-art alternatives. Furthermore,
we apply MaGNet to a real-world case study aimed at extracting task-critical
information from brain activity data, thereby highlighting its effectiveness in
advancing scientific research.",http://arxiv.org/pdf/2309.13459v1
2309.13445v1,cs.AR,AxOMaP: Designing FPGA-based Approximate Arithmetic Operators using Mathematical Programming,2023-09-23 18:23:54+00:00,"With the increasing application of machine learning (ML) algorithms in
embedded systems, there is a rising necessity to design low-cost computer
arithmetic for these resource-constrained systems. As a result, emerging models
of computation, such as approximate and stochastic computing, that leverage the
inherent error-resilience of such algorithms are being actively explored for
implementing ML inference on resource-constrained systems. Approximate
computing (AxC) aims to provide disproportionate gains in the power,
performance, and area (PPA) of an application by allowing some level of
reduction in its behavioral accuracy (BEHAV). Using approximate operators
(AxOs) for computer arithmetic forms one of the more prevalent methods of
implementing AxC. AxOs provide the additional scope for finer granularity of
optimization, compared to only precision scaling of computer arithmetic. To
this end, designing platform-specific and cost-efficient approximate operators
forms an important research goal. Recently, multiple works have reported using
AI/ML-based approaches for synthesizing novel FPGA-based AxOs. However, most of
such works limit usage of AI/ML to designing ML-based surrogate functions used
during iterative optimization processes. To this end, we propose a novel data
analysis-driven mathematical programming-based approach to synthesizing
approximate operators for FPGAs. Specifically, we formulate mixed integer
quadratically constrained programs based on the results of correlation analysis
of the characterization data and use the solutions to enable a more directed
search approach for evolutionary optimization algorithms. Compared to
traditional evolutionary algorithms-based optimization, we report up to 21%
improvement in the hypervolume, for joint optimization of PPA and BEHAV, in the
design of signed 8-bit multipliers.",http://arxiv.org/pdf/2309.13445v1
2309.13444v1,cs.CR,Moving Target Defense based Secured Network Slicing System in the O-RAN Architecture,2023-09-23 18:21:33+00:00,"The open radio access network (O-RAN) architecture's native virtualization
and embedded intelligence facilitate RAN slicing and enable comprehensive
end-to-end services in post-5G networks. However, any vulnerabilities could
harm security. Therefore, artificial intelligence (AI) and machine learning
(ML) security threats can even threaten O-RAN benefits. This paper proposes a
novel approach to estimating the optimal number of predefined VNFs for each
slice while addressing secure AI/ML methods for dynamic service admission
control and power minimization in the O-RAN architecture. We solve this problem
on two-time scales using mathematical methods for determining the predefined
number of VNFs on a large time scale and the proximal policy optimization
(PPO), a Deep Reinforcement Learning algorithm, for solving dynamic service
admission control and power minimization for different slices on a small-time
scale. To secure the ML system for O-RAN, we implement a moving target defense
(MTD) strategy to prevent poisoning attacks by adding uncertainty to the
system. Our experimental results show that the proposed PPO-based service
admission control approach achieves an admission rate above 80\% and that the
MTD strategy effectively strengthens the robustness of the PPO method against
adversarial attacks.",http://arxiv.org/pdf/2309.13444v1
2309.13442v1,cs.LG,How Do Drivers Behave at Roundabouts in a Mixed Traffic? A Case Study Using Machine Learning,2023-09-23 18:02:57+00:00,"Driving behavior is considered a unique driving habit of each driver and has
a significant impact on road safety. Classifying driving behavior and
introducing policies based on the results can reduce the severity of crashes on
the road. Roundabouts are particularly interesting because of the
interconnected interaction between different road users at the area of
roundabouts, which different driving behavior is hypothesized. This study
investigates driving behavior at roundabouts in a mixed traffic environment
using a data-driven unsupervised machine learning to classify driving behavior
at three roundabouts in Germany. We used a dataset of vehicle kinematics to a
group of different vehicles and vulnerable road users (VRUs) at roundabouts and
classified them into three categories (i.e., conservative, normal, and
aggressive). Results showed that most of the drivers proceeding through a
roundabout can be mostly classified into two driving styles: conservative and
normal because traffic speeds in roundabouts are relatively lower than in other
signalized and unsignalized intersections. Results also showed that about 77%
of drivers who interacted with pedestrians or cyclists were classified as
conservative drivers compared to about 42% of conservative drivers that did not
interact or about 51% from all drivers. It seems that drivers tend to behave
abnormally as they interact with VRUs at roundabouts, which increases the risk
of crashes when an intersection is multimodal. Results of this study could be
helpful in improving the safety of roads by allowing policymakers to determine
the effective and suitable safety countermeasures. Results will also be
beneficial for the Advanced Driver Assistance System (ADAS) as the technology
is being deployed in a mixed traffic environment.",http://arxiv.org/pdf/2309.13442v1
2309.13439v1,cs.LG,Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning,2023-09-23 17:42:13+00:00,"The success of contrastive learning is well known to be dependent on data
augmentation. Although the degree of data augmentations has been well
controlled by utilizing pre-defined techniques in some domains like vision,
time-series data augmentation is less explored and remains a challenging
problem due to the complexity of the data generation mechanism, such as the
intricate mechanism involved in the cardiovascular system. Moreover, there is
no widely recognized and general time-series augmentation method that can be
applied across different tasks. In this paper, we propose a novel data
augmentation method for quasi-periodic time-series tasks that aims to connect
intra-class samples together, and thereby find order in the latent space. Our
method builds upon the well-known mixup technique by incorporating a novel
approach that accounts for the periodic nature of non-stationary time-series.
Also, by controlling the degree of chaos created by data augmentation, our
method leads to improved feature representations and performance on downstream
tasks. We evaluate our proposed method on three time-series tasks, including
heart rate estimation, human activity recognition, and cardiovascular disease
detection. Extensive experiments against state-of-the-art methods show that the
proposed approach outperforms prior works on optimal data generation and known
data augmentation techniques in the three tasks, reflecting the effectiveness
of the presented method. Source code:
https://github.com/eth-siplab/Finding_Order_in_Chaos",http://arxiv.org/pdf/2309.13439v1
2309.13438v1,cs.CV,Rethinking superpixel segmentation from biologically inspired mechanisms,2023-09-23 17:29:38+00:00,"Recently, advancements in deep learning-based superpixel segmentation methods
have brought about improvements in both the efficiency and the performance of
segmentation. However, a significant challenge remains in generating
superpixels that strictly adhere to object boundaries while conveying rich
visual significance, especially when cross-surface color correlations may
interfere with objects. Drawing inspiration from neural structure and visual
mechanisms, we propose a biological network architecture comprising an Enhanced
Screening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel
segmentation. The ESM enhances semantic information by simulating the
interactive projection mechanisms of the visual cortex. Additionally, the BAL
emulates the spatial frequency characteristics of visual cortical cells to
facilitate the generation of superpixels with strong boundary adherence. We
demonstrate the effectiveness of our approach through evaluations on both the
BSDS500 dataset and the NYUv2 dataset.",http://arxiv.org/pdf/2309.13438v1
2309.13430v1,cs.CL,Resolving References in Visually-Grounded Dialogue via Text Generation,2023-09-23 17:07:54+00:00,"Vision-language models (VLMs) have shown to be effective at image retrieval
based on simple text queries, but text-image retrieval based on conversational
input remains a challenge. Consequently, if we want to use VLMs for reference
resolution in visually-grounded dialogue, the discourse processing capabilities
of these models need to be augmented. To address this issue, we propose
fine-tuning a causal large language model (LLM) to generate definite
descriptions that summarize coreferential information found in the linguistic
context of references. We then use a pretrained VLM to identify referents based
on the generated descriptions, zero-shot. We evaluate our approach on a
manually annotated dataset of visually-grounded dialogues and achieve results
that, on average, exceed the performance of the baselines we compare against.
Furthermore, we find that using referent descriptions based on larger context
windows has the potential to yield higher returns.",http://arxiv.org/pdf/2309.13430v1
2309.13429v1,cs.LG,Modeling Student Performance in Game-Based Learning Environments,2023-09-23 16:53:07+00:00,"This study investigates game-based learning in the context of the educational
game ""Jo Wilder and the Capitol Case,"" focusing on predicting student
performance using various machine learning models, including K-Nearest
Neighbors (KNN), Multi-Layer Perceptron (MLP), and Random Forest. The research
aims to identify the features most predictive of student performance and
correct question answering. By leveraging gameplay data, we establish complete
benchmarks for these models and explore the importance of applying proper data
aggregation methods. By compressing all numeric data to min/max/mean/sum and
categorical data to first, last, count, and nunique, we reduced the size of the
original training data from 4.6 GB to 48 MB of preprocessed training data,
maintaining high F1 scores and accuracy.
  Our findings suggest that proper preprocessing techniques can be vital in
enhancing the performance of non-deep-learning-based models. The MLP model
outperformed the current state-of-the-art French Touch model, achieving an F-1
score of 0.83 and an accuracy of 0.74, suggesting its suitability for this
dataset. Future research should explore using larger datasets, other
preprocessing techniques, more advanced deep learning techniques, and
real-world applications to provide personalized learning recommendations to
students based on their predicted performance. This paper contributes to the
understanding of game-based learning and provides insights into optimizing
educational game experiences for improved student outcomes and skill
development.",http://arxiv.org/pdf/2309.13429v1
2309.13426v1,cs.CL,A Chat About Boring Problems: Studying GPT-based text normalization,2023-09-23 16:32:59+00:00,"Text normalization - the conversion of text from written to spoken form - is
traditionally assumed to be an ill-formed task for language models. In this
work, we argue otherwise. We empirically show the capacity of Large-Language
Models (LLM) for text normalization in few-shot scenarios. Combining
self-consistency reasoning with linguistic-informed prompt engineering, we find
LLM based text normalization to achieve error rates around 40\% lower than top
normalization systems. Further, upon error analysis, we note key limitations in
the conventional design of text normalization tasks. We create a new taxonomy
of text normalization errors and apply it to results from GPT-3.5-Turbo and
GPT-4.0. Through this new framework, we can identify strengths and weaknesses
of GPT-based TN, opening opportunities for future work.",http://arxiv.org/pdf/2309.13426v1
2309.13421v1,math.OC,Penalties and Rewards for Fair Learning in Paired Kidney Exchange Programs,2023-09-23 16:25:49+00:00,"A kidney exchange program, also called a kidney paired donation program, can
be viewed as a repeated, dynamic trading and allocation mechanism. This
suggests that a dynamic algorithm for transplant exchange selection may have
superior performance in comparison to the repeated use of a static algorithm.
We confirm this hypothesis using a full scale simulation of the Canadian Kidney
Paired Donation Program: learning algorithms, that attempt to learn optimal
patient-donor weights in advance via dynamic simulations, do lead to improved
outcomes. Specifically, our learning algorithms, designed with the objective of
fairness (that is, equity in terms of transplant accessibility across cPRA
groups), also lead to an increased number of transplants and shorter average
waiting times. Indeed, our highest performing learning algorithm improves
egalitarian fairness by 10% whilst also increasing the number of transplants by
6% and decreasing waiting times by 24%. However, our main result is much more
surprising. We find that the most critical factor in determining the
performance of a kidney exchange program is not the judicious assignment of
positive weights (rewards) to patient-donor pairs. Rather, the key factor in
increasing the number of transplants, decreasing waiting times and improving
group fairness is the judicious assignment of a negative weight (penalty) to
the small number of non-directed donors in the kidney exchange program.",http://arxiv.org/pdf/2309.13421v1
2309.13414v1,cs.LG,State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory,2023-09-23 15:55:12+00:00,"State-space models have gained popularity in sequence modelling due to their
simple and efficient network structures. However, the absence of nonlinear
activation along the temporal direction limits the model's capacity. In this
paper, we prove that stacking state-space models with layer-wise nonlinear
activation is sufficient to approximate any continuous sequence-to-sequence
relationship. Our findings demonstrate that the addition of layer-wise
nonlinear activation enhances the model's capacity to learn complex sequence
patterns. Meanwhile, it can be seen both theoretically and empirically that the
state-space models do not fundamentally resolve the exponential decaying memory
issue. Theoretical results are justified by numerical verifications.",http://arxiv.org/pdf/2309.13414v1
2309.13411v1,cs.LG,Towards Attributions of Input Variables in a Coalition,2023-09-23 15:48:35+00:00,"This paper aims to develop a new attribution method to explain the conflict
between individual variables' attributions and their coalition's attribution
from a fully new perspective. First, we find that the Shapley value can be
reformulated as the allocation of Harsanyi interactions encoded by the AI
model. Second, based the re-alloction of interactions, we extend the Shapley
value to the attribution of coalitions. Third we ective. We derive the
fundamental mechanism behind the conflict. This conflict come from the
interaction containing partial variables in their coalition.",http://arxiv.org/pdf/2309.13411v1
2309.13409v1,cs.LG,Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data,2023-09-23 15:42:54+00:00,"This study introduces a novel forecasting strategy that leverages the power
of fractional differencing (FD) to capture both short- and long-term
dependencies in time series data. Unlike traditional integer differencing
methods, FD preserves memory in series while stabilizing it for modeling
purposes. By applying FD to financial data from the SPY index and incorporating
sentiment analysis from news reports, this empirical analysis explores the
effectiveness of FD in conjunction with binary classification of target
variables. Supervised classification algorithms were employed to validate the
performance of FD series. The results demonstrate the superiority of FD over
integer differencing, as confirmed by Receiver Operating Characteristic/Area
Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.",http://arxiv.org/pdf/2309.13409v1
2309.13394v1,cs.CY,Smart City Digital Twin Framework for Real-Time Multi-Data Integration and Wide Public Distribution,2023-09-23 14:53:04+00:00,"Digital Twins are digital replica of real entities and are becoming
fundamental tools to monitor and control the status of entities, predict their
future evolutions, and simulate alternative scenarios to understand the impact
of changes. Thanks to the large deployment of sensors, with the increasing
information it is possible to build accurate reproductions of urban
environments including structural data and real-time information. Such
solutions help city councils and decision makers to face challenges in urban
development and improve the citizen quality of life, by ana-lysing the actual
conditions, evaluating in advance through simulations and what-if analysis the
outcomes of infrastructural or political chang-es, or predicting the effects of
humans and/or of natural events. Snap4City Smart City Digital Twin framework is
capable to respond to the requirements identified in the literature and by the
international forums. Differently from other solutions, the proposed
architecture provides an integrated solution for data gathering, indexing,
computing and information distribution offered by the Snap4City IoT platform,
therefore realizing a continuously updated Digital Twin. 3D building models,
road networks, IoT devices, WoT Entities, point of interests, routes, paths,
etc., as well as results from data analytical processes for traffic density
reconstruction, pollutant dispersion, predictions of any kind, what-if
analysis, etc., are all integrated into an accessible web interface, to support
the citizens participation in the city decision processes. What-If analysis to
let the user performs simulations and observe possible outcomes. As case of
study, the Digital Twin of the city of Florence (Italy) is presented. Snap4City
platform, is released as open-source, and made available through GitHub and as
docker compose.",http://arxiv.org/pdf/2309.13394v1
2309.13393v1,cs.CV,AgriSORT: A Simple Online Real-time Tracking-by-Detection framework for robotics in precision agriculture,2023-09-23 14:35:45+00:00,"The problem of multi-object tracking (MOT) consists in detecting and tracking
all the objects in a video sequence while keeping a unique identifier for each
object. It is a challenging and fundamental problem for robotics. In precision
agriculture the challenge of achieving a satisfactory solution is amplified by
extreme camera motion, sudden illumination changes, and strong occlusions. Most
modern trackers rely on the appearance of objects rather than motion for
association, which can be ineffective when most targets are static objects with
the same appearance, as in the agricultural case. To this end, on the trail of
SORT [5], we propose AgriSORT, a simple, online, real-time
tracking-by-detection pipeline for precision agriculture based only on motion
information that allows for accurate and fast propagation of tracks between
frames. The main focuses of AgriSORT are efficiency, flexibility, minimal
dependencies, and ease of deployment on robotic platforms. We test the proposed
pipeline on a novel MOT benchmark specifically tailored for the agricultural
context, based on video sequences taken in a table grape vineyard, particularly
challenging due to strong self-similarity and density of the instances. Both
the code and the dataset are available for future comparisons.",http://arxiv.org/pdf/2309.13393v1
2309.13391v1,cs.AI,D-Separation for Causal Self-Explanation,2023-09-23 14:23:19+00:00,"Rationalization is a self-explaining framework for NLP models. Conventional
work typically uses the maximum mutual information (MMI) criterion to find the
rationale that is most indicative of the target label. However, this criterion
can be influenced by spurious features that correlate with the causal rationale
or the target label. Instead of attempting to rectify the issues of the MMI
criterion, we propose a novel criterion to uncover the causal rationale, termed
the Minimum Conditional Dependence (MCD) criterion, which is grounded on our
finding that the non-causal features and the target label are
\emph{d-separated} by the causal rationale. By minimizing the dependence
between the unselected parts of the input and the target label conditioned on
the selected rationale candidate, all the causes of the label are compelled to
be selected. In this study, we employ a simple and practical measure of
dependence, specifically the KL-divergence, to validate our proposed MCD
criterion. Empirically, we demonstrate that MCD improves the F1 score by up to
$13.7\%$ compared to previous state-of-the-art MMI-based methods. Our code is
available at: \url{https://github.com/jugechengzi/Rationalization-MCD}.",http://arxiv.org/pdf/2309.13391v1
2309.13378v1,cs.LG,Deciphering Spatio-Temporal Graph Forecasting: A Causal Lens and Treatment,2023-09-23 13:51:09+00:00,"Spatio-Temporal Graph (STG) forecasting is a fundamental task in many
real-world applications. Spatio-Temporal Graph Neural Networks have emerged as
the most popular method for STG forecasting, but they often struggle with
temporal out-of-distribution (OoD) issues and dynamic spatial causation. In
this paper, we propose a novel framework called CaST to tackle these two
challenges via causal treatments. Concretely, leveraging a causal lens, we
first build a structural causal model to decipher the data generation process
of STGs. To handle the temporal OoD issue, we employ the back-door adjustment
by a novel disentanglement block to separate invariant parts and temporal
environments from input data. Moreover, we utilize the front-door adjustment
and adopt the Hodge-Laplacian operator for edge-level convolution to model the
ripple effect of causation. Experiments results on three real-world datasets
demonstrate the effectiveness and practicality of CaST, which consistently
outperforms existing methods with good interpretability.",http://arxiv.org/pdf/2309.13378v1
2309.13365v1,cs.LG,Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs,2023-09-23 13:06:20+00:00,"Interpretability of AI models allows for user safety checks to build trust in
such AIs. In particular, Decision Trees (DTs) provide a global look at the
learned model and transparently reveal which features of the input are critical
for making a decision. However, interpretability is hindered if the DT is too
large. To learn compact trees, a recent Reinforcement Learning (RL) framework
has been proposed to explore the space of DTs using deep RL. This framework
augments a decision problem (e.g. a supervised classification task) with
additional actions that gather information about the features of an otherwise
hidden input. By appropriately penalizing these actions, the agent learns to
optimally trade-off size and performance of DTs. In practice, a reactive policy
for a partially observable Markov decision process (MDP) needs to be learned,
which is still an open problem. We show in this paper that deep RL can fail
even on simple toy tasks of this class. However, when the underlying decision
problem is a supervised classification task, we show that finding the optimal
tree can be cast as a fully observable Markov decision problem and be solved
efficiently, giving rise to a new family of algorithms for learning DTs that go
beyond the classical greedy maximization ones.",http://arxiv.org/pdf/2309.13365v1
2309.13363v1,cs.LG,MLPST: MLP is All You Need for Spatio-Temporal Prediction,2023-09-23 12:58:16+00:00,"Traffic prediction is a typical spatio-temporal data mining task and has
great significance to the public transportation system. Considering the demand
for its grand application, we recognize key factors for an ideal
spatio-temporal prediction method: efficient, lightweight, and effective.
However, the current deep model-based spatio-temporal prediction solutions
generally own intricate architectures with cumbersome optimization, which can
hardly meet these expectations. To accomplish the above goals, we propose an
intuitive and novel framework, MLPST, a pure multi-layer perceptron
architecture for traffic prediction. Specifically, we first capture spatial
relationships from both local and global receptive fields. Then, temporal
dependencies in different intervals are comprehensively considered. Through
compact and swift MLP processing, MLPST can well capture the spatial and
temporal dependencies while requiring only linear computational complexity, as
well as model parameters that are more than an order of magnitude lower than
baselines. Extensive experiments validated the superior effectiveness and
efficiency of MLPST against advanced baselines, and among models with optimal
accuracy, MLPST achieves the best time and space efficiency.",http://arxiv.org/pdf/2309.13363v1
2309.13356v1,cs.CL,Exploring Large Language Models' Cognitive Moral Development through Defining Issues Test,2023-09-23 12:17:10+00:00,"The development of large language models has instilled widespread interest
among the researchers to understand their inherent reasoning and
problem-solving capabilities. Despite good amount of research going on to
elucidate these capabilities, there is a still an appreciable gap in
understanding moral development and judgments of these models. The current
approaches of evaluating the ethical reasoning abilities of these models as a
classification task pose numerous inaccuracies because of over-simplification.
In this study, we built a psychological connection by bridging two disparate
fields-human psychology and AI. We proposed an effective evaluation framework
which can help to delineate the model's ethical reasoning ability in terms of
moral consistency and Kohlberg's moral development stages with the help of
Psychometric Assessment Tool-Defining Issues Test.",http://arxiv.org/pdf/2309.13356v1
2309.13354v1,cs.CL,Lexical Squad@Multimodal Hate Speech Event Detection 2023: Multimodal Hate Speech Detection using Fused Ensemble Approach,2023-09-23 12:06:05+00:00,"With a surge in the usage of social media postings to express opinions,
emotions, and ideologies, there has been a significant shift towards the
calibration of social media as a rapid medium of conveying viewpoints and
outlooks over the globe. Concurrently, the emergence of a multitude of
conflicts between two entities has given rise to a stream of social media
content containing propaganda, hate speech, and inconsiderate views. Thus, the
issue of monitoring social media postings is rising swiftly, attracting major
attention from those willing to solve such problems. One such problem is Hate
Speech detection. To mitigate this problem, we present our novel ensemble
learning approach for detecting hate speech, by classifying text-embedded
images into two labels, namely ""Hate Speech"" and ""No Hate Speech"". We have
incorporated state-of-art models including InceptionV3, BERT, and XLNet. Our
proposed ensemble model yielded promising results with 75.21 and 74.96 as
accuracy and F-1 score (respectively). We also present an empirical evaluation
of the text-embedded images to elaborate on how well the model was able to
predict and classify. We release our codebase here
(https://github.com/M0hammad-Kashif/MultiModalHateSpeech).",http://arxiv.org/pdf/2309.13354v1
2309.13348v1,physics.geo-ph,Accelerating Particle and Fluid Simulations with Differentiable Graph Networks for Solving Forward and Inverse Problems,2023-09-23 11:52:43+00:00,"We leverage physics-embedded differentiable graph network simulators (GNS) to
accelerate particulate and fluid simulations to solve forward and inverse
problems. GNS represents the domain as a graph with particles as nodes and
learned interactions as edges. Compared to modeling global dynamics, GNS
enables learning local interaction laws through edge messages, improving its
generalization to new environments. GNS achieves over 165x speedup for granular
flow prediction compared to parallel CPU numerical simulations. We propose a
novel hybrid GNS/Material Point Method (MPM) to accelerate forward simulations
by minimizing error on a pure surrogate model by interleaving MPM in GNS
rollouts to satisfy conservation laws and minimize errors achieving 24x speedup
compared to pure numerical simulations. The differentiable GNS enables solving
inverse problems through automatic differentiation, identifying material
parameters that result in target runout distances. We demonstrate the ability
of GNS to solve inverse problems by iteratively updating the friction angle (a
material property) by computing the gradient of a loss function based on the
final and target runouts, thereby identifying the friction angle that best
matches the observed runout. The physics-embedded and differentiable simulators
open an exciting new paradigm for AI-accelerated design, control, and
optimization.",http://arxiv.org/pdf/2309.13348v1
2309.13342v1,cs.SE,Evolve the Model Universe of a System Universe,2023-09-23 11:30:26+00:00,"Uncertain, unpredictable, real time, and lifelong evolution causes
operational failures in intelligent software systems, leading to significant
damages, safety and security hazards, and tragedies. To fully unleash the
potential of such systems and facilitate their wider adoption, ensuring the
trustworthiness of their decision making under uncertainty is the prime
challenge. To overcome this challenge, an intelligent software system and its
operating environment should be continuously monitored, tested, and refined
during its lifetime operation. Existing technologies, such as digital twins,
can enable continuous synchronisation with such systems to reflect their most
updated states. Such representations are often in the form of prior knowledge
based and machine learning models, together called model universe. In this
paper, we present our vision of combining techniques from software engineering,
evolutionary computation, and machine learning to support the model universe
evolution.",http://arxiv.org/pdf/2309.13342v1
2309.13340v1,cs.CL,LLMs as Counterfactual Explanation Modules: Can ChatGPT Explain Black-box Text Classifiers?,2023-09-23 11:22:28+00:00,"Large language models (LLMs) are increasingly being used for tasks beyond
text generation, including complex tasks such as data labeling, information
extraction, etc. With the recent surge in research efforts to comprehend the
full extent of LLM capabilities, in this work, we investigate the role of LLMs
as counterfactual explanation modules, to explain decisions of black-box text
classifiers. Inspired by causal thinking, we propose a pipeline for using LLMs
to generate post-hoc, model-agnostic counterfactual explanations in a
principled way via (i) leveraging the textual understanding capabilities of the
LLM to identify and extract latent features, and (ii) leveraging the
perturbation and generation capabilities of the same LLM to generate a
counterfactual explanation by perturbing input features derived from the
extracted latent features. We evaluate three variants of our framework, with
varying degrees of specificity, on a suite of state-of-the-art LLMs, including
ChatGPT and LLaMA 2. We evaluate the effectiveness and quality of the generated
counterfactual explanations, over a variety of text classification benchmarks.
Our results show varied performance of these models in different settings, with
a full two-step feature extraction based variant outperforming others in most
cases. Our pipeline can be used in automated explanation systems, potentially
reducing human effort.",http://arxiv.org/pdf/2309.13340v1
2309.13339v1,cs.CL,Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic,2023-09-23 11:21:12+00:00,"Recent advancements in large language models have showcased their remarkable
generalizability across various domains. However, their reasoning abilities
still have significant room for improvement, especially when confronted with
scenarios requiring multi-step reasoning. Although large language models
possess extensive knowledge, their behavior, particularly in terms of
reasoning, often fails to effectively utilize this knowledge to establish a
coherent thinking paradigm. Generative language models sometimes show
hallucinations as their reasoning procedures are unconstrained by logical
principles. Aiming to improve the zero-shot chain-of-thought reasoning ability
of large language models, we propose Logical Chain-of-Thought (LogiCoT), a
neurosymbolic framework that leverages principles from symbolic logic to verify
and revise the reasoning processes accordingly. Experimental evaluations
conducted on language tasks in diverse domains, including arithmetic,
commonsense, symbolic, causal inference, and social problems, demonstrate the
efficacy of the enhanced reasoning paradigm by logic.",http://arxiv.org/pdf/2309.13339v1
2309.13336v1,cs.CV,FedDrive v2: an Analysis of the Impact of Label Skewness in Federated Semantic Segmentation for Autonomous Driving,2023-09-23 10:58:08+00:00,"We propose FedDrive v2, an extension of the Federated Learning benchmark for
Semantic Segmentation in Autonomous Driving. While the first version aims at
studying the effect of domain shift of the visual features across clients, in
this work, we focus on the distribution skewness of the labels. We propose six
new federated scenarios to investigate how label skewness affects the
performance of segmentation models and compare it with the effect of domain
shift. Finally, we study the impact of using the domain information during
testing.",http://arxiv.org/pdf/2309.13336v1
2309.13325v1,cs.NI,Joint Explainability and Sensitivity-Aware Federated Deep Learning for Transparent 6G RAN Slicing,2023-09-23 10:08:57+00:00,"In recent years, wireless networks are evolving complex, which upsurges the
use of zero-touch artificial intelligence (AI)-driven network automation within
the telecommunication industry. In particular, network slicing, the most
promising technology beyond 5G, would embrace AI models to manage the complex
communication network. Besides, it is also essential to build the
trustworthiness of the AI black boxes in actual deployment when AI makes
complex resource management and anomaly detection. Inspired by closed-loop
automation and Explainable Artificial intelligence (XAI), we design an
Explainable Federated deep learning (FDL) model to predict per-slice RAN
dropped traffic probability while jointly considering the sensitivity and
explainability-aware metrics as constraints in such non-IID setup. In precise,
we quantitatively validate the faithfulness of the explanations via the
so-called attribution-based \emph{log-odds metric} that is included as a
constraint in the run-time FL optimization task. Simulation results confirm its
superiority over an unconstrained integrated-gradient (IG) \emph{post-hoc} FDL
baseline.",http://arxiv.org/pdf/2309.13325v1
2309.13317v1,cs.CV,Class Attendance System in Education with Deep Learning Method,2023-09-23 09:22:58+00:00,"With the advancing technology, the hardware gain of computers and the
increase in the processing capacity of processors have facilitated the
processing of instantaneous and real-time images. Face recognition processes
are also studies in the field of image processing. Facial recognition processes
are frequently used in security applications and commercial applications.
Especially in the last 20 years, the high performances of artificial
intelligence (AI) studies have contributed to the spread of these studies in
many different fields. Education is one of them. The potential and advantages
of using AI in education; can be grouped under three headings: student,
teacher, and institution. One of the institutional studies may be the security
of educational environments and the contribution of automation to education and
training processes. From this point of view, deep learning methods, one of the
sub-branches of AI, were used in this study. For object detection from images,
a pioneering study has been designed and successfully implemented to keep
records of students' entrance to the educational institution and to perform
class attendance with images taken from the camera using image processing
algorithms. The application of the study to real-life problems will be carried
out in a school determined in the 2022-2023 academic year.",http://arxiv.org/pdf/2309.13317v1
2309.13302v1,cs.NE,Gaining the Sparse Rewards by Exploring Binary Lottery Tickets in Spiking Neural Network,2023-09-23 08:24:36+00:00,"Spiking Neural Network (SNN) as a brain-inspired strategy receives lots of
attention because of the high-sparsity and low-power properties derived from
its inherent spiking information state. To further improve the efficiency of
SNN, some works declare that the Lottery Tickets (LTs) Hypothesis, which
indicates that the Artificial Neural Network (ANN) contains a subnetwork
without sacrificing the performance of the original network, also exists in
SNN. However, the spiking information handled by SNN has a natural similarity
and affinity with binarization in sparsification. Therefore, to further explore
SNN efficiency, this paper focuses on (1) the presence or absence of LTs in the
binary SNN, and (2) whether the spiking mechanism is a superior strategy in
terms of handling binary information compared to simple model binarization. To
certify these consumptions, a sparse training method is proposed to find Binary
Weights Spiking Lottery Tickets (BinW-SLT) under different network structures.
Through comprehensive evaluations, we show that BinW-SLT could attain up to
+5.86% and +3.17% improvement on CIFAR-10 and CIFAR-100 compared with binary
LTs, as well as achieve 1.86x and 8.92x energy saving compared with
full-precision SNN and ANN.",http://arxiv.org/pdf/2309.13302v1
2309.13289v1,cs.CV,USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion Segmentation,2023-09-23 07:08:57+00:00,"Unsupervised skin lesion segmentation offers several benefits, including
conserving expert human resources, reducing discrepancies due to subjective
human labeling, and adapting to novel environments. However, segmenting
dermoscopic images without manual labeling guidance presents significant
challenges due to dermoscopic image artifacts such as hair noise, blister
noise, and subtle edge differences. To address these challenges, we introduce
an innovative Uncertainty Self-Learning Network (USL-Net) designed for skin
lesion segmentation. The USL-Net can effectively segment a range of lesions,
eliminating the need for manual labeling guidance. Initially, features are
extracted using contrastive learning, followed by the generation of Class
Activation Maps (CAMs) as saliency maps using these features. The different CAM
locations correspond to the importance of the lesion region based on their
saliency. High-saliency regions in the map serve as pseudo-labels for lesion
regions while low-saliency regions represent the background. However,
intermediate regions can be hard to classify, often due to their proximity to
lesion edges or interference from hair or blisters. Rather than risk potential
pseudo-labeling errors or learning confusion by forcefully classifying these
regions, we consider them as uncertainty regions, exempting them from
pseudo-labeling and allowing the network to self-learn. Further, we employ
connectivity detection and centrality detection to refine foreground
pseudo-labels and reduce noise-induced errors. The application of cycle
refining enhances performance further. Our method underwent thorough
experimental validation on the ISIC-2017, ISIC-2018, and PH2 datasets,
demonstrating that its performance is on par with weakly supervised and
supervised methods, and exceeds that of other existing unsupervised methods.",http://arxiv.org/pdf/2309.13289v1
2309.13285v1,cs.RO,Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning,2023-09-23 06:56:28+00:00,"End-to-end deep reinforcement learning (DRL) for quadrotor control promises
many benefits -- easy deployment, task generalization and real-time execution
capability. Prior end-to-end DRL-based methods have showcased the ability to
deploy learned controllers onto single quadrotors or quadrotor teams
maneuvering in simple, obstacle-free environments. However, the addition of
obstacles increases the number of possible interactions exponentially, thereby
increasing the difficulty of training RL policies. In this work, we propose an
end-to-end DRL approach to control quadrotor swarms in environments with
obstacles. We provide our agents a curriculum and a replay buffer of the
clipped collision episodes to improve performance in obstacle-rich
environments. We implement an attention mechanism to attend to the neighbor
robots and obstacle interactions - the first successful demonstration of this
mechanism on policies for swarm behavior deployed on severely
compute-constrained hardware. Our work is the first work that demonstrates the
possibility of learning neighbor-avoiding and obstacle-avoiding control
policies trained with end-to-end DRL that transfers zero-shot to real
quadrotors. Our approach scales to 32 robots with 80% obstacle density in
simulation and 8 robots with 20% obstacle density in physical deployment. Video
demonstrations are available on the project website at:
https://sites.google.com/view/obst-avoid-swarm-rl.",http://arxiv.org/pdf/2309.13285v1
2309.13276v1,cs.CV,Discwise Active Learning for LiDAR Semantic Segmentation,2023-09-23 06:08:22+00:00,"While LiDAR data acquisition is easy, labeling for semantic segmentation
remains highly time consuming and must therefore be done selectively. Active
learning (AL) provides a solution that can iteratively and intelligently label
a dataset while retaining high performance and a low budget. In this work we
explore AL for LiDAR semantic segmentation. As a human expert is a component of
the pipeline, a practical framework must consider common labeling techniques
such as sequential labeling that drastically improve annotation times. We
therefore propose a discwise approach (DiAL), where in each iteration, we query
the region a single frame covers on global coordinates, labeling all frames
simultaneously. We then tackle the two major challenges that emerge with
discwise AL. Firstly we devise a new acquisition function that takes 3D point
density changes into consideration which arise due to location changes or
ego-vehicle motion. Next we solve a mixed-integer linear program that provides
a general solution to the selection of multiple frames while taking into
consideration the possibilities of disc intersections. Finally we propose a
semi-supervised learning approach to utilize all frames within our dataset and
improve performance.",http://arxiv.org/pdf/2309.13276v1
2309.13272v1,cs.SE,Natural Language Processing for Requirements Formalization: How to Derive New Approaches?,2023-09-23 05:45:19+00:00,"It is a long-standing desire of industry and research to automate the
software development and testing process as much as possible. In this process,
requirements engineering (RE) plays a fundamental role for all other steps that
build on it. Model-based design and testing methods have been developed to
handle the growing complexity and variability of software systems. However,
major effort is still required to create specification models from a large set
of functional requirements provided in natural language. Numerous approaches
based on natural language processing (NLP) have been proposed in the literature
to generate requirements models using mainly syntactic properties. Recent
advances in NLP show that semantic quantities can also be identified and used
to provide better assistance in the requirements formalization process. In this
work, we present and discuss principal ideas and state-of-the-art methodologies
from the field of NLP in order to guide the readers on how to create a set of
rules and methods for the semi-automated formalization of requirements
according to their specific use case and needs. We discuss two different
approaches in detail and highlight the iterative development of rule sets. The
requirements models are represented in a human- and machine-readable format in
the form of pseudocode. The presented methods are demonstrated on two
industrial use cases from the automotive and railway domains. It shows that
using current pre-trained NLP models requires less effort to create a set of
rules and can be easily adapted to specific use cases and domains. In addition,
findings and shortcomings of this research area are highlighted and an outlook
on possible future developments is given.",http://arxiv.org/pdf/2309.13272v1
2309.13269v1,cs.CV,Being Aware of Localization Accuracy By Generating Predicted-IoU-Guided Quality Scores,2023-09-23 05:27:59+00:00,"Localization Quality Estimation (LQE) helps to improve detection performance
as it benefits post processing through jointly considering classification score
and localization accuracy. In this perspective, for further leveraging the
close relationship between localization accuracy and IoU
(Intersection-Over-Union), and for depressing those inconsistent predictions,
we designed an elegant LQE branch to acquire localization quality score guided
by predicted IoU. Distinctly, for alleviating the inconsistency of
classification score and localization quality during training and inference,
under which some predictions with low classification scores but high LQE scores
will impair the performance, instead of separately and independently setting,
we embedded LQE branch into classification branch, producing a joint
classification-localization-quality representation. Then a novel one stage
detector termed CLQ is proposed. Extensive experiments show that CLQ achieves
state-of-the-arts' performance at an accuracy of 47.8 AP and a speed of 11.5
fps with ResNeXt-101 as backbone on COCO test-dev. Finally, we extend CLQ to
ATSS, producing a reliable 1.2 AP gain, showing our model's strong adaptability
and scalability. Codes are released at https://github.com/PanffeeReal/CLQ.",http://arxiv.org/pdf/2309.13269v1
2309.13266v1,cs.RO,Robust Navigation with Cross-Modal Fusion and Knowledge Transfer,2023-09-23 05:16:35+00:00,"Recently, learning-based approaches show promising results in navigation
tasks. However, the poor generalization capability and the simulation-reality
gap prevent a wide range of applications. We consider the problem of improving
the generalization of mobile robots and achieving sim-to-real transfer for
navigation skills. To that end, we propose a cross-modal fusion method and a
knowledge transfer framework for better generalization. This is realized by a
teacher-student distillation architecture. The teacher learns a discriminative
representation and the near-perfect policy in an ideal environment. By
imitating the behavior and representation of the teacher, the student is able
to align the features from noisy multi-modal input and reduce the influence of
variations on navigation policy. We evaluate our method in simulated and
real-world environments. Experiments show that our method outperforms the
baselines by a large margin and achieves robust navigation performance with
varying working conditions.",http://arxiv.org/pdf/2309.13266v1
2309.13264v1,cs.CV,Randomize to Generalize: Domain Randomization for Runway FOD Detection,2023-09-23 05:02:31+00:00,"Tiny Object Detection is challenging due to small size, low resolution,
occlusion, background clutter, lighting conditions and small object-to-image
ratio. Further, object detection methodologies often make underlying assumption
that both training and testing data remain congruent. However, this presumption
often leads to decline in performance when model is applied to
out-of-domain(unseen) data. Techniques like synthetic image generation are
employed to improve model performance by leveraging variations in input data.
Such an approach typically presumes access to 3D-rendered datasets. In
contrast, we propose a novel two-stage methodology Synthetic Randomized Image
Augmentation (SRIA), carefully devised to enhance generalization capabilities
of models encountering 2D datasets, particularly with lower resolution which is
more practical in real-world scenarios. The first stage employs a weakly
supervised technique to generate pixel-level segmentation masks. Subsequently,
the second stage generates a batch-wise synthesis of artificial images,
carefully designed with an array of diverse augmentations. The efficacy of
proposed technique is illustrated on challenging foreign object debris (FOD)
detection. We compare our results with several SOTA models including CenterNet,
SSD, YOLOv3, YOLOv4, YOLOv5, and Outer Vit on a publicly available FOD-A
dataset. We also construct an out-of-distribution test set encompassing 800
annotated images featuring a corpus of ten common categories. Notably, by
harnessing merely 1.81% of objects from source training data and amalgamating
with 29 runway background images, we generate 2227 synthetic images. Subsequent
model retraining via transfer learning, utilizing enriched dataset generated by
domain randomization, demonstrates significant improvement in detection
accuracy. We report that detection accuracy improved from an initial 41% to 92%
for OOD test set.",http://arxiv.org/pdf/2309.13264v1
2309.13259v1,cs.IR,WikiMT++ Dataset Card,2023-09-23 04:46:28+00:00,"WikiMT++ is an expanded and refined version of WikiMusicText (WikiMT),
featuring 1010 curated lead sheets in ABC notation. To expand application
scenarios of WikiMT, we add both objective (album, lyrics, video) and
subjective emotion (12 emotion adjectives) and emo\_4q (Russell 4Q) attributes,
enhancing its usability for music information retrieval, conditional music
generation, automatic composition, and emotion classification, etc.
Additionally, CLaMP is implemented to correct the attributes inherited from
WikiMT to reduce errors introduced during original data collection and enhance
the accuracy and completeness of our dataset.",http://arxiv.org/pdf/2309.13259v1
2309.13256v1,cs.LG,Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks,2023-09-23 04:41:55+00:00,"Pre-trained language models (PLMs) have demonstrated remarkable performance
as few-shot learners. However, their security risks under such settings are
largely unexplored. In this work, we conduct a pilot study showing that PLMs as
few-shot learners are highly vulnerable to backdoor attacks while existing
defenses are inadequate due to the unique challenges of few-shot scenarios. To
address such challenges, we advocate MDP, a novel lightweight, pluggable, and
effective defense for PLMs as few-shot learners. Specifically, MDP leverages
the gap between the masking-sensitivity of poisoned and clean samples: with
reference to the limited few-shot data as distributional anchors, it compares
the representations of given samples under varying masking and identifies
poisoned samples as ones with significant variations. We show analytically that
MDP creates an interesting dilemma for the attacker to choose between attack
effectiveness and detection evasiveness. The empirical evaluation using
benchmark datasets and representative attacks validates the efficacy of MDP.",http://arxiv.org/pdf/2309.13256v1
2309.13246v1,cs.LG,Can I Trust the Explanations? Investigating Explainable Machine Learning Methods for Monotonic Models,2023-09-23 03:59:02+00:00,"In recent years, explainable machine learning methods have been very
successful. Despite their success, most explainable machine learning methods
are applied to black-box models without any domain knowledge. By incorporating
domain knowledge, science-informed machine learning models have demonstrated
better generalization and interpretation. But do we obtain consistent
scientific explanations if we apply explainable machine learning methods to
science-informed machine learning models? This question is addressed in the
context of monotonic models that exhibit three different types of monotonicity.
To demonstrate monotonicity, we propose three axioms. Accordingly, this study
shows that when only individual monotonicity is involved, the baseline Shapley
value provides good explanations; however, when strong pairwise monotonicity is
involved, the Integrated gradients method provides reasonable explanations on
average.",http://arxiv.org/pdf/2309.13246v1
2309.13242v1,cs.CV,UniHead: Unifying Multi-Perception for Detection Heads,2023-09-23 03:22:48+00:00,"The detection head constitutes a pivotal component within object detectors,
tasked with executing both classification and localization functions.
Regrettably, the commonly used parallel head often lacks omni perceptual
capabilities, such as deformation perception, global perception and cross-task
perception. Despite numerous methods attempt to enhance these abilities from a
single aspect, achieving a comprehensive and unified solution remains a
significant challenge. In response to this challenge, we have developed an
innovative detection head, termed UniHead, to unify three perceptual abilities
simultaneously. More precisely, our approach (1) introduces deformation
perception, enabling the model to adaptively sample object features; (2)
proposes a Dual-axial Aggregation Transformer (DAT) to adeptly model long-range
dependencies, thereby achieving global perception; and (3) devises a Cross-task
Interaction Transformer (CIT) that facilitates interaction between the
classification and localization branches, thus aligning the two tasks. As a
plug-and-play method, the proposed UniHead can be conveniently integrated with
existing detectors. Extensive experiments on the COCO dataset demonstrate that
our UniHead can bring significant improvements to many detectors. For instance,
the UniHead can obtain +2.7 AP gains in RetinaNet, +2.9 AP gains in FreeAnchor,
and +2.1 AP gains in GFL. The code will be publicly available. Code Url:
https://github.com/zht8506/UniHead.",http://arxiv.org/pdf/2309.13242v1
2309.13229v1,cs.AI,Heterogeneous Feature Representation for Digital Twin-Oriented Complex Networked Systems,2023-09-23 01:40:56+00:00,"Building models of Complex Networked Systems (CNS) that can accurately
represent reality forms an important research area. To be able to reflect real
world systems, the modelling needs to consider not only the intensity of
interactions between the entities but also features of all the elements of the
system. This study aims to improve the expressive power of node features in
Digital Twin-Oriented Complex Networked Systems (DT-CNSs) with heterogeneous
feature representation principles. This involves representing features with
crisp feature values and fuzzy sets, each describing the objective and the
subjective inductions of the nodes' features and feature differences. Our
empirical analysis builds DT-CNSs to recreate realistic physical contact
networks in different countries from real node feature distributions based on
various representation principles and an optimised feature preference. We also
investigate their respective disaster resilience to an epidemic outbreak
starting from the most popular node. The results suggest that the increasing
flexibility of feature representation with fuzzy sets improves the expressive
power and enables more accurate modelling. In addition, the heterogeneous
features influence the network structure and the speed of the epidemic
outbreak, requiring various mitigation policies targeted at different people.",http://arxiv.org/pdf/2309.13229v1
2309.13224v1,cs.RO,Pick Planning Strategies for Large-Scale Package Manipulation,2023-09-23 00:26:49+00:00,"Automating warehouse operations can reduce logistics overhead costs,
ultimately driving down the final price for consumers, increasing the speed of
delivery, and enhancing the resiliency to market fluctuations.
  This extended abstract showcases a large-scale package manipulation from
unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which is
used for picking and singulating up to 6 million packages per day and so far
has manipulated over 2 billion packages. It describes the various heuristic
methods developed over time and their successor, which utilizes a pick success
predictor trained on real production data.
  To the best of the authors' knowledge, this work is the first large-scale
deployment of learned pick quality estimation methods in a real production
system.",http://arxiv.org/pdf/2309.13224v1
2309.13223v1,cs.IT,Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks,2023-09-23 00:05:39+00:00,"Despite the basic premise that next-generation wireless networks (e.g., 6G)
will be artificial intelligence (AI)-native, to date, most existing efforts
remain either qualitative or incremental extensions to existing ``AI for
wireless'' paradigms. Indeed, creating AI-native wireless networks faces
significant technical challenges due to the limitations of data-driven,
training-intensive AI. These limitations include the black-box nature of the AI
models, their curve-fitting nature, which can limit their ability to reason and
adapt, their reliance on large amounts of training data, and the energy
inefficiency of large neural networks. In response to these limitations, this
article presents a comprehensive, forward-looking vision that addresses these
shortcomings by introducing a novel framework for building AI-native wireless
networks; grounded in the emerging field of causal reasoning. Causal reasoning,
founded on causal discovery, causal representation learning, and causal
inference, can help build explainable, reasoning-aware, and sustainable
wireless networks. Towards fulfilling this vision, we first highlight several
wireless networking challenges that can be addressed by causal discovery and
representation, including ultra-reliable beamforming for terahertz (THz)
systems, near-accurate physical twin modeling for digital twins, training data
augmentation, and semantic communication. We showcase how incorporating causal
discovery can assist in achieving dynamic adaptability, resilience, and
cognition in addressing these challenges. Furthermore, we outline potential
frameworks that leverage causal inference to achieve the overarching objectives
of future-generation networks, including intent management, dynamic
adaptability, human-level cognition, reasoning, and the critical element of
time sensitivity.",http://arxiv.org/pdf/2309.13223v1
2309.13222v1,cs.CL,Hindi to English: Transformer-Based Neural Machine Translation,2023-09-23 00:00:09+00:00,"Machine Translation (MT) is one of the most prominent tasks in Natural
Language Processing (NLP) which involves the automatic conversion of texts from
one natural language to another while preserving its meaning and fluency.
Although the research in machine translation has been going on since multiple
decades, the newer approach of integrating deep learning techniques in natural
language processing has led to significant improvements in the translation
quality. In this paper, we have developed a Neural Machine Translation (NMT)
system by training the Transformer model to translate texts from Indian
Language Hindi to English. Hindi being a low resource language has made it
difficult for neural networks to understand the language thereby leading to a
slow growth in the development of neural machine translators. Thus, to address
this gap, we implemented back-translation to augment the training data and for
creating the vocabulary, we experimented with both word and subword level
tokenization using Byte Pair Encoding (BPE) thereby ending up training the
Transformer in 10 different configurations. This led us to achieve a
state-of-the-art BLEU score of 24.53 on the test set of IIT Bombay
English-Hindi Corpus in one of the configurations.",http://arxiv.org/pdf/2309.13222v1
2309.13220v1,cs.CV,Poster: Self-Supervised Quantization-Aware Knowledge Distillation,2023-09-22 23:52:58+00:00,"Quantization-aware training (QAT) starts with a pre-trained full-precision
model and performs quantization during retraining. However, existing QAT works
require supervision from the labels and they suffer from accuracy loss due to
reduced precision. To address these limitations, this paper proposes a novel
Self-Supervised Quantization-Aware Knowledge Distillation framework (SQAKD).
SQAKD first unifies the forward and backward dynamics of various quantization
functions and then reframes QAT as a co-optimization problem that
simultaneously minimizes the KL-Loss and the discretization error, in a
self-supervised manner. The evaluation shows that SQAKD significantly improves
the performance of various state-of-the-art QAT works. SQAKD establishes
stronger baselines and does not require extensive labeled training data,
potentially making state-of-the-art QAT research more accessible.",http://arxiv.org/pdf/2309.13220v1
2309.13218v1,cs.AI,AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling,2023-09-22 23:45:21+00:00,"Business optimisation is the process of finding and implementing efficient
and cost-effective means of operation to bring a competitive advantage for
businesses. Synthesizing problem formulations is an integral part of business
optimisation which is centred around human expertise, thus with a high
potential of becoming a bottleneck. With the recent advancements in Large
Language Models (LLMs), human expertise needed in problem formulation can
potentially be minimized using Artificial Intelligence (AI). However,
developing a LLM for problem formulation is challenging, due to training data
requirements, token limitations, and the lack of appropriate performance
metrics in LLMs. To minimize the requirement of large training data,
considerable attention has recently been directed towards fine-tuning
pre-trained LLMs for downstream tasks, rather than training a LLM from scratch
for a specific task. In this paper, we adopt this approach and propose an
AI-Copilot for business optimisation by fine-tuning a pre-trained LLM for
problem formulation. To address token limitations, we introduce modularization
and prompt engineering techniques to synthesize complex problem formulations as
modules that fit into the token limits of LLMs. In addition, we design
performance evaluation metrics that are more suitable for assessing the
accuracy and quality of problem formulations compared to existing evaluation
metrics. Experiment results demonstrate that our AI-Copilot can synthesize
complex and large problem formulations for a typical business optimisation
problem in production scheduling.",http://arxiv.org/pdf/2309.13218v1
2309.13216v1,cs.CV,MISFIT-V: Misaligned Image Synthesis and Fusion using Information from Thermal and Visual,2023-09-22 23:41:24+00:00,"Detecting humans from airborne visual and thermal imagery is a fundamental
challenge for Wilderness Search-and-Rescue (WiSAR) teams, who must perform this
function accurately in the face of immense pressure. The ability to fuse these
two sensor modalities can potentially reduce the cognitive load on human
operators and/or improve the effectiveness of computer vision object detection
models. However, the fusion task is particularly challenging in the context of
WiSAR due to hardware limitations and extreme environmental factors. This work
presents Misaligned Image Synthesis and Fusion using Information from Thermal
and Visual (MISFIT-V), a novel two-pronged unsupervised deep learning approach
that utilizes a Generative Adversarial Network (GAN) and a cross-attention
mechanism to capture the most relevant features from each modality.
Experimental results show MISFIT-V offers enhanced robustness against
misalignment and poor lighting/thermal environmental conditions compared to
existing visual-thermal image fusion methods.",http://arxiv.org/pdf/2309.13216v1
2309.13214v1,cs.AI,Assessing the Impact of Personality on Affective States from Video Game Communication,2023-09-22 23:24:37+00:00,"Individual differences in personality determine our preferences, traits and
values, which should similarly hold for the way we express ourselves. With
current advancements and transformations of technology and society, text-based
communication has become ordinary and often even surpasses natural voice
conversations -- with distinct challenges and opportunities. In this
exploratory work, we investigate the impact of personality on the tendency how
players of a team-based collaborative alternate reality game express themselves
affectively. We collected chat logs from eleven players over two weeks, labeled
them according to their affective state, and assessed the connection between
them and the five-factor personality domains and facets. After applying
multi-linear regression, we found a series of reasonable correlations between
(combinations of) personality variables and expressed affect -- as increased
confusion could be predicted by lower self-competence (C1), personal annoyance
by vulnerability to stress (N6) and expressing anger occured more often in
players that are prone to anxiety (N1), less humble and modest (A5), think less
carefully before they act (C6) and have higher neuroticism (N). Expanding the
data set, sample size and input modalities in subsequent work, we aim to
confirm these findings and reveal even more interesting connections that could
inform affective computing and games user research equally.",http://arxiv.org/pdf/2309.13214v1
2309.13206v1,cs.RO,Intent-Aware Autonomous Driving: A Case Study on Highway Merging Scenarios,2023-09-22 23:02:21+00:00,"In this work, we use the communication of intent as a means to facilitate
cooperation between autonomous vehicle agents. Generally speaking, intents can
be any reliable information about its future behavior that a vehicle
communicates with another vehicle. We implement this as an intent-sharing task
atop the merging environment in the simulator of highway-env, which provides a
collection of environments for learning decision-making strategies for
autonomous vehicles. Under a simple setting between two agents, we carefully
investigate how intent-sharing can aid the receiving vehicle in adjusting its
behavior in highway merging scenarios.",http://arxiv.org/pdf/2309.13206v1
2309.13205v1,cs.CL,A Practical Survey on Zero-shot Prompt Design for In-context Learning,2023-09-22 23:00:34+00:00,"The remarkable advancements in large language models (LLMs) have brought
about significant improvements in Natural Language Processing(NLP) tasks. This
paper presents a comprehensive review of in-context learning techniques,
focusing on different types of prompts, including discrete, continuous,
few-shot, and zero-shot, and their impact on LLM performance. We explore
various approaches to prompt design, such as manual design, optimization
algorithms, and evaluation methods, to optimize LLM performance across diverse
tasks. Our review covers key research studies in prompt engineering, discussing
their methodologies and contributions to the field. We also delve into the
challenges faced in evaluating prompt performance, given the absence of a
single ""best"" prompt and the importance of considering multiple metrics. In
conclusion, the paper highlights the critical role of prompt design in
harnessing the full potential of LLMs and provides insights into the
combination of manual design, optimization techniques, and rigorous evaluation
for more effective and efficient use of LLMs in various NLP tasks.",http://arxiv.org/pdf/2309.13205v1
2309.13202v1,cs.CL,Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts,2023-09-22 22:47:32+00:00,"Biomedical literature often uses complex language and inaccessible
professional terminologies. That is why simplification plays an important role
in improving public health literacy. Applying Natural Language Processing (NLP)
models to automate such tasks allows for quick and direct accessibility for lay
readers. In this work, we investigate the ability of state-of-the-art large
language models (LLMs) on the task of biomedical abstract simplification, using
the publicly available dataset for plain language adaptation of biomedical
abstracts (\textbf{PLABA}). The methods applied include domain fine-tuning and
prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and
BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT,
and 3) Control-token mechanisms on BART-based models. We used a range of
automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and
also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT)
mechanisms reported the highest SARI score of 46.54 and T5-base reported the
highest BERTscore 72.62. In human evaluation, BART-L-w-CTs achieved a better
simplicity score over T5-Base (2.9 vs. 2.2), while T5-Base achieved a better
meaning preservation score over BART-L-w-CTs (3.1 vs. 2.6). We also categorised
the system outputs with examples, hoping this will shed some light for future
research on this task. Our code, fine-tuned models, and data splits are
available at \url{https://github.com/HECTA-UoM/PLABA-MU}",http://arxiv.org/pdf/2309.13202v1
2309.13193v1,cs.HC,SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large Language Model,2023-09-22 21:56:00+00:00,"Simulation plays a critical role in the research and development of
autonomous driving and intelligent transportation systems. However, the current
simulation platforms exhibit limitations in the realism and diversity of agent
behaviors, which impede the transfer of simulation outcomes to the real world.
In this paper, we propose a generative driver agent simulation framework based
on large language models (LLMs), capable of perceiving complex traffic
scenarios and providing realistic driving maneuvers. Notably, we conducted
interviews with 24 drivers and used their detailed descriptions of driving
behavior as chain-of-thought prompts to develop a `coach agent' module, which
can evaluate and assist driver agents in accumulating driving experience and
developing human-like driving styles. Through practical simulation experiments
and user experiments, we validate the feasibility of this framework in
generating reliable driver agents and analyze the roles of each module. The
results show that the framework with full architect decreased the collision
rate by 81.04% and increased the human-likeness by 50%. Our research proposes
the first urban context driver agent simulation framework based on LLMs and
provides valuable insights into the future of agent simulation for complex
tasks.",http://arxiv.org/pdf/2309.13193v1
2309.13192v1,cs.LG,Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation,2023-09-22 21:55:18+00:00,"Fine-tuning is the most effective way of adapting pre-trained large language
models (LLMs) to downstream applications. With the fast growth of LLM-enabled
AI applications and democratization of open-souced LLMs, fine-tuning has become
possible for non-expert individuals, but intensively performed LLM fine-tuning
worldwide could result in significantly high energy consumption and carbon
footprint, which may bring large environmental impact. Mitigating such
environmental impact towards Green AI directly correlates to reducing the FLOPs
of fine-tuning, but existing techniques on efficient LLM fine-tuning can only
achieve limited reduction of such FLOPs, due to their ignorance of the
backpropagation cost in fine-tuning. To address this limitation, in this paper
we present GreenTrainer, a new LLM fine-tuning technique that adaptively
evaluates different tensors' backpropagation costs and contributions to the
fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the
most appropriate set of tensors in training. Such selection in GreenTrainer is
made based on a given objective of FLOPs reduction, which can flexibly adapt to
the carbon footprint in energy supply and the need in Green AI. Experiment
results over multiple open-sourced LLM models and abstractive summarization
datasets show that, compared to fine-tuning the whole LLM model, GreenTrainer
can save up to 64% FLOPs in fine-tuning without any noticeable model accuracy
loss. Compared to the existing fine-tuning techniques such as LoRa,
GreenTrainer can achieve up to 4% improvement on model accuracy with on-par
FLOPs reduction.",http://arxiv.org/pdf/2309.13192v1
2309.13188v1,cs.CV,Masked Discriminators for Content-Consistent Unpaired Image-to-Image Translation,2023-09-22 21:32:07+00:00,"A common goal of unpaired image-to-image translation is to preserve content
consistency between source images and translated images while mimicking the
style of the target domain. Due to biases between the datasets of both domains,
many methods suffer from inconsistencies caused by the translation process.
Most approaches introduced to mitigate these inconsistencies do not constrain
the discriminator, leading to an even more ill-posed training setup. Moreover,
none of these approaches is designed for larger crop sizes. In this work, we
show that masking the inputs of a global discriminator for both domains with a
content-based mask is sufficient to reduce content inconsistencies
significantly. However, this strategy leads to artifacts that can be traced
back to the masking process. To reduce these artifacts, we introduce a local
discriminator that operates on pairs of small crops selected with a similarity
sampling strategy. Furthermore, we apply this sampling strategy to sample
global input crops from the source and target dataset. In addition, we propose
feature-attentive denormalization to selectively incorporate content-based
statistics into the generator stream. In our experiments, we show that our
method achieves state-of-the-art performance in photorealistic sim-to-real
translation and weather translation and also performs well in day-to-night
translation. Additionally, we propose the cKVD metric, which builds on the sKVD
metric and enables the examination of translation quality at the class or
category level.",http://arxiv.org/pdf/2309.13188v1
2309.13181v1,cs.LG,Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning,2023-09-22 21:03:33+00:00,"Humans learn by interacting with their environments and perceiving the
outcomes of their actions. A landmark in artificial intelligence has been the
development of deep reinforcement learning (dRL) algorithms capable of doing
the same in video games, on par with or better than humans. However, it remains
unclear whether the successes of dRL models reflect advances in visual
representation learning, the effectiveness of reinforcement learning algorithms
at discovering better policies, or both. To address this question, we introduce
the Learning Challenge Diagnosticator (LCD), a tool that separately measures
the perceptual and reinforcement learning demands of a task. We use LCD to
discover a novel taxonomy of challenges in the Procgen benchmark, and
demonstrate that these predictions are both highly reliable and can instruct
algorithmic development. More broadly, the LCD reveals multiple failure cases
that can occur when optimizing dRL algorithms over entire video game benchmarks
like Procgen, and provides a pathway towards more efficient progress.",http://arxiv.org/pdf/2309.13181v1
2309.13176v1,cs.AI,AI Risk Profiles: A Standards Proposal for Pre-Deployment AI Risk Disclosures,2023-09-22 20:45:15+00:00,"As AI systems' sophistication and proliferation have increased, awareness of
the risks has grown proportionally (Sorkin et al. 2023). In response, calls
have grown for stronger emphasis on disclosure and transparency in the AI
industry (NTIA 2023; OpenAI 2023b), with proposals ranging from standardizing
use of technical disclosures, like model cards (Mitchell et al. 2019), to
yet-unspecified licensing regimes (Sindhu 2023). Since the AI value chain is
complicated, with actors representing various expertise, perspectives, and
values, it is crucial that consumers of a transparency disclosure be able to
understand the risks of the AI system the disclosure concerns. In this paper we
propose a risk profiling standard which can guide downstream decision-making,
including triaging further risk assessment, informing procurement and
deployment, and directing regulatory frameworks. The standard is built on our
proposed taxonomy of AI risks, which reflects a high-level categorization of
the wide variety of risks proposed in the literature. We outline the myriad
data sources needed to construct informative Risk Profiles and propose a
template-based methodology for collating risk information into a standard, yet
flexible, structure. We apply this methodology to a number of prominent AI
systems using publicly available information. To conclude, we discuss design
decisions for the profiles and future work.",http://arxiv.org/pdf/2309.13176v1
2309.13170v1,cs.CR,Investigating Efficient Deep Learning Architectures For Side-Channel Attacks on AES,2023-09-22 20:16:40+00:00,"Over the past few years, deep learning has been getting progressively more
popular for the exploitation of side-channel vulnerabilities in embedded
cryptographic applications, as it offers advantages in terms of the amount of
attack traces required for effective key recovery. A number of effective
attacks using neural networks have already been published, but reducing their
cost in terms of the amount of computing resources and data required is an
ever-present goal, which we pursue in this work. We focus on the ANSSI
Side-Channel Attack Database (ASCAD), and produce a JAX-based framework for
deep-learning-based SCA, with which we reproduce a selection of previous
results and build upon them in an attempt to improve their performance. We also
investigate the effectiveness of various Transformer-based models.",http://arxiv.org/pdf/2309.13170v1
2309.13168v1,cs.RO,FATHER: FActory on THE Road,2023-09-22 20:16:11+00:00,"In most factories today the robotic cells are deployed on well enforced bases
to avoid any external impact on the accuracy of production. In contrast to
that, we evaluate a futuristic concept where the whole robotic cell could work
in a moving platform. Imagine a trailer of a truck moving along the motorway
while exposed to heavy physical impacts due to maneuvering. The key question
here is how the robotic cell behaves and how the productivity is affected. We
propose a system architecture (FATHER) and show some solutions including
network related information and artificial intelligence to make the proposed
futuristic concept feasible to implement.",http://arxiv.org/pdf/2309.13168v1
2309.14334v1,cs.LG,Tasks Makyth Models: Machine Learning Assisted Surrogates for Tipping Points,2023-09-25 17:58:23+00:00,"We present a machine learning (ML)-assisted framework bridging manifold
learning, neural networks, Gaussian processes, and Equation-Free multiscale
modeling, for (a) detecting tipping points in the emergent behavior of complex
systems, and (b) characterizing probabilities of rare events (here,
catastrophic shifts) near them. Our illustrative example is an event-driven,
stochastic agent-based model (ABM) describing the mimetic behavior of traders
in a simple financial market. Given high-dimensional spatiotemporal data --
generated by the stochastic ABM -- we construct reduced-order models for the
emergent dynamics at different scales: (a) mesoscopic Integro-Partial
Differential Equations (IPDEs); and (b) mean-field-type Stochastic Differential
Equations (SDEs) embedded in a low-dimensional latent space, targeted to the
neighborhood of the tipping point. We contrast the uses of the different models
and the effort involved in learning them.",http://arxiv.org/pdf/2309.14334v1
2309.14328v1,cs.GR,pyParaOcean: A System for Visual Analysis of Ocean Data,2023-09-25 17:53:52+00:00,"Visual analysis is well adopted within the field of oceanography for the
analysis of model simulations, detection of different phenomena and events, and
tracking of dynamic processes. With increasing data sizes and the availability
of multivariate dynamic data, there is a growing need for scalable and
extensible tools for visualization and interactive exploration. We describe
pyParaOcean, a visualization system that supports several tasks routinely used
in the visual analysis of ocean data. The system is available as a plugin to
Paraview and is hence able to leverage its distributed computing capabilities
and its rich set of generic analysis and visualization functionalities.
pyParaOcean provides modules to support different visual analysis tasks
specific to ocean data, such as eddy identification and salinity movement
tracking. These modules are available as Paraview filters and this seamless
integration results in a system that is easy to install and use. A case study
on the Bay of Bengal illustrates the utility of the system for the study of
ocean phenomena and processes.",http://arxiv.org/pdf/2309.14328v1
2309.14327v1,cs.CV,DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention,2023-09-25 17:53:29+00:00,"Most of the existing multi-modal models, hindered by their incapacity to
adeptly manage interleaved image-and-text inputs in multi-image, multi-round
dialogues, face substantial constraints in resource allocation for training and
data accessibility, impacting their adaptability and scalability across varied
interaction realms. To address this, we present the DeepSpeed-VisualChat
framework, designed to optimize Large Language Models (LLMs) by incorporating
multi-modal capabilities, with a focus on enhancing the proficiency of Large
Vision and Language Models in handling interleaved inputs. Our framework is
notable for (1) its open-source support for multi-round and multi-image
dialogues, (2) introducing an innovative multi-modal causal attention
mechanism, and (3) utilizing data blending techniques on existing datasets to
assure seamless interactions in multi-round, multi-image conversations.
Compared to existing frameworks, DeepSpeed-VisualChat shows superior
scalability up to 70B parameter language model size, representing a significant
advancement in multi-modal language models and setting a solid foundation for
future explorations.",http://arxiv.org/pdf/2309.14327v1
2309.14326v1,quant-ph,Futility and utility of a few ancillas for Pauli channel learning,2023-09-25 17:53:12+00:00,"In this paper we revisit one of the prototypical tasks for characterizing the
structure of noise in quantum devices, estimating the eigenvalues of an
$n$-qubit Pauli noise channel. Prior work (Chen et al., 2022) established
exponential lower bounds for this task for algorithms with limited quantum
memory. We first improve upon their lower bounds and show:
  (1) Any algorithm without quantum memory must make $\Omega(2^n/\epsilon^2)$
measurements to estimate each eigenvalue within error $\epsilon$. This is tight
and implies the randomized benchmarking protocol is optimal, resolving an open
question of (Flammia and Wallman, 2020).
  (2) Any algorithm with $\le k$ ancilla qubits of quantum memory must make
$\Omega(2^{(n-k)/3})$ queries to the unknown channel. Crucially, unlike in
(Chen et al., 2022), our bound holds even if arbitrary adaptive control and
channel concatenation are allowed.
  In fact these lower bounds, like those of (Chen et al., 2022), hold even for
the easier hypothesis testing problem of determining whether the underlying
channel is completely depolarizing or has exactly one other nontrivial
eigenvalue. Surprisingly, we show that:
  (3) With only $k=2$ ancilla qubits of quantum memory, there is an algorithm
that solves this hypothesis testing task with high probability using a single
measurement.
  Note that (3) does not contradict (2) as the protocol concatenates
exponentially many queries to the channel before the measurement. This result
suggests a novel mechanism by which channel concatenation and $O(1)$ qubits of
quantum memory could work in tandem to yield striking speedups for quantum
process learning that are not possible for quantum state learning.",http://arxiv.org/pdf/2309.14326v1
2309.14324v1,eess.AS,Towards General-Purpose Text-Instruction-Guided Voice Conversion,2023-09-25 17:52:09+00:00,"This paper introduces a novel voice conversion (VC) model, guided by text
instructions such as ""articulate slowly with a deep tone"" or ""speak in a
cheerful boyish voice"". Unlike traditional methods that rely on reference
utterances to determine the attributes of the converted speech, our model adds
versatility and specificity to voice conversion. The proposed VC model is a
neural codec language model which processes a sequence of discrete codes,
resulting in the code sequence of converted speech. It utilizes text
instructions as style prompts to modify the prosody and emotional information
of the given speech. In contrast to previous approaches, which often rely on
employing separate encoders like prosody and content encoders to handle
different aspects of the source speech, our model handles various information
of speech in an end-to-end manner. Experiments have demonstrated the impressive
capabilities of our model in comprehending instructions and delivering
reasonable results.",http://arxiv.org/pdf/2309.14324v1
2309.14323v1,cs.IR,Cluster Language Model for Improved E-Commerce Retrieval and Ranking: Leveraging Query Similarity and Fine-Tuning for Personalized Results,2023-09-25 17:49:35+00:00,"This paper proposes a novel method to improve the accuracy of product search
in e-commerce by utilizing a cluster language model. The method aims to address
the limitations of the bi-encoder architecture while maintaining a minimal
additional training burden. The approach involves labeling top products for
each query, generating semantically similar query clusters using the K-Means
clustering algorithm, and fine-tuning a global language model into cluster
language models on individual clusters. The parameters of each cluster language
model are fine-tuned to learn local manifolds in the feature space efficiently,
capturing the nuances of various query types within each cluster. The inference
is performed by assigning a new query to its respective cluster and utilizing
the corresponding cluster language model for retrieval. The proposed method
results in more accurate and personalized retrieval results, offering a
superior alternative to the popular bi-encoder based retrieval models in
semantic search.",http://arxiv.org/pdf/2309.14323v1
2309.14321v1,cs.RO,Human-Assisted Continual Robot Learning with Foundation Models,2023-09-25 17:45:55+00:00,"Large Language Models (LLMs) have been shown to act like planners that can
decompose high-level instructions into a sequence of executable instructions.
However, current LLM-based planners are only able to operate with a fixed set
of skills. We overcome this critical limitation and present a method for using
LLM-based planners to query new skills and teach robots these skills in a data
and time-efficient manner for rigid object manipulation. Our system can re-use
newly acquired skills for future tasks, demonstrating the potential of open
world and lifelong learning. We evaluate the proposed framework on multiple
tasks in simulation and the real world. Videos are available at:
https://sites.google.com/mit.edu/halp-robot-learning.",http://arxiv.org/pdf/2309.14321v1
2309.14319v1,math.AP,Regularity theory for parabolic operators in the half-space with boundary degeneracy,2023-09-25 17:44:10+00:00,"We study elliptic and parabolic problems governed by the singular elliptic
operators \begin{align*}
  \mathcal L=y^{\alpha_1}\mbox{Tr
}\left(QD^2_xu\right)+2y^{\frac{\alpha_1+\alpha_2}{2}}q\cdot
\nabla_xD_y+y^{\alpha_2}\gamma D_{yy}+y^{\alpha_2-1}v\cdot \nabla \end{align*}
under Neumann or oblique derivative boundary condition, in the half-space
$\mathbb{R}^{N+1}_+=\{(x,y): x \in \mathbb{R}^N, y>0\}$. We prove elliptic and
parabolic $L^p$-estimates and solvability for the associated problems. In the
language of semigroup theory, we prove that $\mathcal L$ generates an analytic
semigroup, characterize its domain as a weighted Sobolev space and show that it
has maximal regularity.",http://arxiv.org/pdf/2309.14319v1
2309.14315v1,math.PR,Spectrum of subblocks of structured random matrices : A free probability approach,2023-09-25 17:36:05+00:00,"We present a new efficient method, based on an extremization problem, for
computing the spectrum of subblocks of large structured random matrices. This
method applies to ensembles of matrices satisfying three fundamental
properties, which we discuss. We present different proofs - combinatorial or
algebraic - of the validity of this method, which all have some connection with
free probability. We illustrate this method with well known examples of
unstructured matrices, including Haar randomly rotated matrices, as well as
with the example of structured random matrices arising in the quantum symmetric
simple exclusion process.",http://arxiv.org/pdf/2309.14315v1
2309.14311v1,cs.DC,Parallelizing a 1-Dim Nagel-Schreckenberg Traffic Model,2023-09-25 17:30:47+00:00,"The Nagel-Schreckenberg model is a stochastic one-dimensional traffic model.
In this assignment, we guide students through the process of implementing a
shared-memory parallel and reproducible version of an existing serial code that
implements this model, and to analyze its scaling behavior. One of the key
elements in this traffic model is the presence of randomness, without which it
would lack realistic phenomena such as traffic jams. Its implementation thus
requires techniques associated with Monte Carlo simulations and pseudo-random
number generation (PRNG). PRNGs are notoriously tricky to deal with in parallel
when combined with the requirement of reproducibility.
  This assignment was created for the graduate course PHY1610 Scientific
Computing for Physicists at the University of Toronto, which had its origin in
the training program of the SciNet HPC Consortium, and is also very suitable
for other scientific disciplines. Several variations of the assignment have
been used over the years.",http://arxiv.org/pdf/2309.14311v1
2309.14308v1,eess.SP,Heart rate measurement using the built-in triaxial accelerometer from a commercial digital writing device,2023-09-25 17:27:02+00:00,"Wearable devices are on the rise. Smart watches and phones, fitness trackers
or smart textiles now provide unprecedented access to our own personal data. As
such, wearable devices can enable health monitoring without disrupting our
daily routines. In clinical settings, electrocardiograms (ECGs) and
photoplethysmographies (PPGs) are used to monitor the heart's and respiratory
behaviors. In more practical settings, accelerometers can be used to estimate
the heartrate when they are attached to the chest. They can also help filter
out some noise in ECG signal from movement. In this work, we compare the heart
rate data extracted from the built-in accelerometer of a commercial smart pen
equipped with sensors (STABILO's DigiPen), with a standard ECG monitor
readouts. We demonstrate that it is possible to accurately predict the heart
rate from the smart pencil. The data collection is done with eight volunteers,
writing the alphabet continuously for five minutes. The signal is processed
with a Butterworth filter to cut off noise. We achieve a mean-squared error
(MSE) better than 6.685x10$^{-3}$ comparing the DigiPen's computed ${\Delta}$t
(time between pulses) with the reference ECG data. The peaks' timestamps for
both signals all maintain a correlation higher than 0.99. All computed heart
rates from the pen accurately correlate with the reference ECG signals.",http://arxiv.org/pdf/2309.14308v1
2309.14306v1,eess.IV,DeepMesh: Mesh-based Cardiac Motion Tracking using Deep Learning,2023-09-25 17:24:18+00:00,"3D motion estimation from cine cardiac magnetic resonance (CMR) images is
important for the assessment of cardiac function and the diagnosis of
cardiovascular diseases. Current state-of-the art methods focus on estimating
dense pixel-/voxel-wise motion fields in image space, which ignores the fact
that motion estimation is only relevant and useful within the anatomical
objects of interest, e.g., the heart. In this work, we model the heart as a 3D
mesh consisting of epi- and endocardial surfaces. We propose a novel learning
framework, DeepMesh, which propagates a template heart mesh to a subject space
and estimates the 3D motion of the heart mesh from CMR images for individual
subjects. In DeepMesh, the heart mesh of the end-diastolic frame of an
individual subject is first reconstructed from the template mesh. Mesh-based 3D
motion fields with respect to the end-diastolic frame are then estimated from
2D short- and long-axis CMR images. By developing a differentiable
mesh-to-image rasterizer, DeepMesh is able to leverage 2D shape information
from multiple anatomical views for 3D mesh reconstruction and mesh motion
estimation. The proposed method estimates vertex-wise displacement and thus
maintains vertex correspondences between time frames, which is important for
the quantitative assessment of cardiac function across different subjects and
populations. We evaluate DeepMesh on CMR images acquired from the UK Biobank.
We focus on 3D motion estimation of the left ventricle in this work.
Experimental results show that the proposed method quantitatively and
qualitatively outperforms other image-based and mesh-based cardiac motion
tracking methods.",http://arxiv.org/pdf/2309.14306v1
2309.14286v1,physics.geo-ph,Virtual hyperspectral images using symmetric autoencoders,2023-09-25 16:51:26+00:00,"Spectral data acquired through remote sensing are invaluable for
environmental and resource studies. However, these datasets are often marred by
nuisance phenomena such as atmospheric inference and other complexities, which
pose significant challenges for accurate analysis. We show that an autoencoder
architecture, called SymAE, which leverages symmetry under reordering of the
pixels, can learn to disentangle the influence of these nuisance from surface
reflectance features on a pixel-by-pixel basis. The disentanglement provides an
alternative to atmospheric correction, without relying on radiative transfer
modelling, through a purely data-driven process. More importantly, the
symmetric autoencoder can generate virtual hyperspectral images by manipulating
the nuisance effects of each pixel. We demonstrate using AVIRIS instrument data
that these virtual images are valuable for subsequent image analysis tasks. We
also show SymAE's ability to extract intra-class invariant features, which is
very useful in clustering and classification tasks, delivering state-of-the-art
classification performance for a purely spectral method.",http://arxiv.org/pdf/2309.14286v1
2309.14279v1,cs.RO,Spring-IMU Fusion Based Proprioception for Feedback Control of Soft Manipulators,2023-09-25 16:42:23+00:00,"This paper presents a novel framework to realize proprioception and
closed-loop control for soft manipulators. Deformations with large elongation
and large bending can be precisely predicted using geometry-based sensor
signals obtained from the inductive springs and the inertial measurement units
(IMUs) with the help of machine learning techniques. Multiple geometric signals
are fused into robust pose estimations, and a data-efficient training process
is achieved after applying the strategy of sim-to-real transfer. As a result,
we can achieve proprioception that is robust to the variation of external
loading and has an average error of 0.7% across the workspace on a
pneumatic-driven soft manipulator. The realized proprioception on soft
manipulator is then contributed to building a sensor-space based algorithm for
closed-loop control. A gradient descent solver is developed to drive the
end-effector to achieve the required poses by iteratively computing a sequence
of reference sensor signals. A conventional controller is employed in the inner
loop of our algorithm to update actuators (i.e., the pressures in chambers) for
approaching a reference signal in the sensor-space. The systematic function of
closed-loop control has been demonstrated in tasks like path following and
pick-and-place under different external loads.",http://arxiv.org/pdf/2309.14279v1
2309.14274v1,eess.SY,Retrodirective Antenna Array Approach to Achieve Maximum Theoretical Beam Efficiency in Microwave Wireless Power Transfer,2023-09-25 16:38:06+00:00,"Efficient long range wireless power transfer (WPT) is realized if the
distance between the source and receiver is less than the Fraunhoffer distance.
This distance increases proportionally to the square root of the antenna size
so to achieve efficient long range WPT, larger antennas are mandatory, but that
comes with difficulty in implementing both the feeding network and beamforming
control of the antenna. Several proposed implementations require power-hungry
processors rendering implementation impractical. An alternative to reduce usage
of digital processing is in the form of retrodirective antenna arrays. Its core
operation is to track an incoming signal's direction of arrival and resend it
to the same direction. This can be implemented by analog circuits.
Retrodirective capability on both the generator and rectenna arrays creates a
feedback loop that produces a high efficiency WPT channel. In this paper, we
characterize the dynamics of this phenomenon using a discrete-time state-space
model based on S-parameters and show that the system can naturally achieve
maximum theoretical WPT efficiency. We further confirmed the theoretical
analysis through a hardware experiment using a 12-port circuit board with
measurable S-parameters mimicking a deterministic wireless channel. The results
of the hardware experiment show agreement with the proposed theoretical
framework.",http://arxiv.org/pdf/2309.14274v1
2309.14268v1,math-ph,A geometric formulation of Schaefer's theory of Cosserat solids,2023-09-25 16:29:11+00:00,"The Cosserat solid is a theoretical model of a continuum whose elementary
constituents are notional rigid bodies. Here we present a formulation of the
mechanics of a Cosserat solid in the language of modern differential geometry
and exterior calculus, motivated by Schaefer's ""motor field"" theory. The solid
is modelled as a principal fibre bundle and configurations are related by
translations and rotations of each constituent. This kinematic property is
described in a coordinate-independent manner by a bundle map. Configurations
are equivalent if this bundle map is a global Euclidean isometry. Inequivalent
configurations, representing deformations of the solid, are characterised by
the local structure of the bundle map. Using Cartan's magic formula we show
that the strain associated with infinitesimal deformations is the Lie
derivative of a connection one-form on the bundle, revealing it to be a Lie
algebra-valued one-form. Extending Schaefer's theory, we derive the finite
strain by integrating the infinitesimal strain along a prescribed path. This is
path independent when the curvature of the connection one-form is zero. Path
dependence signals the presence of topological defects and the non-zero
curvature is then recognised as the density of topological defects. Mechanical
stresses are defined by a virtual work principle in which the Lie
algebra-valued strain one-form is paired with a dual Lie algebra-valued stress
two-form to yield a scalar work volume form. The d'Alembert principle for the
work form provides the balance laws, which is shown to be integrable for a
hyperelastic Cosserat solid. The breakdown of integrability, relevant to active
oriented solids, is briefly examined. Our work elucidates the geometric
structure of Cosserat solids, aids in constitutive modelling of active oriented
materials, and suggests structure-preserving integration schemes.",http://arxiv.org/pdf/2309.14268v1
2309.14265v1,cs.RO,Industrial Application of 6D Pose Estimation for Robotic Manipulation in Automotive Internal Logistics,2023-09-25 16:23:49+00:00,"Despite the advances in robotics a large proportion of the of parts handling
tasks in the automotive industry's internal logistics are not automated but
still performed by humans. A key component to competitively automate these
processes is a 6D pose estimation that can handle a large number of different
parts, is adaptable to new parts with little manual effort, and is sufficiently
accurate and robust with respect to industry requirements. In this context, the
question arises as to the current status quo with respect to these measures. To
address this we built a representative 6D pose estimation pipeline with
state-of-the-art components from economically scalable real to synthetic data
generation to pose estimators and evaluated it on automotive parts with regards
to a realistic sequencing process. We found that using the data generation
approaches, the performance of the trained 6D pose estimators are promising,
but do not meet industry requirements. We reveal that the reason for this is
the inability of the estimators to provide reliable uncertainties for their
poses, rather than the ability of to provide sufficiently accurate poses. In
this context we further analyzed how RGB- and RGB-D-based approaches compare
against this background and show that they are differently vulnerable to the
domain gap induced by synthetic data.",http://arxiv.org/pdf/2309.14265v1
2309.14264v1,hep-ph,Gravitational production of matter and radiation during reheating,2023-09-25 16:23:07+00:00,"I present the production of matter and radiation during reheating after
inflation, considering only gravitational interactions between the inflaton
background and the other sectors. Processes considered are the following: i)
the exchange of a graviton, $h_{\mu \nu}$, involved in the scattering of the
inflaton or particles in the newly created radiation bath; ii) scattering of
the inflaton background and particles in the radiation bath including the
effects of non-minimal couplings to curvature of the Higgs boson and the
inflaton. Requiring the existence of heavy right-handed neutrinos (RHN), I show
that a minimal scenario utilizing only these ""gravitational portals"" is able to
generate simultaneously the observed relic density of Dark Matter (DM), the
baryon asymmetry through leptogenesis, as well as a sufficiently hot thermal
bath after inflation, for generic models of large field inflation.",http://arxiv.org/pdf/2309.14264v1
2309.14263v1,eess.SY,Target Controllability and Target Observability of Structured Network Systems,2023-09-25 16:22:45+00:00,"The duality between controllability and observability enables methods
developed for full-state control to be applied to full-state estimation, and
vice versa. In applications in which control or estimation of all state
variables is unfeasible, the generalized notions of output controllability and
functional observability establish the minimal conditions for the control and
estimation of a target subset of state variables, respectively. Given the
seemly unrelated nature of these properties, thus far methods for target
control and target estimation have been developed independently in the
literature. Here, we characterize the graph-theoretic conditions for target
controllability and target observability (which are, respectively, special
cases of output controllability and functional observability for structured
systems). This allow us to rigorously establish a weak and strong duality
between these generalized properties. When both properties are equivalent
(strongly dual), we show that efficient algorithms developed for target
controllability can be used for target observability, and vice versa, for the
optimal placement of sensors and drivers. These results are applicable to
large-scale networks, in which control and monitoring are often sought for
small subsets of nodes.",http://arxiv.org/pdf/2309.14263v1
2309.14241v1,cs.CV,Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation,2023-09-25 15:56:01+00:00,"Contemporary domain adaptation offers a practical solution for achieving
cross-domain transfer of semantic segmentation between labeled source data and
unlabeled target data. These solutions have gained significant popularity;
however, they require the model to be retrained when the test environment
changes. This can result in unbearable costs in certain applications due to the
time-consuming training process and concerns regarding data privacy. One-shot
domain adaptation methods attempt to overcome these challenges by transferring
the pre-trained source model to the target domain using only one target data.
Despite this, the referring style transfer module still faces issues with
computation cost and over-fitting problems. To address this problem, we propose
a novel framework called Informative Data Mining (IDM) that enables efficient
one-shot domain adaptation for semantic segmentation. Specifically, IDM
provides an uncertainty-based selection criterion to identify the most
informative samples, which facilitates quick adaptation and reduces redundant
training. We then perform a model adaptation method using these selected
samples, which includes patch-wise mixing and prototype-based information
maximization to update the model. This approach effectively enhances adaptation
and mitigates the overfitting problem. In general, we provide empirical
evidence of the effectiveness and efficiency of IDM. Our approach outperforms
existing methods and achieves a new state-of-the-art one-shot performance of
56.7\%/55.4\% on the GTA5/SYNTHIA to Cityscapes adaptation tasks, respectively.
The code will be released at \url{https://github.com/yxiwang/IDM}.",http://arxiv.org/pdf/2309.14241v1
2309.14240v1,cs.LG,Learning to Abstain From Uninformative Data,2023-09-25 15:55:55+00:00,"Learning and decision-making in domains with naturally high noise-to-signal
ratio, such as Finance or Healthcare, is often challenging, while the stakes
are very high. In this paper, we study the problem of learning and acting under
a general noisy generative process. In this problem, the data distribution has
a significant proportion of uninformative samples with high noise in the label,
while part of the data contains useful information represented by low label
noise. This dichotomy is present during both training and inference, which
requires the proper handling of uninformative data during both training and
testing. We propose a novel approach to learning under these conditions via a
loss inspired by the selective learning theory. By minimizing this loss, the
model is guaranteed to make a near-optimal decision by distinguishing
informative data from uninformative data and making predictions. We build upon
the strength of our theoretical guarantees by describing an iterative
algorithm, which jointly optimizes both a predictor and a selector, and
evaluates its empirical performance in a variety of settings.",http://arxiv.org/pdf/2309.14240v1
2309.14233v1,cs.CL,Urdu Poetry Generated by Using Deep Learning Techniques,2023-09-25 15:44:24+00:00,"This study provides Urdu poetry generated using different deep-learning
techniques and algorithms. The data was collected through the Rekhta website,
containing 1341 text files with several couplets. The data on poetry was not
from any specific genre or poet. Instead, it was a collection of mixed Urdu
poems and Ghazals. Different deep learning techniques, such as the model
applied Long Short-term Memory Networks (LSTM) and Gated Recurrent Unit (GRU),
have been used. Natural Language Processing (NLP) may be used in machine
learning to understand, analyze, and generate a language humans may use and
understand. Much work has been done on generating poetry for different
languages using different techniques. The collection and use of data were also
different for different researchers. The primary purpose of this project is to
provide a model that generates Urdu poems by using data completely, not by
sampling data. Also, this may generate poems in pure Urdu, not Roman Urdu, as
in the base paper. The results have shown good accuracy in the poems generated
by the model.",http://arxiv.org/pdf/2309.14233v1
2309.14232v1,cs.SI,"The Governance of Distributed Autonomous Organizations: A Study of Contributors' Influence, Networks, and Shifts in Voting Power",2023-09-25 15:43:17+00:00,"We present a study analyzing the voting behavior of contributors, or vested
users, in Decentralized Autonomous Organizations (DAOs). We evaluate their
involvement in decision-making processes, discovering that in at least 7.54% of
all DAOs, contributors, on average, held the necessary majority to control
governance decisions. Furthermore, contributors have singularly decided at
least one proposal in 20.41% of DAOs. Notably, contributors tend to be
centrally positioned within the DAO governance ecosystem, suggesting the
presence of inner power circles. Additionally, we observed a tendency for
shifts in governance token ownership shortly before governance polls take place
in 1202 (14.81%) of 8116 evaluated proposals. Our findings highlight the
central role of contributors across a spectrum of DAOs, including Decentralized
Finance protocols. Our research also offers important empirical insights
pertinent to ongoing regulatory activities aimed at increasing transparency to
DAO governance frameworks.",http://arxiv.org/pdf/2309.14232v1
2309.14228v1,cs.HC,ID.8: Co-Creating Visual Stories with Generative AI,2023-09-25 15:35:51+00:00,"Storytelling is an integral part of human culture and significantly impacts
cognitive and socio-emotional development and connection. Despite the
importance of interactive visual storytelling, the process of creating such
content requires specialized skills and is labor-intensive. This paper
introduces ID.8, an open-source system designed for the co-creation of visual
stories with generative AI. We focus on enabling an inclusive storytelling
experience by simplifying the content creation process and allowing for
customization. Our user evaluation confirms a generally positive user
experience in domains such as enjoyment and exploration, while highlighting
areas for improvement, particularly in immersiveness, alignment, and
partnership between the user and the AI system. Overall, our findings indicate
promising possibilities for empowering people to create visual stories with
generative AI. This work contributes a novel content authoring system, ID.8,
and insights into the challenges and potential of using generative AI for
multimedia content creation.",http://arxiv.org/pdf/2309.14228v1
2309.14225v1,cs.RO,HumanMimic: Learning Natural Locomotion and Transitions for Humanoid Robot via Wasserstein Adversarial Imitation,2023-09-25 15:31:34+00:00,"Transferring human motion skills to humanoid robots remains a significant
challenge. In this study, we introduce a Wasserstein adversarial imitation
learning system, allowing humanoid robots to replicate natural whole-body
locomotion patterns and execute seamless transitions by mimicking human
motions. First, we present a unified primitive-skeleton motion retargeting to
mitigate morphological differences between arbitrary human demonstrators and
humanoid robots. An adversarial critic component is integrated with
Reinforcement Learning (RL) to guide the control policy to produce behaviors
aligned with the data distribution of mixed reference motions. Additionally, we
employ a specific Integral Probabilistic Metric (IPM), namely the Wasserstein-1
distance with a novel soft boundary constraint to stabilize the training
process and prevent model collapse. Our system is evaluated on a full-sized
humanoid JAXON in the simulator. The resulting control policy demonstrates a
wide range of locomotion patterns, including standing, push-recovery, squat
walking, human-like straight-leg walking, and dynamic running. Notably, even in
the absence of transition motions in the demonstration dataset, robots showcase
an emerging ability to transit naturally between distinct locomotion patterns
as desired speed changes.",http://arxiv.org/pdf/2309.14225v1
2309.14224v2,math.FA,On $k-$WUR and its generalizations,2023-09-25 15:30:55+00:00,"We introduce two notions called $k-$weakly uniform rotundity ($k-$WUR) and
$k-$weakly locally uniform rotundity ($k-$WLUR) in real Banach spaces. These
are natural generalizations of the well-known concepts $k-$UR and WUR. By
introducing two best approximation notions namely $k-$weakly strong
Chebyshevity and $k-$weakly uniform strong Chebyshevity, we generalize some of
the existing results to $k-$WUR and $k-$WLUR spaces. In particular, we present
characterizations of $k-$WUR spaces in terms of $k-$weakly uniformly strong
Chebyshevness. Also, the inheritance of the notions $k-$WUR and $k-$WLUR by
quotient spaces are discussed. Further, we provide a necessary and sufficient
condition for an infinite $\ell_p-$product space to be $k-$WUR (respectively,
$k-$WLUR). As a consequence, we observe that the notions WUR and $k-$WUR
coincide for an infinite $\ell_p-$product of a Banach space.",http://arxiv.org/pdf/2309.14224v2
2309.14223v1,math-ph,Wigner measures of electromagnetic waves in bianisotropic heterogeneous media,2023-09-25 15:28:29+00:00,"We study the propagation of high-frequency electromagnetic waves in randomly
heterogeneous bianisotropic media with dissipative properties. For that purpose
we consider randomly fluctuating optical responses of such media with
correlation lengths comparable to the typical wavelength of the waves. Although
the fluctuations are weak, they induce multiple scattering over long
propagation times and/or distances such that the waves end up travelling in
many different directions with mixed polarizations. We derive the dispersion
and evolution properties of the Wigner measure of the electromagnetic fields,
which describes their angularly-resolved energy density in this propagation
regime. The analysis starts from Maxwell's equations with general constitutive
equations. We first ignore the random fluctuations of the optical response and
obtain uncoupled transport equations for the components of the Wigner measure
on the different propagation modes (polarizations). Then we use a multi-scale
expansion of the Wigner mesure to obtain the radiative transfer equations
satisfied by these components when the fluctuations are no longer ignored. The
radiative transfer equations are coupled through their collisional parts, which
account for the scattering of waves by the random fluctuations and their
possible changes in polarization. The collisional kernels describing these
processes depend on the power and cross-power spectral densities of the
fluctuations at the wavelength scale. The overall derivation is based on the
interpretation of Wigner transforms and Wigner measures in terms of
semiclassical pseudo-differential operators in their standard quantization.",http://arxiv.org/pdf/2309.14223v1
2309.14222v1,gr-qc,Analyzing a higher order $q(t)$ model and its implications in the late evolution of the Universe using recent observational datasets,2023-09-25 15:26:43+00:00,"In this research paper, we explore a well-motivated parametrization of the
time-dependent deceleration parameter, characterized by a cubic form, within
the context of late time cosmic acceleration. The current analysis is based on
the $f(Q,T)$ gravity theory, by considering the background metric as the
homogeneous and isotropic Friedmann Lema\^itre Robertson Walker (FLRW) metric.
Investigating the model reveals intriguing features of the late universe. To
constrain the model, we use the recent observational datasets, including cosmic
chronometer (CC), Supernovae (SNIa), Baryon Acoustic Oscillation (BAO), Cosmic
Microwave Background Radiation (CMB), Gamma Ray Burst (GRB), and Quasar (Q)
datasets. The joint analysis of these datasets results in tighter constraints
for the model parameters, enabling us to discuss both the physical and
geometrical aspects of the model. Moreover, we determine the present values of
the deceleration parameter ($q_0$), the Hubble parameter ($H_0$), and the
transition redshift ($z_t$) from deceleration to acceleration ensuring
consistency with some recent results of Planck 2018. Our statistical analysis
yields highly improved results, surpassing those obtained in previous
investigations. Overall, this study presents valuable insights into the higher
order $q(t)$ model and its implications for late-time cosmic acceleration,
shedding light on the nature of the late universe.",http://arxiv.org/pdf/2309.14222v1
2309.14219v1,physics.ao-ph,Unveiling Significant Shoreline Changes in Lake Michigan After a Record-Setting Water Level Increase using High-Resolution Satellite Images,2023-09-25 15:24:16+00:00,"In this paper, high-resolution multispectral satellite images were used to
uncover a remarkable shoreline transformation in Lake Michigan coastal areas,
driven by a record-setting increase in the water level between 2013 and 2020.
Shoreline change analyses were conducted for eleven different natural beaches
around the lake, unveiling significant variations of shoreline retreat despite
being affected by the same water level increase. The average observed shoreline
retreats between 2013 and 2020 for the beaches ranged between 20 m and 62 m.
When the passive inundation was excluded, the estimated morphological changes
were found to differ significantly from site to site, with some locations
experiencing minimal changes, while others encountered considerable
morphological changes of up to 38m. The examination of the correlation between
the morphological changes and ten hydrodynamic and morphological factors
revealed strong correlations with the offshore slopes and beach width, with
steeply sloping, wide beaches experiencing more erosion. Notably, wave power,
longshore sediment transport divergence, and the number of storms exhibited
moderate correlation with the observed morphological changes. The results of
the shoreline changes and correlation analysis offer valuable insights into the
varied effects of increased water levels on Lake Michigan beaches, including
erosion and passive inundation, while shedding light on the key factors driving
shoreline erosion in this context. These insights can help decision and
policymakers in making informed choices regarding the protection and management
of Lake Michigan coastal areas, particularly in anticipation of future
incidents of water level increase.",http://arxiv.org/pdf/2309.14219v1
2309.14214v1,hep-ph,Analytic results for massive $2\to2$ processes,2023-09-25 15:19:56+00:00,"We discuss recent (semi) analytic results for $2\to 2$ processes with massive
internal and external particles in various regions of phase space. In the
physical applications we restrict ourselves to $gg\to HH$.",http://arxiv.org/pdf/2309.14214v1
2309.14207v1,cs.CV,Automatic Animation of Hair Blowing in Still Portrait Photos,2023-09-25 15:11:40+00:00,"We propose a novel approach to animate human hair in a still portrait photo.
Existing work has largely studied the animation of fluid elements such as water
and fire. However, hair animation for a real image remains underexplored, which
is a challenging problem, due to the high complexity of hair structure and
dynamics. Considering the complexity of hair structure, we innovatively treat
hair wisp extraction as an instance segmentation problem, where a hair wisp is
referred to as an instance. With advanced instance segmentation networks, our
method extracts meaningful and natural hair wisps. Furthermore, we propose a
wisp-aware animation module that animates hair wisps with pleasing motions
without noticeable artifacts. The extensive experiments show the superiority of
our method. Our method provides the most pleasing and compelling viewing
experience in the qualitative experiments and outperforms state-of-the-art
still-image animation methods by a large margin in the quantitative evaluation.
Project url: \url{https://nevergiveu.github.io/AutomaticHairBlowing/}",http://arxiv.org/pdf/2309.14207v1
2309.14198v1,cs.LG,(Predictable) Performance Bias in Unsupervised Anomaly Detection,2023-09-25 14:57:43+00:00,"Background: With the ever-increasing amount of medical imaging data, the
demand for algorithms to assist clinicians has amplified. Unsupervised anomaly
detection (UAD) models promise to aid in the crucial first step of disease
detection. While previous studies have thoroughly explored fairness in
supervised models in healthcare, for UAD, this has so far been unexplored.
  Methods: In this study, we evaluated how dataset composition regarding
subgroups manifests in disparate performance of UAD models along multiple
protected variables on three large-scale publicly available chest X-ray
datasets. Our experiments were validated using two state-of-the-art UAD models
for medical images. Finally, we introduced a novel subgroup-AUROC (sAUROC)
metric, which aids in quantifying fairness in machine learning.
  Findings: Our experiments revealed empirical ""fairness laws"" (similar to
""scaling laws"" for Transformers) for training-dataset composition: Linear
relationships between anomaly detection performance within a subpopulation and
its representation in the training data. Our study further revealed performance
disparities, even in the case of balanced training data, and compound effects
that exacerbate the drop in performance for subjects associated with multiple
adversely affected groups.
  Interpretation: Our study quantified the disparate performance of UAD models
against certain demographic subgroups. Importantly, we showed that this
unfairness cannot be mitigated by balanced representation alone. Instead, the
representation of some subgroups seems harder to learn by UAD models than that
of others. The empirical fairness laws discovered in our study make disparate
performance in UAD models easier to estimate and aid in determining the most
desirable dataset composition.",http://arxiv.org/pdf/2309.14198v1
2309.14193v1,astro-ph.CO,Universal Gravitational Waves from Interacting and Clustered Solitons,2023-09-25 14:53:17+00:00,"Causal soliton formation (e.g. oscillons, Q-balls) in the primordial Universe
is expected to give rise to a universal gravitational wave (GW) background, at
frequencies smaller than scales of nonlinearity. We show that modifications of
the soliton density field, driven by soliton interactions or initial
conditions, can significantly enhance universal GWs. Gravitational clustering
of solitons naturally leads to generation of correlations in the large-scale
soliton density field. As we demonstrate for axion-like particle (ALP)
oscillons, the growing power spectrum amplifies universal GW signals, opening
new avenues for probing the physics of the early Universe with upcoming GW
experiments. Our results are applicable to variety of scenarios, such as
solitons interacting through a long range Yukawa-like fifth force.",http://arxiv.org/pdf/2309.14193v1
2309.14190v1,quant-ph,Quantum Torque on a Non-Reciprocal Body out of Thermal Equilibrium and Induced by a Magnetic Field of Arbitrary Strength,2023-09-25 14:51:08+00:00,"A stationary body that is out of thermal equilibrium with its environment,
and for which the electric susceptibility is non-reciprocal, experiences a
quantum torque. This arises from the spatially non-symmetric electrical
response of the body to its interaction with the non-equilibrium thermal
fluctuations of the electromagnetic field: the non-equilibrium nature of the
thermal field fluctuations results in a net energy flow through the body, and
the spatially non-symmetric nature of the electrical response of the body to
its interaction with these field fluctuations causes that energy flow to be
transformed into a rotational motion. We establish an exact, closed-form,
analytical expression for this torque in the case that the environment is the
vacuum and the material of the body is described by a damped oscillator model,
where the non-reciprocal nature of the electric susceptibility is induced by an
external magnetic field, as for magneto-optical media. We also generalise this
expression to the context in which the body is slowly rotating. By exploring
the high-temperature expansion of the torque, we are able to identify the
separate contributions from the continuous spectral distribution of the
non-reciprocal electric susceptibility, and from the resonance modes. In
particular, we find that the torque persists in the limiting case of zero
damping parameter, due to the contribution of the resonance modes. We also
consider the low-temperature expansion of the torque. This work extends our
previous consideration of this model to an external magnetic field of arbitrary
strength, thereby including non-linear magnetic field effects.",http://arxiv.org/pdf/2309.14190v1
2309.14174v1,cs.CL,Only 5\% Attention Is All You Need: Efficient Long-range Document-level Neural Machine Translation,2023-09-25 14:33:47+00:00,"Document-level Neural Machine Translation (DocNMT) has been proven crucial
for handling discourse phenomena by introducing document-level context
information. One of the most important directions is to input the whole
document directly to the standard Transformer model. In this case, efficiency
becomes a critical concern due to the quadratic complexity of the attention
module. Existing studies either focus on the encoder part, which cannot be
deployed on sequence-to-sequence generation tasks, e.g., Machine Translation
(MT), or suffer from a significant performance drop. In this work, we keep the
translation performance while gaining 20\% speed up by introducing extra
selection layer based on lightweight attention that selects a small portion of
tokens to be attended. It takes advantage of the original attention to ensure
performance and dimension reduction to accelerate inference. Experimental
results show that our method could achieve up to 95\% sparsity (only 5\% tokens
attended) approximately, and save 93\% computation cost on the attention module
compared with the original Transformer, while maintaining the performance.",http://arxiv.org/pdf/2309.14174v1
2309.14172v1,quant-ph,"Error and Disturbance as Irreversibility with Applications: Unified Definition, Wigner--Araki--Yanase Theorem and Out-of-Time-Order Correlator",2023-09-25 14:29:31+00:00,"Error and disturbance are fundamental concepts in quantum measurements. Here
we show that these two concepts can be defined as special cases of the
irreversibility of quantum processes. The re-definitions provide fruitful
byproducts: First, we can unify the existing various definitions of error and
disturbance as special aspects of irreversibility. Second, we extend the
quantitative Wigner--Araki--Yanase theorem -- a universal restriction on
measurement implementation under a conservation law -- to error and disturbance
of arbitrary definitions. Third, we provide a novel treatment of
out-of-time-orderd-correlator (OTOC) -- a measure of quantum chaos in a quantum
many-body system -- as irreversibility, and its experimental evaluation method.",http://arxiv.org/pdf/2309.14172v1
2309.14168v1,astro-ph.SR,The population of young low-mass stars in Trumpler 14,2023-09-25 14:22:31+00:00,"Massive star-forming regions are thought to be the most common birth
environments in the Galaxy and the only birth places of very massive stars.
Their presence in the stellar cluster alters the conditions within the cluster
impacting at the same time the evolution of other cluster members. In
principle, copious amounts of ultraviolet radiation produced by massive stars
can remove material from outer parts of the protoplanetary disks around low-
and intermediate-mass stars in the process of external photoevaporation,
effectively reducing the planet-formation capabilities of those disks. Here, we
present deep VLT/MUSE observations of low-mass stars in Trumpler 14, one of the
most massive, young, and compact clusters in the Carina Nebula Complex. We
provide spectral and stellar properties of 717 sources and based on the
distribution of stellar ages derive the cluster age of $\sim$1~Myr. The
majority of the stars in our sample have masses $\leqslant$1~$M_\odot$, what
makes our spectroscopic catalogue the most deep to date in term of masses, and
proves that detailed investigations of low-mass stars are possible in the
massive but distant regions. Spectroscopic studies of low-mass members of the
whole Carina Nebula Complex are missing. Our work provides an important step
forward towards filling this gap and set the stage for follow-up investigation
of accretion properties in Trumpler 14.",http://arxiv.org/pdf/2309.14168v1
2309.14167v1,cond-mat.mtrl-sci,Ultrafast generation of nonthermal magnons in iron: Ab initio parameterized calculations,2023-09-25 14:22:04+00:00,"Ultrafast laser excitation of ferromagnetic metals gives rise to correlated,
highly non-equilibrium dynamics of electrons, spins and lattice, which are,
however, poorly described by the widely used three-temperature model (3TM).
Here, we develop a fully ab initio parameterized out-of-equilibrium theory
based on a quantum kinetic approach -- termed (N+2) temperature model -- that
describes magnon occupation dynamics due to electron-magnon scattering. We
apply this model to perform quantitative simulations on the ultrafast,
laser-induced generation of magnons in iron and demonstrate that on these
timescales the magnon distribution is non-thermal: predominantly high-energy
magnons are created, while the magnon occupation close to the center of the
Brillouin zone even decreases, due to a repopulation towards higher energy
states via a so-far-overlooked scattering term. Moreover, we show that the 3TM
can be derived from our model and compare it with our microscopic calculations.
In doing so, we demonstrate that the simple relation between magnetization and
temperature computed at equilibrium does not hold in the ultrafast regime and
that the 3TM greatly overestimates the demagnetization. Our ab
initio-parametrized calculations show that ultrafast generation of non-thermal
magnons provides a sizable demagnetization within 200 fs and, thus, emphasize
the importance of magnon excitations for the ultrafast demagnetization process.",http://arxiv.org/pdf/2309.14167v1
2309.14165v1,cs.CL,Towards End-User Development for IoT: A Case Study on Semantic Parsing of Cooking Recipes for Programming Kitchen Devices,2023-09-25 14:21:24+00:00,"Semantic parsing of user-generated instructional text, in the way of enabling
end-users to program the Internet of Things (IoT), is an underexplored area. In
this study, we provide a unique annotated corpus which aims to support the
transformation of cooking recipe instructions to machine-understandable
commands for IoT devices in the kitchen. Each of these commands is a tuple
capturing the semantics of an instruction involving a kitchen device in terms
of ""What"", ""Where"", ""Why"" and ""How"". Based on this corpus, we developed machine
learning-based sequence labelling methods, namely conditional random fields
(CRF) and a neural network model, in order to parse recipe instructions and
extract our tuples of interest from them. Our results show that while it is
feasible to train semantic parsers based on our annotations, most
natural-language instructions are incomplete, and thus transforming them into
formal meaning representation, is not straightforward.",http://arxiv.org/pdf/2309.14165v1
2309.14163v1,math.NA,Uniform multi-penalty regularization for linear ill-posed inverse problems,2023-09-25 14:13:41+00:00,"This study examines, in the framework of variational regularization methods,
a multi-penalty regularization approach which builds upon the Uniform PENalty
(UPEN) method, previously proposed by the authors for Nuclear Magnetic
Resonance (NMR) data processing. The paper introduces two iterative methods,
UpenMM and GUpenMM, formulated within the Majorization-Minimization (MM)
framework. These methods are designed to identify appropriate regularization
parameters and solutions for linear inverse problems utilizing multi-penalty
regularization. The paper demonstrates the convergence of these methods and
illustrates their potential through numerical examples in one and
two-dimensional scenarios, showing the practical utility of point-wise
regularization terms in solving various inverse problems.",http://arxiv.org/pdf/2309.14163v1
2309.14158v1,cs.SD,An Investigation of Distribution Alignment in Multi-Genre Speaker Recognition,2023-09-25 14:08:48+00:00,"Multi-genre speaker recognition is becoming increasingly popular due to its
ability to better represent the complexities of real-world applications.
However, a major challenge is the significant shift in the distribution of
speaker vectors across different genres. While distribution alignment is a
common approach to address this challenge, previous studies have mainly focused
on aligning a source domain with a target domain, and the performance of
multi-genre data is unknown.
  This paper presents a comprehensive study of mainstream distribution
alignment methods on multi-genre data, where multiple distributions need to be
aligned. We analyze various methods both qualitatively and quantitatively. Our
experiments on the CN-Celeb dataset show that within-between distribution
alignment (WBDA) performs relatively better. However, we also found that none
of the investigated methods consistently improved performance in all test
cases. This suggests that solely aligning the distributions of speaker vectors
may not fully address the challenges posed by multi-genre speaker recognition.
Further investigation is necessary to develop a more comprehensive solution.",http://arxiv.org/pdf/2309.14158v1
2309.14156v1,cs.LG,Designing and evaluating an online reinforcement learning agent for physical exercise recommendations in N-of-1 trials,2023-09-25 14:08:21+00:00,"Personalized adaptive interventions offer the opportunity to increase patient
benefits, however, there are challenges in their planning and implementation.
Once implemented, it is an important question whether personalized adaptive
interventions are indeed clinically more effective compared to a fixed gold
standard intervention. In this paper, we present an innovative N-of-1 trial
study design testing whether implementing a personalized intervention by an
online reinforcement learning agent is feasible and effective. Throughout, we
use a new study on physical exercise recommendations to reduce pain in
endometriosis for illustration. We describe the design of a contextual bandit
recommendation agent and evaluate the agent in simulation studies. The results
show that adaptive interventions add complexity to the design and
implementation process, but have the potential to improve patients' benefits
even if only few observations are available. In order to quantify the expected
benefit, data from previous interventional studies is required. We expect our
approach to be transferable to other interventions and clinical interventions.",http://arxiv.org/pdf/2309.14156v1
2309.14149v1,cs.SD,Multi-Domain Adaptation by Self-Supervised Learning for Speaker Verification,2023-09-25 14:02:16+00:00,"In real-world applications, speaker recognition models often face various
domain-mismatch challenges, leading to a significant drop in performance.
Although numerous domain adaptation techniques have been developed to address
this issue, almost all present methods focus on a simple configuration where
the model is trained in one domain and deployed in another. However, real-world
environments are often complex and may contain multiple domains, making the
methods designed for one-to-one adaptation suboptimal. In our paper, we propose
a self-supervised learning method to tackle this multi-domain adaptation
problem. Building upon the basic self-supervised adaptation algorithm, we
designed three strategies to make it suitable for multi-domain adaptation: an
in-domain negative sampling strategy, a MoCo-like memory bank scheme, and a
CORAL-like distribution alignment. We conducted experiments using VoxCeleb2 as
the source domain dataset and CN-Celeb1 as the target multi-domain dataset. Our
results demonstrate that our method clearly outperforms the basic
self-supervised adaptation method, which simply treats the data of CN-Celeb1 as
a single domain. Importantly, the improvement is consistent in nearly all
in-domain tests and cross-domain tests, demonstrating the effectiveness of our
proposed method.",http://arxiv.org/pdf/2309.14149v1
2309.14146v1,cs.CL,Examining Temporal Bias in Abusive Language Detection,2023-09-25 13:59:39+00:00,"The use of abusive language online has become an increasingly pervasive
problem that damages both individuals and society, with effects ranging from
psychological harm right through to escalation to real-life violence and even
death. Machine learning models have been developed to automatically detect
abusive language, but these models can suffer from temporal bias, the
phenomenon in which topics, language use or social norms change over time. This
study aims to investigate the nature and impact of temporal bias in abusive
language detection across various languages and explore mitigation methods. We
evaluate the performance of models on abusive data sets from different time
periods. Our results demonstrate that temporal bias is a significant challenge
for abusive language detection, with models trained on historical data showing
a significant drop in performance over time. We also present an extensive
linguistic analysis of these abusive data sets from a diachronic perspective,
aiming to explore the reasons for language evolution and performance decline.
This study sheds light on the pervasive issue of temporal bias in abusive
language detection across languages, offering crucial insights into language
evolution and temporal bias mitigation.",http://arxiv.org/pdf/2309.14146v1
2309.14140v1,cond-mat.mtrl-sci,"Magnetic States and Electronic Properties of Manganese-Based Intermetallic Compounds Mn$_2$YAl and Mn$_3$Z (Y = V, Cr, Fe, Co, Ni; Z = Al, Ge, Sn, Si, Pt)",2023-09-25 13:51:30+00:00,"We present a brief review of experimental and theoretical papers on studies
of electron transport and magnetic properties in manganese-based compounds
Mn$_2$YZ and Mn$_3$Z (Y = V, Cr, Fe, Co, Ni, etc.; Z = Al, Ge, Sn, Si, Pt,
etc.). It has been shown that in the electronic subsystem of Mn$_2$YZ
compounds, the states of a half-metallic ferromagnet and a spin gapless
semiconductor can arise with the realization of various magnetic states, such
as a ferromagnet, a compensated ferrimagnet, and a frustrated antiferromagnet.
Binary compounds Mn$_3$Z have the properties of a half-metallic ferromagnet and
a topological semimetal with a large anomalous Hall effect, spin Hall effect,
spin Nernst effect, and thermal Hall effect. Their magnetic states are also
very diverse: from a ferrimagnet and an antiferromagnet to a compensated
ferrimagnet and a frustrated antiferromagnet, as well as an antiferromagnet
with a kagome-type lattice. It has been demonstrated that the electronic and
magnetic properties of such materials are very sensitive to external influences
(temperature, magnetic field, external pressure), as well as the processing
method (cast, rapidly quenched, nanostructured, etc.). Knowledge of the
regularities in the behavior of the electronic and magnetic characteristics of
Mn$_2$YAl and Mn$_3$Z compounds can be used for applications in micro- and
nanoelectronics and spintronics.",http://arxiv.org/pdf/2309.14140v1
2309.14138v1,astro-ph.SR,Stellar Activity Cycles,2023-09-25 13:50:03+00:00,"The magnetic field of the Sun is generated by internal dynamo process with a
cyclic period of 11 years or a 22 year magnetic cycle. The signatures of the
Sun's magnetic cycle are observed in the different layers of its atmosphere and
in its internal layers. In this review, we use the same diagnostics to
understand the magnetic cycles of other stars with the same internal structure
as the Sun. We review what is currently known about mapping the surface
magnetic fields, chromospheric and coronal indicators, cycles in photometry and
asteroseismology. We conclude our review with an outlook for the future.",http://arxiv.org/pdf/2309.14138v1
2309.14137v1,cs.CV,IEBins: Iterative Elastic Bins for Monocular Depth Estimation,2023-09-25 13:48:39+00:00,"Monocular depth estimation (MDE) is a fundamental topic of geometric computer
vision and a core technique for many downstream applications. Recently, several
methods reframe the MDE as a classification-regression problem where a linear
combination of probabilistic distribution and bin centers is used to predict
depth. In this paper, we propose a novel concept of iterative elastic bins
(IEBins) for the classification-regression-based MDE. The proposed IEBins aims
to search for high-quality depth by progressively optimizing the search range,
which involves multiple stages and each stage performs a finer-grained depth
search in the target bin on top of its previous stage. To alleviate the
possible error accumulation during the iterative process, we utilize a novel
elastic target bin to replace the original target bin, the width of which is
adjusted elastically based on the depth uncertainty. Furthermore, we develop a
dedicated framework composed of a feature extractor and an iterative optimizer
that has powerful temporal context modeling capabilities benefiting from the
GRU-based architecture. Extensive experiments on the KITTI, NYU-Depth-v2 and
SUN RGB-D datasets demonstrate that the proposed method surpasses prior
state-of-the-art competitors. The source code is publicly available at
https://github.com/ShuweiShao/IEBins.",http://arxiv.org/pdf/2309.14137v1
2309.14133v1,cond-mat.supr-con,Induced Superconductivity in Hybrid Au/YBa2Cu3O7-x Electrodes on Vicinal Substrates,2023-09-25 13:39:19+00:00,"Superconducting electrodes are an integral part of hybrid Josephson junctions
used in many applications including quantum technologies. We report on the
fabrication and characterization of superconducting hybrid Au/YBa2Cu3O7-x
(YBCO) electrodes on vicinal substrates. In these structures, superconducting
CuO2-planes face the gold film, resulting in a higher value and smaller
variation of the induced energy gap compared to the conventional Au/YBCO
electrodes based on films with the c-axis normal to the substrate surface.
Using scanning tunneling microscopy, we observe an energy gap of about 10-17
meV at the surface of the 15- nm-thick gold layer deposited in situ atop the
YBCO film. To study the origin of this gap, we fabricate nanoconstrictions from
the Au/YBCO heterostructure and measure their electrical transport
characteristics. The conductance of the nanoconstrictions shows a series of
dips due to multiple Andreev reflections in YBCO and gold providing clear
evidence of the superconducting nature of the gap in gold. We consider the
Au/YBCO electrodes to be a versatile platform for hybrid Josephson devices with
a high operating temperature.",http://arxiv.org/pdf/2309.14133v1
2309.14130v1,cs.SD,On the Relation between Internal Language Model and Sequence Discriminative Training for Neural Transducers,2023-09-25 13:35:28+00:00,"Internal language model (ILM) subtraction has been widely applied to improve
the performance of the RNN-Transducer with external language model (LM) fusion
for speech recognition. In this work, we show that sequence discriminative
training has a strong correlation with ILM subtraction from both theoretical
and empirical points of view. Theoretically, we derive that the global optimum
of maximum mutual information (MMI) training shares a similar formula as ILM
subtraction. Empirically, we show that ILM subtraction and sequence
discriminative training achieve similar performance across a wide range of
experiments on Librispeech, including both MMI and minimum Bayes risk (MBR)
criteria, as well as neural transducers and LMs of both full and limited
context. The benefit of ILM subtraction also becomes much smaller after
sequence discriminative training. We also provide an in-depth study to show
that sequence discriminative training has a minimal effect on the commonly used
zero-encoder ILM estimation, but a joint effect on both encoder and prediction
+ joint network for posterior probability reshaping including both ILM and
blank suppression.",http://arxiv.org/pdf/2309.14130v1
2309.14129v1,eess.AS,Speaker anonymization using neural audio codec language models,2023-09-25 13:32:09+00:00,"The vast majority of approaches to speaker anonymization involve the
extraction of fundamental frequency estimates, linguistic features and a
speaker embedding which is perturbed to obfuscate the speaker identity before
an anonymized speech waveform is resynthesized using a vocoder. Recent work has
shown that x-vector transformations are difficult to control consistently:
other sources of speaker information contained within fundamental frequency and
linguistic features are re-entangled upon vocoding, meaning that anonymized
speech signals still contain speaker information. We propose an approach based
upon neural audio codecs (NACs), which are known to generate high-quality
synthetic speech when combined with language models. NACs use quantized codes,
which are known to effectively bottleneck speaker-related information: we
demonstrate the potential of speaker anonymization systems based on NAC
language modeling by applying the evaluation framework of the Voice Privacy
Challenge 2022.",http://arxiv.org/pdf/2309.14129v1
2309.14127v1,math.RA,Dual digraphs of finite meet-distributive and modular lattices,2023-09-25 13:29:15+00:00,"We describe the digraphs that are dual representations of finite lattices
satisfying conditions related to meet-distributivity and modularity. This is
done using the dual digraph representation of finite lattices by Craig, Gouveia
and Haviar (2015). These digraphs, known as TiRS digraphs, have their origins
in the dual representations of lattices by Urquhart (1978) and Plo\v{s}\v{c}ica
(1995). We describe two properties of finite lattices which are weakenings of
(upper) semimodularity and lower semimodularity respectively, and then show how
these properties have a simple description in the dual digraphs. Combined with
previous work on dual digraphs of semidistributive lattices (2022), it leads to
a dual representation of finite meet-distributive lattices. This provides a
natural link to finite convex geometries. In addition, we present two
sufficient conditions on a finite TiRS digraph for its dual lattice to be
modular. We close by posing four open problems.",http://arxiv.org/pdf/2309.14127v1
2309.14122v1,cs.CV,SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution,2023-09-25 13:20:15+00:00,"Advanced text-to-image models such as DALL-E 2 and Midjourney possess the
capacity to generate highly realistic images, raising significant concerns
regarding the potential proliferation of unsafe content. This includes adult,
violent, or deceptive imagery of political figures. Despite claims of rigorous
safety mechanisms implemented in these models to restrict the generation of
not-safe-for-work (NSFW) content, we successfully devise and exhibit the first
prompt attacks on Midjourney, resulting in the production of abundant
photorealistic NSFW images. We reveal the fundamental principles of such prompt
attacks and suggest strategically substituting high-risk sections within a
suspect prompt to evade closed-source safety measures. Our novel framework,
SurrogatePrompt, systematically generates attack prompts, utilizing large
language models, image-to-text, and image-to-image modules to automate attack
prompt creation at scale. Evaluation results disclose an 88% success rate in
bypassing Midjourney's proprietary safety filter with our attack prompts,
leading to the generation of counterfeit images depicting political figures in
violent scenarios. Both subjective and objective assessments validate that the
images generated from our attack prompts present considerable safety hazards.",http://arxiv.org/pdf/2309.14122v1
2309.14120v1,math.ST,Regression with Variable Dimension Covariates,2023-09-25 13:19:53+00:00,"Regression is one of the most fundamental statistical inference problems. A
broad definition of regression problems is as estimation of the distribution of
an outcome using a family of probability models indexed by covariates. Despite
the ubiquitous nature of regression problems and the abundance of related
methods and results there is a surprising gap in the literature. There are no
well established methods for regression with a varying dimension covariate
vectors, despite the common occurrence of such problems. In this paper we
review some recent related papers proposing varying dimension regression by way
of random partitions.",http://arxiv.org/pdf/2309.14120v1
2309.14109v1,eess.AS,Haha-Pod: An Attempt for Laughter-based Non-Verbal Speaker Verification,2023-09-25 13:04:39+00:00,"It is widely acknowledged that discriminative representation for speaker
verification can be extracted from verbal speech. However, how much speaker
information that non-verbal vocalization carries is still a puzzle. This paper
explores speaker verification based on the most ubiquitous form of non-verbal
voice, laughter. First, we use a semi-automatic pipeline to collect a new
Haha-Pod dataset from open-source podcast media. The dataset contains over 240
speakers' laughter clips with corresponding high-quality verbal speech. Second,
we propose a Two-Stage Teacher-Student (2S-TS) framework to minimize the
within-speaker embedding distance between verbal and non-verbal (laughter)
signals. Considering Haha-Pod as a test set, two trials (S2L-Eval) are designed
to verify the speaker's identity through laugh sounds. Experimental results
demonstrate that our method can significantly improve the performance of the
S2L-Eval test set with only a minor degradation on the VoxCeleb1 test set. The
Haha-Pod dataset is open to access on
https://drive.google.com/file/d/1J-HBRTsm_yWrcbkXupy-tiWRt5gE2LzG/view?usp=drive_link.",http://arxiv.org/pdf/2309.14109v1
2309.14107v1,eess.AS,Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech,2023-09-25 13:00:33+00:00,"Automatic detection and severity level classification of dysarthria directly
from acoustic speech signals can be used as a tool in medical diagnosis. In
this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor
to build detection and severity level classification systems for dysarthric
speech. The experiments were carried out with the popularly used UA-speech
database. In the detection experiments, the results revealed that the best
performance was obtained using the embeddings from the first layer of the
wav2vec model that yielded an absolute improvement of 1.23% in accuracy
compared to the best performing baseline feature (spectrogram). In the studied
severity level classification task, the results revealed that the embeddings
from the final layer gave an absolute improvement of 10.62% in accuracy
compared to the best baseline features (mel-frequency cepstral coefficients).",http://arxiv.org/pdf/2309.14107v1
2309.14098v1,cs.CY,Computer Science Framework to Teach Community-Based Environmental Literacy and Data Literacy to Diverse Students,2023-09-25 12:49:39+00:00,"This study introduces an integrated curriculum designed to empower
underrepresented students by combining environmental literacy, data literacy,
and computer science. The framework promotes environmental awareness, data
literacy, and civic engagement using a culturally sustaining approach. This
integrated curriculum is embedded with resources to support language
development, technology skills, and coding skills to accommodate the diverse
needs of students. To evaluate the effectiveness of this curriculum, we
conducted a pilot study in a 5th-grade special education classroom with
multilingual Latinx students. During the pilot, students utilized Scratch, a
block-based coding language, to create interactive projects that showcased
locally collected data, which they used to communicate environmental challenges
and propose solutions to community leaders. This approach allowed students to
engage with environmental literacy at a deeper level, harnessing their
creativity and community knowledge in the digital learning environment.
Moreover, this curriculum equipped students with the skills to critically
analyze political and socio-cultural factors impacting environmental
sustainability. Students not only gained knowledge within the classroom but
also applied their learning to address real environmental issues within their
community. The results of the pilot study underscore the efficacy of this
integrated approach.",http://arxiv.org/pdf/2309.14098v1
2309.14097v1,cs.OH,How do users design scientific workflows? The Case of Snakemake,2023-09-25 12:48:49+00:00,"Scientific workflows automate the analysis of large-scale scientific data,
fostering the reuse of data processing operators as well as the reproducibility
and traceability of analysis results. In exploratory research, however,
workflows are continuously adapted, utilizing a wide range of tools and
software libraries, to test scientific hypotheses. Script-based workflow
engines cater to the required flexibility through direct integration of
programming primitives but lack abstractions for interactive exploration of the
workflow design by a user during workflow execution. To derive requirements for
such interactive workflows, we conduct an empirical study on the use of
Snakemake, a popular Python-based workflow engine. Based on workflows collected
from 1602 GitHub repositories, we present insights on common structures of
Snakemake workflows, as well as the language features typically adopted in
their specification.",http://arxiv.org/pdf/2309.14097v1
2309.14095v1,astro-ph.GA,Velocity-resolved high-J CO emission from massive star-forming clumps,2023-09-25 12:42:32+00:00,"(Abridged) Context. Massive star formation is associated with energetic
processes, which result in significant gas cooling via far-infrared (IR) lines.
Velocity-resolved observations can constrain the kinematics of the gas,
allowing the identification of the physical mechanisms responsible for gas
heating. Aims. Our aim is to quantify far-infrared CO line emission toward
high-mass star-forming regions, identify the high-velocity gas component
associated with outflows, and estimate the physical conditions required for the
excitation of the observed lines. Methods. Velocity-resolved SOFIA/GREAT
spectra of 13 high-mass star forming clumps of various luminosities and
evolutionary stages are studied using CO 11-10 and 16-15 lines. Results. All
targets show strong high-J CO emission in the far-IR, characterized by broad
line wings associated with outflows, thereby significantly increasing the
sample of sources with velocity-resolved high-J CO spectra. The contribution of
the emission in the line wings does not correlate with the envelope mass or
evolutionary stage. Gas rotational temperatures cover a narrow range of 120-220
K for the line wings. The non-LTE radiative transfer models indicate gas
densities of 1e5-1e7 cm-3 and N(CO) of 1e17- 1e18 cm-2, similar to physical
conditions in deeply-embedded low- and high-mass protostars. The
velocity-integrated CO line fluxes correlate with the bolometric luminosity
over 7 orders of magnitude including data on the low-mass protostars,
suggesting similar processes are responsible for the high-J CO excitation over
a significant range of physical scales. Conclusions. Velocity-resolved line
profiles allow the detection of outflows toward massive star-forming clumps
spanning a broad range of evolutionary stages. The lack of clear evolutionary
trends suggest that mass accretion and ejection prevail during the entire
lifetime of star-forming clumps.",http://arxiv.org/pdf/2309.14095v1
2309.14094v1,cs.SD,VoiceLens: Controllable Speaker Generation and Editing with Flow,2023-09-25 12:37:03+00:00,"Currently, many multi-speaker speech synthesis and voice conversion systems
address speaker variations with an embedding vector. Modeling it directly
allows new voices outside of training data to be synthesized. GMM based
approaches such as Tacospawn are favored in literature for this generation
task, but there are still some limitations when difficult conditionings are
involved. In this paper, we propose VoiceLens, a semi-supervised flow-based
approach, to model speaker embedding distributions for multi-conditional
speaker generation. VoiceLens maps speaker embeddings into a combination of
independent attributes and residual information. It allows new voices
associated with certain attributes to be \textit{generated} for existing TTS
models, and attributes of known voices to be meaningfully \textit{edited}. We
show in this paper, VoiceLens displays an unconditional generation capacity
that is similar to Tacospawn while obtaining higher controllability and
flexibility when used in a conditional manner. In addition, we show
synthesizing less noisy speech from known noisy speakers without re-training
the TTS model is possible via solely editing their embeddings with a SNR
conditioned VoiceLens model. Demos are available at
sos1sos2sixteen.github.io/voicelens.",http://arxiv.org/pdf/2309.14094v1
2309.14089v1,eess.AS,BiSinger: Bilingual Singing Voice Synthesis,2023-09-25 12:31:05+00:00,"Although Singing Voice Synthesis (SVS) has made great strides with
Text-to-Speech (TTS) techniques, multilingual singing voice modeling remains
relatively unexplored. This paper presents BiSinger, a bilingual SVS system for
English and Chinese Mandarin. Current systems require separate models per
language and cannot accurately represent both Chinese and English, hindering
code-switch SVS. To address this gap, we design a shared representation between
Chinese and English singing voices, achieved by using the CMU dictionary with
mapping rules. We fuse monolingual singing datasets with established singing
voice conversion techniques to generate bilingual singing voices while also
exploring the potential use of bilingual speech data. Experiments affirm that
our language-independent representation and incorporation of related datasets
enable a single model with enhanced performance in English and code-switch SVS
while maintaining Chinese song performance. Audio samples are available at
https://bisinger-svs.github.io.",http://arxiv.org/pdf/2309.14089v1
2309.14088v1,cs.LG,REPA: Client Clustering without Training and Data Labels for Improved Federated Learning in Non-IID Settings,2023-09-25 12:30:43+00:00,"Clustering clients into groups that exhibit relatively homogeneous data
distributions represents one of the major means of improving the performance of
federated learning (FL) in non-independent and identically distributed
(non-IID) data settings. Yet, the applicability of current state-of-the-art
approaches remains limited as these approaches cluster clients based on
information, such as the evolution of local model parameters, that is only
obtainable through actual on-client training. On the other hand, there is a
need to make FL models available to clients who are not able to perform the
training themselves, as they do not have the processing capabilities required
for training, or simply want to use the model without participating in the
training. Furthermore, the existing alternative approaches that avert the
training still require that individual clients have a sufficient amount of
labeled data upon which the clustering is based, essentially assuming that each
client is a data annotator. In this paper, we present REPA, an approach to
client clustering in non-IID FL settings that requires neither training nor
labeled data collection. REPA uses a novel supervised autoencoder-based method
to create embeddings that profile a client's underlying data-generating
processes without exposing the data to the server and without requiring local
training. Our experimental analysis over three different datasets demonstrates
that REPA delivers state-of-the-art model performance while expanding the
applicability of cluster-based FL to previously uncovered use cases.",http://arxiv.org/pdf/2309.14088v1
2309.14083v1,physics.optics,High-order aberrations of vortex constellations,2023-09-25 12:22:34+00:00,"When reflected from an interface, a laser beam generally drifts and tilts
away from the path predicted by ray optics, an intriguing consequence of its
finite transverse extent. Such beam shifts manifest more dramatically for
structured light fields, and in particular for optical vortices. Upon
reflection, a field containing a high-order optical vortex is expected to
experience not only geometrical shifts, but an additional splitting of its
high-order vortex into a constellation of unit-charge vortices, a phenomenon
known as topological aberration. In this article, we report on the first direct
observation of the topological aberration effect, measured through the
transformation of a vortex constellation upon reflection. We develop a general
theoretical framework to study topological aberrations in terms of the
elementary symmetric polynomials of the coordinates of a vortex constellation,
a mathematical abstraction which we prove to be the physical quantity of
interest. Using this approach, we are able to verify experimentally the
aberration of constellations of up to three vortices reflected from a thin
metallic film. Our work not only deepens the understanding of the reflection of
naturally occurring structured light fields such as vortex constellations but
also sets forth a potential method for studying the interaction of twisted
light fields with matter.",http://arxiv.org/pdf/2309.14083v1
2309.14080v1,eess.AS,Analysis and Detection of Pathological Voice using Glottal Source Features,2023-09-25 12:14:25+00:00,"Automatic detection of voice pathology enables objective assessment and
earlier intervention for the diagnosis. This study provides a systematic
analysis of glottal source features and investigates their effectiveness in
voice pathology detection. Glottal source features are extracted using glottal
flows estimated with the quasi-closed phase (QCP) glottal inverse filtering
method, using approximate glottal source signals computed with the zero
frequency filtering (ZFF) method, and using acoustic voice signals directly. In
addition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from
the glottal source waveforms computed by QCP and ZFF to effectively capture the
variations in glottal source spectra of pathological voice. Experiments were
carried out using two databases, the Hospital Universitario Principe de
Asturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database.
Analysis of features revealed that the glottal source contains information that
discriminates normal and pathological voice. Pathology detection experiments
were carried out using support vector machine (SVM). From the detection
experiments it was observed that the performance achieved with the studied
glottal source features is comparable or better than that of conventional MFCCs
and perceptual linear prediction (PLP) features. The best detection performance
was achieved when the glottal source features were combined with the
conventional MFCCs and PLP features, which indicates the complementary nature
of the features.",http://arxiv.org/pdf/2309.14080v1
2309.14074v2,cs.DC,FlexCast: genuine overlay-based atomic multicast,2023-09-25 12:09:54+00:00,"Atomic multicast is a communication abstraction where messages are propagated
to groups of processes with reliability and order guarantees. Atomic multicast
is at the core of strongly consistent storage and transactional systems. This
paper presents FlexCast, the first genuine overlay-based atomic multicast
protocol. Genuineness captures the essence of atomic multicast in that only the
sender of a message and the message's destinations coordinate to order the
message, leading to efficient protocols. Overlay-based protocols restrict how
process groups can communicate. Limiting communication leads to simpler
protocols and reduces the amount of information each process must keep about
the rest of the system. FlexCast implements genuine atomic multicast using a
complete DAG overlay. We experimentally evaluate FlexCast in a geographically
distributed environment using gTPC-C, a variation of the TPC-C benchmark that
takes into account geographical distribution and locality. We show that, by
exploiting genuineness and workload locality, FlexCast outperforms
well-established atomic multicast protocols without the inherent communication
overhead of state-of-the-art non-genuine multicast protocols.",http://arxiv.org/pdf/2309.14074v2
2309.14069v1,hep-th,Rényi Topology of Charged-flat Black Hole: Hawking-Page and Van-der-Waals Phase Transitions,2023-09-25 12:04:03+00:00,"In this paper, we extend the proposed setup in [1,2] for finding the
topological charges associated with the Hawking-Page and Van-der-Waals
transition points as well as equilibrium phases to catch the nonextensive
nature of the black hole entropy, Rigorously speaking we incorporate the
R\'enyi statistics formalism in off-shell Bragg-Williams free energy landscape
to examine topologically the Hawking-Page phase transition related to the
uncharged/charged-flat black hole in grand canonical and the Van-der- Waals
transition in the canonical ensemble and where a vortex/anti-vortex structure
is found. For this purpose, we introduce three mappings, the $\psi$- and
$\xi$-mapping, for phase transitions classification and the $\eta$-mapping for
equilibrium phases classification. We found that Hawking-Page and Van-der-Waals
phase transitions belong to different topological classes and exhibit an
interplay of total charge values hinting to a possible new correspondence. Our
topological study provides further substantiation for a possible conjecture
positing a correspondence between the thermodynamic characteristics of black
holes in asymptotically flat spacetime using R\'enyi statistics, and those in
asymptotically Anti-de-Sitter spacetime employing Gibbs-Boltzmann statistics.",http://arxiv.org/pdf/2309.14069v1
2309.14067v1,cond-mat.mes-hall,Charge dynamics in the 2D/3D semiconductor heterostructure WSe$_2$/GaAs,2023-09-25 12:03:24+00:00,"Understanding the relaxation and recombination processes of excited states in
two-dimensional (2D)/three-dimensional (3D) semiconductor heterojunctions is
essential for developing efficient optical and (opto)electronic devices which
integrate new 2D materials with more conventional 3D ones. In this work, we
unveil the carrier dynamics and charge transfer in a monolayer of WSe$_2$ on a
GaAs substrate. We use time-resolved differential reflectivity to study the
charge relaxation processes involved in the junction and how they change when
compared to an electrically decoupled heterostructure, WSe$_2$/hBN/GaAs. We
observe that the monolayer in direct contact with the GaAs substrate presents
longer optically-excited carrier lifetimes (3.5 ns) when compared with the
hBN-isolated region (1 ns), consistent with a strong reduction of radiative
decay and a fast charge transfer of a single polarity. Through low-temperature
measurements, we find evidence of a type-II band alignment for this
heterostructure with an exciton dissociation that accumulates electrons in the
GaAs and holes in the WSe$_2$. The type-II band alignment and fast
photo-excited carrier dissociation shown here indicate that WSe$_2$/GaAs is a
promising junction for new photovoltaic and other optoelectronic devices,
making use of the best properties of new (2D) and conventional (3D)
semiconductors.",http://arxiv.org/pdf/2309.14067v1
2309.14061v1,gr-qc,Physically consistent gravitational waveform for capturing beyond general relativity effects in the compact object merger phase,2023-09-25 11:53:59+00:00,"The merger phase of compact binary coalescences is the strongest gravity
regime that can be observed. To test the validity of general relativity (GR) in
strong gravitational fields, we propose a gravitational waveform parameterized
for deviations from GR in the dynamical and nonlinear regime of gravity. Our
fundamental idea is that perturbative modifications to a GR waveform can
capture possible deviations in the merger phase that are difficult to model in
a specific theory of gravity. One of notable points is that our waveform is
physically consistent in the sense that the additional radiative losses of
energy and angular momentum associated with beyond-GR modifications are
included. Our prescription to ensure physical consistency in the whole
coalescence process is expected to be applicable to any deviation from the
standard model of compact binary coalescence, such as the extended models of
gravity or the environmental effects of compact objects, as long as
perturbative modifications are considered. Based on the Fisher analysis and the
compatibility with Einstein-dilaton Gauss-Bonnet waveforms, we show that our
parameterization is a physically-consistent minimal one that captures the
deviations in the nonlinear regime.",http://arxiv.org/pdf/2309.14061v1
2309.14059v1,cs.IT,Single-Antenna Jammers in MIMO-OFDM Can Resemble Multi-Antenna Jammers,2023-09-25 11:53:32+00:00,"In multiple-input multiple-output (MIMO) wireless systems with frequency-flat
channels, a single-antenna jammer causes receive interference that is confined
to a one-dimensional subspace. Such a jammer can thus be nulled using linear
spatial filtering at the cost of one degree of freedom. Frequency-selective
channels are often transformed into multiple frequency-flat subcarriers with
orthogonal frequency-division multiplexing (OFDM). We show that when a
single-antenna jammer violates the OFDM protocol by not sending a cyclic
prefix, the interference received on each subcarrier by a multi-antenna
receiver is, in general, not confined to a subspace of dimension one (as a
single-antenna jammer in a frequency-flat scenario would be), but of dimension
L, where L is the jammer's number of channel taps. In MIMO-OFDM systems, a
single-antenna jammer can therefore resemble an L-antenna jammer. Simulations
corroborate our theoretical results. These findings imply that mitigating
jammers with large delay spread through linear spatial filtering is infeasible.
We discuss some (im)possibilities for the way forward.",http://arxiv.org/pdf/2309.14059v1
2309.14050v2,eess.SY,NNgTL: Neural Network Guided Optimal Temporal Logic Task Planning for Mobile Robots,2023-09-25 11:24:40+00:00,"In this work, we investigate task planning for mobile robots under linear
temporal logic (LTL) specifications. This problem is particularly challenging
when robots navigate in continuous workspaces due to the high computational
complexity involved. Sampling-based methods have emerged as a promising avenue
for addressing this challenge by incrementally constructing random trees,
thereby sidestepping the need to explicitly explore the entire state-space.
However, the performance of this sampling-based approach hinges crucially on
the chosen sampling strategy, and a well-informed heuristic can notably enhance
sample efficiency. In this work, we propose a novel neural-network guided
(NN-guided) sampling strategy tailored for LTL planning. Specifically, we
employ a multi-modal neural network capable of extracting features concurrently
from both the workspace and the B\""{u}chi automaton. This neural network
generates predictions that serve as guidance for random tree construction,
directing the sampling process toward more optimal directions. Through
numerical experiments, we compare our approach with existing methods and
demonstrate its superior efficiency, requiring less than 15% of the time of the
existing methods to find a feasible solution.",http://arxiv.org/pdf/2309.14050v2
2309.14049v1,cs.HC,How Novices Use LLM-Based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment,2023-09-25 11:24:30+00:00,"As Large Language Models (LLMs) gain in popularity, it is important to
understand how novice programmers use them. We present a thematic analysis of
33 learners, aged 10-17, independently learning Python through 45
code-authoring tasks using Codex, an LLM-based code generator. We explore
several questions related to how learners used these code generators and
provide an analysis of the properties of the written prompts and the generated
code. Specifically, we explore (A) the context in which learners use Codex, (B)
what learners are asking from Codex, (C) properties of their prompts in terms
of relation to task description, language, and clarity, and prompt crafting
patterns, (D) the correctness, complexity, and accuracy of the AI-generated
code, and (E) how learners utilize AI-generated code in terms of placement,
verification, and manual modifications. Furthermore, our analysis reveals four
distinct coding approaches when writing code with an AI code generator: AI
Single Prompt, where learners prompted Codex once to generate the entire
solution to a task; AI Step-by-Step, where learners divided the problem into
parts and used Codex to generate each part; Hybrid, where learners wrote some
of the code themselves and used Codex to generate others; and Manual coding,
where learners wrote the code themselves. The AI Single Prompt approach
resulted in the highest correctness scores on code-authoring tasks, but the
lowest correctness scores on subsequent code-modification tasks during
training. Our results provide initial insight into how novice learners use AI
code generators and the challenges and opportunities associated with
integrating them into self-paced learning environments. We conclude with
various signs of over-reliance and self-regulation, as well as opportunities
for curriculum and tool development.",http://arxiv.org/pdf/2309.14049v1
2309.14048v2,cs.LO,"Synchronous Agents, Verification, and Blame -- A Deontic View",2023-09-25 11:23:59+00:00,"A question we can ask of multi-agent systems is whether the agents'
collective interaction satisfies particular goals or specifications, which can
be either individual or collective. When a collaborative goal is not reached,
or a specification is violated, a pertinent question is whether any agent is to
blame. This paper considers a two-agent synchronous setting and a formal
language to specify when agents' collaboration is required. We take a deontic
approach and use obligations, permissions, and prohibitions to capture notions
of non-interference between agents. We also handle reparations, allowing
violations to be corrected or compensated. We give trace semantics to our
logic, and use it to define blame assignment for violations. We give an
automaton construction for the logic, which we use as the base for model
checking and blame analysis. We also further provide quantitative semantics
that is able to compare different interactions in terms of the required
reparations.",http://arxiv.org/pdf/2309.14048v2
2309.14041v1,nucl-ex,Beam Charge Asymmetries for Deeply Virtual Compton Scattering on the Proton at CLAS12,2023-09-25 11:19:01+00:00,"The parameterization of the nucleon structure through Generalized Parton
Distributions (GPDs) shed a new light on the nucleon internal dynamics. For its
direct interpretation, Deeply Virtual Compton Scattering (DVCS) is the golden
channel for GPDs investigation. The DVCS process interferes with the
Bethe-Heitler (BH) mechanism to constitute the leading order amplitude of the
$eN \to eN\gamma$ process. The study of the $ep\gamma$ reaction with polarized
positron and electron beams gives a complete set of unique observables to
unravel the different contributions to the $ep \gamma$ cross section. This
separates the different reaction amplitudes, providing a direct access to their
real and imaginary parts which procures crucial constraints on the model
dependences and associated systematic uncertainties on GPDs extraction. The
real part of the BH-DVCS interference amplitude is particularly sensitive to
the $D$-term which parameterizes the Gravitational Form Factors of the nucleon.
The separation of the imaginary parts of the interference and DVCS amplitudes
provides insights on possible higher-twist effects. We propose to measure the
unpolarized and polarized Beam Charge Asymmetries (BCAs) of the $\vec{e}^{\pm}p
\to e^{\pm}p \gamma$ process on an unpolarized hydrogen target with {\tt
CLAS12}, using polarized positron and electron beams at 10.6~GeV. The azimuthal
and $t$-dependences of the unpolarized and polarized BCAs will be measured over
a large $(x_B,Q^2)$ phase space using a 100 day run with a luminosity of
0.66$\times 10^{35}$cm$^{-2}\cdot$s$^{-1}$.",http://arxiv.org/pdf/2309.14041v1
2309.14040v1,physics.flu-dyn,Mixing as a correlated aggregation process,2023-09-25 11:18:03+00:00,"Mixing describes the process by which scalars, such as solute concentration
or fluid temperature, evolve from an initial heterogeneous state to uniformity
under the stirring action of a fluid flow. Mixing occurs initially through the
formation of scalar lamellae as a result of fluid stretching and later by their
coalescence due to molecular diffusion. Owing to the linearity of the
advection-diffusion equation, scalar coalescence can be envisioned as an
aggregation process. While random aggregation models have been shown to capture
scalar mixing across a range of turbulent flows, we demonstrate here that they
are not accurate for most chaotic flows. In particular, we show that the
spatial distribution of the number of lamellae in aggregates is highly
correlated with their elongation and is also influenced by the fractal geometry
that arises from the chaotic flow. The presence of correlations makes mixing
less efficient than a completely random aggregation process because lamellae
with similar elongations and scalar levels tend to remain isolated from each
other. Based on these observations, we propose a correlated aggregation
framework that captures the asymptotic mixing dynamics of chaotic flows and
predicts the evolution of the scalar pdf based on the flow stretching
statistics. We show that correlated aggregation is uniquely determined by a
single exponent which quantifies the effective number of random aggregation
events, and is dependent on the fractal dimension of the flow. These findings
expand aggregation theories to a larger class of systems, which have relevance
to various fundamental and applied mixing problems.",http://arxiv.org/pdf/2309.14040v1
2309.14034v1,cs.DM,A unified worst case for classical simplex and policy iteration pivot rules,2023-09-25 11:01:20+00:00,"We construct a family of Markov decision processes for which the policy
iteration algorithm needs an exponential number of improving switches with
Dantzig's rule, with Bland's rule, and with the Largest Increase pivot rule.
This immediately translates to a family of linear programs for which the
simplex algorithm needs an exponential number of pivot steps with the same
three pivot rules. Our results yield a unified construction that simultaneously
reproduces well-known lower bounds for these classical pivot rules, and we are
able to infer that any (deterministic or randomized) combination of them cannot
avoid an exponential worst-case behavior. Regarding the policy iteration
algorithm, pivot rules typically switch multiple edges simultaneously and our
lower bound for Dantzig's rule and the Largest Increase rule, which perform
only single switches, seem novel. Regarding the simplex algorithm, the
individual lower bounds were previously obtained separately via deformed
hypercube constructions. In contrast to previous bounds for the simplex
algorithm via Markov decision processes, our rigorous analysis is reasonably
concise.",http://arxiv.org/pdf/2309.14034v1
2309.14030v1,cs.HC,DeWave: Discrete EEG Waves Encoding for Brain Dynamics to Text Translation,2023-09-25 10:52:28+00:00,"The translation of brain dynamics into natural language is pivotal for
brain-computer interfaces (BCIs), a field that has seen substantial growth in
recent years. With the swift advancement of large language models, such as
ChatGPT, the need to bridge the gap between the brain and languages becomes
increasingly pressing. Current methods, however, require eye-tracking fixations
or event markers to segment brain dynamics into word-level features, which can
restrict the practical application of these systems. These event markers may
not be readily available or could be challenging to acquire during real-time
inference, and the sequence of eye fixations may not align with the order of
spoken words. To tackle these issues, we introduce a novel framework, DeWave,
that integrates discrete encoding sequences into open-vocabulary EEG-to-text
translation tasks. DeWave uses a quantized variational encoder to derive
discrete codex encoding and align it with pre-trained language models. This
discrete codex representation brings forth two advantages: 1) it alleviates the
order mismatch between eye fixations and spoken words by introducing text-EEG
contrastive alignment training, and 2) it minimizes the interference caused by
individual differences in EEG waves through an invariant discrete codex. Our
model surpasses the previous baseline (40.1 and 31.7) by 3.06% and 6.34%,
respectively, achieving 41.35 BLEU-1 and 33.71 Rouge-F on the ZuCo Dataset.
Furthermore, this work is the first to facilitate the translation of entire EEG
signal periods without needing word-level order markers (e.g., eye fixations),
scoring 20.5 BLEU-1 and 29.5 Rouge-1 on the ZuCo Dataset, respectively. Codes
and the final paper will be public soon.",http://arxiv.org/pdf/2309.14030v1
2309.14023v1,cond-mat.quant-gas,Natural orbitals and their occupation numbers in a non-interacting two-anyon system in the magnetic gauge,2023-09-25 10:37:36+00:00,"We investigate the properties of natural orbitals and their occupation
numbers of the ground state of two non-interacting anyons characterised by the
fractional exchange parameter $\alpha$ and confined in a harmonic trap. We work
in the boson magnetic gauge where the anyons are modelled as composite bosons
with magnetic flux quanta attached to their positions. We derive an asymptotic
form of the weakly occupied natural orbitals, and show that their corresponding
(ordered descendingly) occupation numbers decay according to the power law
$n^{-(4+2\alpha)}$, where $n$ is the index of the natural orbital. We find
remarkable numerical agreement of the theory with the natural orbitals and
their occupation numbers computed from the spectral decomposition of the
system's wavefunction. We explain that the same results apply to the fermion
magnetic gauge.",http://arxiv.org/pdf/2309.14023v1
2309.14016v1,cs.NI,Virtuoso: High Resource Utilization and μs-scale Performance Isolation in a Shared Virtual Machine TCP Network Stack,2023-09-25 10:29:06+00:00,"Virtualization improves resource efficiency and ensures security and
performance isolation for cloud applications. To that end, operators today use
a layered architecture that runs a separate network stack instance in each VM
and container connected to a separate virtual switch. Decoupling through
layering reduces complexity, but induces performance and resource overheads
that are at odds with increasing demands for network bandwidth, communication
requirements for large distributed applications, and low latency.
  We present Virtuoso, a new software networking stack for VMs and containers.
Virtuoso performs a fundamental re-organization of the networking stack to
maximize CPU utilization, enforce isolation, and minimize networking stack
overheads. We maximize utilization by running one elastically shared network
stack instance on dedicated cores; we enforce isolation by performing central
and fine-grained per-packet resource accounting and scheduling; we reduce
overheads by building a single-layer data path with a one-shot fast-path
incorporating all processing from the TCP transport layer through network
virtualization and virtual switching. Virtuoso improves resource utilization by
up to 50%, latencies by up to 42% compared to other virtualized network stacks
without sacrificing isolation, and keeps processing overhead within 11.5% of
unvirtualized network stacks.",http://arxiv.org/pdf/2309.14016v1
2309.14012v1,eess.SP,Beam Squint Assisted User Localization in Near-Field Integrated Sensing and Communications Systems,2023-09-25 10:26:17+00:00,"Integrated sensing and communication (ISAC) has been regarded as a key
technology for 6G wireless communications, in which large-scale multiple input
and multiple output (MIMO) array with higher and wider frequency bands will be
adopted. However, recent studies show that the beam squint phenomenon can not
be ignored in wideband MIMO system, which generally deteriorates the
communications performance. In this paper, we find that with the aid of
true-time-delay lines (TTDs), the range and trajectory of the beam squint in
near-field communications systems can be freely controlled, and hence it is
possible to reversely utilize the beam squint for user localization. We derive
the trajectory equation for near-field beam squint points and design a way to
control such trajectory. With the proposed design, beamforming from different
subcarriers would purposely point to different angles and different distances,
such that users from different positions would receive the maximum power at
different subcarriers. Hence, one can simply localize multiple users from the
beam squint effect in frequency domain, and thus reduce the beam sweeping
overhead as compared to the conventional time domain beam search based
approach. Furthermore, we utilize the phase difference of the maximum power
subcarriers received by the user at different frequencies in several times beam
sweeping to obtain a more accurate distance estimation result, ultimately
realizing high accuracy and low beam sweeping overhead user localization.
Simulation results demonstrate the effectiveness of the proposed schemes.",http://arxiv.org/pdf/2309.14012v1
2309.14011v1,cs.LO,A Truly Concurrent Semantics for Reversible CCS,2023-09-25 10:25:43+00:00,"Reversible CCS (RCCS) is a well-established, formal model for reversible
communicating systems, which has been built on top of the classical Calculus of
Communicating Systems (CCS). In its original formulation, each CCS process is
equipped with a memory that records its performed actions, which is then used
to reverse computations. More recently, abstract models for RCCS have been
proposed in the literature, basically, by directly associating RCCS processes
with (reversible versions of) event structures. In this paper we propose a
different abstract model: starting from one of the well-known encoding of CCS
into Petri nets we apply a recently proposed approach to incorporate
causally-consistent reversibility to Petri nets, obtaining as result the
(reversible) net counterpart of every RCCS term.",http://arxiv.org/pdf/2309.14011v1
2309.14010v1,cs.CV,Variational Inference for Scalable 3D Object-centric Learning,2023-09-25 10:23:40+00:00,"We tackle the task of scalable unsupervised object-centric representation
learning on 3D scenes. Existing approaches to object-centric representation
learning show limitations in generalizing to larger scenes as their learning
processes rely on a fixed global coordinate system. In contrast, we propose to
learn view-invariant 3D object representations in localized object coordinate
systems. To this end, we estimate the object pose and appearance representation
separately and explicitly map object representations across views while
maintaining object identities. We adopt an amortized variational inference
pipeline that can process sequential input and scalably update object latent
distributions online. To handle large-scale scenes with a varying number of
objects, we further introduce a Cognitive Map that allows the registration and
query of objects on a per-scene global map to achieve scalable representation
learning. We explore the object-centric neural radiance field (NeRF) as our 3D
scene representation, which is jointly modeled within our unsupervised
object-centric learning framework. Experimental results on synthetic and real
datasets show that our proposed method can infer and maintain object-centric
representations of 3D scenes and outperforms previous models.",http://arxiv.org/pdf/2309.14010v1
2309.14009v1,econ.GN,Discounting and Impatience,2023-09-25 10:20:23+00:00,"Understanding how people actually trade off time for money is perhaps the
major question in the field of time discounting. There is indeed a vast body of
work devoted to explore the underlying mechanisms of the individual decision
making process in an intertemporal context. This paper presents a family of new
discount functions whereof we derive a formal axiomatization. Applying the
framework proposed by Bleichrodt, Rohde and Wakker, we further extend their
formulation of CADI and CRDI functions, making discounting a function not only
of time delay but, simultaneously, also of time distortion. Our main purpose
is, in practice, to provide a tractable setting within which individual
intertemporal preferences can be outlined. Furthermore, we apply our models to
study the relation between individual time preferences and personality traits.
For the CADI-CADI, results show that the habit of smoking is heavily related
with both impatience and time perception. Within the Big-Five framework,
conscientiousness, agreeableness and openness are positively related with
patience (low r, initial discount rate).",http://arxiv.org/pdf/2309.14009v1
2309.14006v1,cs.CL,Multiple evolutionary pressures shape identical consonant avoidance in the world's languages,2023-09-25 10:16:30+00:00,"Languages disfavor word forms containing sequences of similar or identical
consonants, due to the biomechanical and cognitive difficulties posed by
patterns of this sort. However, the specific evolutionary processes responsible
for this phenomenon are not fully understood. Words containing sequences of
identical consonants may be more likely to arise than those without; processes
of word form mutation may be more likely to remove than create sequences of
identical consonants in word forms; finally, words containing identical
consonants may die out more frequently than those without. Phylogenetic
analyses of the evolution of homologous word forms indicate that words with
identical consonants arise less frequently than those without, and processes
which mutate word forms are more likely to remove sequences of identical
consonants than introduce them. However, words with identical consonants do not
die out more frequently than those without. Further analyses reveal that forms
with identical consonants are replaced in basic meaning functions more
frequently than words without. Taken together, results suggest that the under
representation of sequences of identical consonants is overwhelmingly a
byproduct of constraints on word form coinage, though processes related to word
usage also serve to ensure that such patterns are infrequent in more salient
vocabulary items. These findings clarify previously unknown aspects of
processes of lexical evolution and competition that take place during language
change, optimizing communicative systems.",http://arxiv.org/pdf/2309.14006v1
2309.14001v1,cond-mat.soft,A discrete model for layered growth,2023-09-25 10:07:53+00:00,"In this work we present a discrete model that captures the fundamental
properties of additively manufactured solids in a minimal setting. The model is
based on simplified kinematics and allows for the onset of incompatible
deformations between discrete layers of an additively manufactured stack.
Thanks to the discrete nature of the model, we obtain an averaged formulation
of mechanical equilibrium for the growing stack, leading to closed-form
solutions that are both analytically simple and physically transparent. In
particular, we are able to explain the origin of residual stresses by the
accumulation of incompatible deformations between adjacent layers. At the same
time, we are able to formulate the technologically relevant inverse problem
that provides the deposition protocol required to produce a desired state of
internal stress in the manufactured stack. Another important aspect analyzed in
the work is the role played by an ideal ``glue'' between the layers, whose
presence is fundamental to prevent their sliding and whose mechanical behavior
can quantitatively influence the final stress distribution in the stack.
Although the model is an elementary approximation of additive manufacturing,
its simplicity makes it possible to highlight how the controls exerted during
deposition will have qualitative or quantitative effects on the final stress
state of the stack. This understanding is crucial in shedding light on the
complex mechanical behavior of additive manufactured solids.",http://arxiv.org/pdf/2309.14001v1
2309.13994v1,eess.AS,Unsupervised Accent Adaptation Through Masked Language Model Correction Of Discrete Self-Supervised Speech Units,2023-09-25 09:51:59+00:00,"Self-supervised pre-trained speech models have strongly improved speech
recognition, yet they are still sensitive to domain shifts and accented or
atypical speech. Many of these models rely on quantisation or clustering to
learn discrete acoustic units. We propose to correct the discovered discrete
units for accented speech back to a standard pronunciation in an unsupervised
manner. A masked language model is trained on discrete units from a standard
accent and iteratively corrects an accented token sequence by masking
unexpected cluster sequences and predicting their common variant. Small accent
adapter blocks are inserted in the pre-trained model and fine-tuned by
predicting the corrected clusters, which leads to an increased robustness of
the pre-trained model towards a target accent, and this without supervision. We
are able to improve a state-of-the-art HuBERT Large model on a downstream
accented speech recognition task by altering the training regime with the
proposed method.",http://arxiv.org/pdf/2309.13994v1
2309.13993v1,cs.LG,Identification of Mixtures of Discrete Product Distributions in Near-Optimal Sample and Time Complexity,2023-09-25 09:50:15+00:00,"We consider the problem of identifying, from statistics, a distribution of
discrete random variables $X_1,\ldots,X_n$ that is a mixture of $k$ product
distributions. The best previous sample complexity for $n \in O(k)$ was
$(1/\zeta)^{O(k^2 \log k)}$ (under a mild separation assumption parameterized
by $\zeta$). The best known lower bound was $\exp(\Omega(k))$. It is known that
$n\geq 2k-1$ is necessary and sufficient for identification. We show, for any
$n\geq 2k-1$, how to achieve sample complexity and run-time complexity
$(1/\zeta)^{O(k)}$. We also extend the known lower bound of $e^{\Omega(k)}$ to
match our upper bound across a broad range of $\zeta$. Our results are obtained
by combining (a) a classic method for robust tensor decomposition, (b) a novel
way of bounding the condition number of key matrices called Hadamard
extensions, by studying their action only on flattened rank-1 tensors.",http://arxiv.org/pdf/2309.13993v1
2309.13984v1,eess.SP,Near-field Hybrid Beamforming for Terahertz-band Integrated Sensing and Communications,2023-09-25 09:36:41+00:00,"Terahertz (THz) band communications and integrated sensing and communications
(ISAC) are two main facets of the sixth generation wireless networks. In order
to compensate the severe attenuation, the THz wireless systems employ large
arrays, wherein the near-field beam-squint severely degrades the beamforming
accuracy. Contrary to prior works that examine only either narrowband ISAC
beamforming or far-field models, we introduce an alternating optimization
technique for hybrid beamforming design in near-field THz-ISAC scenario. We
also propose an efficient approach to compensate near-field beam-squint via
baseband beamformers. Via numerical simulations, we show that the proposed
approach achieves satisfactory spectral efficiency performance while accurately
estimating the near-field beamformers and mitigating the beam-squint without
additional hardware components.",http://arxiv.org/pdf/2309.13984v1
2309.13983v1,astro-ph.SR,"Numerical studies on the link between radioisotopic signatures on Earth and the formation of the Local Bubble. II. Advanced modeling of interstellar 26Al, 53Mn, 60Fe, and 244Pu influxes as traces of past supernovae in the solar neighborhood",2023-09-25 09:35:52+00:00,"Measurements of long-lived radioisotopes provide a means, completely
independent of other observational channels, to draw conclusions about
near-Earth supernovae (SNe) and thus the origin of the Local Bubble (LB). First
and foremost in this context is 60Fe, which has already been detected across
the Earth and on the Moon. Using Gaia EDR3, we identified 14 SN explosions,
with 13 occurring in UCL/LCC, and one in V1062 Sco, all being subgroups of the
Sco-Cen OB association. The timing of these explosions was obtained by us
through interpolation of rotating stellar evolution tracks via the initial
masses of the already exploded massive stars. We further developed a new Monte
Carlo-type approach for deriving the trajectories of the SN progenitors. We
then performed 3D hydrodynamic simulations based on these initial conditions to
explore the evolution of the LB in an inhomogeneous local interstellar medium
and the transport of radioisotopes to Earth. The simulations include the
stellar winds from the SN progenitors and additional radioisotopes (26Al, 53Mn,
and 244Pu) besides 60Fe. We find that (i) our simulations are consistent with
measurements of 60Fe, in particular, a peak 2-3 Myr before present, as well as
26Al, 53Mn, and 244Pu data, (ii) stellar winds contribute to the distribution
of radioisotopes and also to the dynamics of the LB, (iii) the solar system
(SS) entered the LB about 4.6 Myr ago, and (iv) the measured recent influx of
60Fe can be naturally explained by turbulent radioisotopic transport. Our
simulations not only support the recent hypothesis that the LB triggered star
formation in the solar vicinity through its expansion, but also suggest that
the second, separate 60Fe peak measured at 6-9 Myr ago was generated by the
passage of the SS through a neighboring superbubble (SB), possibly the
Orion-Eridanus SB, prior to its current residence in the LB.",http://arxiv.org/pdf/2309.13983v1
2309.13980v1,eess.IV,Better Generalization of White Matter Tract Segmentation to Arbitrary Datasets with Scaled Residual Bootstrap,2023-09-25 09:31:34+00:00,"White matter (WM) tract segmentation is a crucial step for brain connectivity
studies. It is performed on diffusion magnetic resonance imaging (dMRI), and
deep neural networks (DNNs) have achieved promising segmentation accuracy.
Existing DNN-based methods use an annotated dataset for model training.
However, the performance of the trained model on a different test dataset may
not be optimal due to distribution shift, and it is desirable to design WM
tract segmentation approaches that allow better generalization of the
segmentation model to arbitrary test datasets. In this work, we propose a WM
tract segmentation approach that improves the generalization with scaled
residual bootstrap. The difference between dMRI scans in training and test
datasets is most noticeably caused by the different numbers of diffusion
gradients and noise levels. Since both of them lead to different
signal-to-noise ratios (SNRs) between the training and test data, we propose to
augment the training scans by adjusting the noise magnitude and develop an
adapted residual bootstrap strategy for the augmentation. To validate the
proposed approach, two dMRI datasets were used, and the experimental results
show that our method consistently improved the generalization of WM tract
segmentation under various settings.",http://arxiv.org/pdf/2309.13980v1
2309.13977v1,cs.DC,The Computational Power of Distributed Shared-Memory Models with Bounded-Size Registers,2023-09-25 09:22:54+00:00,"The celebrated Asynchronous Computability Theorem of Herlihy and Shavit (STOC
1993 and STOC 1994) provided a topological characterization of the tasks that
are solvable in a distributed system where processes are communicating by
writing and reading shared registers, and where any number of processes can
fail by crashing. However, this characterization assumes the use of
full-information protocols, that is, protocols in which each time any of the
processes writes in the shared memory, it communicates everything it learned
since the beginning of the execution. Thus, the characterization implicitly
assumes that each register in the shared memory is of unbounded size. Whether
unbounded size registers are unavoidable for the model of computation to be
universal is the central question studied in this paper. Specifically, is any
task that is solvable using unbounded registers solvable using registers of
bounded size? More generally, when at most $t$ processes can crash, is the
model with bounded size registers universal? These are the questions answered
in this paper.",http://arxiv.org/pdf/2309.13977v1
2309.13976v1,physics.flu-dyn,Heat transfer in drop-laden turbulence,2023-09-25 09:22:29+00:00,"Heat transfer by large deformable drops in a turbulent flow is a complex and
rich in physics system, in which drops deformation, breakage and coalescence
influence the transport of heat. We study this problem coupling direct
numerical simulations (DNS) of turbulence, with a phase-field method for the
interface description. Simulations are run at fixed shear Reynolds and Weber
numbers. To evaluate the influence of microscopic flow properties, like
momentum/thermal diffusivity, on macroscopic flow properties, like mean
temperature or heat transfer rates, we consider four different values of the
Prandtl number, which is the momentum to thermal diffusivity ratio: Pr=1, Pr=2,
Pr=4 and Pr=8. The drops volume fraction is Phi=5.4% for all cases. Drops are
initially warmer than the turbulent carrier fluid, and release heat at
different rates, depending on the value of Pr, but also on their size and on
their own dynamics (topology, breakage, drop-drop interaction). Computing the
time behavior of the drops and carrier fluid average temperatures, we clearly
show that an increase of Pr slows down the heat transfer process. We explain
our results by a simplified phenomenological model: we show that the time
behavior of the drops average temperature is self similar, and a universal
behavior can be found upon rescaling by t/Pr^2/3.",http://arxiv.org/pdf/2309.13976v1
2309.13974v1,cs.SE,Deriving Product Line Requirements: the RED-PL Guidance Approach,2023-09-25 09:16:45+00:00,"Product lines (PL) modeling have proven to be an effective approach to reuse
in software development.Several variability approaches were developed to plan
requirements reuse, but only little of them actuallyaddress the issue of
deriving product requirements.This paper presents a method, RED-PL that intends
to support requirements derivation. The originality ofthe proposed approach is
that (i) it is user-oriented, (ii) it guides product requirements elicitation
andderivation as a decision making activity, and (iii) it provides systematic
and interactive guidance assistinganalysts in taking decisions about
requirements. The RED-PL methodological process was validatedin an industrial
setting by considering the requirement engineering phase of a product line of
blood analyzers.",http://arxiv.org/pdf/2309.13974v1
2309.13963v2,eess.AS,Connecting Speech Encoder and Large Language Model for ASR,2023-09-25 08:57:07+00:00,"The impressive capability and versatility of large language models (LLMs)
have aroused increasing attention in automatic speech recognition (ASR), with
several pioneering studies attempting to build integrated ASR models by
connecting a speech encoder with an LLM. This paper presents a comparative
study of three commonly used structures as connectors, including fully
connected layers, multi-head cross-attention, and Q-Former. Speech encoders
from the Whisper model series as well as LLMs from the Vicuna model series with
different model sizes were studied. Experiments were performed on the commonly
used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with
Q-Formers demonstrated consistent and considerable word error rate (WER)
reductions over LLMs with other connector structures. Q-Former-based LLMs can
generalise well to out-of-domain datasets, where 12% relative WER reductions
over the Whisper baseline ASR model were achieved on the Eval2000 test set
without using any in-domain training data from Switchboard. Moreover, a novel
segment-level Q-Former is proposed to enable LLMs to recognise speech segments
with a duration exceeding the limitation of the encoders, which results in 17%
relative WER reductions over other connector structures on 90-second-long
speech data.",http://arxiv.org/pdf/2309.13963v2
2309.13962v1,cs.CV,Egocentric RGB+Depth Action Recognition in Industry-Like Settings,2023-09-25 08:56:22+00:00,"Action recognition from an egocentric viewpoint is a crucial perception task
in robotics and enables a wide range of human-robot interactions. While most
computer vision approaches prioritize the RGB camera, the Depth modality -
which can further amplify the subtleties of actions from an egocentric
perspective - remains underexplored. Our work focuses on recognizing actions
from egocentric RGB and Depth modalities in an industry-like environment. To
study this problem, we consider the recent MECCANO dataset, which provides a
wide range of assembling actions. Our framework is based on the 3D Video SWIN
Transformer to encode both RGB and Depth modalities effectively. To address the
inherent skewness in real-world multimodal action occurrences, we propose a
training strategy using an exponentially decaying variant of the focal loss
modulating factor. Additionally, to leverage the information in both RGB and
Depth modalities, we opt for late fusion to combine the predictions from each
modality. We thoroughly evaluate our method on the action recognition task of
the MECCANO dataset, and it significantly outperforms the prior work. Notably,
our method also secured first place at the multimodal action recognition
challenge at ICIAP 2023.",http://arxiv.org/pdf/2309.13962v1
2309.13957v1,q-bio.BM,Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design,2023-09-25 08:43:13+00:00,"Generative molecular design has moved from proof-of-concept to real-world
applicability, as marked by the surge in very recent papers reporting
experimental validation. Key challenges in explainability and sample efficiency
present opportunities to enhance generative design to directly optimize
expensive high-fidelity oracles and provide actionable insights to domain
experts. Here, we propose Beam Enumeration to exhaustively enumerate the most
probable sub-sequences from language-based molecular generative models and show
that molecular substructures can be extracted. When coupled with reinforcement
learning, extracted substructures become meaningful, providing a source of
explainability and improving sample efficiency through self-conditioned
generation. Beam Enumeration is generally applicable to any language-based
molecular generative model and notably further improves the performance of the
recently reported Augmented Memory algorithm, which achieved the new
state-of-the-art on the Practical Molecular Optimization benchmark for sample
efficiency. The combined algorithm generates more high reward molecules and
faster, given a fixed oracle budget. Beam Enumeration is the first method to
jointly address explainability and sample efficiency for molecular design.",http://arxiv.org/pdf/2309.13957v1
2309.13945v1,quant-ph,Measuring the quantum state of photoelectrons,2023-09-25 08:25:28+00:00,"A photoelectron, emitted due to the absorption of light quanta as described
by the photoelectric effect, is often characterized experimentally by a
classical quantity, its momentum. However, since the photoelectron is a quantum
object, its rigorous characterization requires the reconstruction of the
complete quantum state, the photoelectron's density matrix. Here, we use
quantum state tomography to fully characterize photoelectrons emitted from
helium and argon atoms upon absorption of ultrashort, extreme ultraviolet light
pulses. While in helium we measure a pure photoelectronic state, in argon,
spin-orbit interaction induces entanglement between the ion and the
photoelectron, leading to a reduced purity of the photoelectron state. Our work
shows how state tomography gives new insights into the fundamental quantum
aspects of light-induced electronic processes in matter, bridging the fields of
photoelectron spectroscopy and quantum information, and offering new
spectroscopic possibilities for quantum technology.",http://arxiv.org/pdf/2309.13945v1
2309.13942v1,cs.CV,Speed Co-Augmentation for Unsupervised Audio-Visual Pre-training,2023-09-25 08:22:30+00:00,"This work aims to improve unsupervised audio-visual pre-training. Inspired by
the efficacy of data augmentation in visual contrastive learning, we propose a
novel speed co-augmentation method that randomly changes the playback speeds of
both audio and video data. Despite its simplicity, the speed co-augmentation
method possesses two compelling attributes: (1) it increases the diversity of
audio-visual pairs and doubles the size of negative pairs, resulting in a
significant enhancement in the learned representations, and (2) it changes the
strict correlation between audio-visual pairs but introduces a partial
relationship between the augmented pairs, which is modeled by our proposed
SoftInfoNCE loss to further boost the performance. Experimental results show
that the proposed method significantly improves the learned representations
when compared to vanilla audio-visual contrastive learning.",http://arxiv.org/pdf/2309.13942v1
2309.13938v1,eess.AS,Evaluating Classification Systems Against Soft Labels with Fuzzy Precision and Recall,2023-09-25 08:16:01+00:00,"Classification systems are normally trained by minimizing the cross-entropy
between system outputs and reference labels, which makes the Kullback-Leibler
divergence a natural choice for measuring how closely the system can follow the
data. Precision and recall provide another perspective for measuring the
performance of a classification system. Non-binary references can arise from
various sources, and it is often beneficial to use the soft labels for training
instead of the binarized data. However, the existing definitions for precision
and recall require binary reference labels, and binarizing the data can cause
erroneous interpretations. We present a novel method to calculate precision,
recall and F-score without quantizing the data. The proposed metrics extend the
well established metrics as the definitions coincide when used with binary
labels. To understand the behavior of the metrics we show simple example cases
and an evaluation of different sound event detection models trained on real
data with soft labels.",http://arxiv.org/pdf/2309.13938v1
2309.13931v1,gr-qc,Generalized free energy and dynamical state transition of the dyonic AdS black hole in the grand canonical ensemble,2023-09-25 08:01:23+00:00,"We study the generalized free energy of the dyonic AdS black hole in an
ensemble with varying electric charge $q_E$ and fixed magnetic charge $q_M$.
When we adjust the temperature $T$ and the electric potential $\Phi_E$ of the
ensemble, the Ricci scalar curvature $R$ and electromagnetic potential $A_u$
usually diverge at the horizon. We regularize them and incorporate the
off-shell corrections into the Einstein-Hilbert action. Alternatively, we find
that the off-shell corrections can also be obtained by adding a boundary near
the horizon to exclude the singularities. Ultimately, we derive the generalized
free energy which is consistent with the definition of the thermodynamic
relations. Based on the generalized free energy landscape, we can describe the
dynamics of state transition as a stochastic process quantified by the Langevin
equation. The path integral framework can be formulated to derive the
time-dependent trajectory of the order parameter and the time evolution of the
transition probability. By comparing the probability with the result of the
classical master equation, we attribute the contribution to the probability of
one pseudomolecule or antipseudomolecule (the instanton and anti-instanton
pair) to the rate of state transition. These results are consistent with the
qualitative analysis of the free energy landscape.",http://arxiv.org/pdf/2309.13931v1
2309.13922v1,eess.SP,Track-before-detect Algorithm based on Cost-reference Particle Filter Bank for Weak Target Detection,2023-09-25 07:45:39+00:00,"Detecting weak target is an important and challenging problem in many
applications such as radar, sonar etc. However, conventional detection methods
are often ineffective in this case because of low signal-to-noise ratio (SNR).
This paper presents a track-before-detect (TBD) algorithm based on an improved
particle filter, i.e. cost-reference particle filter bank (CRPFB), which turns
the problem of target detection to the problem of two-layer hypothesis testing.
The first layer is implemented by CRPFB for state estimation of possible
target. CRPFB has entirely parallel structure, consisting amounts of
cost-reference particle filters with different hypothesized prior information.
The second layer is to compare a test metric with a given threshold, which is
constructed from the output of the first layer and fits GEV distribution. The
performance of our proposed TBD algorithm and the existed TBD algorithms are
compared according to the experiments on nonlinear frequency modulated (NLFM)
signal detection and tracking. Simulation results show that the proposed TBD
algorithm has better performance than the state-of-the-arts in detection,
tracking, and time efficiency.",http://arxiv.org/pdf/2309.13922v1
2309.13920v1,cs.SD,Real-Time Emergency Vehicle Detection using Mel Spectrograms and Regular Expressions,2023-09-25 07:40:19+00:00,"In emergency situations, the movement of vehicles through city streets can be
problematic due to vehicular traffic. This paper presents a method for
detecting emergency vehicle sirens in real time. To derive a siren Hi-Lo audio
fingerprint it was necessary to apply digital signal processing techniques and
signal symbolization, contrasting against a deep neural network audio
classifier feeding 280 environmental sounds and 38 Hi-Lo sirens. In both
methods, their precision was evaluated based on a confusion matrix and various
metrics. The precision of the developed DSP algorithm presented a greater
ability to discriminate between signal and noise, compared to the CNN model.",http://arxiv.org/pdf/2309.13920v1
2309.13918v1,cond-mat.stat-mech,Signatures of spectral crossovers in the short- and long-range spectral correlations of a disordered spin-chain with Kramers degeneracy,2023-09-25 07:36:00+00:00,"We investigate several distinct spectral crossovers amongst various
integrable and quantum-chaotic limits of a 1D disordered quantum spin-1/2
model, by tuning the relative amplitudes of various Hamiltonian parameters to
retain or break relevant unitary and antiunitary symmetries. Since we are
specially interested in crossovers involving a Gaussian symplectic ensemble
(GSE) limit, we carry out all our calculations with an odd number of spins that
naturally results in eigenspectra with Kramers degeneracies. The various
crossovers are investigated via detailed studies of both short-range (NNSD) and
long-range (spectral rigidity and number variance) spectral correlations. The
short-range studies show excellent agreement with RMT predictions. One of the
highlights of this study is the systematic investigation of the consequences of
retaining both eigenvalues corresponding to every Kramers doublet, in a
crossover involving the GSE limit, and see how it evolves to a limit where the
KD is naturally lifted. The NNSD plot in the GSE limit exhibits a Dirac delta
peak at zero splitting and a renormalized GSE hump at finite splitting, whose
general analytical form is derived. With an increasing symmetry breaking
magnetic field the NNSD shows an interesting, dynamic two-peaked structure that
finally converges to the standard GUE lineshape. We explain this trend in terms
of a competition between the splittings amongst distinct Kramers doublets and
the Zeeman-like splittings induced by a breaking of time-reversal symmetry. In
the long-range spectral correlation studies, we shed light on the extent of
agreement between our physical spin systems and RMT predictions. Our studies
also show that the long-range correlations may serve to distinguish between the
two Poissonian limits (nonlocalized and localized) in the reentrant crossovers,
which the short-range correlations fail to distinguish.",http://arxiv.org/pdf/2309.13918v1
2309.13917v1,eess.SP,Online Resource Allocation for Semantic-Aware Edge Computing Systems,2023-09-25 07:35:20+00:00,"In this paper, we propose a semantic-aware joint communication and
computation resource allocation framework for MEC systems. In the considered
system, random tasks arrive at each terminal device (TD), which needs to be
computed locally or offloaded to the MEC server. To further release the
transmission burden, each TD sends the small-size extracted semantic
information of tasks to the server instead of the original large-size raw data.
An optimization problem of joint semanticaware division factor, communication
and computation resource management is formulated. The problem aims to minimize
the energy consumption of the whole system, while satisfying longterm delay and
processing rate constraints. To solve this problem, an online low-complexity
algorithm is proposed. In particular, Lyapunov optimization is utilized to
decompose the original coupled long-term problem into a series of decoupled
deterministic problems without requiring the realizations of future task
arrivals and channel gains. Then, the block coordinate descent method and
successive convex approximation algorithm are adopted to solve the current time
slot deterministic problem by observing the current system states. Moreover,
the closed-form optimal solution of each optimization variable is provided.
Simulation results show that the proposed algorithm yields up to 41.8% energy
reduction compared to its counterpart without semantic-aware allocation.",http://arxiv.org/pdf/2309.13917v1
2309.13916v1,eess.AS,Frame-wise streaming end-to-end speaker diarization with non-autoregressive self-attention-based attractors,2023-09-25 07:33:54+00:00,"This work proposes a frame-wise online/streaming end-to-end neural
diarization (FS-EEND) method in a frame-in-frame-out fashion. To frame-wisely
detect a flexible number of speakers and extract/update their corresponding
attractors, we propose to leverage a causal speaker embedding encoder and an
online non-autoregressive self-attention-based attractor decoder. A look-ahead
mechanism is adopted to allow leveraging some future frames for effectively
detecting new speakers in real time and adaptively updating speaker attractors.
The proposed method processes the audio stream frame by frame, and has a low
inference latency caused by the look-ahead frames. Experiments show that,
compared with the recently proposed block-wise online methods, our method
FS-EEND achieves state-of-the-art diarization results, with a low inference
latency and computational cost.",http://arxiv.org/pdf/2309.13916v1
2309.13914v1,cs.LG,Matrix Factorization in Tropical and Mixed Tropical-Linear Algebras,2023-09-25 07:29:59+00:00,"Matrix Factorization (MF) has found numerous applications in Machine Learning
and Data Mining, including collaborative filtering recommendation systems,
dimensionality reduction, data visualization, and community detection.
Motivated by the recent successes of tropical algebra and geometry in machine
learning, we investigate two problems involving matrix factorization over the
tropical algebra. For the first problem, Tropical Matrix Factorization (TMF),
which has been studied already in the literature, we propose an improved
algorithm that avoids many of the local optima. The second formulation
considers the approximate decomposition of a given matrix into the product of
three matrices where a usual matrix product is followed by a tropical product.
This formulation has a very interesting interpretation in terms of the learning
of the utility functions of multiple users. We also present numerical results
illustrating the effectiveness of the proposed algorithms, as well as an
application to recommendation systems with promising results.",http://arxiv.org/pdf/2309.13914v1
2309.13912v1,astro-ph.GA,Merging Filaments and Hub Formation in the G083.097$+$03.270 Molecular Complex,2023-09-25 07:21:05+00:00,"We uncover a hub-filament system associated with massive star formation in
the G083.097$+$03.270. Diagnosed with simultaneous $^{12}$CO, $^{13}$CO, and
C$^{18}$O line observations, the region is found to host two distinct and
elongated filaments having separate velocity components, interacting spatially
and kinematically, that appear to have seeded the formation of a dense hub at
the intersection. A large velocity spread at the hub in addition to clear
bridging feature connecting the filaments in velocity are indicating merging of
filaments. Along the filaments axis, the velocity gradient reveals a global gas
motion with an increasing velocity dispersion inward to the hub signifying
turbulence. Altogether, the clustering of Class I sources, a high excitation
temperature, a high column density, and presence of a massive outflow at the
central hub suggest enhanced star formation. We propose that merging of
large-scale filaments and velocity gradients along filaments are the driving
factors in the mass accumulation process at the hub that have sequentially led
to the massive star formation. With two giant filaments merging to coincide
with a hub therein with ongoing star formation, this site serves as a benchmark
for the `filaments to clusters' star-forming paradigm.",http://arxiv.org/pdf/2309.13912v1
2309.13910v1,math.PR,Uniqueness of distributional solutions to the 2D vorticity Navier-Stokes equation and its associated nonlinear Markov process,2023-09-25 07:19:06+00:00,"In this work we prove uniqueness of distributional solutions to $2D$
Navier-Stokes equations in vorticity form $u_t-\nu\Delta u+ div (K(u)u)=0$ on
$(0,\infty)\times\mathbb{R}^2$ with Radon measures as initial data, where $K$
is the Biot-Savart operator in 2-D. As a consequence, one gets the uniqueness
of probabilistically weak solutions to the corresponding McKean-Vlasov
stochastic differential equations. It is also proved that for initial
conditions with density in $L^4$ these solutions are strong, so can be written
as a functional of the Wiener process, and that pathwise uniqueness holds in
the class of weak solutions, whose time marginal law densities are in
$L^{\frac43}$ in space-time. In particular, one derives a stochastic
representation of the vorticity $u$ of the fluid flow in terms of a solution to
the McKean-Vlasov SDE. Finally, it is proved that the family
$\mathbb{P}_{s,\zeta},$ $s \geq 0$, $\zeta=$probability measure on
$\mathbb{R}^d$, of path laws of the solutions to the McKean-Vlasov SDE, started
with $\zeta$ at $s$, form a nonlinear Markov process in the sense of McKean.",http://arxiv.org/pdf/2309.13910v1
2309.13907v1,cs.SD,HiGNN-TTS: Hierarchical Prosody Modeling with Graph Neural Networks for Expressive Long-form TTS,2023-09-25 07:07:02+00:00,"Recent advances in text-to-speech, particularly those based on Graph Neural
Networks (GNNs), have significantly improved the expressiveness of short-form
synthetic speech. However, generating human-parity long-form speech with high
dynamic prosodic variations is still challenging. To address this problem, we
expand the capabilities of GNNs with a hierarchical prosody modeling approach,
named HiGNN-TTS. Specifically, we add a virtual global node in the graph to
strengthen the interconnection of word nodes and introduce a contextual
attention mechanism to broaden the prosody modeling scope of GNNs from
intra-sentence to inter-sentence. Additionally, we perform hierarchical
supervision from acoustic prosody on each node of the graph to capture the
prosodic variations with a high dynamic range. Ablation studies show the
effectiveness of HiGNN-TTS in learning hierarchical prosody. Both objective and
subjective evaluations demonstrate that HiGNN-TTS significantly improves the
naturalness and expressiveness of long-form synthetic speech",http://arxiv.org/pdf/2309.13907v1
2309.13906v1,hep-ph,Strong decays of low-lying $D$-wave $Ξ_b/Ξ_b'$ baryons with QPC model,2023-09-25 07:05:44+00:00,"For further decoding the inner structure of the two excited $\Xi_b$ states
observed by LHCb, we perform a systematical study of the strong decays of the
low-lying $1D$-wave $\Xi_b$ and $\Xi_b'$ excitations using the quark pair
creation model within the $j-j$ coupling scheme. Combining with the measured
masses and decay properties of $\Xi_{b}(6327)^{0}$ and $\Xi_{b}(6327)^{0}$, the
two excited states can be explained as $1D$ $\lambda$-mode $\Xi_b$ states
$\Xi_{b}|J^{P}=\frac{3}{2}^{+},2\rangle_{\lambda\lambda}$ and
$\Xi_{b}|J^{P}=\frac{5}{2}^{+},2\rangle_{\lambda\lambda}$, respectively. If
such a view were correct, $\Xi_b'\pi$ and $\Xi_b'^*\pi$ could be another
interesting channels for experimental exploring of the $\Xi_{b}(6327)^{0}$ and
$\Xi_{b}(6327)^{0}$, respectively. Those calculations are good consistent with
the results within the chiral quark model. In addition, for the other missing
$1D$-wave $\Xi_b$ and $\Xi_b'$ excitations, our predictions indicate that:(i)
the two $\rho$-mode $1D$ $\Xi_b$ states are likely to be moderate states with a
width of $\Gamma\sim50$ MeV. The $J^P=3/2^+$ state dominantly decays into
$\Sigma_bK$ and $\Xi_b'\pi$, while the $J^P=5/2^+$ state decays primarily
through $\Sigma_b^*K$ and $\Xi_b'^*\pi$. (ii) The $\lambda$-mode $1D$ $\Xi_b'$
states may be moderate states with a widths of about several to dozens of MeV.
Most of the $\lambda$-mode $1D$ $\Xi_b'$ states mainly decay into the $1P$-wave
bottomed baryon via the pionic decay processes. Meanwhile, several
$\lambda$-mode $1D$ $\Xi_b'$ states have significant decay rates into $\Lambda
B$. (iii) While, the $\rho$-mode $1D$ $\Xi_b'$ states are predicted to be very
broad states with a width of about several hundreds MeV. It will be a great
challenge to explore the $\rho$-mode $1D$ $\Xi_b'$ states in experiments for
their broad widths.",http://arxiv.org/pdf/2309.13906v1
2309.13905v1,eess.AS,AutoPrep: An Automatic Preprocessing Framework for In-the-Wild Speech Data,2023-09-25 07:01:10+00:00,"Recently, the utilization of extensive open-sourced text data has
significantly advanced the performance of text-based large language models
(LLMs). However, the use of in-the-wild large-scale speech data in the speech
technology community remains constrained. One reason for this limitation is
that a considerable amount of the publicly available speech data is compromised
by background noise, speech overlapping, lack of speech segmentation
information, missing speaker labels, and incomplete transcriptions, which can
largely hinder their usefulness. On the other hand, human annotation of speech
data is both time-consuming and costly. To address this issue, we introduce an
automatic in-the-wild speech data preprocessing framework (AutoPrep) in this
paper, which is designed to enhance speech quality, generate speaker labels,
and produce transcriptions automatically. The proposed AutoPrep framework
comprises six components: speech enhancement, speech segmentation, speaker
clustering, target speech extraction, quality filtering and automatic speech
recognition. Experiments conducted on the open-sourced WenetSpeech and our
self-collected AutoPrepWild corpora demonstrate that the proposed AutoPrep
framework can generate preprocessed data with similar DNSMOS and PDNSMOS scores
compared to several open-sourced TTS datasets. The corresponding TTS system can
achieve up to 0.68 in-domain speaker similarity.",http://arxiv.org/pdf/2309.13905v1
2309.13899v1,math.PR,Branching stable processes and motion by mean curvature flow,2023-09-25 06:39:02+00:00,"We prove a new result relating solutions of the scaled fractional Allen--Cahn
equation to motion by mean curvature flow, motivated by the motion of hybrid
zones in populations that exhibit long range dispersal. Our proof is purely
probabilistic and takes inspiration from Etheridge et al. to describe solutions
of the fractional Allen--Cahn equation in terms of ternary branching
$\alpha$-stable motions. To overcome technical difficulties arising from the
heavy-tailed nature of the stable distribution, we couple ternary branching
stable motions to ternary branching Brownian motions subordinated by truncated
stable subordinators.",http://arxiv.org/pdf/2309.13899v1
2309.13898v1,physics.bio-ph,Extrinsic vs Intrinsic Criticality in Systems with Many Components,2023-09-25 06:30:23+00:00,"Biological systems with many components often exhibit seemingly critical
behaviors, characterized by atypically large correlated fluctuations. Yet the
underlying causes remain unclear. Here we define and examine two types of
criticality. Intrinsic criticality arises from interactions within the system
which are fine-tuned to a critical point. Extrinsic criticality, in contrast,
emerges without fine tuning when observable degrees of freedom are coupled to
unobserved fluctuating variables. We unify both types of criticality using the
language of learning and information theory. We show that critical
correlations, intrinsic or extrinsic, lead to diverging mutual information
between two halves of the system, and are a feature of learning problems, in
which the unobserved fluctuations are inferred from the observable degrees of
freedom. We argue that extrinsic criticality is equivalent to standard
inference, whereas intrinsic criticality describes fractional learning, in
which the amount to be learned depends on the system size. We show further that
both types of criticality are on the same continuum, connected by a smooth
crossover. In addition, we investigate the observability of Zipf's law, a
power-law rank-frequency distribution often used as an empirical signature of
criticality. We find that Zipf's law is a robust feature of extrinsic
criticality but can be nontrivial to observe for some intrinsically critical
systems, including critical mean-field models. We further demonstrate that
models with global dynamics, such as oscillatory models, can produce observable
Zipf's law without relying on either external fluctuations or fine tuning. Our
findings suggest that while possible in theory, fine tuning is not the only,
nor the most likely, explanation for the apparent ubiquity of criticality in
biological systems with many components.",http://arxiv.org/pdf/2309.13898v1
2309.13897v1,math.NA,Error Distribution for One-Dimensional Stochastic Differential Equation Driven By Fractional Brownian Motion,2023-09-25 06:27:54+00:00,"We can define the error distribution as the limiting distribution of the
error between the solution $Y$ of a given stochastic differential equation
(SDE) and its numerical approximation $\hat{Y}^{(m)}$, weighted by the
convergence rate between the two. A goal when studying the error distribution
is to provide a way of determination for error distributions for any SDE and
numerical scheme that converge to the exact solution. By dividing the error
into a main term and a remainder term in a particular way, the author shows
that the remainder term can be negligible compared to the main term under
certain suitable conditions. Under these conditions, deriving the error
distribution reduces to deriving the limiting distribution of the main term.
Even if the dimension is one, there are unsolved problems about the asymptotic
behavior of the error when the SDE has a drift term and $0<H\leq 1/3$, but our
result in the one-dimensional case can be adapted to any Hurst exponent. The
main idea of the proof is to define a stochastic process $Y^{m, \rho}$ with the
parameter $\rho$ interpolating between $Y$ and $\hat{Y}^{(m)}$ and to estimate
the asymptotic expansion for it. Using this estimate, we determine the error
distribution of the ($k$)-Milstein scheme and of the Crank-Nicholson scheme in
unsolved cases.",http://arxiv.org/pdf/2309.13897v1
2309.13894v1,gr-qc,"Black Holes, Equilibrium, and Cosmology",2023-09-25 06:19:37+00:00,"We trace the origins and development of black hole thermodynamics across the
past half-century, emphasizing the framework's relation to classical
thermodynamics, and the vital role played by the notions of equilibrium,
stationarity, and symmetry. We discuss different interpretations of the first
law of black hole mechanics, and assess the validity of its mechanical,
process-based interpretation for evaporating black holes. We bring these ideas
to the cosmological realm, and highlight the various difficulties that arise
when formulating thermodynamics for black holes in asymptotically de Sitter
backgrounds. We discuss a number of proposed solutions and the open questions
that arise therein.",http://arxiv.org/pdf/2309.13894v1
2309.13890v2,cs.CV,Bitstream-Corrupted Video Recovery: A Novel Benchmark Dataset and Method,2023-09-25 06:06:26+00:00,"The past decade has witnessed great strides in video recovery by specialist
technologies, like video inpainting, completion, and error concealment.
However, they typically simulate the missing content by manual-designed error
masks, thus failing to fill in the realistic video loss in video communication
(e.g., telepresence, live streaming, and internet video) and multimedia
forensics. To address this, we introduce the bitstream-corrupted video (BSCV)
benchmark, the first benchmark dataset with more than 28,000 video clips, which
can be used for bitstream-corrupted video recovery in the real world. The BSCV
is a collection of 1) a proposed three-parameter corruption model for video
bitstream, 2) a large-scale dataset containing rich error patterns, multiple
corruption levels, and flexible dataset branches, and 3) a plug-and-play module
in video recovery framework that serves as a benchmark. We evaluate
state-of-the-art video inpainting methods on the BSCV dataset, demonstrating
existing approaches' limitations and our framework's advantages in solving the
bitstream-corrupted video recovery problem. The benchmark and dataset are
released at https://github.com/LIUTIGHE/BSCV-Dataset.",http://arxiv.org/pdf/2309.13890v2
2309.13888v1,cs.SI,Graph Representation Learning Towards Patents Network Analysis,2023-09-25 05:49:40+00:00,"Patent analysis has recently been recognized as a powerful technique for
large companies worldwide to lend them insight into the age of competition
among various industries. This technique is considered a shortcut for
developing countries since it can significantly accelerate their technology
development. Therefore, as an inevitable process, patent analysis can be
utilized to monitor rival companies and diverse industries. This research
employed a graph representation learning approach to create, analyze, and find
similarities in the patent data registered in the Iranian Official Gazette. The
patent records were scrapped and wrangled through the Iranian Official Gazette
portal. Afterward, the key entities were extracted from the scrapped patents
dataset to create the Iranian patents graph from scratch based on novel natural
language processing and entity resolution techniques. Finally, thanks to the
utilization of novel graph algorithms and text mining methods, we identified
new areas of industry and research from Iranian patent data, which can be used
extensively to prevent duplicate patents, familiarity with similar and
connected inventions, Awareness of legal entities supporting patents and
knowledge of researchers and linked stakeholders in a particular research
field.",http://arxiv.org/pdf/2309.13888v1
2309.13883v1,hep-ex,Measurement of the $e^{+}e^{-} \to K_{S}^{0} K_{L}^{0} π^{0}$ cross sections from $\sqrt{s}=$ 2.000 to 3.080 GeV,2023-09-25 05:34:36+00:00,"Based on $e^{+}e^{-}$ collision data collected at center-of-mass energies
from 2.000 to 3.080 GeV by the BESIII detector at the BEPCII collider, a
partial wave analysis is performed for the process $e^{+}e^{-}\to K_{S}^{0}
K_{L}^{0} \pi^{0}$. The results allow the Born cross sections of the process
$e^{+}e^{-}\to K_{S}^{0} K_{L}^{0} \pi^{0}$, as well as its subprocesses
$e^{+}e^{-}\to K^{*}(892)^{0}\bar{K}$ and $K^{*}_{2}(1430)^{0}\bar{K}$ to be
measured. The Born cross sections for $e^{+}e^{-}\to K_{S}^{0}
K_{L}^{0}\pi^{0}$ are consistent with previous measurements by BaBar and SND,
but with substantially improved precision. The Born cross section lineshape of
the process $e^{+}e^{-}\to K^{*}(892)^{0}\bar{K}$ is consistent with a vector
meson state around 2.2 GeV with a statistical significance of 3.2$\sigma$. A
Breit-Wigner fit determines its mass as
$M_Y=(2164.1\pm9.6\pm3.1)~{\rm{MeV}}/c^{2}$ and its width as
$\Gamma_{Y}=(32.4\pm21.1\pm1.5)~\rm{MeV}$, where the first uncertainties are
statistical and the second ones are systematic, respectively.",http://arxiv.org/pdf/2309.13883v1
2309.13882v1,cs.RO,FC-Planner: A Skeleton-guided Planning Framework for Fast Aerial Coverage of Complex 3D Scenes,2023-09-25 05:25:55+00:00,"3D coverage path planning for UAVs is a crucial problem in diverse practical
applications. However, existing methods have shown unsatisfactory system
simplicity, computation efficiency, and path quality in large and complex
scenes. To address these challenges, we propose FC-Planner, a skeleton-guided
planning framework that can achieve fast aerial coverage of complex 3D scenes
without pre-processing. We decompose the scene into several simple subspaces by
a skeleton-based space decomposition (SSD). Additionally, the skeleton guides
us to effortlessly determine free space. We utilize the skeleton to efficiently
generate a minimal set of specialized and informative viewpoints for complete
coverage. Based on SSD, a hierarchical planner effectively divides the large
planning problem into independent sub-problems, enabling parallel planning for
each subspace. The carefully designed global and local planning strategies are
then incorporated to guarantee both high quality and efficiency in path
generation. We conduct extensive benchmark and real-world tests, where
FC-Planner computes over 10 times faster compared to state-of-the-art methods
with shorter path and more complete coverage. The source code will be open at
https://github.com/HKUST-Aerial-Robotics/FC-Planner.",http://arxiv.org/pdf/2309.13882v1
2309.13879v1,cs.HC,"LLM-Powered Conversational Voice Assistants: Interaction Patterns, Opportunities, Challenges, and Design Guidelines",2023-09-25 05:10:50+00:00,"Conventional Voice Assistants (VAs) rely on traditional language models to
discern user intent and respond to their queries, leading to interactions that
often lack a broader contextual understanding, an area in which Large Language
Models (LLMs) excel. However, current LLMs are largely designed for text-based
interactions, thus making it unclear how user interactions will evolve if their
modality is changed to voice. In this work, we investigate whether LLMs can
enrich VA interactions via an exploratory study with participants (N=20) using
a ChatGPT-powered VA for three scenarios (medical self-diagnosis, creative
planning, and debate) with varied constraints, stakes, and objectivity. We
observe that LLM-powered VA elicits richer interaction patterns that vary
across tasks, showing its versatility. Notably, LLMs absorb the majority of VA
intent recognition failures. We additionally discuss the potential of
harnessing LLMs for more resilient and fluid user-VA interactions and provide
design guidelines for tailoring LLMs for voice assistance.",http://arxiv.org/pdf/2309.13879v1
2309.13878v1,math.ST,On improved estimation of the larger location parameter,2023-09-25 05:05:50+00:00,"This paper investigates the problem of estimating the larger location
parameter of two general location families from a decision-theoretic
perspective. In this estimation problem, we use the criteria of minimizing the
risk function and the Pitman closeness under a general bowl-shaped loss
function. Inadmissibility of a general location and equivariant estimators is
provided. We prove that a natural estimator (analogue of the BLEE of unordered
location parameters) is inadmissible, under certain conditions on underlying
densities, and propose a dominating estimator. We also derive a class of
improved estimators using the Kubokawa's IERD approach and observe that the
boundary estimator of this class is the Brewster-Zidek type estimator.
Additionally, under the generalized Pitman criterion, we show that the natural
estimator is inadmissible and obtain improved estimators. The results are
implemented for different loss functions, and explicit expressions for the
dominating estimators are provided. We explore the applications of these
results to for exponential and normal distribution under specified loss
functions. A simulation is also conducted to compare the risk performance of
the proposed estimators. Finally, we present a real-life data analysis to
illustrate the practical applications of the paper's findings.",http://arxiv.org/pdf/2309.13878v1
2309.13876v1,cs.CL,Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data,2023-09-25 05:01:34+00:00,"Pre-training speech models on large volumes of data has achieved remarkable
success. OpenAI Whisper is a multilingual multitask model trained on 680k hours
of supervised speech data. It generalizes well to various speech recognition
and translation benchmarks even in a zero-shot setup. However, the full
pipeline for developing such models (from data collection to training) is not
publicly accessible, which makes it difficult for researchers to further
improve its performance and address training-related issues such as efficiency,
robustness, fairness, and bias. This work presents an Open Whisper-style Speech
Model (OWSM), which reproduces Whisper-style training using an open-source
toolkit and publicly available data. OWSM even supports more translation
directions and can be more efficient to train. We will publicly release all
scripts used for data preparation, training, inference, and scoring as well as
pre-trained models and training logs to promote open science.",http://arxiv.org/pdf/2309.13876v1
2309.13875v1,physics.plasm-ph,Diffusion metamaterials for plasma transport,2023-09-25 04:59:36+00:00,"Plasma technology has found widespread applications in numerous domains, yet
the techniques to manipulate plasma transport predominantly rely on magnetic
control. In this review, we present a streamlined diffusion-migration method to
characterize plasma transport. Based on this framework, the viability of the
transformation theory for plasma transport is demonstrated. Highlighted within
are three model devices designed to cloak, concentrate, and rotate plasmas
without significantly altering the density profile of background plasmas.
Additionally, insights regarding potential implications for novel physics are
discussed. This review aims to contribute to advancements in plasma technology,
especially in sectors like medicine and chemistry.",http://arxiv.org/pdf/2309.13875v1
2309.13874v1,eess.AS,Diffusion Conditional Expectation Model for Efficient and Robust Target Speech Extraction,2023-09-25 04:58:38+00:00,"Target Speech Extraction (TSE) is a crucial task in speech processing that
focuses on isolating the clean speech of a specific speaker from complex
mixtures. While discriminative methods are commonly used for TSE, they can
introduce distortion in terms of speech perception quality. On the other hand,
generative approaches, particularly diffusion-based methods, can enhance speech
quality perceptually but suffer from slower inference speed. We propose an
efficient generative approach named Diffusion Conditional Expectation Model
(DCEM) for TSE. It can handle multi- and single-speaker scenarios in both noisy
and clean conditions. Additionally, we introduce Regenerate-DCEM (R-DCEM) that
can regenerate and optimize speech quality based on pre-processed speech from a
discriminative model. Our method outperforms conventional methods in terms of
both intrusive and non-intrusive metrics and demonstrates notable strengths in
inference efficiency and robustness to unseen tasks. Audio examples are
available online (https://vivian556123.github.io/dcem).",http://arxiv.org/pdf/2309.13874v1
2309.13872v1,eess.IV,Attention and Pooling based Sigmoid Colon Segmentation in 3D CT images,2023-09-25 04:52:46+00:00,"Segmentation of the sigmoid colon is a crucial aspect of treating
diverticulitis. It enables accurate identification and localisation of
inflammation, which in turn helps healthcare professionals make informed
decisions about the most appropriate treatment options. This research presents
a novel deep learning architecture for segmenting the sigmoid colon from
Computed Tomography (CT) images using a modified 3D U-Net architecture. Several
variations of the 3D U-Net model with modified hyper-parameters were examined
in this study. Pyramid pooling (PyP) and channel-spatial Squeeze and Excitation
(csSE) were also used to improve the model performance. The networks were
trained using manually annotated sigmoid colon. A five-fold cross-validation
procedure was used on a test dataset to evaluate the network's performance. As
indicated by the maximum Dice similarity coefficient (DSC) of 56.92+/-1.42%,
the application of PyP and csSE techniques improves segmentation precision. We
explored ensemble methods including averaging, weighted averaging, majority
voting, and max ensemble. The results show that average and majority voting
approaches with a threshold value of 0.5 and consistent weight distribution
among the top three models produced comparable and optimal results with DSC of
88.11+/-3.52%. The results indicate that the application of a modified 3D U-Net
architecture is effective for segmenting the sigmoid colon in Computed
Tomography (CT) images. In addition, the study highlights the potential
benefits of integrating ensemble methods to improve segmentation precision.",http://arxiv.org/pdf/2309.13872v1
2309.13871v1,physics.geo-ph,Inexact Augmented Lagrangian Method-Based Full-waveform Inversion with Randomized Singular Value Decomposition,2023-09-25 04:50:14+00:00,"Full Waveform Inversion (FWI) is a modeling algorithm used for seismic data
processing and subsurface structure inversion. Theoretically, the main
advantage of FWI is its ability to obtain useful subsurface structure
information, such as velocity and density, from complex seismic data through
inversion simulation. However, under complex conditions, FWI is difficult to
achieve high-resolution imaging results, and most of the cases are due to
random noise, initial model, or inversion parameters and so on. Therefore, we
consider an effective image processing and dimension reduction tool, randomized
singular value decomposition (rSVD) - weighted truncated nuclear norm
regularization (WTNNR), for embedding FWI to achieve high-resolution imaging
results. This algorithm obtains a truncated matrix approximating the original
matrix by reducing the rank of the velocity increment matrix, thus achieving
the truncation of noisy data, with the truncation range controlled by WTNNR.
Subsequently, we employ an inexact augmented Lagrangian method (iALM) algorithm
in the optimization to compress the solution space range, thus relaxing the
dependence of FWI and rSVD-WTNNR on the initial model and accelerating the
convergence rate of the objective function. We tested on two sets of synthetic
data, and the results show that compared with traditional FWI, our method can
more effectively suppress the impact of random noise, thus obtaining higher
resolution and more accurate subsurface model information. Meanwhile, due to
the introduction of iALM, our method also significantly improves the
convergence rate. This work indicates that the combination of rSVD-WTNNR and
FWI is an effective imaging strategy which can help to solve the challenges
faced by traditional FWI.",http://arxiv.org/pdf/2309.13871v1
2309.13869v1,cs.CL,PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration,2023-09-25 04:42:39+00:00,"Document-level relation extraction (DocRE) aims to extract relations of all
entity pairs in a document. A key challenge in DocRE is the cost of annotating
such data which requires intensive human effort. Thus, we investigate the case
of DocRE in a low-resource setting, and we find that existing models trained on
low data overestimate the NA (""no relation"") label, causing limited
performance. In this work, we approach the problem from a calibration
perspective and propose PRiSM, which learns to adapt logits based on relation
semantic information. We evaluate our method on three DocRE datasets and
demonstrate that integrating existing models with PRiSM improves performance by
as much as 26.38 F1 score, while the calibration error drops as much as 36
times when trained with about 3% of data. The code is publicly available at
https://github.com/brightjade/PRiSM.",http://arxiv.org/pdf/2309.13869v1
2309.13864v1,cs.CR,PA-iMFL: Communication-Efficient Privacy Amplification Method against Data Reconstruction Attack in Improved Multi-Layer Federated Learning,2023-09-25 04:28:13+00:00,"Recently, big data has seen explosive growth in the Internet of Things (IoT).
Multi-layer FL (MFL) based on cloud-edge-end architecture can promote model
training efficiency and model accuracy while preserving IoT data privacy. This
paper considers an improved MFL, where edge layer devices own private data and
can join the training process. iMFL can improve edge resource utilization and
also alleviate the strict requirement of end devices, but suffers from the
issues of Data Reconstruction Attack (DRA) and unacceptable communication
overhead. This paper aims to address these issues with iMFL. We propose a
Privacy Amplification scheme on iMFL (PA-iMFL). Differing from standard MFL, we
design privacy operations in end and edge devices after local training,
including three sequential components, local differential privacy with Laplace
mechanism, privacy amplification subsample, and gradient sign reset.
Benefitting from privacy operations, PA-iMFL reduces communication overhead and
achieves privacy-preserving. Extensive results demonstrate that against
State-Of-The-Art (SOTA) DRAs, PA-iMFL can effectively mitigate private data
leakage and reach the same level of protection capability as the SOTA defense
model. Moreover, due to adopting privacy operations in edge devices, PA-iMFL
promotes up to 2.8 times communication efficiency than the SOTA compression
method without compromising model accuracy.",http://arxiv.org/pdf/2309.13864v1
2309.13859v1,hep-th,Massless Rotating Spacetimes in Four-Dimensional Horava Gravity,2023-09-25 04:04:09+00:00,"We study a particular exact solution for rotating spacetimes in
four-dimensional Horava gravity, which has been proposed as a renormalizable
gravity model without the ghost problem. We show that the massless Kerr
spacetime or the massless Kerr-(A)dS spacetime in Einstein gravity is an exact
solution in four-dimensional Horava for an arbitrary IR Lorentz-violation
parameter lambda, but with an appropriate cosmological constant. In particular,
for the massless topological Kerr-AdS black hole solution with the hyperbolic
horizon topology or the massless Kerr-dS cosmological solution with the
spherical horizon topology, there exist the ergosphere and the non-vanishing
Hawking temperature, which imply the existence of negative mass black holes as
well as positive mass spacetimes, by losing its mass from the massless ones via
the Hawking radiation or Penrose process in the ergosphere.",http://arxiv.org/pdf/2309.13859v1
2309.13847v1,cs.CV,Tuning Multi-mode Token-level Prompt Alignment across Modalities,2023-09-25 03:20:09+00:00,"Prompt tuning pre-trained vision-language models have demonstrated
significant potential in improving open-world visual concept understanding.
However, prior works only primarily focus on single-mode (only one prompt for
each modality) and holistic level (image or sentence) semantic alignment, which
fails to capture the sample diversity, leading to sub-optimal prompt discovery.
To address the limitation, we propose a multi-mode token-level tuning framework
that leverages the optimal transportation to learn and align a set of prompt
tokens across modalities. Specifically, we rely on two essential factors: 1)
multi-mode prompts discovery, which guarantees diverse semantic
representations, and 2) token-level alignment, which helps explore fine-grained
similarity. Thus, the similarity can be calculated as a hierarchical
transportation problem between the modality-specific sets. Extensive
experiments on popular image recognition benchmarks show the superior
generalization and few-shot abilities of our approach. The qualitative analysis
demonstrates that the learned prompt tokens have the ability to capture diverse
visual concepts.",http://arxiv.org/pdf/2309.13847v1
2309.13844v2,physics.optics,Measurement of the complex polar magneto-optical Kerr effect using weak measurement,2023-09-25 03:10:19+00:00,"Polar magneto-optical Kerr effect (PMOKE) is one of the most widely being
applied magneto-optical Kerr effects (MOKE) due to the induced complex MOKE
signal, consisting of the Kerr rotation angle and the ellipticity, is very
sensitive to the magnetization component perpendicular to the magnetic surface.
However, the Kerr rotation angle and the ellipticity invariably coexist and
pose a challenge in their separation. This dual presence plays a pivotal role
in defining the light intensity detected, ultimately restricting the
advancements in the measurement precision. In this paper, we propose a weak
measurement (WM) scheme to measure the complex MOKE in the pure polar
configuration. Unlike the traditional MOKE or WM method using a
quarter-wave-plate to measure the Kerr rotation angle and the ellipticity
separately, we realize the simultaneous measurement of these two parameters in
a single WM process using two new pointers, which possesses a larger linear
response region compared with the previous amplified shift pointer. The
measurement precision for the complex PMOKE angle reaches to $10^{-4}$ deg in
our experiment. Besides, the complex magneto-optical constant Q is also
calculated. This work is of great significance for the measurement of the
complex PMOKE with high efficiency, ultra-precision, low cost, and is an
important attempt to obtain complex physical quantities using WM.",http://arxiv.org/pdf/2309.13844v2
2309.13840v1,astro-ph.HE,Constraining the Charge of a Black Hole with Electromagnetic Radiation from a Black Hole-Neutron Star System,2023-09-25 02:54:32+00:00,"Black hole-neutron star (BH-NS) mergers are expected to emit
gravitational-wave (GW) and electromagnetic (EM) counterparts when the NS is
tidally disrupted or plunges into the BH. Recently, GW 200105 and GW200115 were
claimed as originating in BH-NS mergers, even GW 200105 remains in debate.
Several optical source candidates are reported to possible associate with the
two GW events, but not confirmed yet. In this work, we assume that the BH is
charged (the NS is naturally charged) and try to constrain the charge of the BH
by using the possible associated EM emission from the charged BH and NS system
working in the inspiral regime. We adopt electric and magnetic dipole
radiations for the binaries which power a Poynting-flux-dominated outflow to
accelerate electrons. Then, it produces the observed EM radiation via
synchrotron radiation. We find that the conversion efficiency in the X-ray band
is much higher than that of the ultraviolet (UV), near-infrared, and radio
bands. The estimated maximum charge-to-mass ratio (the charge for unit mass) of
the BH is $1.12\times 10^{-6}$ and $1.53\times 10^{-6}$ esu for the binary
systems of GW200105 and GW200115, respectively, if magnetic field strength
$B_{p}\lesssim ~10^{16}$ G and period $P>~1$ ms for the NS spin.",http://arxiv.org/pdf/2309.13840v1
2309.13839v1,eess.IV,Fill the K-Space and Refine the Image: Prompting for Dynamic and Multi-Contrast MRI Reconstruction,2023-09-25 02:51:00+00:00,"The key to dynamic or multi-contrast magnetic resonance imaging (MRI)
reconstruction lies in exploring inter-frame or inter-contrast information.
Currently, the unrolled model, an approach combining iterative MRI
reconstruction steps with learnable neural network layers, stands as the
best-performing method for MRI reconstruction. However, there are two main
limitations to overcome: firstly, the unrolled model structure and GPU memory
constraints restrict the capacity of each denoising block in the network,
impeding the effective extraction of detailed features for reconstruction;
secondly, the existing model lacks the flexibility to adapt to variations in
the input, such as different contrasts, resolutions or views, necessitating the
training of separate models for each input type, which is inefficient and may
lead to insufficient reconstruction. In this paper, we propose a two-stage MRI
reconstruction pipeline to address these limitations. The first stage involves
filling the missing k-space data, which we approach as a physics-based
reconstruction problem. We first propose a simple yet efficient baseline model,
which utilizes adjacent frames/contrasts and channel attention to capture the
inherent inter-frame/-contrast correlation. Then, we extend the baseline model
to a prompt-based learning approach, PromptMR, for all-in-one MRI
reconstruction from different views, contrasts, adjacent types, and
acceleration factors. The second stage is to refine the reconstruction from the
first stage, which we treat as a general video restoration problem to further
fuse features from neighboring frames/contrasts in the image domain. Extensive
experiments show that our proposed method significantly outperforms previous
state-of-the-art accelerated MRI reconstruction methods.",http://arxiv.org/pdf/2309.13839v1
2309.13836v1,cs.IT,On the Energy Efficiency of THz-NOMA enhanced UAV Cooperative Network with SWIPT,2023-09-25 02:49:13+00:00,"This paper considers the energy efficiency (EE) maximization of a
simultaneous wireless information and power transfer (SWIPT)-assisted unmanned
aerial vehicles (UAV) cooperative network operating at TeraHertz (THz)
frequencies. The source performs SWIPT enabling the UAV to receive both power
and information while also transmitting the information to a designated
destination node. Subsequently, the UAV utilizes the harvested energy to relay
the data to the intended destination node effectively. Specifically, we
maximize EE by optimizing the non-orthogonal multiple access (NOMA) power
allocation coefficients, SWIPT power splitting (PS) ratio, and UAV trajectory.
The main problem is broken down into a two-stage optimization problem and
solved using an alternating optimization approach. In the first stage,
optimization of the PS ratio and trajectory is performed by employing
successive convex approximation using a lower bound on the exponential factor
in the THz channel model. In the second phase, the NOMA power coefficients are
optimized using a quadratic transform approach. Numerical results demonstrate
the effectiveness of our proposed resource allocation algorithm compared to the
baselines where there is no trajectory optimization or no NOMA power or PS
optimization.",http://arxiv.org/pdf/2309.13836v1
2309.13835v1,eess.IV,IBVC: Interpolation-driven B-frame Video Compression,2023-09-25 02:45:51+00:00,"Learned B-frame video compression aims to adopt bi-directional motion
estimation and motion compensation (MEMC) coding for middle frame
reconstruction. However, previous learned approaches often directly extend
neural P-frame codecs to B-frame relying on bi-directional optical-flow
estimation or video frame interpolation. They suffer from inaccurate quantized
motions and inefficient motion compensation. To address these issues, we
propose a simple yet effective structure called Interpolation-driven B-frame
Video Compression (IBVC). Our approach only involves two major operations:
video frame interpolation and artifact reduction compression. IBVC introduces a
bit-rate free MEMC based on interpolation, which avoids optical-flow
quantization and additional compression distortions. Later, to reduce duplicate
bit-rate consumption and focus on unaligned artifacts, a residual guided
masking encoder is deployed to adaptively select the meaningful contexts with
interpolated multi-scale dependencies. In addition, a conditional
spatio-temporal decoder is proposed to eliminate location errors and artifacts
instead of using MEMC coding in other methods. The experimental results on
B-frame coding demonstrate that IBVC has significant improvements compared to
the relevant state-of-the-art methods. Meanwhile, our approach can save bit
rates compared with the random access (RA) configuration of H.266 (VTM). The
code will be available at https://github.com/ruhig6/IBVC.",http://arxiv.org/pdf/2309.13835v1
2309.13831v1,physics.plasm-ph,Evidence of lower-hybrid rotating spoke oscillations in a direct current magnetron microdischarge,2023-09-25 02:37:17+00:00,"High frequency current-carrying spokes are observed propagating in the
E$\times$B direction in a neon direct current magnetron discharge. Two modes
are found with distinct frequencies and behaviors. At low discharge currents,
we see highly coherent 60 MHz fluctuations. Above a distinct current threshold,
secondary 5 - 10 MHz fluctuations emerge in addition to turbulent fluctuations
spanning the 60 - 100 MHz range. The presence of lower-hybrid waves is invoked
to explain the high frequency oscillations. We attribute the appearance of the
low frequency axial modes concomittant with the onset of the high frequency
turbulence to an inverse cascade process, as suggested by recent simulations.",http://arxiv.org/pdf/2309.13831v1
2309.13830v1,math.OC,Deep Neural Newsvendor,2023-09-25 02:34:13+00:00,"We consider a data-driven newsvendor problem, where one has access to past
demand data and the associated feature information. We solve the problem by
estimating the target quantile function using a deep neural network (DNN). The
remarkable representational power of DNN allows our framework to incorporate or
approximate various extant data-driven models. We provide theoretical
guarantees in terms of excess risk bounds for the DNN solution characterized by
the network structure and sample size in a non-asymptotic manner, which justify
the applicability of DNNs in the relevant contexts. Specifically, the
convergence rate of the excess risk bound with respect to the sample size
increases in the smoothness of the target quantile function but decreases in
the dimension of feature variables. This rate can be further accelerated when
the target function possesses a composite structure. Compared to other typical
models, the nonparametric DNN method can effectively avoid or significantly
reduce the model misspecification error. In particular, our theoretical
framework can be extended to accommodate the data-dependent scenarios, where
the data-generating process is time-dependent but not necessarily identical
over time. Finally, we apply the DNN method to a real-world dataset obtained
from a food supermarket. Our numerical experiments demonstrate that (1) the DNN
method consistently outperforms other alternatives across a wide range of cost
parameters, and (2) it also exhibits good performance when the sample size is
either very large or relatively limited.",http://arxiv.org/pdf/2309.13830v1
2309.13825v1,stat.ML,NSOTree: Neural Survival Oblique Tree,2023-09-25 02:14:15+00:00,"Survival analysis is a statistical method employed to scrutinize the duration
until a specific event of interest transpires, known as time-to-event
information characterized by censorship. Recently, deep learning-based methods
have dominated this field due to their representational capacity and
state-of-the-art performance. However, the black-box nature of the deep neural
network hinders its interpretability, which is desired in real-world survival
applications but has been largely neglected by previous works. In contrast,
conventional tree-based methods are advantageous with respect to
interpretability, while consistently grappling with an inability to approximate
the global optima due to greedy expansion. In this paper, we leverage the
strengths of both neural networks and tree-based methods, capitalizing on their
ability to approximate intricate functions while maintaining interpretability.
To this end, we propose a Neural Survival Oblique Tree (NSOTree) for survival
analysis. Specifically, the NSOTree was derived from the ReLU network and can
be easily incorporated into existing survival models in a plug-and-play
fashion. Evaluations on both simulated and real survival datasets demonstrated
the effectiveness of the proposed method in terms of performance and
interpretability.",http://arxiv.org/pdf/2309.13825v1
2309.13821v1,astro-ph.GA,"An ALMA resoved view of 7,000 au Protostellar Gas Ring around the Class I source CrA-IRS 2 as a possible sign of magnetic flux advection",2023-09-25 02:03:37+00:00,"Transferring a significant fraction of the magnetic flux from a dense cloud
core is essential in the star formation process. A ring-like structure produced
by magnetic flux loss has been predicted theoretically, but no observational
identification has been presented. We have performed ALMA observations of the
Class I protostar IRS 2 in the Corona Australis star-forming region and
resolved a distinctive gas ring in the C$^{18}$O ($J$ = 2-1) line emission. The
center of this gas ring is $\sim$5,000 au away from the protostar, with a
diameter of $\sim$7,000 au. The radial velocity of the gas is $\lesssim1$ km
s$^{-1}$ blueshifted from that of the protostar, with a possible expanding
feature judged from the velocity-field (moment 1) map and position-velocity
diagram. These features are either observationally new or have been discovered
but not discussed in depth because they are difficult to explain by
well-studied protostellar phenomena such as molecular outflows and accretion
streamers. A plausible interpretation is a magnetic wall created by the
advection of magnetic flux which is theoretically expected in the Class 0/I
phase during star formation as a removal mechanism of magnetic flux. Similar
structures reported in the other young stellar sources could likely be
candidates formed by the same mechanism, encouraging us to revisit the issue of
magnetic flux transport in the early stages of star formation from an
observational perspective.",http://arxiv.org/pdf/2309.13821v1
2309.13820v1,math.PR,Strongly Efficient Rare-Event Simulation for Multiple-Jump Events in Regularly Varying Lévy Processes with Infinite Activities,2023-09-25 02:02:56+00:00,"In this paper, we address rare-event simulation for heavy-tailed L\'evy
processes with infinite activities. Specifically, the presence of infinite
activities poses a significant computational challenge, making it impractical
to simulate or store the sample path of the L\'evy process. Building upon the
importance sampling scheme in Chen et al. (2019), we present a rare-event
simulation algorithm that incorporates the sample path large deviations for
heavy-tailed L\'evy processes, the stick-breaking approximation for the extrema
of L\'evy processes, the Asmussen-Rosi\'nski approximation for small-jump
L\'evy processes, and the randomized debiasing Monte-Carlo scheme. By
establishing a novel characterization for the Lipschitz continuity of the law
of L\'evy processes, we show that the proposed algorithm is unbiased and
strongly efficient under mild conditions, and hence applicable to a broad class
of L\'evy processes. In numerical experiments, our algorithm demonstrates
significant improvements in efficiency when compared to crude Monte-Carlo
method.",http://arxiv.org/pdf/2309.13820v1
2309.13819v1,eess.AS,A Two-Step Approach for Narrowband Source Localization in Reverberant Rooms,2023-09-25 02:00:40+00:00,"This paper presents a two-step approach for narrowband source localization
within reverberant rooms. The first step involves dereverberation by modeling
the homogeneous component of the sound field by an equivalent decomposition of
planewaves using Iteratively Reweighted Least Squares (IRLS), while the second
step focuses on source localization by modeling the dereverberated component as
a sparse representation of point-source distribution using Orthogonal Matching
Pursuit (OMP). The proposed method enhances localization accuracy with fewer
measurements, particularly in environments with strong reverberation. A
numerical simulation in a conference room scenario, using a uniform microphone
array affixed to the wall, demonstrates real-world feasibility. Notably, the
proposed method and microphone placement effectively localize sound sources
within the 2D-horizontal plane without requiring prior knowledge of boundary
conditions and room geometry, making it versatile for application in different
room types.",http://arxiv.org/pdf/2309.13819v1
2309.13818v2,physics.app-ph,Observing parity-time symmetry in diffusive systems,2023-09-25 01:57:12+00:00,"Phase modulation has scarcely been mentioned in diffusive systems since the
diffusion process does not carry momentum like waves. Recently, the
non-Hermitian physics provides a new perspective for understanding diffusion
and shows prospects in the phase regulation of heat flow, for example, the
discovery of anti-parity-time (APT) symmetry in diffusive systems. The precise
control of thermal phase however remains elusive hitherto and can hardly be
realized in APT-symmetric thermal systems due to the existence of phase
oscillation. Here we construct the counterpart of APT-symmetric diffusive
systems, i.e., PT-symmetric diffusive systems, which can achieve complete
suppression of thermal phase oscillation. We find the real coupling of
diffusive fields can be readily established through a strong convective
background, where the decay-rate detuning is enabled by thermal metamaterial
design. Moreover, we observe phase transition of PT symmetry breaking in
diffusive systems with the symmetry-determined amplitude distribution and phase
regulation of coupled temperature fields. Our work uncovers the existence of
PT-symmetry in dissipative energy exchanges and provides a unique approach for
harnessing the mass transfer of particles, the wave propagation in strongly
scattering systems as well as thermal conduction.",http://arxiv.org/pdf/2309.13818v2
2309.13817v1,eess.IV,MMA-Net: Multiple Morphology-Aware Network for Automated Cobb Angle Measurement,2023-09-25 01:56:53+00:00,"Scoliosis diagnosis and assessment depend largely on the measurement of the
Cobb angle in spine X-ray images. With the emergence of deep learning
techniques that employ landmark detection, tilt prediction, and spine
segmentation, automated Cobb angle measurement has become increasingly popular.
However, these methods encounter difficulties such as high noise sensitivity,
intricate computational procedures, and exclusive reliance on a single type of
morphological information. In this paper, we introduce the Multiple
Morphology-Aware Network (MMA-Net), a novel framework that improves Cobb angle
measurement accuracy by integrating multiple spine morphology as attention
information. In the MMA-Net, we first feed spine X-ray images into the
segmentation network to produce multiple morphological information (spine
region, centerline, and boundary) and then concatenate the original X-ray image
with the resulting segmentation maps as input for the regression module to
perform precise Cobb angle measurement. Furthermore, we devise joint loss
functions for our segmentation and regression network training, respectively.
We evaluate our method on the AASCE challenge dataset and achieve superior
performance with the SMAPE of 7.28% and the MAE of 3.18{\deg}, indicating a
strong competitiveness compared to other outstanding methods. Consequently, we
can offer clinicians automated, efficient, and reliable Cobb angle measurement.",http://arxiv.org/pdf/2309.13817v1
2309.13807v1,cs.LG,Forecasting large collections of time series: feature-based methods,2023-09-25 01:23:02+00:00,"In economics and many other forecasting domains, the real world problems are
too complex for a single model that assumes a specific data generation process.
The forecasting performance of different methods changes depending on the
nature of the time series. When forecasting large collections of time series,
two lines of approaches have been developed using time series features, namely
feature-based model selection and feature-based model combination. This chapter
discusses the state-of-the-art feature-based methods, with reference to
open-source software implementations.",http://arxiv.org/pdf/2309.13807v1
2309.14339v1,cs.CV,Chop & Learn: Recognizing and Generating Object-State Compositions,2023-09-25 17:59:43+00:00,"Recognizing and generating object-state compositions has been a challenging
task, especially when generalizing to unseen compositions. In this paper, we
study the task of cutting objects in different styles and the resulting object
state changes. We propose a new benchmark suite Chop & Learn, to accommodate
the needs of learning objects and different cut styles using multiple
viewpoints. We also propose a new task of Compositional Image Generation, which
can transfer learned cut styles to different objects, by generating novel
object-state images. Moreover, we also use the videos for Compositional Action
Recognition, and show valuable uses of this dataset for multiple video tasks.
Project website: https://chopnlearn.github.io.",http://arxiv.org/pdf/2309.14339v1
2309.14338v1,cs.CV,3D Indoor Instance Segmentation in an Open-World,2023-09-25 17:59:26+00:00,"Existing 3D instance segmentation methods typically assume that all semantic
classes to be segmented would be available during training and only seen
categories are segmented at inference. We argue that such a closed-world
assumption is restrictive and explore for the first time 3D indoor instance
segmentation in an open-world setting, where the model is allowed to
distinguish a set of known classes as well as identify an unknown object as
unknown and then later incrementally learning the semantic category of the
unknown when the corresponding category labels are available. To this end, we
introduce an open-world 3D indoor instance segmentation method, where an
auto-labeling scheme is employed to produce pseudo-labels during training and
induce separation to separate known and unknown category labels. We further
improve the pseudo-labels quality at inference by adjusting the unknown class
probability based on the objectness score distribution. We also introduce
carefully curated open-world splits leveraging realistic scenarios based on
inherent object distribution, region-based indoor scene exploration and
randomness aspect of open-world classes. Extensive experiments reveal the
efficacy of the proposed contributions leading to promising open-world 3D
instance segmentation performance.",http://arxiv.org/pdf/2309.14338v1
2309.14330v1,cs.CV,"Noise-in, Bias-out: Balanced and Real-time MoCap Solving",2023-09-25 17:55:24+00:00,"Real-time optical Motion Capture (MoCap) systems have not benefited from the
advances in modern data-driven modeling. In this work we apply machine learning
to solve noisy unstructured marker estimates in real-time and deliver robust
marker-based MoCap even when using sparse affordable sensors. To achieve this
we focus on a number of challenges related to model training, namely the
sourcing of training data and their long-tailed distribution. Leveraging
representation learning we design a technique for imbalanced regression that
requires no additional data or labels and improves the performance of our model
in rare and challenging poses. By relying on a unified representation, we show
that training such a model is not bound to high-end MoCap training data
acquisition, and exploit the advances in marker-less MoCap to acquire the
necessary data. Finally, we take a step towards richer and affordable MoCap by
adapting a body model-based inverse kinematics solution to account for
measurement and inference uncertainty, further improving performance and
robustness. Project page: https://moverseai.github.io/noise-tail",http://arxiv.org/pdf/2309.14330v1
2309.14317v1,cs.GT,Online and Offline Dynamic Influence Maximization Games Over Social Networks,2023-09-25 17:38:21+00:00,"In this work, we consider dynamic influence maximization games over social
networks with multiple players (influencers). The goal of each influencer is to
maximize their own reward subject to their limited total budget rate
constraints. Thus, influencers need to carefully design their investment
policies considering individuals' opinion dynamics and other influencers'
investment strategies, leading to a dynamic game problem. We first consider the
case of a single influencer who wants to maximize its utility subject to a
total budget rate constraint. We study both offline and online versions of the
problem where the opinion dynamics are either known or not known a priori. In
the singe-influencer case, we propose an online no-regret algorithm, meaning
that as the number of campaign opportunities grows, the average utilities
obtained by the offline and online solutions converge. Then, we consider the
game formulation with multiple influencers in offline and online settings. For
the offline setting, we show that the dynamic game admits a unique Nash
equilibrium policy and provide a method to compute it. For the online setting
and with two influencers, we show that if each influencer applies the same
no-regret online algorithm proposed for the single-influencer maximization
problem, they will converge to the set of $\epsilon$-Nash equilibrium policies
where $\epsilon=O(\frac{1}{\sqrt{K}})$ scales in average inversely with the
number of campaign times $K$ considering the average utilities of the
influencers. Moreover, we extend this result to any finite number of
influencers under more strict requirements on the information structure.
Finally, we provide numerical analysis to validate our results under various
settings.",http://arxiv.org/pdf/2309.14317v1
2309.14303v1,cs.CV,Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation,2023-09-25 17:19:26+00:00,"Preparing training data for deep vision models is a labor-intensive task. To
address this, generative models have emerged as an effective solution for
generating synthetic data. While current generative models produce image-level
category labels, we propose a novel method for generating pixel-level semantic
segmentation labels using the text-to-image generative model Stable Diffusion
(SD). By utilizing the text prompts, cross-attention, and self-attention of SD,
we introduce three new techniques: \textit{class-prompt appending},
\textit{class-prompt cross-attention}, and \textit{self-attention
exponentiation}. These techniques enable us to generate segmentation maps
corresponding to synthetic images. These maps serve as pseudo-labels for
training semantic segmenters, eliminating the need for labor-intensive
pixel-wise annotation. To account for the imperfections in our pseudo-labels,
we incorporate uncertainty regions into the segmentation, allowing us to
disregard loss from those regions. We conduct evaluations on two datasets,
PASCAL VOC and MSCOCO, and our approach significantly outperforms concurrent
work. Our benchmarks and code will be released at
https://github.com/VinAIResearch/Dataset-Diffusion",http://arxiv.org/pdf/2309.14303v1
2309.14302v1,cond-mat.mtrl-sci,Bayesian parameter estimation for characterising mobile ion vacancies in perovskite solar cells,2023-09-25 17:18:05+00:00,"To overcome the challenges associated with poor temporal stability of
perovskite solar cells, methods are required that allow for fast iteration of
fabrication and characterisation, such that optimal device performance and
stability may be actively pursued. Currently, establishing the causes of
underperformance is both complex and time-consuming, and optimisation of device
fabrication thus inherently slow. Here, we present a means of computational
device characterisation of mobile halide ion parameters from room temperature
current-voltage (J-V) measurements only, requiring $\sim 2$ hours of
computation on basic computing resources. With our approach, the physical
parameters of the device may be reverse modelled from experimental J-V
measurements. In a drift-diffusion model, the set of coupled drift-diffusion
partial differential equations cannot be inverted explicitly, so a method for
inverting the drift-diffusion simulation is required. We show how Bayesian
Parameter Estimation (BPE) coupled with a drift-diffusion perovskite solar cell
model can determine the extent to which device parameters affect performance
measured by J-V characteristics. Our method is demonstrated by investigating
the extent to which device performance is influenced by mobile halide ions for
a specific fabricated device. The ion vacancy density $N_0$ and diffusion
coefficient $D_I$ were found to be precisely characterised for both simulated
and fabricated devices. This result opens up the possibility of pinpointing
origins of degradation by finding which parameters most influence device J-V
curves as the cell degrades.",http://arxiv.org/pdf/2309.14302v1
2309.14292v1,cs.AR,On the Non-Associativity of Analog Computations,2023-09-25 17:04:09+00:00,"The energy efficiency of analog forms of computing makes it one of the most
promising candidates to deploy resource-hungry machine learning tasks on
resource-constrained system such as mobile or embedded devices. However, it is
well known that for analog computations the safety net of discretization is
missing, thus all analog computations are exposed to a variety of imperfections
of corresponding implementations. Examples include non-linearities, saturation
effect and various forms of noise. In this work, we observe that the ordering
of input operands of an analog operation also has an impact on the output
result, which essentially makes analog computations non-associative, even
though the underlying operation might be mathematically associative. We conduct
a simple test by creating a model of a real analog processor which captures
such ordering effects. With this model we assess the importance of ordering by
comparing the test accuracy of a neural network for keyword spotting, which is
trained based either on an ordered model, on a non-ordered variant, and on real
hardware. The results prove the existence of ordering effects as well as their
high impact, as neglecting ordering results in substantial accuracy drops.",http://arxiv.org/pdf/2309.14292v1
2309.14291v1,cs.CV,Tiled Multiplane Images for Practical 3D Photography,2023-09-25 16:56:40+00:00,"The task of synthesizing novel views from a single image has useful
applications in virtual reality and mobile computing, and a number of
approaches to the problem have been proposed in recent years. A Multiplane
Image (MPI) estimates the scene as a stack of RGBA layers, and can model
complex appearance effects, anti-alias depth errors and synthesize soft edges
better than methods that use textured meshes or layered depth images. And
unlike neural radiance fields, an MPI can be efficiently rendered on graphics
hardware. However, MPIs are highly redundant and require a large number of
depth layers to achieve plausible results. Based on the observation that the
depth complexity in local image regions is lower than that over the entire
image, we split an MPI into many small, tiled regions, each with only a few
depth planes. We call this representation a Tiled Multiplane Image (TMPI). We
propose a method for generating a TMPI with adaptive depth planes for
single-view 3D photography in the wild. Our synthesized results are comparable
to state-of-the-art single-view MPI methods while having lower computational
overhead.",http://arxiv.org/pdf/2309.14291v1
2309.14290v1,cs.DC,Automated Market Makers for Cross-chain DeFi and Sharded Blockchains,2023-09-25 16:53:37+00:00,"In this paper we provide an execution framework for Automated Market Maker
(AMM) to be deployed across independent blockchain platforms as well as
concurrent sharding within the same blockchain platform. The framework provides
economic incentives to participate through a mechanism that guarantee fixed
prices across pairwise liquidity pools.",http://arxiv.org/pdf/2309.14290v1
2309.14289v1,cs.CV,CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free,2023-09-25 16:52:59+00:00,"The emergence of CLIP has opened the way for open-world image perception. The
zero-shot classification capabilities of the model are impressive but are
harder to use for dense tasks such as image segmentation. Several methods have
proposed different modifications and learning schemes to produce dense output.
Instead, we propose in this work an open-vocabulary semantic segmentation
method, dubbed CLIP-DIY, which does not require any additional training or
annotations, but instead leverages existing unsupervised object localization
approaches. In particular, CLIP-DIY is a multi-scale approach that directly
exploits CLIP classification abilities on patches of different sizes and
aggregates the decision in a single map. We further guide the segmentation
using foreground/background scores obtained using unsupervised object
localization methods. With our method, we obtain state-of-the-art zero-shot
semantic segmentation results on PASCAL VOC and perform on par with the best
methods on COCO.",http://arxiv.org/pdf/2309.14289v1
2309.14285v1,math.PR,On the variation of the sum of digits in the Zeckendorf representation: an algorithm to compute the distribution and mixing properties,2023-09-25 16:49:48+00:00,"We study probability measures defined by the variation of the sum of digits
in the Zeckendorf representation. For {r\ge 0} and {d\in} Z, we consider
{\mu^{(r)}(d)} the density of integers n{\in}N for which the sum of digits
increases by d when r is added to n. We give a probabilistic interpretation of
{\mu^{(r)}} via the dynamical system provided by the odometer of
Zeckendorf-adic integers and its unique invariant measure. We give an algorithm
for computing {\mu^{(r)}} and we deduce a control on the tail of the negative
distribution of {\mu^{(r)}}, as well as the formula {\mu^{(F_{\ell})} =
{\mu^{(1)} where {F_{\ell}} is a term in the Fibonacci sequence. Finally, we
decompose the Zeckendorf representation of an integer r into so-called ""blocks""
and show that when added to an adic Zeckendorf integer, the successive actions
of these blocks can be seen as a sequence of mixing random variables.",http://arxiv.org/pdf/2309.14285v1
2309.14283v1,astro-ph.IM,Using the Gerchberg-Saxton algorithm to reconstruct non-modulated pyramid wavefront sensor measurements,2023-09-25 16:48:21+00:00,"Adaptive optics (AO) is a technique to improve the resolution of ground-based
telescopes by correcting, in real-time, optical aberrations due to atmospheric
turbulence and the telescope itself. With the rise of Giant Segmented Mirror
Telescopes (GSMT), AO is needed more than ever to reach the full potential of
these future observatories. One of the main performance drivers of an AO system
is the wavefront sensing operation, consisting of measuring the shape of the
above mentioned optical aberrations. Aims. The non-modulated pyramid wavefront
sensor (nPWFS) is a wavefront sensor with high sensitivity, allowing the limits
of AO systems to be pushed. The high sensitivity comes at the expense of its
dynamic range, which makes it a highly non-linear sensor. We propose here a
novel way to invert nPWFS signals by using the principle of reciprocity of
light propagation and the Gerchberg-Saxton (GS) algorithm. We test the
performance of this reconstructor in two steps: the technique is first
implemented in simulations, where some of its basic properties are studied.
Then, the GS reconstructor is tested on the Santa Cruz Extreme Adaptive optics
Laboratory (SEAL) testbed located at the University of California Santa Cruz.
This new way to invert the nPWFS measurements allows us to drastically increase
the dynamic range of the reconstruction for the nPWFS, pushing the dynamics
close to a modulated PWFS. The reconstructor is an iterative algorithm
requiring heavy computational burden, which could be an issue for real-time
purposes in its current implementation. However, this new reconstructor could
still be helpful in the case of many wavefront control operations. This
reconstruction technique has also been successfully tested on the Santa Cruz
Extreme AO Laboratory (SEAL) bench where it is now used as the standard way to
invert nPWFS signal.",http://arxiv.org/pdf/2309.14283v1
2309.14282v1,cs.CV,Calibration-based Dual Prototypical Contrastive Learning Approach for Domain Generalization Semantic Segmentation,2023-09-25 16:48:09+00:00,"Prototypical contrastive learning (PCL) has been widely used to learn
class-wise domain-invariant features recently. These methods are based on the
assumption that the prototypes, which are represented as the central value of
the same class in a certain domain, are domain-invariant. Since the prototypes
of different domains have discrepancies as well, the class-wise
domain-invariant features learned from the source domain by PCL need to be
aligned with the prototypes of other domains simultaneously. However, the
prototypes of the same class in different domains may be different while the
prototypes of different classes may be similar, which may affect the learning
of class-wise domain-invariant features. Based on these observations, a
calibration-based dual prototypical contrastive learning (CDPCL) approach is
proposed to reduce the domain discrepancy between the learned class-wise
features and the prototypes of different domains for domain generalization
semantic segmentation. It contains an uncertainty-guided PCL (UPCL) and a
hard-weighted PCL (HPCL). Since the domain discrepancies of the prototypes of
different classes may be different, we propose an uncertainty probability
matrix to represent the domain discrepancies of the prototypes of all the
classes. The UPCL estimates the uncertainty probability matrix to calibrate the
weights of the prototypes during the PCL. Moreover, considering that the
prototypes of different classes may be similar in some circumstances, which
means these prototypes are hard-aligned, the HPCL is proposed to generate a
hard-weighted matrix to calibrate the weights of the hard-aligned prototypes
during the PCL. Extensive experiments demonstrate that our approach achieves
superior performance over current approaches on domain generalization semantic
segmentation tasks.",http://arxiv.org/pdf/2309.14282v1
2309.14277v1,cs.CV,SINCERE: Supervised Information Noise-Contrastive Estimation REvisited,2023-09-25 16:40:56+00:00,"The information noise-contrastive estimation (InfoNCE) loss function provides
the basis of many self-supervised deep learning methods due to its strong
empirical results and theoretic motivation. Previous work suggests a supervised
contrastive (SupCon) loss to extend InfoNCE to learn from available class
labels. This SupCon loss has been widely-used due to reports of good empirical
performance. However, in this work we suggest that the specific SupCon loss
formulated by prior work has questionable theoretic justification, because it
can encourage images from the same class to repel one another in the learned
embedding space. This problematic behavior gets worse as the number of inputs
sharing one class label increases. We propose the Supervised InfoNCE REvisited
(SINCERE) loss as a remedy. SINCERE is a theoretically justified solution for a
supervised extension of InfoNCE that never causes images from the same class to
repel one another. We further show that minimizing our new loss is equivalent
to maximizing a bound on the KL divergence between class conditional embedding
distributions. We compare SINCERE and SupCon losses in terms of learning
trajectories during pretraining and in ultimate linear classifier performance
after finetuning. Our proposed SINCERE loss better separates embeddings from
different classes during pretraining while delivering competitive accuracy.",http://arxiv.org/pdf/2309.14277v1
2309.14271v1,stat.CO,Generative Filtering for Recursive Bayesian Inference with Streaming Data,2023-09-25 16:31:47+00:00,"In the streaming data setting, where data arrive continuously or in frequent
batches and there is no pre-determined amount of total data, Bayesian models
can employ recursive updates, incorporating each new batch of data into the
model parameters' posterior distribution. Filtering methods are currently used
to perform these updates efficiently, however, they suffer from eventual
degradation as the number of unique values within the filtered samples
decreases. We propose Generative Filtering, a method for efficiently performing
recursive Bayesian updates in the streaming setting. Generative Filtering
retains the speed of a filtering method while using parallel updates to avoid
degenerate distributions after repeated applications. We derive rates of
convergence for Generative Filtering and conditions for the use of sufficient
statistics instead of fully storing all past data. We investigate the
alleviation of filtering degradation through simulation and Ecological species
count data.",http://arxiv.org/pdf/2309.14271v1
2309.14267v1,cs.CV,Identity-preserving Editing of Multiple Facial Attributes by Learning Global Edit Directions and Local Adjustments,2023-09-25 16:28:39+00:00,"Semantic facial attribute editing using pre-trained Generative Adversarial
Networks (GANs) has attracted a great deal of attention and effort from
researchers in recent years. Due to the high quality of face images generated
by StyleGANs, much work has focused on the StyleGANs' latent space and the
proposed methods for facial image editing. Although these methods have achieved
satisfying results for manipulating user-intended attributes, they have not
fulfilled the goal of preserving the identity, which is an important challenge.
We present ID-Style, a new architecture capable of addressing the problem of
identity loss during attribute manipulation. The key components of ID-Style
include Learnable Global Direction (LGD), which finds a shared and semi-sparse
direction for each attribute, and an Instance-Aware Intensity Predictor (IAIP)
network, which finetunes the global direction according to the input instance.
Furthermore, we introduce two losses during training to enforce the LGD to find
semi-sparse semantic directions, which along with the IAIP, preserve the
identity of the input instance. Despite reducing the size of the network by
roughly 95% as compared to similar state-of-the-art works, it outperforms
baselines by 10% and 7% in Identity preserving metric (FRS) and average
accuracy of manipulation (mACC), respectively.",http://arxiv.org/pdf/2309.14267v1
2309.14260v1,cond-mat.soft,What keeps nanopores boiling,2023-09-25 16:19:15+00:00,"The liquid to vapour transition can occur at unexpected conditions in
nanopores, opening the door to fundamental questions and new technologies. The
physics of boiling in confinement is progressively introduced, starting from
classical nucleation theory, passing through nanoscale effects, and terminating
to the material and external parameters which affect the boiling conditions.
The relevance of boiling in specific nanoconfined systems is discussed,
focusing on heterogeneous lyophobic systems, chromatographic columns, and ion
channels. The current level of control of boiling in nanopores enabled by
microporous materials, as metal organic frameworks, and biological nanopores
paves the way to thrilling theoretical challenges and to new technological
opportunities in the fields of energy, neuromorphic computing, and sensing.",http://arxiv.org/pdf/2309.14260v1
2309.14255v1,physics.chem-ph,"The resolution of the weak-exchange limit made rigorous, simple and general in binuclear complexes",2023-09-25 16:13:52+00:00,"The correct interpretation of magnetic properties in the weak-exchange regime
has remained a challenging task for several decades. In this regime, the
effective exchange interaction between local spins is quite weak, of the same
order of magnitude or smaller than the various anisotropic terms, which
\textit{in fine} generates a complex set of levels characterized by spin
intercalation if not significant spin mixing. Although the model multispin
Hamiltonian, \hms{} = \js{} + \da{} +\db{} + \dab{}, is considered good enough
to map the experimental energies at zero field and in the strong-exchange
limit, theoretical works pointed out limitations of this simple model. This
work revives the use of \hms{} from a new theoretical perspective, detailing
point-by-point a strategy to correctly map the computational energies and wave
functions onto \hms{}, thus validating it regardless of the exchange regime. We
will distinguish two cases, based on experimentally characterized dicobalt(II)
complexes from the literature. If centrosymmetry imposes alignment of the
various rank-2 tensors constitutive of \hms{} in the first case, the absence of
any symmetry element prevents such alignment in the second case. In such a
context, the strategy provided herein becomes a powerful tool to rationalize
the experimental magnetic data, since it is capable of fully and rigorously
extracting the multispin model without any assumption on the orientation of its
constitutive tensors. Finally, previous theoretical data related to a known
dinickel(II) complex is reinterpreted, clarifying initial wanderings regarding
the weak-exchange limit.",http://arxiv.org/pdf/2309.14255v1
2309.14254v1,physics.data-an,End-to-end deep learning inference with CMSSW via ONNX using docker,2023-09-25 16:13:35+00:00,"Deep learning techniques have been proven to provide excellent performance
for a variety of high-energy physics applications, such as particle
identification, event reconstruction and trigger operations. Recently, we
developed an end-to-end deep learning approach to identify various particles
using low-level detector information from high-energy collisions. These models
will be incorporated in the CMS software framework (CMSSW) to enable their use
for particle reconstruction or for trigger operation in real-time.
Incorporating these computational tools in the experimental framework presents
new challenges. This paper reports an implementation of the end-to-end deep
learning inference with the CMS software framework. The inference has been
implemented on GPU for faster computation using ONNX. We have benchmarked the
ONNX inference with GPU and CPU using NERSCs Perlmutter cluster by building a
docker image of the CMS software framework.",http://arxiv.org/pdf/2309.14254v1
2309.14245v1,cs.HC,Do We Run How We Say We Run? Formalization and Practice of Governance in OSS Communities,2023-09-25 16:04:11+00:00,"Open Source Software (OSS) communities often resist regulation typical of
traditional organizations. Yet formal governance systems are being increasingly
adopted among communities, particularly through non-profit mentor foundations.
Our study looks at the Apache Software Foundation Incubator program and 208
projects it supports. We assemble a scalable, semantic pipeline to discover and
analyze the governance behavior of projects from their mailing lists. We then
investigate the reception of formal policies among communities, through their
own governance priorities and internalization of the policies. Our findings
indicate that while communities observe formal requirements and policies as
extensively as they are defined, their day-to-day governance focus does not
dwell on topics that see most formal policy-making. Moreover formalization, be
it dedicating governance focus or adopting policy, has limited association with
project sustenance.",http://arxiv.org/pdf/2309.14245v1
2309.14236v1,cs.RO,MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation,2023-09-25 15:51:29+00:00,"Robotic systems that aspire to operate in uninstrumented real-world
environments must perceive the world directly via onboard sensing. Vision-based
learning systems aim to eliminate the need for environment instrumentation by
building an implicit understanding of the world based on raw pixels, but
navigating the contact-rich high-dimensional search space from solely sparse
visual reward signals significantly exacerbates the challenge of exploration.
The applicability of such systems is thus typically restricted to simulated or
heavily engineered environments since agent exploration in the real-world
without the guidance of explicit state estimation and dense rewards can lead to
unsafe behavior and safety faults that are catastrophic. In this study, we
isolate the root causes behind these limitations to develop a system, called
MoDem-V2, capable of learning contact-rich manipulation directly in the
uninstrumented real world. Building on the latest algorithmic advancements in
model-based reinforcement learning (MBRL), demo-bootstrapping, and effective
exploration, MoDem-V2 can acquire contact-rich dexterous manipulation skills
directly in the real world. We identify key ingredients for leveraging
demonstrations in model learning while respecting real-world safety
considerations -- exploration centering, agency handover, and actor-critic
ensembles. We empirically demonstrate the contribution of these ingredients in
four complex visuo-motor manipulation problems in both simulation and the real
world. To the best of our knowledge, our work presents the first successful
system for demonstration-augmented visual MBRL trained directly in the real
world. Visit https://sites.google.com/view/modem-v2 for videos and more
details.",http://arxiv.org/pdf/2309.14236v1
2309.14211v1,cs.RO,QuadricsNet: Learning Concise Representation for Geometric Primitives in Point Clouds,2023-09-25 15:18:08+00:00,"This paper presents a novel framework to learn a concise geometric primitive
representation for 3D point clouds. Different from representing each type of
primitive individually, we focus on the challenging problem of how to achieve a
concise and uniform representation robustly. We employ quadrics to represent
diverse primitives with only 10 parameters and propose the first end-to-end
learning-based framework, namely QuadricsNet, to parse quadrics in point
clouds. The relationships between quadrics mathematical formulation and
geometric attributes, including the type, scale and pose, are insightfully
integrated for effective supervision of QuaidricsNet. Besides, a novel
pattern-comprehensive dataset with quadrics segments and objects is collected
for training and evaluation. Experiments demonstrate the effectiveness of our
concise representation and the robustness of QuadricsNet. Our code is available
at \url{https://github.com/MichaelWu99-lab/QuadricsNet}",http://arxiv.org/pdf/2309.14211v1
2309.14205v1,hep-ph,An alternative evaluation of the leading-order hadronic contribution to the muon g-2 with MUonE,2023-09-25 15:09:02+00:00,"We propose an alternative method to extract the leading-order hadronic
contribution to the muon g-2, $a_{\mu}^\text{HLO}$, with the MUonE experiment.
In contrast to the traditional method based on the integral of the hadronic
contribution to the running of the effective fine-structure constant
$\Delta\alpha_{had}$ in the space-like region, our approach relies on the
computation of the derivatives of $\Delta\alpha_{had}(t)$ at zero squared
momentum transfer $t$. We show that this approach allows to extract $\sim 99\%$
of the total value of $a_{\mu}^\text{HLO}$ from the MUonE data, while the
remaining $\sim 1\%$ can be computed combining perturbative QCD and data on
$e^+e^-$ annihilation to hadrons. This leads to a competitive evaluation of
$a_{\mu}^\text{HLO}$ which is robust against the parameterization used to model
$\Delta\alpha_{had}(t)$ in the MUonE kinematic region, thanks to the
analyticity properties of $\Delta\alpha_{had}(t)$, which can be expanded as a
polynomial at $t\sim 0$.",http://arxiv.org/pdf/2309.14205v1
2309.14203v1,cs.CV,Detecting and Grounding Multi-Modal Media Manipulation and Beyond,2023-09-25 15:05:46+00:00,"Misinformation has become a pressing issue. Fake media, in both visual and
textual forms, is widespread on the web. While various deepfake detection and
text fake news detection methods have been proposed, they are only designed for
single-modality forgery based on binary classification, let alone analyzing and
reasoning subtle forgery traces across different modalities. In this paper, we
highlight a new research problem for multi-modal fake media, namely Detecting
and Grounding Multi-Modal Media Manipulation (DGM^4). DGM^4 aims to not only
detect the authenticity of multi-modal media, but also ground the manipulated
content, which requires deeper reasoning of multi-modal media manipulation. To
support a large-scale investigation, we construct the first DGM^4 dataset,
where image-text pairs are manipulated by various approaches, with rich
annotation of diverse manipulations. Moreover, we propose a novel HierArchical
Multi-modal Manipulation rEasoning tRansformer (HAMMER) to fully capture the
fine-grained interaction between different modalities. HAMMER performs 1)
manipulation-aware contrastive learning between two uni-modal encoders as
shallow manipulation reasoning, and 2) modality-aware cross-attention by
multi-modal aggregator as deep manipulation reasoning. Dedicated manipulation
detection and grounding heads are integrated from shallow to deep levels based
on the interacted multi-modal information. To exploit more fine-grained
contrastive learning for cross-modal semantic alignment, we further integrate
Manipulation-Aware Contrastive Loss with Local View and construct a more
advanced model HAMMER++. Finally, we build an extensive benchmark and set up
rigorous evaluation metrics for this new research problem. Comprehensive
experiments demonstrate the superiority of HAMMER and HAMMER++.",http://arxiv.org/pdf/2309.14203v1
2309.14201v1,cs.GT,Towards a Theory of Maximal Extractable Value II: Uncertainty,2023-09-25 15:01:11+00:00,"Maximal Extractable Value (MEV) is value extractable by temporary monopoly
power commonly found in decentralized systems. This extraction stems from a
lack of user privacy upon transaction submission and the ability of a
monopolist validator to reorder, add, and/or censor transactions. There are two
main directions to reduce MEV: reduce the flexibility of the miner to reorder
transactions by enforcing ordering rules and/or introduce a competitive market
for the right to reorder, add, and/or censor transactions. In this work, we
unify these approaches via \emph{uncertainty principles}, akin to those found
in harmonic analysis and physics. This provides a quantitative trade-off
between the freedom to reorder transactions and the complexity of an economic
payoff to a user in a decentralized network. This trade off is analogous to the
Nyquist-Shannon sampling theorem and demonstrates that sequencing rules in
blockchains need to be application specific. Our results suggest that neither
so-called fair ordering techniques nor economic mechanisms can individually
mitigate MEV for arbitrary payoff functions.",http://arxiv.org/pdf/2309.14201v1
2309.14196v1,quant-ph,Learning Restricted Boltzmann Machines with greedy quantum search,2023-09-25 14:56:30+00:00,"Restricted Boltzmann Machines (RBMs) are widely used probabilistic undirected
graphical models with visible and latent nodes, playing an important role in
statistics and machine learning. The task of structure learning for RBMs
involves inferring the underlying graph by using samples from the visible
nodes. Specifically, learning the two-hop neighbors of each visible node allows
for the inference of the graph structure. Prior research has addressed the
structure learning problem for specific classes of RBMs, namely ferromagnetic
and locally consistent RBMs. In this paper, we extend the scope to the quantum
computing domain and propose corresponding quantum algorithms for this problem.
Our study demonstrates that the proposed quantum algorithms yield a polynomial
speedup compared to the classical algorithms for learning the structure of
these two classes of RBMs.",http://arxiv.org/pdf/2309.14196v1
2309.14187v1,cs.LO,Two tricks to trivialize higher-indexed families,2023-09-25 14:47:47+00:00,"The conventional general syntax of indexed families in dependent type
theories follow the style of ""constructors returning a special case"", as in
Agda, Lean, Idris, Coq, and probably many other systems. Fording is a method to
encode indexed families of this style with index-free inductive types and an
identity type. There is another trick that merges interleaved higher
inductive-inductive types into a single big family of types. It makes use of a
small universe as the index to distinguish the original types. In this paper,
we show that these two methods can trivialize some very fancy-looking indexed
families with higher inductive indices (which we refer to as higher indexed
families).",http://arxiv.org/pdf/2309.14187v1
2309.14179v1,physics.ao-ph,Anticipation of Oligocene's climate heartbeat by simplified eigenvalue estimation,2023-09-25 14:41:52+00:00,"The Eocene-Oligocene transition marks a watershed point of earth's climate
history. The climate shifts from a greenhouse state to an icehouse state in
which Antarctica glaciated for the first time and periodic dynamics arise which
are still relevant for our current climate. We analyse a $CaCO_3$ concentration
time series which covers the Eocene-Oligocene transition and which is obtained
from a Pacific sediment core at site DSDP1218. Therefore, we introduce a
simplified autoregression-based variant of the dominant eigenvalue (DEV)
estimation procedure. The DEV works as leading indicator of bifurcation-induced
transitions and enables us to identify the bifurcation type. We confirm its
reliability in a methodological study and demonstrate the crucial importance of
proper detrending to obtain unbiased results. As a remark, we discuss also
possible pathways to estimate the stability of limit cycles based on the DEV
and the alternative drift slope as a proof of principle. Finally, we present
the DEV analysis results of the $CaCO_3$ concentration time series which are
reproducible in a wide parameter range. Our findings demonstrate that the onset
of Oligocene's periodic dynamics might be announced by a Neimark-Sacker/Hopf
bifurcation in course of the Eocene-Oligocene transition 34 mya. (We follow the
convention and use mya$\widehat{=}$""million years ago"" and
Ma$\widehat{=}$""million years"" throughout the article.)",http://arxiv.org/pdf/2309.14179v1
2309.14176v1,cs.LG,Federated Learning Under Restricted User Availability,2023-09-25 14:40:27+00:00,"Federated Learning (FL) is a decentralized machine learning framework that
enables collaborative model training while respecting data privacy. In various
applications, non-uniform availability or participation of users is unavoidable
due to an adverse or stochastic environment, the latter often being
uncontrollable during learning. Here, we posit a generic user selection
mechanism implementing a possibly randomized, stationary selection policy,
suggestively termed as a Random Access Model (RAM). We propose a new
formulation of the FL problem which effectively captures and mitigates limited
participation of data originating from infrequent, or restricted users, at the
presence of a RAM. By employing the Conditional Value-at-Risk (CVaR) over the
(unknown) RAM distribution, we extend the expected loss FL objective to a
risk-aware objective, enabling the design of an efficient training algorithm
that is completely oblivious to the RAM, and with essentially identical
complexity as FedAvg. Our experiments on synthetic and benchmark datasets show
that the proposed approach achieves significantly improved performance as
compared with standard FL, under a variety of setups.",http://arxiv.org/pdf/2309.14176v1
2309.14169v1,math.NA,Extrapolated regularization of nearly singular integrals on surfaces,2023-09-25 14:24:29+00:00,"We present a method for computing nearly singular integrals that occur when
single or double layer surface integrals, for harmonic potentials or Stokes
flow, are evaluated at points nearby. Such values could be needed in solving an
integral equation when one surface is close to another or to obtain values at
grid points. We replace the singular kernel with a regularized version having a
length parameter $\delta$ in order to control discretization error. Analysis
near the singularity leads to an expression for the error due to regularization
which has terms with unknown coefficients multiplying known quantities. By
computing the integral with three choices of $\delta$ we can solve for an
extrapolated value that has regularization error reduced to $O(\delta^5)$. In
examples with $\delta/h$ constant and moderate resolution we observe total
error about $O(h^5)$. For convergence as $h \to 0$ we can choose $\delta$
proportional to $h^q$ with $q < 1$ to ensure the discretization error is
dominated by the regularization error. With $q = 4/5$ we find errors about
$O(h^4)$. For harmonic potentials we extend the approach to a version with
$O(\delta^7)$ regularization; it typically has smaller errors but the order of
accuracy is less predictable.",http://arxiv.org/pdf/2309.14169v1
2309.14159v1,astro-ph.HE,"Long-term monitoring of the radio-galaxy M87 in gamma-rays: joint analysis of MAGIC, VERITAS and Fermi-LAT data",2023-09-25 14:08:55+00:00,"M87 was discovered in the very-high-energy band (VHE, E > 100 GeV) with HEGRA
in 2003, long before its emission was detected in the high-energy band (HE, E >
100 MeV) with Fermi-LAT in 2009, opening the window to a new family of
extragalactic sources with tilted jets. After a series of major VHE flares in
2005, 2008, and 2010, which were detected in multiple bands, the source has
been found in a low activity state, interrupted only by comparatively
smaller-scale flares. MAGIC and VERITAS, two stereoscopic Cherenkov telescope
arrays located at Roque de los Muchachos Observatory (Canary Islands, Spain)
and the Fred Lawrence Whipple Observatory (Arizona, US), have monitored M87
continuously and in coordination for more than 10 years. In this work, we
present the data for 4 years of MAGIC and VERITAS observations corresponding to
2019, 2020, 2021 and 2022. The resulting light curves are shown in daily and
monthly scales where no significant variability is observed. In addition, we
show the first joint analysis using combined event data from the two VHE
instruments and Fermi-LAT to compute the spectral energy distribution.",http://arxiv.org/pdf/2309.14159v1
2309.14157v1,cs.CV,LAPP: Layer Adaptive Progressive Pruning for Compressing CNNs from Scratch,2023-09-25 14:08:45+00:00,"Structured pruning is a commonly used convolutional neural network (CNN)
compression approach. Pruning rate setting is a fundamental problem in
structured pruning. Most existing works introduce too many additional learnable
parameters to assign different pruning rates across different layers in CNN or
cannot control the compression rate explicitly. Since too narrow network blocks
information flow for training, automatic pruning rate setting cannot explore a
high pruning rate for a specific layer. To overcome these limitations, we
propose a novel framework named Layer Adaptive Progressive Pruning (LAPP),
which gradually compresses the network during initial training of a few epochs
from scratch. In particular, LAPP designs an effective and efficient pruning
strategy that introduces a learnable threshold for each layer and FLOPs
constraints for network. Guided by both task loss and FLOPs constraints, the
learnable thresholds are dynamically and gradually updated to accommodate
changes of importance scores during training. Therefore the pruning strategy
can gradually prune the network and automatically determine the appropriate
pruning rates for each layer. What's more, in order to maintain the expressive
power of the pruned layer, before training starts, we introduce an additional
lightweight bypass for each convolutional layer to be pruned, which only adds
relatively few additional burdens. Our method demonstrates superior performance
gains over previous compression methods on various datasets and backbone
architectures. For example, on CIFAR-10, our method compresses ResNet-20 to
40.3% without accuracy drop. 55.6% of FLOPs of ResNet-18 are reduced with 0.21%
top-1 accuracy increase and 0.40% top-5 accuracy increase on ImageNet.",http://arxiv.org/pdf/2309.14157v1
2309.14136v1,cs.CV,Masked Image Residual Learning for Scaling Deeper Vision Transformers,2023-09-25 13:45:28+00:00,"Deeper Vision Transformers (ViTs) are more challenging to train. We expose a
degradation problem in deeper layers of ViT when using masked image modeling
(MIM) for pre-training. To ease the training of deeper ViTs, we introduce a
self-supervised learning framework called \textbf{M}asked \textbf{I}mage
\textbf{R}esidual \textbf{L}earning (\textbf{MIRL}), which significantly
alleviates the degradation problem, making scaling ViT along depth a promising
direction for performance upgrade. We reformulate the pre-training objective
for deeper layers of ViT as learning to recover the residual of the masked
image. We provide extensive empirical evidence showing that deeper ViTs can be
effectively optimized using MIRL and easily gain accuracy from increased depth.
With the same level of computational complexity as ViT-Base and ViT-Large, we
instantiate 4.5{$\times$} and 2{$\times$} deeper ViTs, dubbed ViT-S-54 and
ViT-B-48. The deeper ViT-S-54, costing 3{$\times$} less than ViT-Large,
achieves performance on par with ViT-Large. ViT-B-48 achieves 86.2\% top-1
accuracy on ImageNet. On one hand, deeper ViTs pre-trained with MIRL exhibit
excellent generalization capabilities on downstream tasks, such as object
detection and semantic segmentation. On the other hand, MIRL demonstrates high
pre-training efficiency. With less pre-training time, MIRL yields competitive
performance compared to other approaches.",http://arxiv.org/pdf/2309.14136v1
2309.14110v1,physics.chem-ph,On the thermodynamic theory of curvature-dependent surface tension,2023-09-25 13:08:08+00:00,"An exact equation for determining the Tolman length (TL) as a function of
radius is obtained and a computational procedure for solving it is proposed. As
a result of implementing this procedure, the dependences of the TL and surface
tension on radius are obtained for the drop and bubble cases and various
equations of state. As one of the results of the thermodynamic study, a new
equation for the dependence of surface tension on radius (curvature effect),
alternative to the corresponding Tolman equation and associated with the
spinodal point, is obtained. The Kelvin type equation for the equimolecular
radius is shown to be exact over the entire metastability region and serves as
the basis for the TL equation. The expansions of surface tension near the
spinodal and binodal points show that the correction to Rusanov s linear
asymptotics in the first case is a series in cubes of the radius, whereas a
series in curvature holds in the second case. As a result of the analysis of
these expansions, the fundamental impossibility to determine the curvature
effect analytically from the binodal point is established; the computational
procedure determines it from the spinodal point. It is shown that just the
characteristics of the system on the spinodal, mainly the TL value at zero
radius, determine the curvature effect. In general, the theory reveals a close
connection between surface and bulk properties.",http://arxiv.org/pdf/2309.14110v1
2309.14104v1,cs.HC,Affective Game Computing: A Survey,2023-09-25 12:52:48+00:00,"This paper surveys the current state of the art in affective computing
principles, methods and tools as applied to games. We review this emerging
field, namely affective game computing, through the lens of the four core
phases of the affective loop: game affect elicitation, game affect sensing,
game affect detection and game affect adaptation. In addition, we provide a
taxonomy of terms, methods and approaches used across the four phases of the
affective game loop and situate the field within this taxonomy. We continue
with a comprehensive review of available affect data collection methods with
regards to gaming interfaces, sensors, annotation protocols, and available
corpora. The paper concludes with a discussion on the current limitations of
affective game computing and our vision for the most promising future research
directions in the field.",http://arxiv.org/pdf/2309.14104v1
2309.14103v1,math.CO,Upper clique transversals in graphs,2023-09-25 12:52:38+00:00,"A clique transversal in a graph is a set of vertices intersecting all maximal
cliques. The problem of determining the minimum size of a clique transversal
has received considerable attention in the literature. In this paper, we
initiate the study of the ""upper"" variant of this parameter, the upper clique
transversal number, defined as the maximum size of a minimal clique
transversal. We investigate this parameter from the algorithmic and complexity
points of view, with a focus on various graph classes. We show that the
corresponding decision problem is NP-complete in the classes of chordal graphs,
chordal bipartite graphs, and line graphs of bipartite graphs, but solvable in
linear time in the classes of split graphs and proper interval graphs.",http://arxiv.org/pdf/2309.14103v1
2309.14101v1,cond-mat.mtrl-sci,Coulomb potential screening via charged carriers and charge-neutral dipoles/excitons in two-dimensional case,2023-09-25 12:50:49+00:00,"With the shrink of dimensionality, Coulomb interaction displays a distinct
role owing to the reduced dielectric screening in out-of-plane direction. Apart
from the dielectric screening, the free charge carriers and/or dipoles can also
make nonnegligible contribution to Coulomb interaction. While the Thomas Fermi
model is effective in describing charge carrier screening in three dimensions,
the extent of screening to two-dimension resulting from charge-neutral dipoles
and carriers remains quantitatively unclear. To address this gap, we present a
simple analytical solution based on linear response theory, offering a
comprehensive depiction of the Coulomb screened potential in both 2D and 3D
systems, where screening effects from both charge carriers and charge-neutral
dipoles are addressed. Our work provides a handy tool for directly analysing
and evaluating Coulomb interaction strength in atomically thin materials and
particularly in the context of electronic and optoelectronic engineering. As a
demonstration, we utilize the derived modified Coulomb potential for the
exciton system to estimate the exciton binding energy variation arising from
exciton density fluctuation and the temperature dependent exciton
polarizability, yielding excellent agreement with the experimental and
computational findings.",http://arxiv.org/pdf/2309.14101v1
2309.14090v1,cs.LG,Convolutional autoencoder-based multimodal one-class classification,2023-09-25 12:31:18+00:00,"One-class classification refers to approaches of learning using data from a
single class only. In this paper, we propose a deep learning one-class
classification method suitable for multimodal data, which relies on two
convolutional autoencoders jointly trained to reconstruct the positive input
data while obtaining the data representations in the latent space as compact as
possible. During inference, the distance of the latent representation of an
input to the origin can be used as an anomaly score. Experimental results using
a multimodal macroinvertebrate image classification dataset show that the
proposed multimodal method yields better results as compared to the unimodal
approach. Furthermore, study the effect of different input image sizes, and we
investigate how recently proposed feature diversity regularizers affect the
performance of our approach. We show that such regularizers improve
performance.",http://arxiv.org/pdf/2309.14090v1
2309.14072v1,cs.CV,BoIR: Box-Supervised Instance Representation for Multi-Person Pose Estimation,2023-09-25 12:06:19+00:00,"Single-stage multi-person human pose estimation (MPPE) methods have shown
great performance improvements, but existing methods fail to disentangle
features by individual instances under crowded scenes. In this paper, we
propose a bounding box-level instance representation learning called BoIR,
which simultaneously solves instance detection, instance disentanglement, and
instance-keypoint association problems. Our new instance embedding loss
provides a learning signal on the entire area of the image with bounding box
annotations, achieving globally consistent and disentangled instance
representation. Our method exploits multi-task learning of bottom-up keypoint
estimation, bounding box regression, and contrastive instance embedding
learning, without additional computational cost during inference. BoIR is
effective for crowded scenes, outperforming state-of-the-art on COCO val (0.8
AP), COCO test-dev (0.5 AP), CrowdPose (4.9 AP), and OCHuman (3.5 AP). Code
will be available at https://github.com/uyoung-jeong/BoIR",http://arxiv.org/pdf/2309.14072v1
2309.14070v1,hep-lat,Scalar and tensor charmonium resonances in coupled-channel scattering from QCD,2023-09-25 12:04:37+00:00,"We determine $J^{PC}=0^{++}$ and $2^{++}$ hadron-hadron scattering amplitudes
in the charmonium energy region up to 4100 MeV using lattice QCD, a
first-principles approach to QCD. Working at $m_\pi\approx 391$ MeV, more than
200 finite-volume energy levels are computed and these are used in extensions
of the L\""uscher formalism to determine infinite-volume coupled-channel
scattering amplitudes. We find that this energy region contains a single
$\chi_{c0}$ and a single $\chi_{c2}$ resonance. Both are found as pole
singularities on the closest unphysical Riemann sheet, just below 4000 MeV with
widths around 70 MeV. The largest couplings are to kinematically-closed $D^*
\bar{D}^*$ channels in $S$-wave, and couplings to several decay channels
consisting of pairs of open-charm mesons are found to be large and significant
in both cases. Above the ground state $\chi_{c0}$, no other scalar bound-states
or near-$D\bar{D}$ threshold resonances are found, in contrast to several
theoretical and experimental studies.",http://arxiv.org/pdf/2309.14070v1
2309.14068v1,cs.LG,Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models,2023-09-25 12:03:32+00:00,"Because diffusion models have shown impressive performances in a number of
tasks, such as image synthesis, there is a trend in recent works to prove (with
certain assumptions) that these models have strong approximation capabilities.
In this paper, we show that current diffusion models actually have an
expressive bottleneck in backward denoising and some assumption made by
existing theoretical guarantees is too strong. Based on this finding, we prove
that diffusion models have unbounded errors in both local denoising and global
approximation. In light of our theoretical studies, we introduce soft mixture
denoising (SMD), an expressive and efficient model for backward denoising. SMD
not only permits diffusion models to well approximate any Gaussian mixture
distributions in theory, but also is simple and efficient for implementation.
Our experiments on multiple image datasets show that SMD significantly improves
different types of diffusion models (e.g., DDPM), especially in the situation
of few backward iterations.",http://arxiv.org/pdf/2309.14068v1
2309.14062v1,cs.CV,FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning,2023-09-25 11:54:33+00:00,"Exemplar-free class-incremental learning (CIL) poses several challenges since
it prohibits the rehearsal of data from previous tasks and thus suffers from
catastrophic forgetting. Recent approaches to incrementally learning the
classifier by freezing the feature extractor after the first task have gained
much attention. In this paper, we explore prototypical networks for CIL, which
generate new class prototypes using the frozen feature extractor and classify
the features based on the Euclidean distance to the prototypes. In an analysis
of the feature distributions of classes, we show that classification based on
Euclidean metrics is successful for jointly trained features. However, when
learning from non-stationary data, we observe that the Euclidean metric is
suboptimal and that feature distributions are heterogeneous. To address this
challenge, we revisit the anisotropic Mahalanobis distance for CIL. In
addition, we empirically show that modeling the feature covariance relations is
better than previous attempts at sampling features from normal distributions
and training a linear classifier. Unlike existing methods, our approach
generalizes to both many- and few-shot CIL settings, as well as to
domain-incremental settings. Interestingly, without updating the backbone
network, our method obtains state-of-the-art results on several standard
continual learning benchmarks. Code is available at
https://github.com/dipamgoswami/FeCAM.",http://arxiv.org/pdf/2309.14062v1
2309.14058v1,math.GT,"Knot Floer homology and the fundamental group of $ (1,1) $ knots",2023-09-25 11:53:19+00:00,"We give an algorithm for computing the knot Floer homology of a $ (1,1) $
knot from a particular presentation of its fundamental group.",http://arxiv.org/pdf/2309.14058v1
2309.14057v1,cs.CV,Weakly Supervised Semantic Segmentation by Knowledge Graph Inference,2023-09-25 11:50:19+00:00,"Currently, existing efforts in Weakly Supervised Semantic Segmentation (WSSS)
based on Convolutional Neural Networks (CNNs) have predominantly focused on
enhancing the multi-label classification network stage, with limited attention
given to the equally important downstream segmentation network. Furthermore,
CNN-based local convolutions lack the ability to model the extensive
inter-category dependencies. Therefore, this paper introduces a graph
reasoning-based approach to enhance WSSS. The aim is to improve WSSS
holistically by simultaneously enhancing both the multi-label classification
and segmentation network stages. In the multi-label classification network
segment, external knowledge is integrated, coupled with GCNs, to globally
reason about inter-class dependencies. This encourages the network to uncover
features in non-salient regions of images, thereby refining the completeness of
generated pseudo-labels. In the segmentation network segment, the proposed
Graph Reasoning Mapping (GRM) module is employed to leverage knowledge obtained
from textual databases, facilitating contextual reasoning for class
representation within image regions. This GRM module enhances feature
representation in high-level semantics of the segmentation network's local
convolutions, while dynamically learning semantic coherence for individual
samples. Using solely image-level supervision, we have achieved
state-of-the-art performance in WSSS on the PASCAL VOC 2012 and MS-COCO
datasets. Extensive experimentation on both the multi-label classification and
segmentation network stages underscores the effectiveness of the proposed graph
reasoning approach for advancing WSSS.",http://arxiv.org/pdf/2309.14057v1
2309.14056v1,physics.optics,Unveiling the dynamical diversity of quantum dot lasers subject to optoelectronic feedback,2023-09-25 11:49:17+00:00,"This paper investigates experimentally and numerically the nonlinear dynamics
of an epitaxial quantum dot laser on silicon subjected to optoelectronic
feedback. Experimental results showcase a diverse range of dynamics,
encompassing square wave patterns, quasi-chaotic states, and mixed waveforms
exhibiting fast and slow oscillations. These measurements unequivocally
demonstrate that quantum dot lasers on silicon readily and stably generate a
more extensive repertoire of nonlinear dynamics compared to quantum well
lasers. This pronounced sensitivity of quantum dot lasers to optoelectronic
feedback represents a notable departure from their inherent insensitivity to
optical feedback arising from reflections. Moreover, based on the Ikeda-like
model, our simulations illustrate that the inherent characteristics of quantum
dot lasers on silicon enable rapid and diverse dynamic transformations in
response to optoelectronic feedback. The emergence of these exotic dynamics
paves the way for further applications like integrated optical clocks, optical
logic, and optical computing.",http://arxiv.org/pdf/2309.14056v1
2309.14052v1,cs.CV,Single Image Test-Time Adaptation for Segmentation,2023-09-25 11:31:18+00:00,"Test-Time Adaptation (TTA) methods improve the robustness of deep neural
networks to domain shift on a variety of tasks such as image classification or
segmentation. This work explores adapting segmentation models to a single
unlabelled image with no other data available at test-time. In particular, this
work focuses on adaptation by optimizing self-supervised losses at test-time.
Multiple baselines based on different principles are evaluated under diverse
conditions and a novel adversarial training is introduced for adaptation with
mask refinement. Our additions to the baselines result in a 3.51 and 3.28 %
increase over non-adapted baselines, without these improvements, the increase
would be 1.7 and 2.16 % only.",http://arxiv.org/pdf/2309.14052v1
2309.14051v1,physics.flu-dyn,Point torque representations of ciliary flows,2023-09-25 11:26:02+00:00,"Ciliary flows are generated by a vast array of eukaryotic organisms, from
unicellular algae to mammals, and occur in a range of different geometrical
configurations. We employ a point torque -- or `rotlet' -- model to capture the
time-averaged ciliary flow above a planar rigid wall. We demonstrate the
advantages (i.e. accuracy and computational efficiency) of using this, arguably
simpler approach compared to other singularity-based models in Stokes flows.
Then, in order to model ciliary flows in confined spaces, we extend the point
torque solution to a bounded domain between two plane parallel no-slip walls.
The flow field is resolved using the method of images and Fourier transforms,
and we analyze the role of confinement by comparing the resultant fluid
velocity to that of a rotlet near a single wall. Our results suggest that the
flow field of a single cilium is not changed significantly by the confinement,
even when the distance between the walls is commensurate with the cilium's
length.",http://arxiv.org/pdf/2309.14051v1
2309.14047v1,cond-mat.dis-nn,Random-Energy Secret Sharing via Extreme Synergy,2023-09-25 11:23:16+00:00,"The random-energy model (REM), a solvable spin-glass model, has impacted an
incredibly diverse set of problems, from protein folding to combinatorial
optimization to many-body localization. Here, we explore a new connection to
secret sharing. We formulate a secret-sharing scheme, based on the REM, and
analyze its information-theoretic properties. Our analyses reveal that the
correlations between subsystems of the REM are highly synergistic and form the
basis for secure secret-sharing schemes. We derive the ranges of temperatures
and secret lengths over which the REM satisfies the requirement of secure
secret sharing. We show further that a special point in the phase diagram
exists at which the REM-based scheme is optimal in its information encoding.
Our analytical results for the thermodynamic limit are in good qualitative
agreement with numerical simulations of finite systems, for which the strict
security requirement is replaced by a tradeoff between secrecy and
recoverability. Our work offers a further example of information theory as a
unifying concept, connecting problems in statistical physics to those in
computation.",http://arxiv.org/pdf/2309.14047v1
2309.14036v1,hep-th,Taking the Null-Hypersurface Limit in the Parikh-Wilczek Membrane Approach,2023-09-25 11:03:41+00:00,"We consider subtleties of the horizon (null-hypersurface) limit in the
Parikh-Wilczek Membrane Approach to Black Holes. Specifically, we refine the
correspondence between the (projected) Einstein equations of gravity with
matter and the Raychaudhuri-Damour-Navier-Stokes (RDNS) equations of
relativistic hydrodynamics. For a general configuration of gravity with matter
we obtain additional terms in the hydrodynamic equations, which include
logarithmic derivarives of a parameter (the regularization function)
determining the proximity of a stretched membrane to the BH horizon. Direct
computation of the new terms for exact (Schwarzschild and Kerr) solutions to
the Einstein equations results in vanishing the additions to the RDNS equations
in the horizon limit. For spacetimes, which are not exact solutions to the
Einstein equations, as, for instance, for space-time configurations mimicking
black holes, taking into account new terms in the RDNS equations is the
mandatory operation. We also comment the correspondence between the horizon
limit of the Parikh-Wilczek Membrane Approach and the Gourgoulhon-Jaramillo
method of a null-hypersurface description, as well as the link of the obtained
results to our previous work on the Kerr black holes.",http://arxiv.org/pdf/2309.14036v1
2309.14035v1,cond-mat.str-el,Applicability and limitations of cluster perturbation theory for Hubbard models,2023-09-25 11:02:02+00:00,"We present important use cases and limitations when considering results
obtained from Cluster Perturbation Theory (CPT). CPT combines the solutions of
small individual clusters of an infinite lattice system with the Bloch theory
of conventional band theory in order to provide an approximation for the
Green's function in the thermodynamic limit. To this end we are investigating
single-band and multi-band Hubbard models in one- and two-dimensional systems.
A special interest is taken in the supposed pseudo gap regime of the
two-dimensional square lattice at half filling and intermediate interaction
strength ($U \leq 3t$) as well as the metal-insulator transition. We point out
that the finite-size level spacing of the cluster limits the resolution of
spectral features within CPT. This restricts the investigation of asymptotic
properties of the metal-insulator transition, as it would require much larger
cluster sizes that are beyond computational capabilities.",http://arxiv.org/pdf/2309.14035v1
2309.14031v1,math.NA,Phase-space iterative solvers,2023-09-25 10:52:31+00:00,"I introduce a new iterative method to solve problems in small-strain
non-linear elasticity. The method is inspired by recent work in data-driven
computational mechanics, which reformulated the classic boundary value problem
of continuum mechanics using the concept of ""phase space"". The latter is an
abstract metric space, whose coordinates are indexed by strains and stress
components, where each possible state of the discretized body corresponds to a
point. Since the phase space is associated to the discretized body, it is
finite dimensional. Two subsets are then defined: an affine space termed
""physically-admissible set"" made up by those points that satisfy equilibrium
and a ""materially-admissible set"" containing points that satisfy the
constitutive law. Solving the boundary-value problem amounts to finding the
intersection between these two subdomains. In the linear-elastic setting, this
can be achieved through the solution of a set of linear equations; when
material non-linearity enters the picture, such is not the case anymore and
iterative solution approaches are necessary. Our iterative method consists on
projecting points alternatively from one set to the other, until convergence.
The method is similar in spirit to the ""method of alternative projections"" and
to the ""method of projections onto convex sets"", for which there is a solid
mathematical foundation that furnishes conditions for existence and uniqueness
of solutions, upon which we rely to uphold our new method's performance. We
present two examples to illustrate the applicability of the method, and to
showcase its strengths when compared to the classic Newton-Raphson method, the
usual tool of choice in non-linear continuum mechanics.",http://arxiv.org/pdf/2309.14031v1
2309.14022v1,cs.CV,Hashing Neural Video Decomposition with Multiplicative Residuals in Space-Time,2023-09-25 10:36:14+00:00,"We present a video decomposition method that facilitates layer-based editing
of videos with spatiotemporally varying lighting and motion effects. Our neural
model decomposes an input video into multiple layered representations, each
comprising a 2D texture map, a mask for the original video, and a
multiplicative residual characterizing the spatiotemporal variations in
lighting conditions. A single edit on the texture maps can be propagated to the
corresponding locations in the entire video frames while preserving other
contents' consistencies. Our method efficiently learns the layer-based neural
representations of a 1080p video in 25s per frame via coordinate hashing and
allows real-time rendering of the edited result at 71 fps on a single GPU.
Qualitatively, we run our method on various videos to show its effectiveness in
generating high-quality editing effects. Quantitatively, we propose to adopt
feature-tracking evaluation metrics for objectively assessing the consistency
of video editing. Project page: https://lightbulb12294.github.io/hashing-nvd/",http://arxiv.org/pdf/2309.14022v1
2309.14002v1,cond-mat.mtrl-sci,Calculating the Circular Dichroism of Chiral Halide Perovskites: A Tight-Binding Approach,2023-09-25 10:10:17+00:00,"Chiral metal halide perovskites have emerged as promising optoelectronic
materials for emission and detection of circular polarized visible light.
Despite chirality being realized by adding chiral organic cations or ligands,
the chiroptical activity originates from the metal halide framework. The
mechanism is not well understood, as an overarching modeling framework is
lacking. Capturing chirality requires going beyond electric dipole transitions,
the common approximation in condensed matter calculations. We present a density
functional theory (DFT) parameterized tight-binding (TB) model, which allows us
to calculate optical properties including circular dichroism (CD) at low
computational cost. Comparing Pb-based chiral perovskites with different
organic cations and halide anions, we find that the structural helicity within
the metal halide layers determines the size of the CD. Our results mark an
important step in understanding the complex correlations of structural,
electronic and optical properties of chiral perovskites, and provide a useful
tool to predict new compounds with desired properties for novel optoelectronic
applications.",http://arxiv.org/pdf/2309.14002v1
2309.13992v1,cond-mat.mes-hall,All-electrical detection of the spin-charge conversion in nanodevices based on SrTiO3 two-dimensional electron gases,2023-09-25 09:47:00+00:00,"The Magnetoelectric Spin-Orbit (MESO) technology aims to bring logic into
memory by combining a ferromagnet with a magnetoelectric (ME) element for
information writing, and a spin-orbit (SO) element for information read-out
through spin-charge conversion. Among candidate SO materials to achieve a large
MESO output signal, oxide Rashba two-dimensional electron gases (2DEGs) have
shown very large spin-charge conversion efficiencies, albeit mostly in
spin-pumping experiments. Here, we report all-electrical spin-injection and
spin-charge conversion experiments in nanoscale devices harnessing the inverse
Edelstein effect of SrTiO3 2DEGs. We have designed, patterned and fabricated
nanodevices in which a spin current injected from a cobalt layer into the 2DEG
is converted into a charge current. We optimized the spin-charge conversion
signal by applying back-gate voltages, and studied its temperature evolution.
We further disentangled the inverse Edelstein contribution from spurious
effects such as the planar Hall effect, the anomalous Hall effect or the
anisotropic magnetoresistance. The combination of non-volatility and high
energy efficiency of these devices could potentially lead to new technology
paradigms for beyond-CMOS computing architectures.",http://arxiv.org/pdf/2309.13992v1
2309.13990v1,physics.comp-ph,"Supervised, semi-supervised, and unsupervised learning of the Domany-Kinzel model",2023-09-25 09:41:15+00:00,"The Domany Kinzel (DK) model encompasses several types of non-equilibrium
phase transitions, depending on the selected parameters. We apply supervised,
semi-supervised, and unsupervised learning methods to studying the phase
transitions and critical behaviors of the (1 + 1)-dimensional DK model. The
supervised and the semi-supervised learning methods permit the estimations of
the critical points, the spatial and temporal correlation exponents, concerning
labelled and unlabelled DK configurations, respectively. Furthermore, we also
predict the critical points by employing principal component analysis (PCA) and
autoencoder. The PCA and autoencoder can produce results in good agreement with
simulated particle number density.",http://arxiv.org/pdf/2309.13990v1
2309.13985v1,cs.LG,Physics-Driven ML-Based Modelling for Correcting Inverse Estimation,2023-09-25 09:37:19+00:00,"When deploying machine learning estimators in science and engineering (SAE)
domains, it is critical to avoid failed estimations that can have disastrous
consequences, e.g., in aero engine design. This work focuses on detecting and
correcting failed state estimations before adopting them in SAE inverse
problems, by utilizing simulations and performance metrics guided by physical
laws. We suggest to flag a machine learning estimation when its physical model
error exceeds a feasible threshold, and propose a novel approach, GEESE, to
correct it through optimization, aiming at delivering both low error and high
efficiency. The key designs of GEESE include (1) a hybrid surrogate error model
to provide fast error estimations to reduce simulation cost and to enable
gradient based backpropagation of error feedback, and (2) two generative models
to approximate the probability distributions of the candidate states for
simulating the exploitation and exploration behaviours. All three models are
constructed as neural networks. GEESE is tested on three real-world SAE inverse
problems and compared to a number of state-of-the-art optimization/search
approaches. Results show that it fails the least number of times in terms of
finding a feasible state correction, and requires physical evaluations less
frequently in general.",http://arxiv.org/pdf/2309.13985v1
2309.13975v1,cs.CV,Diverse Semantic Image Editing with Style Codes,2023-09-25 09:22:18+00:00,"Semantic image editing requires inpainting pixels following a semantic map.
It is a challenging task since this inpainting requires both harmony with the
context and strict compliance with the semantic maps. The majority of the
previous methods proposed for this task try to encode the whole information
from erased images. However, when an object is added to a scene such as a car,
its style cannot be encoded from the context alone. On the other hand, the
models that can output diverse generations struggle to output images that have
seamless boundaries between the generated and unerased parts. Additionally,
previous methods do not have a mechanism to encode the styles of visible and
partially visible objects differently for better performance. In this work, we
propose a framework that can encode visible and partially visible objects with
a novel mechanism to achieve consistency in the style encoding and final
generations. We extensively compare with previous conditional image generation
and semantic image editing algorithms. Our extensive experiments show that our
method significantly improves over the state-of-the-art. Our method not only
achieves better quantitative results but also provides diverse results. Please
refer to the project web page for the released code and demo:
https://github.com/hakansivuk/DivSem.",http://arxiv.org/pdf/2309.13975v1
2309.13967v1,quant-ph,No free lunch theorems for quantum state measurements as resources in classical sampling and generative modelling,2023-09-25 09:05:16+00:00,"We prove that $\textit{almost all}$ quantum states, when sampled according to
the Haar measure over the unitary group, have the following property: if copies
of the state are measured to provide latent random variables which are taken as
an input in a classical generative model or sampling algorithm, then any
alternative state whose measurements can generate the same set of target
distributions will do so with the same overall cost. Here, we define the
overall cost as the aggregate computational complexity of sampling from all
possible distributions that can be prepared from the given input distribution.
Our result holds for any length of input and output bitstring and when a
uniformly random bitstring of any length is optionally provided as an
additional resource. As it is easy to construct scenarios where a pair of
alternative candidate states are such that classical simulation of the
preparation thereof is easy in one case and hard in the other, the result can
be viewed as decoupling how hard it is to obtain a latent random variable, and
how useful it is as a resource in classical sampling and generative modelling.",http://arxiv.org/pdf/2309.13967v1
2309.13961v1,quant-ph,A hybrid quantum-classical approach to warm-starting optimization,2023-09-25 08:53:54+00:00,"The Quantum Approximate Optimization Algorithm (QAOA) is a promising
candidate for solving combinatorial optimization problems more efficiently than
classical computers. Recent studies have shown that warm-starting the standard
algorithm improves the performance. In this paper we compare the performance of
standard QAOA with that of warm-start QAOA in the context of portfolio
optimization and investigate the warm-start approach for different problem
instances. In particular, we analyze the extent to which the improved
performance of warm-start QAOA is due to quantum effects, and show that the
results can be reproduced or even surpassed by a purely classical preprocessing
of the original problem followed by standard QAOA.",http://arxiv.org/pdf/2309.13961v1
2309.13956v1,cs.CV,In-Domain GAN Inversion for Faithful Reconstruction and Editability,2023-09-25 08:42:06+00:00,"Generative Adversarial Networks (GANs) have significantly advanced image
synthesis through mapping randomly sampled latent codes to high-fidelity
synthesized images. However, applying well-trained GANs to real image editing
remains challenging. A common solution is to find an approximate latent code
that can adequately recover the input image to edit, which is also known as GAN
inversion. To invert a GAN model, prior works typically focus on reconstructing
the target image at the pixel level, yet few studies are conducted on whether
the inverted result can well support manipulation at the semantic level. This
work fills in this gap by proposing in-domain GAN inversion, which consists of
a domain-guided encoder and a domain-regularized optimizer, to regularize the
inverted code in the native latent space of the pre-trained GAN model. In this
way, we manage to sufficiently reuse the knowledge learned by GANs for image
reconstruction, facilitating a wide range of editing applications without any
retraining. We further make comprehensive analyses on the effects of the
encoder structure, the starting inversion point, as well as the inversion
parameter space, and observe the trade-off between the reconstruction quality
and the editing property. Such a trade-off sheds light on how a GAN model
represents an image with various semantics encoded in the learned latent
distribution. Code, models, and demo are available at the project page:
https://genforce.github.io/idinvert/.",http://arxiv.org/pdf/2309.13956v1
2309.13955v1,math.NA,Deep Reinforcement Learning for the Heat Transfer Control of Pulsating Impinging Jets,2023-09-25 08:41:50+00:00,"This research study explores the applicability of Deep Reinforcement Learning
(DRL) for thermal control based on Computational Fluid Dynamics. To accomplish
that, the forced convection on a hot plate prone to a pulsating cooling jet
with variable velocity has been investigated. We begin with evaluating the
efficiency and viability of a vanilla Deep Q-Network (DQN) method for thermal
control. Subsequently, a comprehensive comparison between different variants of
DRL is conducted. Soft Double and Duel DQN achieved better thermal control
performance among all the variants due to their efficient learning and action
prioritization capabilities. Results demonstrate that the soft Double DQN
outperforms the hard Double DQN. Moreover, soft Double and Duel can maintain
the temperature in the desired threshold for more than 98% of the control
cycle. These findings demonstrate the promising potential of DRL in effectively
addressing thermal control systems.",http://arxiv.org/pdf/2309.13955v1
2309.13940v1,cs.CV,A Lightweight Recurrent Grouping Attention Network for Video Super-Resolution,2023-09-25 08:21:49+00:00,"Effective aggregation of temporal information of consecutive frames is the
core of achieving video super-resolution. Many scholars have utilized
structures such as sliding windows and recurrent to gather spatio-temporal
information of frames. However, although the performance of the constructed VSR
models is improving, the size of the models is also increasing, exacerbating
the demand on the equipment. Thus, to reduce the stress on the device, we
propose a novel lightweight recurrent grouping attention network. The
parameters of this model are only 0.878M, which is much lower than the current
mainstream model for studying video super-resolution. We design forward feature
extraction module and backward feature extraction module to collect temporal
information between consecutive frames from two directions. Moreover, a new
grouping mechanism is proposed to efficiently collect spatio-temporal
information of the reference frame and its neighboring frames. The attention
supplementation module is presented to further enhance the information
gathering range of the model. The feature reconstruction module aims to
aggregate information from different directions to reconstruct high-resolution
features. Experiments demonstrate that our model achieves state-of-the-art
performance on multiple datasets.",http://arxiv.org/pdf/2309.13940v1
2309.13935v1,math.AG,Spherical Geometry of Hilbert Schemes of Conics in Adjoint Varieties,2023-09-25 08:07:42+00:00,"For each adjoint variety not of type $A$ or $C$, we study the irreducible
component of the Hilbert scheme which parametrizes all smooth conics. We prove
that its normalization is a spherical variety by using contact geometry, and
then compute the colored fan of the normalization. As a corollary, we describe
the conjugacy classes of conics in the adjoint variety and show smoothness of
the normalization. Similar results on the Chow scheme of the adjoint variety
are also presented.",http://arxiv.org/pdf/2309.13935v1
2309.13930v1,cs.LG,SAMN: A Sample Attention Memory Network Combining SVM and NN in One Architecture,2023-09-25 08:01:05+00:00,"Support vector machine (SVM) and neural networks (NN) have strong
complementarity. SVM focuses on the inner operation among samples while NN
focuses on the operation among the features within samples. Thus, it is
promising and attractive to combine SVM and NN, as it may provide a more
powerful function than SVM or NN alone. However, current work on combining them
lacks true integration. To address this, we propose a sample attention memory
network (SAMN) that effectively combines SVM and NN by incorporating sample
attention module, class prototypes, and memory block to NN. SVM can be viewed
as a sample attention machine. It allows us to add a sample attention module to
NN to implement the main function of SVM. Class prototypes are representatives
of all classes, which can be viewed as alternatives to support vectors. The
memory block is used for the storage and update of class prototypes. Class
prototypes and memory block effectively reduce the computational cost of sample
attention and make SAMN suitable for multi-classification tasks. Extensive
experiments show that SAMN achieves better classification performance than
single SVM or single NN with similar parameter sizes, as well as the previous
best model for combining SVM and NN. The sample attention mechanism is a
flexible module that can be easily deepened and incorporated into neural
networks that require it.",http://arxiv.org/pdf/2309.13930v1
2309.13927v1,quant-ph,ZZ-Interaction-Free Single-Qubit-Gate Optimization in Superconducting Qubits,2023-09-25 07:49:27+00:00,"Overcoming the issue of qubit-frequency fluctuations is essential to realize
stable and practical quantum computing with solid-state qubits. Static ZZ
interaction, which causes a frequency shift of a qubit depending on the state
of neighboring qubits, is one of the major obstacles to integrating
fixed-frequency transmon qubits. Here we propose and experimentally demonstrate
ZZ-interaction-free single-qubit-gate operations on a superconducting transmon
qubit by utilizing a semi-analytically optimized pulse based on a perturbative
analysis. The gate is designed to be robust against slow qubit-frequency
fluctuations. The robustness of the optimized gate spans a few MHz, which is
sufficient for suppressing the adverse effects of the ZZ interaction. Our
result paves the way for an efficient approach to overcoming the issue of ZZ
interaction without any additional hardware overhead.",http://arxiv.org/pdf/2309.13927v1
2309.13924v1,cs.CV,Recursive Counterfactual Deconfounding for Object Recognition,2023-09-25 07:46:41+00:00,"Image recognition is a classic and common task in the computer vision field,
which has been widely applied in the past decade. Most existing methods in
literature aim to learn discriminative features from labeled images for
classification, however, they generally neglect confounders that infiltrate
into the learned features, resulting in low performances for discriminating
test images. To address this problem, we propose a Recursive Counterfactual
Deconfounding model for object recognition in both closed-set and open-set
scenarios based on counterfactual analysis, called RCD. The proposed model
consists of a factual graph and a counterfactual graph, where the relationships
among image features, model predictions, and confounders are built and updated
recursively for learning more discriminative features. It performs in a
recursive manner so that subtler counterfactual features could be learned and
eliminated progressively, and both the discriminability and generalization of
the proposed model could be improved accordingly. In addition, a negative
correlation constraint is designed for alleviating the negative effects of the
counterfactual features further at the model training stage. Extensive
experimental results on both closed-set recognition task and open-set
recognition task demonstrate that the proposed RCD model performs better than
11 state-of-the-art baselines significantly in most cases.",http://arxiv.org/pdf/2309.13924v1
2309.13904v1,cs.CV,Subspace-Aware Feature Reconstruction for Unsupervised Anomaly Localization,2023-09-25 06:58:57+00:00,"Unsupervised anomaly localization, which plays a critical role in industrial
manufacturing, is to identify anomalous regions that deviate from patterns
established exclusively from nominal samples. Recent mainstream methods focus
on approximating the target feature distribution by leveraging embeddings from
ImageNet models. However, a common issue in many anomaly localization methods
is the lack of adaptability of the feature approximations to specific targets.
Consequently, their ability to effectively identify anomalous regions relies
significantly on the data coverage provided by the finite resources in a memory
bank. In this paper, we propose a novel subspace-aware feature reconstruction
framework for anomaly localization. To achieve adaptive feature approximation,
our proposed method involves the reconstruction of the feature representation
through the self-expressive model designed to learn low-dimensional subspaces.
Importantly, the sparsity of the subspace representation contributes to
covering feature patterns from the same subspace with fewer resources, leading
to a reduction in the memory bank. Extensive experiments across three
industrial benchmark datasets demonstrate that our approach achieves
competitive anomaly localization performance compared to state-of-the-art
methods by adaptively reconstructing target features with a small number of
samples.",http://arxiv.org/pdf/2309.13904v1
2309.13889v1,eess.SY,Resilient State Estimation for Nonlinear Discrete-Time Systems via Input and State Interval Observer Synthesis,2023-09-25 06:03:15+00:00,"This paper addresses the problem of resilient state estimation and attack
reconstruction for bounded-error nonlinear discrete-time systems with nonlinear
observations/ constraints, where both sensors and actuators can be compromised
by false data injection attack signals/unknown inputs. By leveraging
mixed-monotone decomposition of nonlinear functions, as well as affine parallel
outer-approximation of the observation functions, along with introducing
auxiliary states to cancel out the effect of the attacks/unknown inputs, our
proposed observer recursively computes interval estimates that by construction,
contain the true states and unknown inputs of the system. Moreover, we provide
several semi-definite programs to synthesize observer gains to ensure
input-to-state stability of the proposed observer and optimality of the design
in the sense of minimum $\mathcal{H}_{\infty}$ gain.",http://arxiv.org/pdf/2309.13889v1
2309.13881v2,cs.CV,Skip-Connected Neural Networks with Layout Graphs for Floor Plan Auto-Generation,2023-09-25 05:20:57+00:00,"With the advent of AI and computer vision techniques, the quest for automated
and efficient floor plan designs has gained momentum. This paper presents a
novel approach using skip-connected neural networks integrated with layout
graphs. The skip-connected layers capture multi-scale floor plan information,
and the encoder-decoder networks with GNN facilitate pixel-level
probability-based generation. Validated on the MSD dataset, our approach
achieved a 93.9 mIoU score in the 1st CVAAD workshop challenge. Code and
pre-trained models are publicly available at
https://github.com/yuntaeJ/SkipNet-FloorPlanGe.",http://arxiv.org/pdf/2309.13881v2
2309.13873v1,cs.SY,Guaranteed Privacy-Preserving $\mathcal{H}_{\infty}$-Optimal Interval Observer Design for Bounded-Error LTI Systems,2023-09-25 04:55:48+00:00,"This paper furthers current research into the notion of guaranteed privacy,
which provides a deterministic characterization of the privacy of output
signals of a dynamical system or mechanism. Unlike stochastic differential
privacy, guaranteed privacy offers strict bounds on the proximity between the
ranges of two sets of estimated data. Our approach relies on synthesizing an
interval observer for linear time-invariant (LTI) bounded-error systems. The
design procedure incorporates a bounded noise perturbation factor computation
and an observer gain synthesis. The observer simultaneously provides guaranteed
private and stable interval-valued estimates for the desired variable. We
demonstrate the optimality of our design by minimizing the
$\mathcal{H}_{\infty}$ norm of the observer error system. Lastly, we assess the
accuracy of our proposed mechanism by quantifying the loss incurred when
considering guaranteed privacy specifications, and illustrate our approach
outperformance to differential privacy through simulations.",http://arxiv.org/pdf/2309.13873v1
2309.13867v1,math.RT,Cellularity of KLR and weighted KLRW algebras via crystals,2023-09-25 04:31:36+00:00,"We prove that the weighted KLRW algebras of finite type, and their cyclotomic
quotients, are cellular algebras. The cellular bases are explicitly described
using crystal graphs. As a special case, this proves that the KLR algebras of
finite type are cellular. As one application, we compute the graded
decomposition numbers of the cyclotomic algebras.",http://arxiv.org/pdf/2309.13867v1
2309.13866v2,cs.LG,On Calibration of Modern Quantized Efficient Neural Networks,2023-09-25 04:30:18+00:00,"We explore calibration properties at various precisions for three
architectures: ShuffleNetv2, GhostNet-VGG, and MobileOne; and two datasets:
CIFAR-100 and PathMNIST. The quality of calibration is observed to track the
quantization quality; it is well-documented that performance worsens with lower
precision, and we observe a similar correlation with poorer calibration. This
becomes especially egregious at 4-bit activation regime. GhostNet-VGG is shown
to be the most robust to overall performance drop at lower precision. We find
that temperature scaling can improve calibration error for quantized networks,
with some caveats. We hope that these preliminary insights can lead to more
opportunities for explainable and reliable EdgeML.",http://arxiv.org/pdf/2309.13866v2
2309.13863v1,cs.CV,SuPerPM: A Large Deformation-Robust Surgical Perception Framework Based on Deep Point Matching Learned from Physical Constrained Simulation Data,2023-09-25 04:27:06+00:00,"Manipulation of tissue with surgical tools often results in large
deformations that current methods in tracking and reconstructing algorithms
have not effectively addressed. A major source of tracking errors during large
deformations stems from wrong data association between observed sensor
measurements with previously tracked scene. To mitigate this issue, we present
a surgical perception framework, SuPerPM, that leverages learning-based
non-rigid point cloud matching for data association, thus accommodating larger
deformations. The learning models typically require training data with ground
truth point cloud correspondences, which is challenging or even impractical to
collect in surgical environments. Thus, for tuning the learning model, we
gather endoscopic data of soft tissue being manipulated by a surgical robot and
then establish correspondences between point clouds at different time points to
serve as ground truth. This was achieved by employing a position-based dynamics
(PBD) simulation to ensure that the correspondences adhered to physical
constraints. The proposed framework is demonstrated on several challenging
surgical datasets that are characterized by large deformations, achieving
superior performance over state-of-the-art surgical scene tracking algorithms.",http://arxiv.org/pdf/2309.13863v1
2309.13857v1,cs.CV,Adversarial Attacks on Video Object Segmentation with Hard Region Discovery,2023-09-25 03:52:15+00:00,"Video object segmentation has been applied to various computer vision tasks,
such as video editing, autonomous driving, and human-robot interaction.
However, the methods based on deep neural networks are vulnerable to
adversarial examples, which are the inputs attacked by almost
human-imperceptible perturbations, and the adversary (i.e., attacker) will fool
the segmentation model to make incorrect pixel-level predictions. This will
rise the security issues in highly-demanding tasks because small perturbations
to the input video will result in potential attack risks. Though adversarial
examples have been extensively used for classification, it is rarely studied in
video object segmentation. Existing related methods in computer vision either
require prior knowledge of categories or cannot be directly applied due to the
special design for certain tasks, failing to consider the pixel-wise region
attack. Hence, this work develops an object-agnostic adversary that has
adversarial impacts on VOS by first-frame attacking via hard region discovery.
Particularly, the gradients from the segmentation model are exploited to
discover the easily confused region, in which it is difficult to identify the
pixel-wise objects from the background in a frame. This provides a hardness map
that helps to generate perturbations with a stronger adversarial power for
attacking the first frame. Empirical studies on three benchmarks indicate that
our attacker significantly degrades the performance of several state-of-the-art
video object segmentation models.",http://arxiv.org/pdf/2309.13857v1
2309.13853v1,cs.ET,A Ferroelectric Compute-in-Memory Annealer for Combinatorial Optimization Problems,2023-09-25 03:46:19+00:00,"Computationally hard combinatorial optimization problems (COPs) are
ubiquitous in many applications, including logistical planning, resource
allocation, chip design, drug explorations, and more. Due to their critical
significance and the inability of conventional hardware in efficiently handling
scaled COPs, there is a growing interest in developing computing hardware
tailored specifically for COPs, including digital annealers, dynamical Ising
machines, and quantum/photonic systems. However, significant hurdles still
remain, such as the memory access issue, the system scalability and restricted
applicability to certain types of COPs, and VLSI-incompatibility, respectively.
Here, a ferroelectric field effect transistor (FeFET) based compute-in-memory
(CiM) annealer is proposed. After converting COPs into quadratic unconstrained
binary optimization (QUBO) formulations, a hardware-algorithm co-design is
conducted, yielding an energy-efficient, versatile, and scalable hardware for
COPs. To accelerate the core vector-matrix-vector (VMV) multiplication of QUBO
formulations, a FeFET based CiM array is exploited, which can accelerate the
intended operation in-situ due to its unique three-terminal structure. In
particular, a lossless compression technique is proposed to prune typically
sparse QUBO matrix to reduce hardware cost. Furthermore, a multi-epoch
simulated annealing (MESA) algorithm is proposed to replace conventional
simulated annealing for its faster convergence and better solution quality. The
effectiveness of the proposed techniques is validated through the utilization
of developed chip prototypes for successfully solving graph coloring problem,
indicating great promise of FeFET CiM annealer in solving general COPs.",http://arxiv.org/pdf/2309.13853v1
2309.13850v1,stat.ML,Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts,2023-09-25 03:28:01+00:00,"Top-K sparse softmax gating mixture of experts has been widely used for
scaling up massive deep-learning architectures without increasing the
computational cost. Despite its popularity in real-world applications, the
theoretical understanding of that gating function has remained an open problem.
The main challenge comes from the structure of the top-K sparse softmax gating
function, which partitions the input space into multiple regions with distinct
behaviors. By focusing on a Gaussian mixture of experts, we establish
theoretical results on the effects of the top-K sparse softmax gating function
on both density and parameter estimations. Our results hinge upon defining
novel loss functions among parameters to capture different behaviors of the
input regions. When the true number of experts $k_{\ast}$ is known, we
demonstrate that the convergence rates of density and parameter estimations are
both parametric on the sample size. However, when $k_{\ast}$ becomes unknown
and the true model is over-specified by a Gaussian mixture of $k$ experts where
$k > k_{\ast}$, our findings suggest that the number of experts selected from
the top-K sparse softmax gating function must exceed the total cardinality of a
certain number of Voronoi cells associated with the true parameters to
guarantee the convergence of the density estimation. Moreover, while the
density estimation rate remains parametric under this setting, the parameter
estimation rates become substantially slow due to an intrinsic interaction
between the softmax gating and expert functions.",http://arxiv.org/pdf/2309.13850v1
2309.13842v1,cs.RO,Traj-LO: In Defense of LiDAR-Only Odometry Using an Effective Continuous-Time Trajectory,2023-09-25 03:05:06+00:00,"LiDAR Odometry is an essential component in many robotic applications. Unlike
the mainstreamed approaches that focus on improving the accuracy by the
additional inertial sensors, this letter explores the capability of LiDAR-only
odometry through a continuous-time perspective. Firstly, the measurements of
LiDAR are regarded as streaming points continuously captured at high frequency.
Secondly, the LiDAR movement is parameterized by a simple yet effective
continuous-time trajectory. Therefore, our proposed Traj-LO approach tries to
recover the spatial-temporal consistent movement of LiDAR by tightly coupling
the geometric information from LiDAR points and kinematic constraints from
trajectory smoothness. This framework is generalized for different kinds of
LiDAR as well as multi-LiDAR systems. Extensive experiments on the public
datasets demonstrate the robustness and effectiveness of our proposed
LiDAR-only approach, even in scenarios where the kinematic state exceeds the
IMU's measuring range. Our implementation is open-sourced on GitHub.",http://arxiv.org/pdf/2309.13842v1
2309.13838v1,stat.AP,Penalized Principal Component Analysis using Nesterov Smoothing,2023-09-25 02:50:22+00:00,"Principal components computed via PCA (principal component analysis) are
traditionally used to reduce dimensionality in genomic data or to correct for
population stratification. In this paper, we explore the penalized eigenvalue
problem (PEP) which reformulates the computation of the first eigenvector as an
optimization problem and adds an L1 penalty constraint. The contribution of our
article is threefold. First, we extend PEP by applying Nesterov smoothing to
the original LASSO-type L1 penalty. This allows one to compute analytical
gradients which enable faster and more efficient minimization of the objective
function associated with the optimization problem. Second, we demonstrate how
higher order eigenvectors can be calculated with PEP using established results
from singular value decomposition (SVD). Third, using data from the 1000 Genome
Project dataset, we empirically demonstrate that our proposed smoothed PEP
allows one to increase numerical stability and obtain meaningful eigenvectors.
We further investigate the utility of the penalized eigenvector approach over
traditional PCA.",http://arxiv.org/pdf/2309.13838v1
2309.14322v1,cs.LG,Small-scale proxies for large-scale Transformer training instabilities,2023-09-25 17:48:51+00:00,"Teams that have trained large Transformer-based models have reported training
instabilities at large scale that did not appear when training with the same
hyperparameters at smaller scales. Although the causes of such instabilities
are of scientific interest, the amount of resources required to reproduce them
has made investigation difficult. In this work, we seek ways to reproduce and
study training stability and instability at smaller scales. First, we focus on
two sources of training instability described in previous work: the growth of
logits in attention layers (Dehghani et al., 2023) and divergence of the output
logits from the log probabilities (Chowdhery et al., 2022). By measuring the
relationship between learning rate and loss across scales, we show that these
instabilities also appear in small models when training at high learning rates,
and that mitigations previously employed at large scales are equally effective
in this regime. This prompts us to investigate the extent to which other known
optimizer and model interventions influence the sensitivity of the final loss
to changes in the learning rate. To this end, we study methods such as warm-up,
weight decay, and the $\mu$Param (Yang et al., 2022), and combine techniques to
train small models that achieve similar losses across orders of magnitude of
learning rate variation. Finally, to conclude our exploration we study two
cases where instabilities can be predicted before they emerge by examining the
scaling behavior of model activation and gradient norms.",http://arxiv.org/pdf/2309.14322v1
2309.14320v1,cs.RO,MUTEX: Learning Unified Policies from Multimodal Task Specifications,2023-09-25 17:45:31+00:00,"Humans use different modalities, such as speech, text, images, videos, etc.,
to communicate their intent and goals with teammates. For robots to become
better assistants, we aim to endow them with the ability to follow instructions
and understand tasks specified by their human partners. Most robotic policy
learning methods have focused on one single modality of task specification
while ignoring the rich cross-modal information. We present MUTEX, a unified
approach to policy learning from multimodal task specifications. It trains a
transformer-based architecture to facilitate cross-modal reasoning, combining
masked modeling and cross-modal matching objectives in a two-stage training
procedure. After training, MUTEX can follow a task specification in any of the
six learned modalities (video demonstrations, goal images, text goal
descriptions, text instructions, speech goal descriptions, and speech
instructions) or a combination of them. We systematically evaluate the benefits
of MUTEX in a newly designed dataset with 100 tasks in simulation and 50 tasks
in the real world, annotated with multiple instances of task specifications in
different modalities, and observe improved performance over methods trained
specifically for any single modality. More information at
https://ut-austin-rpl.github.io/MUTEX/",http://arxiv.org/pdf/2309.14320v1
2309.14307v2,cs.LG,A post-selection algorithm for improving dynamic ensemble selection methods,2023-09-25 17:25:39+00:00,"Dynamic Ensemble Selection (DES) is a Multiple Classifier Systems (MCS)
approach that aims to select an ensemble for each query sample during the
selection phase. Even with the proposal of several DES approaches, no
particular DES technique is the best choice for different problems. Thus, we
hypothesize that selecting the best DES approach per query instance can lead to
better accuracy. To evaluate this idea, we introduce the Post-Selection Dynamic
Ensemble Selection (PS-DES) approach, a post-selection scheme that evaluates
ensembles selected by several DES techniques using different metrics.
Experimental results show that using accuracy as a metric to select the
ensembles, PS-DES performs better than individual DES techniques. PS-DES source
code is available in a GitHub repository",http://arxiv.org/pdf/2309.14307v2
2309.14298v1,stat.ML,Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures,2023-09-25 17:13:46+00:00,"We present improved algorithms with worst-case regret guarantees for the
stochastic linear bandit problem. The widely used ""optimism in the face of
uncertainty"" principle reduces a stochastic bandit problem to the construction
of a confidence sequence for the unknown reward function. The performance of
the resulting bandit algorithm depends on the size of the confidence sequence,
with smaller confidence sets yielding better empirical performance and stronger
regret guarantees. In this work, we use a novel tail bound for adaptive
martingale mixtures to construct confidence sequences which are suitable for
stochastic bandits. These confidence sequences allow for efficient action
selection via convex programming. We prove that a linear bandit algorithm based
on our confidence sequences is guaranteed to achieve competitive worst-case
regret. We show that our confidence sequences are tighter than competitors,
both empirically and theoretically. Finally, we demonstrate that our tighter
confidence sequences give improved performance in several hyperparameter tuning
tasks.",http://arxiv.org/pdf/2309.14298v1
2309.14294v1,astro-ph.SR,AspGap: Augmented Stellar Parameters and Abundances for 23 million RGB stars from Gaia XP low-resolution spectra,2023-09-25 17:06:01+00:00,"We present AspGap, a new approach to infer stellar labels from low-resolution
Gaia XP spectra, including precise [$\alpha$/M] estimates for the first time.
AspGap is a neural-network based regression model trained on APOGEE spectra. In
the training step, AspGap learns to use XP spectra not only to predict stellar
labels but also the high-resolution APOGEE spectra that lead to the same
stellar labels. The inclusion of this last model component -- dubbed the
hallucinator -- creates a more physically motivated mapping and significantly
improves the prediction of stellar labels in the validation, particularly of
[$\alpha$/M]. For giant stars, we find cross-validated rms accuracies for Teff,
log g, [M/H], [$\alpha$/M] of ~1%, 0.12 dex, 0.07 dex, 0.03 dex, respectively.
We also validate our labels through comparison with external datasets and
through a range of astrophysical tests that demonstrate that we are indeed
determining [$\alpha$/M] from the XP spectra, rather than just inferring it
indirectly from correlations with other labels. We publicly release the AspGap
codebase, along with our stellar parameter catalog for all giants observed by
Gaia XP. AspGap enables new insights into the formation and chemo-dynamics of
our Galaxy by providing precise [$\alpha$/M] estimates for 23 million giant
stars, including 12 million with radial velocities from Gaia.",http://arxiv.org/pdf/2309.14294v1
2309.14248v1,eess.SY,Transcending the Acceleration-Bandwidth Trade-off: Lightweight Precision Stages with Active Control of Flexible Dynamics,2023-09-25 16:09:30+00:00,"Micro/Nano-positioning stages are of great importance in a wide range of
manufacturing machines and instruments. In recent years, the drastically
growing demand for higher throughput and reduced power consumption in various
IC manufacturing equipment calls for the development of next-generation
precision positioning systems with unprecedented acceleration capability while
maintaining exceptional positioning accuracy and high control bandwidth.
Reducing the stage's weight is an effective approach to achieving this goal.
However, the reduction of stages' weight tends to decrease its structural
resonance frequency, which limits the closed-loop control bandwidth and can
even cause stability issues. Aiming to overcome the aforementioned challenge
and thus create new lightweight precision stages with substantially improved
acceleration capability without sacrificing stage control performance, this
research presents a novel sequential structure and control design framework for
lightweight stages with low-frequency flexible modes of the stage being
actively controlled. Additional actuators and sensors are placed to actively
control the flexible structural dynamics of the lightweight stage to attain
high control bandwidth. A case study is simulated to evaluate the effectiveness
of the proposed approach, where a stage weight reduction of 24% is demonstrated
compared to a baseline case, which demonstrates the potential of the proposed
design framework. Experimental evaluation of the designed stage's motion
performance will be performed on a magnetically levitated linear motor platform
for performance demonstration.",http://arxiv.org/pdf/2309.14248v1
2309.14246v1,cs.RO,Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning,2023-09-25 16:05:32+00:00,"Deployment in hazardous environments requires robots to understand the risks
associated with their actions and movements to prevent accidents. Despite its
importance, these risks are not explicitly modeled by currently deployed
locomotion controllers for legged robots. In this work, we propose a risk
sensitive locomotion training method employing distributional reinforcement
learning to consider safety explicitly. Instead of relying on a value
expectation, we estimate the complete value distribution to account for
uncertainty in the robot's interaction with the environment. The value
distribution is consumed by a risk metric to extract risk sensitive value
estimates. These are integrated into Proximal Policy Optimization (PPO) to
derive our method, Distributional Proximal Policy Optimization (DPPO). The risk
preference, ranging from risk-averse to risk-seeking, can be controlled by a
single parameter, which enables to adjust the robot's behavior dynamically.
Importantly, our approach removes the need for additional reward function
tuning to achieve risk sensitivity. We show emergent risk sensitive locomotion
behavior in simulation and on the quadrupedal robot ANYmal.",http://arxiv.org/pdf/2309.14246v1
2309.14237v1,cs.RO,Hierarchical Reinforcement Learning based on Planning Operators,2023-09-25 15:54:32+00:00,"Long-horizon manipulation tasks such as stacking represent a longstanding
challenge in the field of robotic manipulation, particularly when using
reinforcement learning (RL) methods which often struggle to learn the correct
sequence of actions for achieving these complex goals. To learn this sequence,
symbolic planning methods offer a good solution based on high-level reasoning,
however, planners often fall short in addressing the low-level control
specificity needed for precise execution. This paper introduces a novel
framework that integrates symbolic planning with hierarchical RL through the
cooperation of high-level operators and low-level policies. Our contribution
integrates planning operators (e.g. preconditions and effects) as part of the
hierarchical RL algorithm based on the Scheduled Auxiliary Control (SAC-X)
method. We developed a dual-purpose high-level operator, which can be used both
in holistic planning and as independent, reusable policies. Our approach offers
a flexible solution for long-horizon tasks, e.g., stacking a cube. The
experimental results show that our proposed method obtained an average of 97.2%
success rate for learning and executing the whole stack sequence, and the
success rate for learning independent policies, e.g. reach (98.9%), lift
(99.7%), stack (85%), etc. The training time is also reduced by 68% when using
our proposed approach.",http://arxiv.org/pdf/2309.14237v1
2309.14204v1,physics.ao-ph,A Machine Learning Framework for Extending Wave Height Time Series Using Historical Wind Records,2023-09-25 15:07:20+00:00,"This study presents a novel machine learning-based framework that utilizes
the ConvLSTM-1D model to extend the hindcast of wave height time series by
leveraging historical wind records. This approach was applied to Lake Michigan
by incorporating wind data from multiple Automatic Surface Observation Systems
(ASOS) stations as input features. A wave height time series from the Wave
Information System model (WIS) served as the training, validation, and testing
dataset for the proposed model. Several models were developed, considering
different numbers of wind stations, revealing the importance of incorporating
stations with variable distances and orientations to enhance prediction
accuracy. Notably, the improvement in the model performance plateaued after a
certain number of stations, underscoring the importance of selecting an optimal
number of wind stations. Additionally, an ensemble learning technique was
employed to combine multiple models, resulting in further enhancements in
prediction accuracy. The developed model added 30 years of wave height
predictions to the existing time series, expanding it by 70% which allows
insights into the long-term wave climatology of the Lake Michigan. This
framework offers a promising avenue for utilizing historical wind records
worldwide to extend wave height time series, in turn improving coastal
resilience and coastal management plans.",http://arxiv.org/pdf/2309.14204v1
2309.14200v1,astro-ph.SR,101 Eclipsing Quadruple Star Candidates Discovered in TESS Full Frame Images,2023-09-25 15:00:42+00:00,"We present our second catalog of quadruple star candidates, containing 101
systems discovered in TESS Full-Frame Image data. The targets were initially
detected as eclipsing binary stars with the help of supervised machine learning
methods applied to sectors Sectors 1 through 54. A dedicated team of citizen
scientists subsequently identified through visual inspection two sets of
eclipses following two different periods. All 101 systems presented here pass
comprehensive photocenter motion tests confirming that both sets of eclipses
originate from the target star. Some of the systems exhibit prominent eclipse
time variations suggesting dynamical interactions between the two component
binary stars. One target is an eclipsing quintuple candidate with a (2+1)+2
hierarchical configuration, such that the (2+1) subsystem produces eclipses on
the triple orbit as well. Another has recently been confirmed as the second
shortest period quadruple reported to date. This catalog provides ephemerides,
eclipse depths and durations, sample statistics, and highlights potentially
interesting targets for future studies.",http://arxiv.org/pdf/2309.14200v1
2309.14155v1,math.OC,Extragradient Type Methods for Riemannian Variational Inequality Problems,2023-09-25 14:08:02+00:00,"Riemannian convex optimization and minimax optimization have recently drawn
considerable attention. Their appeal lies in their capacity to adeptly manage
the non-convexity of the objective function as well as constraints inherent in
the feasible set in the Euclidean sense. In this work, we delve into monotone
Riemannian Variational Inequality Problems (RVIPs), which encompass both
Riemannian convex optimization and minimax optimization as particular cases. In
the context of Euclidean space, it is established that the last-iterates of
both the extragradient (EG) and past extragradient (PEG) methods converge to
the solution of monotone variational inequality problems at a rate of
$O\left(\frac{1}{\sqrt{T}}\right)$ (Cai et al., 2022). However, analogous
behavior on Riemannian manifolds remains an open question. To bridge this gap,
we introduce the Riemannian extragradient (REG) and Riemannian past
extragradient (RPEG) methods. We demonstrate that both exhibit
$O\left(\frac{1}{\sqrt{T}}\right)$ last-iterate convergence. Additionally, we
show that the average-iterate convergence of both REG and RPEG is
$O\left(\frac{1}{{T}}\right)$, aligning with observations in the Euclidean case
(Mokhtari et al., 2020). These results are enabled by judiciously addressing
the holonomy effect so that additional complications in Riemannian cases can be
reduced and the Euclidean proof inspired by the performance estimation problem
(PEP) technique or the sum-of-squares (SOS) technique can be applied again.",http://arxiv.org/pdf/2309.14155v1
2309.14150v1,cs.RO,Learned Contextual LiDAR Informed Visual Search in Unseen Environments,2023-09-25 14:04:31+00:00,"This paper presents LIVES: LiDAR Informed Visual Search, an autonomous
planner for unknown environments. We consider the pixel-wise environment
perception problem where one is given 2D range data from LiDAR scans and must
label points contextually as map or non-map in the surroundings for visual
planning. LIVES classifies incoming 2D scans from the wide Field of View (FoV)
LiDAR in unseen environments without prior map information. The
map-generalizable classifier is trained from expert data collected using a
simple cart platform equipped with a map-based classifier in real environments.
A visual planner takes contextual data from scans and uses this information to
plan viewpoints more likely to yield detection of the search target. While
conventional frontier based methods for LiDAR and multi sensor exploration
effectively map environments, they are not tailored to search for people
indoors, which we investigate in this paper. LIVES is baselined against several
existing exploration methods in simulation to verify its performance. Finally,
it is validated in real-world experiments with a Spot robot in a 20x30m indoor
apartment setting. Videos of experimental validation can be found on our
project website at https://sites.google.com/view/lives-icra-2024/home.",http://arxiv.org/pdf/2309.14150v1
2309.14125v1,eess.SY,Driving behavior-guided battery health monitoring for electric vehicles using machine learning,2023-09-25 13:24:53+00:00,"An accurate estimation of the state of health (SOH) of batteries is critical
to ensuring the safe and reliable operation of electric vehicles (EVs).
Feature-based machine learning methods have exhibited enormous potential for
rapidly and precisely monitoring battery health status. However, simultaneously
using various health indicators (HIs) may weaken estimation performance due to
feature redundancy. Furthermore, ignoring real-world driving behaviors can lead
to inaccurate estimation results as some features are rarely accessible in
practical scenarios. To address these issues, we proposed a feature-based
machine learning pipeline for reliable battery health monitoring, enabled by
evaluating the acquisition probability of features under real-world driving
conditions. We first summarized and analyzed various individual HIs with
mechanism-related interpretations, which provide insightful guidance on how
these features relate to battery degradation modes. Moreover, all features were
carefully evaluated and screened based on estimation accuracy and correlation
analysis on three public battery degradation datasets. Finally, the
scenario-based feature fusion and acquisition probability-based practicality
evaluation method construct a useful tool for feature extraction with
consideration of driving behaviors. This work highlights the importance of
balancing the performance and practicality of HIs during the development of
feature-based battery health monitoring algorithms.",http://arxiv.org/pdf/2309.14125v1
2309.14118v1,cs.LG,"MultiModN- Multimodal, Multi-Task, Interpretable Modular Networks",2023-09-25 13:16:57+00:00,"Predicting multiple real-world tasks in a single model often requires a
particularly diverse feature space. Multimodal (MM) models aim to extract the
synergistic predictive potential of multiple data types to create a shared
feature space with aligned semantic meaning across inputs of drastically
varying sizes (i.e. images, text, sound). Most current MM architectures fuse
these representations in parallel, which not only limits their interpretability
but also creates a dependency on modality availability. We present MultiModN, a
multimodal, modular network that fuses latent representations in a sequence of
any number, combination, or type of modality while providing granular real-time
predictive feedback on any number or combination of predictive tasks.
MultiModN's composable pipeline is interpretable-by-design, as well as innately
multi-task and robust to the fundamental issue of biased missingness. We
perform four experiments on several benchmark MM datasets across 10 real-world
tasks (predicting medical diagnoses, academic performance, and weather), and
show that MultiModN's sequential MM fusion does not compromise performance
compared with a baseline of parallel fusion. By simulating the challenging bias
of missing not-at-random (MNAR), this work shows that, contrary to MultiModN,
parallel fusion baselines erroneously learn MNAR and suffer catastrophic
failure when faced with different patterns of MNAR at inference. To the best of
our knowledge, this is the first inherently MNAR-resistant approach to MM
modeling. In conclusion, MultiModN provides granular insights, robustness, and
flexibility without compromising performance.",http://arxiv.org/pdf/2309.14118v1
2309.14113v1,hep-ph,HyperTrack: Neural Combinatorics for High Energy Physics,2023-09-25 13:12:08+00:00,"Combinatorial inverse problems in high energy physics span enormous
algorithmic challenges. This work presents a new deep learning driven
clustering algorithm that utilizes a space-time non-local trainable graph
constructor, a graph neural network, and a set transformer. The model is
trained with loss functions at the graph node, edge and object level, including
contrastive learning and meta-supervision. The algorithm can be applied to
problems such as charged particle tracking, calorimetry, pile-up
discrimination, jet physics, and beyond. We showcase the effectiveness of this
cutting-edge AI approach through particle tracking simulations. The code is
available online.",http://arxiv.org/pdf/2309.14113v1
2309.14096v1,cs.LG,Tracking Control for a Spherical Pendulum via Curriculum Reinforcement Learning,2023-09-25 12:48:47+00:00,"Reinforcement Learning (RL) allows learning non-trivial robot control laws
purely from data. However, many successful applications of RL have relied on
ad-hoc regularizations, such as hand-crafted curricula, to regularize the
learning performance. In this paper, we pair a recent algorithm for
automatically building curricula with RL on massively parallelized simulations
to learn a tracking controller for a spherical pendulum on a robotic arm via
RL. Through an improved optimization scheme that better respects the
non-Euclidean task structure, we allow the method to reliably generate
curricula of trajectories to be tracked, resulting in faster and more robust
learning compared to an RL baseline that does not exploit this form of
structured learning. The learned policy matches the performance of an optimal
control baseline on the real system, demonstrating the potential of curriculum
RL to jointly learn state estimation and control for non-linear tracking tasks.",http://arxiv.org/pdf/2309.14096v1
2309.14091v1,cs.LG,On the Benefit of Optimal Transport for Curriculum Reinforcement Learning,2023-09-25 12:31:37+00:00,"Curriculum reinforcement learning (CRL) allows solving complex tasks by
generating a tailored sequence of learning tasks, starting from easy ones and
subsequently increasing their difficulty. Although the potential of curricula
in RL has been clearly shown in various works, it is less clear how to generate
them for a given learning environment, resulting in various methods aiming to
automate this task. In this work, we focus on framing curricula as
interpolations between task distributions, which has previously been shown to
be a viable approach to CRL. Identifying key issues of existing methods, we
frame the generation of a curriculum as a constrained optimal transport problem
between task distributions. Benchmarks show that this way of curriculum
generation can improve upon existing CRL methods, yielding high performance in
various tasks with different characteristics.",http://arxiv.org/pdf/2309.14091v1
2309.14046v1,cs.IR,Diversify and Conquer: Bandits and Diversity for an Enhanced E-commerce Homepage Experience,2023-09-25 11:22:19+00:00,"In the realm of e-commerce, popular platforms utilize widgets to recommend
advertisements and products to their users. However, the prevalence of mobile
device usage on these platforms introduces a unique challenge due to the
limited screen real estate available. Consequently, the positioning of relevant
widgets becomes pivotal in capturing and maintaining customer engagement. Given
the restricted screen size of mobile devices, widgets placed at the top of the
interface are more prominently displayed and thus attract greater user
attention. Conversely, widgets positioned further down the page require users
to scroll, resulting in reduced visibility and subsequent lower impression
rates. Therefore it becomes imperative to place relevant widgets on top.
However, selecting relevant widgets to display is a challenging task as the
widgets can be heterogeneous, widgets can be introduced or removed at any given
time from the platform. In this work, we model the vertical widget reordering
as a contextual multi-arm bandit problem with delayed batch feedback. The
objective is to rank the vertical widgets in a personalized manner. We present
a two-stage ranking framework that combines contextual bandits with a diversity
layer to improve the overall ranking. We demonstrate its effectiveness through
offline and online A/B results, conducted on proprietary data from Myntra, a
major fashion e-commerce platform in India.",http://arxiv.org/pdf/2309.14046v1
2309.14027v1,physics.ins-det,TomOpt: Differential optimisation for task- and constraint-aware design of particle detectors in the context of muon tomography,2023-09-25 10:44:09+00:00,"We describe a software package, TomOpt, developed to optimise the geometrical
layout and specifications of detectors designed for tomography by scattering of
cosmic-ray muons. The software exploits differentiable programming for the
modeling of muon interactions with detectors and scanned volumes, the inference
of volume properties, and the optimisation cycle performing the loss
minimisation. In doing so, we provide the first demonstration of
end-to-end-differentiable and inference-aware optimisation of particle physics
instruments. We study the performance of the software on a relevant benchmark
scenarios and discuss its potential applications.",http://arxiv.org/pdf/2309.14027v1
2309.14003v1,cs.LG,Hierarchical Imitation Learning for Stochastic Environments,2023-09-25 10:10:34+00:00,"Many applications of imitation learning require the agent to generate the
full distribution of behaviour observed in the training data. For example, to
evaluate the safety of autonomous vehicles in simulation, accurate and diverse
behaviour models of other road users are paramount. Existing methods that
improve this distributional realism typically rely on hierarchical policies.
These condition the policy on types such as goals or personas that give rise to
multi-modal behaviour. However, such methods are often inappropriate for
stochastic environments where the agent must also react to external factors:
because agent types are inferred from the observed future trajectory during
training, these environments require that the contributions of internal and
external factors to the agent behaviour are disentangled and only internal
factors, i.e., those under the agent's control, are encoded in the type.
Encoding future information about external factors leads to inappropriate agent
reactions during testing, when the future is unknown and types must be drawn
independently from the actual future. We formalize this challenge as
distribution shift in the conditional distribution of agent types under
environmental stochasticity. We propose Robust Type Conditioning (RTC), which
eliminates this shift with adversarial training under randomly sampled types.
Experiments on two domains, including the large-scale Waymo Open Motion
Dataset, show improved distributional realism while maintaining or improving
task performance compared to state-of-the-art baselines.",http://arxiv.org/pdf/2309.14003v1
2309.13998v1,stat.ME,Linked shrinkage to improve estimation of interaction effects in regression models,2023-09-25 10:03:39+00:00,"We address a classical problem in statistics: adding two-way interaction
terms to a regression model. As the covariate dimension increases
quadratically, we develop an estimator that adapts well to this increase, while
providing accurate estimates and appropriate inference. Existing strategies
overcome the dimensionality problem by only allowing interactions between
relevant main effects. Building on this philosophy, we implement a softer link
between the two types of effects using a local shrinkage model. We empirically
show that borrowing strength between the amount of shrinkage for main effects
and their interactions can strongly improve estimation of the regression
coefficients. Moreover, we evaluate the potential of the model for inference,
which is notoriously hard for selection strategies. Large-scale cohort data are
used to provide realistic illustrations and evaluations. Comparisons with other
methods are provided. The evaluation of variable importance is not trivial in
regression models with many interaction terms. Therefore, we derive a new
analytical formula for the Shapley value, which enables rapid assessment of
individual-specific variable importance scores and their uncertainties.
Finally, while not targeting for prediction, we do show that our models can be
very competitive to a more advanced machine learner, like random forest, even
for fairly large sample sizes. The implementation of our method in RStan is
fairly straightforward, allowing for adjustments to specific needs.",http://arxiv.org/pdf/2309.13998v1
2309.13989v1,cs.LG,A Novel Approach for Effective Multi-View Clustering with Information-Theoretic Perspective,2023-09-25 09:41:11+00:00,"Multi-view clustering (MVC) is a popular technique for improving clustering
performance using various data sources. However, existing methods primarily
focus on acquiring consistent information while often neglecting the issue of
redundancy across multiple views. This study presents a new approach called
Sufficient Multi-View Clustering (SUMVC) that examines the multi-view
clustering framework from an information-theoretic standpoint. Our proposed
method consists of two parts. Firstly, we develop a simple and reliable
multi-view clustering method SCMVC (simple consistent multi-view clustering)
that employs variational analysis to generate consistent information. Secondly,
we propose a sufficient representation lower bound to enhance consistent
information and minimise unnecessary information among views. The proposed
SUMVC method offers a promising solution to the problem of multi-view
clustering and provides a new perspective for analyzing multi-view data.
  To verify the effectiveness of our model, we conducted a theoretical analysis
based on the Bayes Error Rate, and experiments on multiple multi-view datasets
demonstrate the superior performance of SUMVC.",http://arxiv.org/pdf/2309.13989v1
2309.13950v1,cs.LG,Local and Global Trend Bayesian Exponential Smoothing Models,2023-09-25 08:31:50+00:00,"This paper describes a family of seasonal and non-seasonal time series models
that can be viewed as generalisations of additive and multiplicative
exponential smoothing models. Their development is motivated by fast-growing,
volatile time series, and facilitated by state-of-the-art Bayesian fitting
techniques. When applied to the M3 competition data set, they outperform the
best algorithms in the competition as well as other benchmarks, thus achieving
to the best of our knowledge the best results of univariate methods on this
dataset in the literature.",http://arxiv.org/pdf/2309.13950v1
2309.13949v1,cs.LG,Characterising User Transfer Amid Industrial Resource Variation: A Bayesian Nonparametric Approach,2023-09-25 08:31:14+00:00,"In a multitude of industrial fields, a key objective entails optimising
resource management whilst satisfying user requirements. Resource management by
industrial practitioners can result in a passive transfer of user loads across
resource providers, a phenomenon whose accurate characterisation is both
challenging and crucial. This research reveals the existence of user clusters,
which capture macro-level user transfer patterns amid resource variation. We
then propose CLUSTER, an interpretable hierarchical Bayesian nonparametric
model capable of automating cluster identification, and thereby predicting user
transfer in response to resource variation. Furthermore, CLUSTER facilitates
uncertainty quantification for further reliable decision-making. Our method
enables privacy protection by functioning independently of personally
identifiable information. Experiments with simulated and real-world data from
the communications industry reveal a pronounced alignment between prediction
results and empirical observations across a spectrum of resource management
scenarios. This research establishes a solid groundwork for advancing resource
management strategy development.",http://arxiv.org/pdf/2309.13949v1
2309.13944v1,cs.LG,Provable Training for Graph Contrastive Learning,2023-09-25 08:23:53+00:00,"Graph Contrastive Learning (GCL) has emerged as a popular training approach
for learning node embeddings from augmented graphs without labels. Despite the
key principle that maximizing the similarity between positive node pairs while
minimizing it between negative node pairs is well established, some fundamental
problems are still unclear. Considering the complex graph structure, are some
nodes consistently well-trained and following this principle even with
different graph augmentations? Or are there some nodes more likely to be
untrained across graph augmentations and violate the principle? How to
distinguish these nodes and further guide the training of GCL? To answer these
questions, we first present experimental evidence showing that the training of
GCL is indeed imbalanced across all nodes. To address this problem, we propose
the metric ""node compactness"", which is the lower bound of how a node follows
the GCL principle related to the range of augmentations. We further derive the
form of node compactness theoretically through bound propagation, which can be
integrated into binary cross-entropy as a regularization. To this end, we
propose the PrOvable Training (POT) for GCL, which regularizes the training of
GCL to encode node embeddings that follows the GCL principle better. Through
extensive experiments on various benchmarks, POT consistently improves the
existing GCL approaches, serving as a friendly plugin.",http://arxiv.org/pdf/2309.13944v1
2309.13915v1,cs.LG,Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds,2023-09-25 07:31:22+00:00,"Policy-based algorithms equipped with deep neural networks have achieved
great success in solving high-dimensional policy optimization problems in
reinforcement learning. However, current analyses cannot explain why they are
resistant to the curse of dimensionality. In this work, we study the sample
complexity of the neural policy mirror descent (NPMD) algorithm with
convolutional neural networks (CNN) as function approximators. Motivated by the
empirical observation that many high-dimensional environments have state spaces
possessing low-dimensional structures, such as those taking images as states,
we consider the state space to be a $d$-dimensional manifold embedded in the
$D$-dimensional Euclidean space with intrinsic dimension $d\ll D$. We show that
in each iteration of NPMD, both the value function and the policy can be well
approximated by CNNs. The approximation errors are controlled by the size of
the networks, and the smoothness of the previous networks can be inherited. As
a result, by properly choosing the network size and hyperparameters, NPMD can
find an $\epsilon$-optimal policy with
$\widetilde{O}(\epsilon^{-\frac{d}{\alpha}-2})$ samples in expectation, where
$\alpha\in(0,1]$ indicates the smoothness of environment. Compared to previous
work, our result exhibits that NPMD can leverage the low-dimensional structure
of state space to escape from the curse of dimensionality, providing an
explanation for the efficacy of deep policy-based algorithms.",http://arxiv.org/pdf/2309.13915v1
2309.13909v1,cs.HC,Chinese herb medicine in augmented reality,2023-09-25 07:12:58+00:00,"Augmented reality becomes popular in education gradually, which provides a
contextual and adaptive learning experience. Here, we develop a Chinese herb
medicine AR platform based the 3dsMax and the Unity that allows users to
visualize and interact with the herb model and learn the related information.
The users use their mobile camera to scan the 2D herb picture to trigger the
presentation of 3D AR model and corresponding text information on the screen in
real-time. The system shows good performance and has high accuracy for the
identification of herbal medicine after interference test and occlusion test.
Users can interact with the herb AR model by rotating, scaling, and viewing
transformation, which effectively enhances learners' interest in Chinese herb
medicine.",http://arxiv.org/pdf/2309.13909v1
2309.13896v1,cs.LG,Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts,2023-09-25 06:22:28+00:00,"Standard contextual bandit problem assumes that all the relevant contexts are
observed before the algorithm chooses an arm. This modeling paradigm, while
useful, often falls short when dealing with problems in which valuable
additional context can be observed after arm selection. For example, content
recommendation platforms like Youtube, Instagram, Tiktok also observe valuable
follow-up information pertinent to the user's reward after recommendation
(e.g., how long the user stayed, what is the user's watch speed, etc.). To
improve online learning efficiency in these applications, we study a novel
contextual bandit problem with post-serving contexts and design a new
algorithm, poLinUCB, that achieves tight regret under standard assumptions.
Core to our technical proof is a robustified and generalized version of the
well-known Elliptical Potential Lemma (EPL), which can accommodate noise in
data. Such robustification is necessary for tackling our problem, and we
believe it could also be of general interest. Extensive empirical tests on both
synthetic and real-world datasets demonstrate the significant benefit of
utilizing post-serving contexts as well as the superior performance of our
algorithm over the state-of-the-art approaches.",http://arxiv.org/pdf/2309.13896v1
2309.13886v1,cs.LG,Can Class-Priors Help Single-Positive Multi-Label Learning?,2023-09-25 05:45:57+00:00,"Single-positive multi-label learning (SPMLL) is a typical weakly supervised
multi-label learning problem, where each training example is annotated with
only one positive label. Existing SPMLL methods typically assign pseudo-labels
to unannotated labels with the assumption that prior probabilities of all
classes are identical. However, the class-prior of each category may differ
significantly in real-world scenarios, which makes the predictive model not
perform as well as expected due to the unrealistic assumption on real-world
application. To alleviate this issue, a novel framework named {\proposed},
i.e., Class-pRiors Induced Single-Positive multi-label learning, is proposed.
Specifically, a class-priors estimator is introduced, which could estimate the
class-priors that are theoretically guaranteed to converge to the ground-truth
class-priors. In addition, based on the estimated class-priors, an unbiased
risk estimator for classification is derived, and the corresponding risk
minimizer could be guaranteed to approximately converge to the optimal risk
minimizer on fully supervised data. Experimental results on ten MLL benchmark
datasets demonstrate the effectiveness and superiority of our method over
existing SPMLL approaches.",http://arxiv.org/pdf/2309.13886v1
2309.13884v1,cs.LG,Estimating Treatment Effects Under Heterogeneous Interference,2023-09-25 05:44:17+00:00,"Treatment effect estimation can assist in effective decision-making in
e-commerce, medicine, and education. One popular application of this estimation
lies in the prediction of the impact of a treatment (e.g., a promotion) on an
outcome (e.g., sales) of a particular unit (e.g., an item), known as the
individual treatment effect (ITE). In many online applications, the outcome of
a unit can be affected by the treatments of other units, as units are often
associated, which is referred to as interference. For example, on an online
shopping website, sales of an item will be influenced by an advertisement of
its co-purchased item. Prior studies have attempted to model interference to
estimate the ITE accurately, but they often assume a homogeneous interference,
i.e., relationships between units only have a single view. However, in
real-world applications, interference may be heterogeneous, with multi-view
relationships. For instance, the sale of an item is usually affected by the
treatment of its co-purchased and co-viewed items. We hypothesize that ITE
estimation will be inaccurate if this heterogeneous interference is not
properly modeled. Therefore, we propose a novel approach to model heterogeneous
interference by developing a new architecture to aggregate information from
diverse neighbors. Our proposed method contains graph neural networks that
aggregate same-view information, a mechanism that aggregates information from
different views, and attention mechanisms. In our experiments on multiple
datasets with heterogeneous interference, the proposed method significantly
outperforms existing methods for ITE estimation, confirming the importance of
modeling heterogeneous interference.",http://arxiv.org/pdf/2309.13884v1
2309.13855v1,math.OC,Adaptive Softassign via Hadamard-Equipped Sinkhorn,2023-09-25 03:47:32+00:00,"Softassign is a crucial step in several popular algorithms for graph matching
or other learning targets. Such softassign-based algorithms perform very well
for small graph matching tasks. However, the performance of such algorithms is
sensitive to a parameter in the softassign in large-scale problems, especially
when handling noised data. Turning the parameter is difficult and almost done
empirically. This paper constructs an adaptive softassign method by delicately
taking advantage of Hadamard operations in Sinkhorn. Compared with the previous
state-of-the-art algorithms such as the scalable Gromov-Wasserstein Learning
(S-GWL), the resulting algorithm enjoys both a higher accuracy and a
significant improvement in efficiency for large graph matching problems. In
particular, on the protein network matching benchmark problems (1004 nodes),
our algorithm can improve the accuracy from $56.3\%$ by the S-GWL to $75.1\%$,
at the same time, it can achieve 3X+ speedup in efficiency.",http://arxiv.org/pdf/2309.13855v1
2309.13841v1,cs.CR,On the Effectiveness of Adversarial Samples against Ensemble Learning-based Windows PE Malware Detectors,2023-09-25 02:57:27+00:00,"Recently, there has been a growing focus and interest in applying machine
learning (ML) to the field of cybersecurity, particularly in malware detection
and prevention. Several research works on malware analysis have been proposed,
offering promising results for both academic and practical applications. In
these works, the use of Generative Adversarial Networks (GANs) or Reinforcement
Learning (RL) can aid malware creators in crafting metamorphic malware that
evades antivirus software. In this study, we propose a mutation system to
counteract ensemble learning-based detectors by combining GANs and an RL model,
overcoming the limitations of the MalGAN model. Our proposed FeaGAN model is
built based on MalGAN by incorporating an RL model called the Deep Q-network
anti-malware Engines Attacking Framework (DQEAF). The RL model addresses three
key challenges in performing adversarial attacks on Windows Portable Executable
malware, including format preservation, executability preservation, and
maliciousness preservation. In the FeaGAN model, ensemble learning is utilized
to enhance the malware detector's evasion ability, with the generated
adversarial patterns. The experimental results demonstrate that 100\% of the
selected mutant samples preserve the format of executable files, while certain
successes in both executability preservation and maliciousness preservation are
achieved, reaching a stable success rate.",http://arxiv.org/pdf/2309.13841v1
2309.13837v1,cs.LG,Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations,2023-09-25 02:50:20+00:00,"This article introduces an advanced analytical approach for predicting
backorders in inventory management. Backorder refers to an order that cannot be
immediately fulfilled due to stock depletion. Multiple classification
techniques, including Balanced Bagging Classifiers, Fuzzy Logic, Variational
Autoencoder - Generative Adversarial Networks, and Multi-layer Perceptron
classifiers, are assessed in this work using performance evaluation metrics
such as ROC-AUC and PR-AUC. Moreover, this work incorporates a profit function
and misclassification costs, considering the financial implications and costs
associated with inventory management and backorder handling. The results
demonstrate the effectiveness of the predictive model in enhancing inventory
system service levels, which leads to customer satisfaction and overall
organizational performance. Considering interpretability is a significant
aspect of using AI in commercial applications, permutation importance is
applied to the selected model to determine the importance of features. This
research contributes to the advancement of predictive analytics and offers
valuable insights for future investigations in backorder forecasting and
inventory control optimization for decision-making.",http://arxiv.org/pdf/2309.13837v1
2309.13822v1,cs.CV,PARTICLE: Part Discovery and Contrastive Learning for Fine-grained Recognition,2023-09-25 02:08:48+00:00,"We develop techniques for refining representations for fine-grained
classification and segmentation tasks in a self-supervised manner. We find that
fine-tuning methods based on instance-discriminative contrastive learning are
not as effective, and posit that recognizing part-specific variations is
crucial for fine-grained categorization. We present an iterative learning
approach that incorporates part-centric equivariance and invariance objectives.
First, pixel representations are clustered to discover parts. We analyze the
representations from convolutional and vision transformer networks that are
best suited for this task. Then, a part-centric learning step aggregates and
contrasts representations of parts within an image. We show that this improves
the performance on image classification and part segmentation tasks across
datasets. For example, under a linear-evaluation scheme, the classification
accuracy of a ResNet50 trained on ImageNet using DetCon, a self-supervised
learning approach, improves from 35.4% to 42.0% on the Caltech-UCSD Birds, from
35.5% to 44.1% on the FGVC Aircraft, and from 29.7% to 37.4% on the Stanford
Cars. We also observe significant gains in few-shot part segmentation tasks
using the proposed technique, while instance-discriminative learning was not as
effective. Smaller, yet consistent, improvements are also observed for stronger
networks based on transformers.",http://arxiv.org/pdf/2309.13822v1
2309.13814v1,cs.CV,DVI-SLAM: A Dual Visual Inertial SLAM Network,2023-09-25 01:42:54+00:00,"Recent deep learning based visual simultaneous localization and mapping
(SLAM) methods have made significant progress. However, how to make full use of
visual information as well as better integrate with inertial measurement unit
(IMU) in visual SLAM has potential research value. This paper proposes a novel
deep SLAM network with dual visual factors. The basic idea is to integrate both
photometric factor and re-projection factor into the end-to-end differentiable
structure through multi-factor data association module. We show that the
proposed network dynamically learns and adjusts the confidence maps of both
visual factors and it can be further extended to include the IMU factors as
well. Extensive experiments validate that our proposed method significantly
outperforms the state-of-the-art methods on several public datasets, including
TartanAir, EuRoC and ETH3D-SLAM. Specifically, when dynamically fusing the
three factors together, the absolute trajectory error for both monocular and
stereo configurations on EuRoC dataset has reduced by 45.3% and 36.2%
respectively.",http://arxiv.org/pdf/2309.13814v1
2309.13810v1,cs.CV,Boundary-Aware Proposal Generation Method for Temporal Action Localization,2023-09-25 01:41:09+00:00,"The goal of Temporal Action Localization (TAL) is to find the categories and
temporal boundaries of actions in an untrimmed video. Most TAL methods rely
heavily on action recognition models that are sensitive to action labels rather
than temporal boundaries. More importantly, few works consider the background
frames that are similar to action frames in pixels but dissimilar in semantics,
which also leads to inaccurate temporal boundaries. To address the challenge
above, we propose a Boundary-Aware Proposal Generation (BAPG) method with
contrastive learning. Specifically, we define the above background frames as
hard negative samples. Contrastive learning with hard negative mining is
introduced to improve the discrimination of BAPG. BAPG is independent of the
existing TAL network architecture, so it can be applied plug-and-play to
mainstream TAL models. Extensive experimental results on THUMOS14 and
ActivityNet-1.3 demonstrate that BAPG can significantly improve the performance
of TAL.",http://arxiv.org/pdf/2309.13810v1
2309.13794v1,cs.LG,Projected Randomized Smoothing for Certified Adversarial Robustness,2023-09-25 01:12:55+00:00,"Randomized smoothing is the current state-of-the-art method for producing
provably robust classifiers. While randomized smoothing typically yields robust
$\ell_2$-ball certificates, recent research has generalized provable robustness
to different norm balls as well as anisotropic regions. This work considers a
classifier architecture that first projects onto a low-dimensional
approximation of the data manifold and then applies a standard classifier. By
performing randomized smoothing in the low-dimensional projected space, we
characterize the certified region of our smoothed composite classifier back in
the high-dimensional input space and prove a tractable lower bound on its
volume. We show experimentally on CIFAR-10 and SVHN that classifiers without
the initial projection are vulnerable to perturbations that are normal to the
data manifold and yet are captured by the certified regions of our method. We
compare the volume of our certified regions against various baselines and show
that our method improves on the state-of-the-art by many orders of magnitude.",http://arxiv.org/pdf/2309.13794v1
2309.13793v1,cs.LG,ReMasker: Imputing Tabular Data with Masked Autoencoding,2023-09-25 01:03:45+00:00,"We present ReMasker, a new method of imputing missing values in tabular data
by extending the masked autoencoding framework. Compared with prior work,
ReMasker is both simple -- besides the missing values (i.e., naturally masked),
we randomly ``re-mask'' another set of values, optimize the autoencoder by
reconstructing this re-masked set, and apply the trained model to predict the
missing values; and effective -- with extensive evaluation on benchmark
datasets, we show that ReMasker performs on par with or outperforms
state-of-the-art methods in terms of both imputation fidelity and utility under
various missingness settings, while its performance advantage often increases
with the ratio of missing data. We further explore theoretical justification
for its effectiveness, showing that ReMasker tends to learn
missingness-invariant representations of tabular data. Our findings indicate
that masked modeling represents a promising direction for further research on
tabular data imputation. The code is publicly available.",http://arxiv.org/pdf/2309.13793v1
2309.13786v1,cs.LG,Distribution-Free Statistical Dispersion Control for Societal Applications,2023-09-25 00:31:55+00:00,"Explicit finite-sample statistical guarantees on model performance are an
important ingredient in responsible machine learning. Previous work has focused
mainly on bounding either the expected loss of a predictor or the probability
that an individual prediction will incur a loss value in a specified range.
However, for many high-stakes applications, it is crucial to understand and
control the dispersion of a loss distribution, or the extent to which different
members of a population experience unequal effects of algorithmic decisions. We
initiate the study of distribution-free control of statistical dispersion
measures with societal implications and propose a simple yet flexible framework
that allows us to handle a much richer class of statistical functionals beyond
previous work. Our methods are verified through experiments in toxic comment
detection, medical imaging, and film recommendation.",http://arxiv.org/pdf/2309.13786v1
2309.13777v1,eess.IV,Diffeomorphic Multi-Resolution Deep Learning Registration for Applications in Breast MRI,2023-09-24 23:16:38+00:00,"In breast surgical planning, accurate registration of MR images across
patient positions has the potential to improve the localisation of tumours
during breast cancer treatment. While learning-based registration methods have
recently become the state-of-the-art approach for most medical image
registration tasks, these methods have yet to make inroads into breast image
registration due to certain difficulties-the lack of rich texture information
in breast MR images and the need for the deformations to be diffeomophic. In
this work, we propose learning strategies for breast MR image registration that
are amenable to diffeomorphic constraints, together with early experimental
results from in-silico and in-vivo experiments. One key contribution of this
work is a registration network which produces superior registration outcomes
for breast images in addition to providing diffeomorphic guarantees.",http://arxiv.org/pdf/2309.13777v1
2309.13775v1,cs.LG,"The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance",2023-09-24 23:09:48+00:00,"Quantifying variable importance is essential for answering high-stakes
questions in fields like genetics, public policy, and medicine. Current methods
generally calculate variable importance for a given model trained on a given
dataset. However, for a given dataset, there may be many models that explain
the target outcome equally well; without accounting for all possible
explanations, different researchers may arrive at many conflicting yet equally
valid conclusions given the same data. Additionally, even when accounting for
all possible explanations for a given dataset, these insights may not
generalize because not all good explanations are stable across reasonable data
perturbations. We propose a new variable importance framework that quantifies
the importance of a variable across the set of all good models and is stable
across the data distribution. Our framework is extremely flexible and can be
integrated with most existing model classes and global variable importance
metrics. We demonstrate through experiments that our framework recovers
variable importance rankings for complex simulation setups where other methods
fail. Further, we show that our framework accurately estimates the true
importance of a variable for the underlying data distribution. We provide
theoretical guarantees on the consistency and finite sample error rates for our
estimator. Finally, we demonstrate its utility with a real-world case study
exploring which genes are important for predicting HIV load in persons with
HIV, highlighting an important gene that has not previously been studied in
connection with HIV. Code is available here.",http://arxiv.org/pdf/2309.13775v1
2309.13772v1,cs.CV,Motion Segmentation from a Moving Monocular Camera,2023-09-24 22:59:05+00:00,"Identifying and segmenting moving objects from a moving monocular camera is
difficult when there is unknown camera motion, different types of object
motions and complex scene structures. To tackle these challenges, we take
advantage of two popular branches of monocular motion segmentation approaches:
point trajectory based and optical flow based methods, by synergistically
fusing these two highly complementary motion cues at object level. By doing
this, we are able to model various complex object motions in different scene
structures at once, which has not been achieved by existing methods. We first
obtain object-specific point trajectories and optical flow mask for each common
object in the video, by leveraging the recent foundational models in object
recognition, segmentation and tracking. We then construct two robust affinity
matrices representing the pairwise object motion affinities throughout the
whole video using epipolar geometry and the motion information provided by
optical flow. Finally, co-regularized multi-view spectral clustering is used to
fuse the two affinity matrices and obtain the final clustering. Our method
shows state-of-the-art performance on the KT3DMoSeg dataset, which contains
complex motions and scene structures. Being able to identify moving objects
allows us to remove them for map building when using visual SLAM or SFM.",http://arxiv.org/pdf/2309.13772v1
2309.13770v1,cs.LG,Devil in the Number: Towards Robust Multi-modality Data Filter,2023-09-24 22:52:35+00:00,"In order to appropriately filter multi-modality data sets on a web-scale, it
becomes crucial to employ suitable filtering methods to boost performance and
reduce training costs. For instance, LAION papers employs the CLIP score filter
to select data with CLIP scores surpassing a certain threshold. On the other
hand, T-MARS achieves high-quality data filtering by detecting and masking text
within images and then filtering by CLIP score. Through analyzing the dataset,
we observe a significant proportion of redundant information, such as numbers,
present in the textual content. Our experiments on a subset of the data unveil
the profound impact of these redundant elements on the CLIP scores. A logical
approach would involve reevaluating the CLIP scores after eliminating these
influences. Experimentally, our text-based CLIP filter outperforms the
top-ranked method on the ``small scale"" of DataComp (a data filtering
benchmark) on ImageNet distribution shifts, achieving a 3.6% performance
improvement. The results also demonstrate that our proposed text-masked filter
outperforms the original CLIP score filter when selecting the top 40% of the
data. The impact of numbers on CLIP and their handling provide valuable
insights for improving the effectiveness of CLIP training, including language
rewrite techniques.",http://arxiv.org/pdf/2309.13770v1
2309.13761v1,cs.CL,Text Classification: A Perspective of Deep Learning Methods,2023-09-24 21:49:51+00:00,"In recent years, with the rapid development of information on the Internet,
the number of complex texts and documents has increased exponentially, which
requires a deeper understanding of deep learning methods in order to accurately
classify texts using deep learning techniques, and thus deep learning methods
have become increasingly important in text classification. Text classification
is a class of tasks that automatically classifies a set of documents into
multiple predefined categories based on their content and subject matter. Thus,
the main goal of text classification is to enable users to extract information
from textual resources and process processes such as retrieval, classification,
and machine learning techniques together in order to classify different
categories. Many new techniques of deep learning have already achieved
excellent results in natural language processing. The success of these learning
algorithms relies on their ability to understand complex models and non-linear
relationships in data. However, finding the right structure, architecture, and
techniques for text classification is a challenge for researchers. This paper
introduces deep learning-based text classification algorithms, including
important steps required for text classification tasks such as feature
extraction, feature reduction, and evaluation strategies and methods. At the
end of the article, different deep learning text classification methods are
compared and summarized.",http://arxiv.org/pdf/2309.13761v1
