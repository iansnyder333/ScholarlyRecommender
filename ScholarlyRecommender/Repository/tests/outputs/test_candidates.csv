Id,Category,Title,Published,Abstract,URL
2309.15112v1,cs.CV,InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition,2023-09-26 17:58:20+00:00,"We propose InternLM-XComposer, a vision-language large model that enables
advanced image-text comprehension and composition. The innovative nature of our
model is highlighted by three appealing properties: 1) Interleaved Text-Image
Composition: InternLM-XComposer can effortlessly generate coherent and
contextual articles that seamlessly integrate images, providing a more engaging
and immersive reading experience. Simply provide a title, and our system will
generate the corresponding manuscript. It can intelligently identify the areas
in the text where images would enhance the content and automatically insert the
most appropriate visual candidates. 2) Comprehension with Rich Multilingual
Knowledge: The text-image comprehension is empowered by training on extensive
multi-modal multilingual concepts with carefully crafted strategies, resulting
in a deep understanding of visual content. 3) State-of-the-art Performance: Our
model consistently achieves state-of-the-art results across various mainstream
benchmarks for vision-language foundational models, including MME Benchmark,
MMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).
Collectively, InternLM-XComposer seamlessly blends advanced text-image
comprehension and composition, revolutionizing vision-language interaction and
offering new insights and opportunities. The InternLM-XComposer models with 7B
parameters are publicly available at
https://github.com/InternLM/InternLM-XComposer.",http://arxiv.org/pdf/2309.15112v1
2309.15110v1,cs.CV,Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow,2023-09-26 17:56:31+00:00,"Dense visual correspondence plays a vital role in robotic perception. This
work focuses on establishing the dense correspondence between a pair of images
that captures dynamic scenes undergoing substantial transformations. We
introduce Doduo to learn general dense visual correspondence from in-the-wild
images and videos without ground truth supervision. Given a pair of images, it
estimates the dense flow field encoding the displacement of each pixel in one
image to its corresponding pixel in the other image. Doduo uses flow-based
warping to acquire supervisory signals for the training. Incorporating semantic
priors with self-supervised flow training, Doduo produces accurate dense
correspondence robust to the dynamic changes of the scenes. Trained on an
in-the-wild video dataset, Doduo illustrates superior performance on
point-level correspondence estimation over existing self-supervised
correspondence learning baselines. We also apply Doduo to articulation
estimation and zero-shot goal-conditioned manipulation, underlining its
practical applications in robotics. Code and additional visualizations are
available at https://ut-austin-rpl.github.io/Doduo",http://arxiv.org/pdf/2309.15110v1
2309.15098v1,cs.CL,Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models,2023-09-26 17:48:55+00:00,"We investigate the internal behavior of Transformer-based Large Language
Models (LLMs) when they generate factually incorrect text. We propose modeling
factual queries as Constraint Satisfaction Problems and use this framework to
investigate how the model interacts internally with factual constraints.
Specifically, we discover a strong positive relation between the model's
attention to constraint tokens and the factual accuracy of its responses. In
our curated suite of 11 datasets with over 40,000 prompts, we study the task of
predicting factual errors with the Llama-2 family across all scales (7B, 13B,
70B). We propose SAT Probe, a method probing self-attention patterns, that can
predict constraint satisfaction and factual errors, and allows early error
identification. The approach and findings demonstrate how using the mechanistic
understanding of factuality in LLMs can enhance reliability.",http://arxiv.org/pdf/2309.15098v1
2309.15091v1,cs.CV,VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning,2023-09-26 17:36:26+00:00,"Although recent text-to-video (T2V) generation methods have seen significant
advancements, most of these works focus on producing short video clips of a
single event with a single background (i.e., single-scene videos). Meanwhile,
recent large language models (LLMs) have demonstrated their capability in
generating layouts and programs to control downstream visual modules such as
image generation models. This raises an important question: can we leverage the
knowledge embedded in these LLMs for temporally consistent long video
generation? In this paper, we propose VideoDirectorGPT, a novel framework for
consistent multi-scene video generation that uses the knowledge of LLMs for
video content planning and grounded video generation. Specifically, given a
single text prompt, we first ask our video planner LLM (GPT-4) to expand it
into a 'video plan', which involves generating the scene descriptions, the
entities with their respective layouts, the background for each scene, and
consistency groupings of the entities and backgrounds. Next, guided by this
output from the video planner, our video generator, Layout2Vid, has explicit
control over spatial layouts and can maintain temporal consistency of
entities/backgrounds across scenes, while only trained with image-level
annotations. Our experiments demonstrate that VideoDirectorGPT framework
substantially improves layout and movement control in both single- and
multi-scene video generation and can generate multi-scene videos with visual
consistency across scenes, while achieving competitive performance with SOTAs
in open-domain single-scene T2V generation. We also demonstrate that our
framework can dynamically control the strength for layout guidance and can also
generate videos with user-provided images. We hope our framework can inspire
future work on better integrating the planning ability of LLMs into consistent
long video generation.",http://arxiv.org/pdf/2309.15091v1
2309.15056v1,quant-ph,QUILT: Effective Multi-Class Classification on Quantum Computers Using an Ensemble of Diverse Quantum Classifiers,2023-09-26 16:36:11+00:00,"Quantum computers can theoretically have significant acceleration over
classical computers; but, the near-future era of quantum computing is limited
due to small number of qubits that are also error prone. Quilt is a framework
for performing multi-class classification task designed to work effectively on
current error-prone quantum computers. Quilt is evaluated with real quantum
machines as well as with projected noise levels as quantum machines become more
noise-free. Quilt demonstrates up to 85% multi-class classification accuracy
with the MNIST dataset on a five-qubit system.",http://arxiv.org/pdf/2309.15056v1
2309.15054v1,cs.RO,Near Real-Time Position Tracking for Robot-Guided Evacuation,2023-09-26 16:34:18+00:00,"During the evacuation of a building, the rapid and accurate tracking of human
evacuees can be used by a guide robot to increase the effectiveness of the
evacuation [1],[2]. This paper introduces a near real-time human position
tracking solution tailored for evacuation robots. Using a pose detector, our
system first identifies human joints in the camera frame in near real-time and
then translates the position of these pixels into real-world coordinates via a
simple calibration process. We run multiple trials of the system in action in
an indoor lab environment and show that the system can achieve an accuracy of
0.55 meters when compared to ground truth. The system can also achieve an
average of 3 frames per second (FPS) which was sufficient for our study on
robot-guided human evacuation. The potential of our approach extends beyond
mere tracking, paving the way for evacuee motion prediction, allowing the robot
to proactively respond to human movements during an evacuation.",http://arxiv.org/pdf/2309.15054v1
2309.15049v1,cs.RO,When Prolog meets generative models: a new approach for managing knowledge and planning in robotic applications,2023-09-26 16:26:17+00:00,"In this paper, we propose a robot oriented knowledge management system based
on the use of the Prolog language. Our framework hinges on a special
organisation of knowledge base that enables: 1. its efficient population from
natural language texts using semi-automated procedures based on Large Language
Models, 2. the bumpless generation of temporal parallel plans for multi-robot
systems through a sequence of transformations, 3. the automated translation of
the plan into an executable formalism (the behaviour trees). The framework is
supported by a set of open source tools and is shown on a realistic
application.",http://arxiv.org/pdf/2309.15049v1
2309.15048v1,cs.LG,Class Incremental Learning via Likelihood Ratio Based Task Prediction,2023-09-26 16:25:57+00:00,"Class incremental learning (CIL) is a challenging setting of continual
learning, which learns a series of tasks sequentially. Each task consists of a
set of unique classes. The key feature of CIL is that no task identifier (or
task-id) is provided at test time for each test sample. Predicting the task-id
for each test sample is a challenging problem. An emerging theoretically
justified and effective approach is to train a task-specific model for each
task in a shared network for all tasks based on a task-incremental learning
(TIL) method to deal with forgetting. The model for each task in this approach
is an out-of-distribution (OOD) detector rather than a conventional classifier.
The OOD detector can perform both within-task (in-distribution (IND)) class
prediction and OOD detection. The OOD detection capability is the key for
task-id prediction during inference for each test sample. However, this paper
argues that using a traditional OOD detector for task-id prediction is
sub-optimal because additional information (e.g., the replay data and the
learned tasks) available in CIL can be exploited to design a better and
principled method for task-id prediction. We call the new method TPLR (Task-id
Prediction based on Likelihood Ratio}). TPLR markedly outperforms strong CIL
baselines.",http://arxiv.org/pdf/2309.15048v1
2309.15039v1,cs.LG,Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data,2023-09-26 16:15:54+00:00,"Purely medical cancer screening methods are often costly, time-consuming, and
weakly applicable on a large scale. Advanced Artificial Intelligence (AI)
methods greatly help cancer detection but require specific or deep medical
data. These aspects affect the mass implementation of cancer screening methods.
For these reasons, it is a disruptive change for healthcare to apply AI methods
for mass personalized assessment of the cancer risk among patients based on the
existing Electronic Health Records (EHR) volume.
  This paper presents a novel method for mass cancer risk prediction using EHR
data. Among other methods, our one stands out by the minimum data greedy
policy, requiring only a history of medical service codes and diagnoses from
EHR. We formulate the problem as a binary classification. This dataset contains
175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, we
implement a solution based on a recurrent neural network (RNN). We propose a
method that combines machine learning and survival analysis since these
approaches are less computationally heavy, can be combined into an ensemble
(the Survival Ensemble), and can be reproduced in most medical institutions.
  We test the Survival Ensemble in some studies. Firstly, we obtain a
significant difference between values of the primary metric (Average Precision)
with 22.8% (ROC AUC 83.7%, F1 17.8%) for the Survival Ensemble versus 15.1%
(ROC AUC 84.9%, F1 21.4%) for the Baseline. Secondly, the performance of the
Survival Ensemble is also confirmed during the ablation study. Thirdly, our
method exceeds age baselines by a significant margin. Fourthly, in the blind
retrospective out-of-time experiment, the proposed method is reliable in cancer
patient detection (9 out of 100 selected). Such results exceed the estimates of
medical screenings, e.g., the best Number Needed to Screen (9 out of 1000
screenings).",http://arxiv.org/pdf/2309.15039v1
2309.15037v1,cs.IT,STAR-RIS Assisted Full-Duplex Communication Networks,2023-09-26 16:07:38+00:00,"Different from conventional reconfigurable intelligent surfaces (RIS), a
recent innovation called simultaneous transmitting and reflecting
reconfigurable intelligent surface (STAR-RIS) has emerged, aimed at achieving
complete 360-degree coverage in communication networks. Additionally,
fullduplex (FD) technology is recognized as a potent approach for enhancing
spectral efficiency by enabling simultaneous transmission and reception within
the same time and frequency resources. In this study, we investigate the
performance of a STAR-RIS-assisted FD communication system. The STAR-RIS is
strategically placed at the cell-edge to facilitate communication for users
located in this challenging region, while cell-center users can communicate
directly with the FD base station (BS). We employ a non-orthogonal multiple
access (NOMA) pairing scheme and account for system impairments, such as
self-interference at the BS and imperfect successive interference cancellation
(SIC). We derive closed-form expressions for the ergodic rates in both the
up-link and down-link communications and extend our analysis to bidirectional
communication between cell-center and cell-edge users. Furthermore, we
formulate an optimization problem aimed at maximizing the ergodic sum-rate.
This optimization involves adjusting the amplitudes and phase-shifts of the
STAR-RIS elements and allocating total transmit power efficiently. To gain
deeper insights into the achievable rates of STAR-RIS-aided FD systems, we
explore the impact of various system parameters through numerical results.",http://arxiv.org/pdf/2309.15037v1
2309.15028v1,cs.CL,Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding,2023-09-26 15:57:57+00:00,"Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may
seem unnecessary when generating natural language text based on
state-of-the-art reinforcement learning such as Proximal Policy Optimization
(PPO). In this paper, we demonstrate that it is possible to get extra mileage
out of PPO by integrating MCTS on top. The key idea is not to throw out the
value network, a byproduct of PPO training for evaluating partial output
sequences, when decoding text out of the policy network. More concretely, we
present a novel value-guided decoding algorithm called PPO-MCTS, which can
integrate the value network from PPO to work closely with the policy network
during inference-time generation. Compared to prior approaches based on MCTS
for controlled text generation, the key strength of our approach is to reduce
the fundamental mismatch of the scoring mechanisms of the partial outputs
between training and test. Evaluation on four text generation tasks demonstrate
that PPO-MCTS greatly improves the preferability of generated text compared to
the standard practice of using only the PPO policy. Our results demonstrate the
promise of search algorithms even on top of the aligned language models from
PPO, and the under-explored benefit of the value network.",http://arxiv.org/pdf/2309.15028v1
2309.15025v1,cs.CL,Large Language Model Alignment: A Survey,2023-09-26 15:49:23+00:00,"Recent years have witnessed remarkable progress made in large language models
(LLMs). Such advancements, while garnering significant attention, have
concurrently elicited various concerns. The potential of these models is
undeniably vast; however, they may yield texts that are imprecise, misleading,
or even detrimental. Consequently, it becomes paramount to employ alignment
techniques to ensure these models to exhibit behaviors consistent with human
values.
  This survey endeavors to furnish an extensive exploration of alignment
methodologies designed for LLMs, in conjunction with the extant capability
research in this domain. Adopting the lens of AI alignment, we categorize the
prevailing methods and emergent proposals for the alignment of LLMs into outer
and inner alignment. We also probe into salient issues including the models'
interpretability, and potential vulnerabilities to adversarial attacks. To
assess LLM alignment, we present a wide variety of benchmarks and evaluation
methodologies. After discussing the state of alignment research for LLMs, we
finally cast a vision toward the future, contemplating the promising avenues of
research that lie ahead.
  Our aspiration for this survey extends beyond merely spurring research
interests in this realm. We also envision bridging the gap between the AI
alignment research community and the researchers engrossed in the capability
exploration of LLMs for both capable and safe LLMs.",http://arxiv.org/pdf/2309.15025v1
2309.15018v1,cs.CV,Unidirectional brain-computer interface: Artificial neural network encoding natural images to fMRI response in the visual cortex,2023-09-26 15:38:26+00:00,"While significant advancements in artificial intelligence (AI) have catalyzed
progress across various domains, its full potential in understanding visual
perception remains underexplored. We propose an artificial neural network
dubbed VISION, an acronym for ""Visual Interface System for Imaging Output of
Neural activity,"" to mimic the human brain and show how it can foster
neuroscientific inquiries. Using visual and contextual inputs, this multimodal
model predicts the brain's functional magnetic resonance imaging (fMRI) scan
response to natural images. VISION successfully predicts human hemodynamic
responses as fMRI voxel values to visual inputs with an accuracy exceeding
state-of-the-art performance by 45%. We further probe the trained networks to
reveal representational biases in different visual areas, generate
experimentally testable hypotheses, and formulate an interpretable metric to
associate these hypotheses with cortical functions. With both a model and
evaluation metric, the cost and time burdens associated with designing and
implementing functional analysis on the visual cortex could be reduced. Our
work suggests that the evolution of computational models may shed light on our
fundamental understanding of the visual cortex and provide a viable approach
toward reliable brain-machine interfaces.",http://arxiv.org/pdf/2309.15018v1
2309.15004v1,cs.CL,Automating question generation from educational text,2023-09-26 15:18:44+00:00,"The use of question-based activities (QBAs) is wide-spread in education,
traditionally forming an integral part of the learning and assessment process.
In this paper, we design and evaluate an automated question generation tool for
formative and summative assessment in schools. We present an expert survey of
one hundred and four teachers, demonstrating the need for automated generation
of QBAs, as a tool that can significantly reduce the workload of teachers and
facilitate personalized learning experiences. Leveraging the recent
advancements in generative AI, we then present a modular framework employing
transformer based language models for automatic generation of multiple-choice
questions (MCQs) from textual content. The presented solution, with distinct
modules for question generation, correct answer prediction, and distractor
formulation, enables us to evaluate different language models and generation
techniques. Finally, we perform an extensive quantitative and qualitative
evaluation, demonstrating trade-offs in the use of different techniques and
models.",http://arxiv.org/pdf/2309.15004v1
2309.15001v1,math.ST,Convergence guarantees for forward gradient descent in the linear regression model,2023-09-26 15:15:10+00:00,"Renewed interest in the relationship between artificial and biological neural
networks motivates the study of gradient-free methods. Considering the linear
regression model with random design, we theoretically analyze in this work the
biologically motivated (weight-perturbed) forward gradient scheme that is based
on random linear combination of the gradient. If d denotes the number of
parameters and k the number of samples, we prove that the mean squared error of
this method converges for $k\gtrsim d^2\log(d)$ with rate $d^2\log(d)/k.$
Compared to the dimension dependence d for stochastic gradient descent, an
additional factor $d\log(d)$ occurs.",http://arxiv.org/pdf/2309.15001v1
2309.14994v1,cs.LG,Measurement Models For Sailboats Price vs. Features And Regional Areas,2023-09-26 15:03:05+00:00,"In this study, we investigated the relationship between sailboat technical
specifications and their prices, as well as regional pricing influences.
Utilizing a dataset encompassing characteristics like length, beam, draft,
displacement, sail area, and waterline, we applied multiple machine learning
models to predict sailboat prices. The gradient descent model demonstrated
superior performance, producing the lowest MSE and MAE. Our analysis revealed
that monohulled boats are generally more affordable than catamarans, and that
certain specifications such as length, beam, displacement, and sail area
directly correlate with higher prices. Interestingly, lower draft was
associated with higher listing prices. We also explored regional price
determinants and found that the United States tops the list in average sailboat
prices, followed by Europe, Hong Kong, and the Caribbean. Contrary to our
initial hypothesis, a country's GDP showed no direct correlation with sailboat
prices. Utilizing a 50% cross-validation method, our models yielded consistent
results across test groups. Our research offers a machine learning-enhanced
perspective on sailboat pricing, aiding prospective buyers in making informed
decisions.",http://arxiv.org/pdf/2309.14994v1
2309.14972v1,cs.CV,Improving Unsupervised Visual Program Inference with Code Rewriting Families,2023-09-26 14:44:48+00:00,"Programs offer compactness and structure that makes them an attractive
representation for visual data. We explore how code rewriting can be used to
improve systems for inferring programs from visual data. We first propose
Sparse Intermittent Rewrite Injection (SIRI), a framework for unsupervised
bootstrapped learning. SIRI sparsely applies code rewrite operations over a
dataset of training programs, injecting the improved programs back into the
training set. We design a family of rewriters for visual programming domains:
parameter optimization, code pruning, and code grafting. For three shape
programming languages in 2D and 3D, we show that using SIRI with our family of
rewriters improves performance: better reconstructions and faster convergence
rates, compared with bootstrapped learning methods that do not use rewriters or
use them naively. Finally, we demonstrate that our family of rewriters can be
effectively used at test time to improve the output of SIRI predictions. For 2D
and 3D CSG, we outperform or match the reconstruction performance of recent
domain-specific neural architectures, while producing more parsimonious
programs that use significantly fewer primitives.",http://arxiv.org/pdf/2309.14972v1
2309.14970v1,cs.LG,Recurrent Hypernetworks are Surprisingly Strong in Meta-RL,2023-09-26 14:42:28+00:00,"Deep reinforcement learning (RL) is notoriously impractical to deploy due to
sample inefficiency. Meta-RL directly addresses this sample inefficiency by
learning to perform few-shot learning when a distribution of related tasks is
available for meta-training. While many specialized meta-RL methods have been
proposed, recent work suggests that end-to-end learning in conjunction with an
off-the-shelf sequential model, such as a recurrent network, is a surprisingly
strong baseline. However, such claims have been controversial due to limited
supporting evidence, particularly in the face of prior work establishing
precisely the opposite. In this paper, we conduct an empirical investigation.
While we likewise find that a recurrent network can achieve strong performance,
we demonstrate that the use of hypernetworks is crucial to maximizing their
potential. Surprisingly, when combined with hypernetworks, the recurrent
baselines that are far simpler than existing specialized methods actually
achieve the strongest performance of all methods evaluated.",http://arxiv.org/pdf/2309.14970v1
2309.14966v1,cs.CL,Interactively Learning Social Media Representations Improves News Source Factuality Detection,2023-09-26 14:36:19+00:00,"The rise of social media has enabled the widespread propagation of fake news,
text that is published with an intent to spread misinformation and sway
beliefs. Rapidly detecting fake news, especially as new events arise, is
important to prevent misinformation.
  While prior works have tackled this problem using supervised learning
systems, automatedly modeling the complexities of the social media landscape
that enables the spread of fake news is challenging. On the contrary, having
humans fact check all news is not scalable. Thus, in this paper, we propose to
approach this problem interactively, where humans can interact to help an
automated system learn a better social media representation quality. On real
world events, our experiments show performance improvements in detecting
factuality of news sources, even after few human interactions.",http://arxiv.org/pdf/2309.14966v1
2309.14954v1,q-bio.BM,Addressing preferred orientation in single-particle cryo-EM through AI-generated auxiliary particles,2023-09-26 14:14:09+00:00,"The single-particle cryo-EM field faces the persistent challenge of preferred
orientation, lacking general computational solutions. We introduce cryoPROS, an
AI-based approach designed to address the above issue. By generating the
auxiliary particles with a conditional deep generative model, cryoPROS
addresses the intrinsic bias in orientation estimation for the observed
particles. We effectively employed cryoPROS in the cryo-EM single particle
analysis of the hemagglutinin trimer, showing the ability to restore the
near-atomic resolution structure on non-tilt data. Moreover, the enhanced
version named cryoPROS-MP significantly improves the resolution of the membrane
protein NaX using the no-tilted data that contains the effects of micelles.
Compared to the classical approaches, cryoPROS does not need special
experimental or image acquisition techniques, providing a purely computational
yet effective solution for the preferred orientation problem. Finally, we
conduct extensive experiments that establish the low risk of model bias and the
high robustness of cryoPROS.",http://arxiv.org/pdf/2309.14954v1
2309.14950v1,cs.CV,Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher,2023-09-26 14:08:03+00:00,"Adapting visual object detectors to operational target domains is a
challenging task, commonly achieved using unsupervised domain adaptation (UDA)
methods. When the labeled dataset is coming from multiple source domains,
treating them as separate domains and performing a multi-source domain
adaptation (MSDA) improves the accuracy and robustness over mixing these source
domains and performing a UDA, as observed by recent studies in MSDA. Existing
MSDA methods learn domain invariant and domain-specific parameters (for each
source domain) for the adaptation. However, unlike single-source UDA methods,
learning domain-specific parameters makes them grow significantly proportional
to the number of source domains used. This paper proposes a novel MSDA method
called Prototype-based Mean-Teacher (PMT), which uses class prototypes instead
of domain-specific subnets to preserve domain-specific information. These
prototypes are learned using a contrastive loss, aligning the same categories
across domains and separating different categories far apart. Because of the
use of prototypes, the parameter size of our method does not increase
significantly with the number of source domains, thus reducing memory issues
and possible overfitting. Empirical studies show PMT outperforms
state-of-the-art MSDA methods on several challenging object detection datasets.",http://arxiv.org/pdf/2309.14950v1
2309.14931v1,cs.RO,Interaction-Aware Sampling-Based MPC with Learned Local Goal Predictions,2023-09-26 13:36:45+00:00,"Motion planning for autonomous robots in tight, interaction-rich, and mixed
human-robot environments is challenging. State-of-the-art methods typically
separate prediction and planning, predicting other agents' trajectories first
and then planning the ego agent's motion in the remaining free space. However,
agents' lack of awareness of their influence on others can lead to the freezing
robot problem. We build upon Interaction-Aware Model Predictive Path Integral
(IA-MPPI) control and combine it with learning-based trajectory predictions,
thereby relaxing its reliance on communicated short-term goals for other
agents. We apply this framework to Autonomous Surface Vessels (ASVs) navigating
urban canals. By generating an artificial dataset in real sections of
Amsterdam's canals, adapting and training a prediction model for our domain,
and proposing heuristics to extract local goals, we enable effective
cooperation in planning. Our approach improves autonomous robot navigation in
complex, crowded environments, with potential implications for multi-agent
systems and human-robot interaction.",http://arxiv.org/pdf/2309.14931v1
2309.14921v1,cs.HC,A Democratic Platform for Engaging with Disabled Community in Generative AI Development,2023-09-26 13:30:57+00:00,"Artificial Intelligence (AI) systems, especially generative AI technologies
are becoming more relevant in our society. Tools like ChatGPT are being used by
members of the disabled community e.g., Autistic people may use it to help
compose emails. The growing impact and popularity of generative AI tools have
prompted us to examine their relevance within the disabled community. The
design and development phases often neglect this marginalized group, leading to
inaccurate predictions and unfair discrimination directed towards them. This
could result from bias in data sets, algorithms, and systems at various phases
of creation and implementation. This workshop paper proposes a platform to
involve the disabled community while building generative AI systems. With this
platform, our aim is to gain insight into the factors that contribute to bias
in the outputs generated by generative AI when used by the disabled community.
Furthermore, we expect to comprehend which algorithmic factors are the main
contributors to the output's incorrectness or irrelevancy. The proposed
platform calls on both disabled and non-disabled people from various
geographical and cultural backgrounds to collaborate asynchronously and
remotely in a democratic approach to decision-making.",http://arxiv.org/pdf/2309.14921v1
2309.14907v1,cs.LG,Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs against Learning Bias,2023-09-26 13:09:43+00:00,"Node representation learning on attributed graphs -- whose nodes are
associated with rich attributes (e.g., texts and protein sequences) -- plays a
crucial role in many important downstream tasks. To encode the attributes and
graph structures simultaneously, recent studies integrate pre-trained models
with graph neural networks (GNNs), where pre-trained models serve as node
encoders (NEs) to encode the attributes. As jointly training large NEs and GNNs
on large-scale graphs suffers from severe scalability issues, many methods
propose to train NEs and GNNs separately. Consequently, they do not take
feature convolutions in GNNs into consideration in the training phase of NEs,
leading to a significant learning bias from that by the joint training. To
address this challenge, we propose an efficient label regularization technique,
namely Label Deconvolution (LD), to alleviate the learning bias by a novel and
highly scalable approximation to the inverse mapping of GNNs. The inverse
mapping leads to an objective function that is equivalent to that by the joint
training, while it can effectively incorporate GNNs in the training phase of
NEs against the learning bias. More importantly, we show that LD converges to
the optimal objective function values by thejoint training under mild
assumptions. Experiments demonstrate LD significantly outperforms
state-of-the-art methods on Open Graph Benchmark datasets.",http://arxiv.org/pdf/2309.14907v1
2309.14880v1,cs.LG,Credit Card Fraud Detection with Subspace Learning-based One-Class Classification,2023-09-26 12:26:28+00:00,"In an increasingly digitalized commerce landscape, the proliferation of
credit card fraud and the evolution of sophisticated fraudulent techniques have
led to substantial financial losses. Automating credit card fraud detection is
a viable way to accelerate detection, reducing response times and minimizing
potential financial losses. However, addressing this challenge is complicated
by the highly imbalanced nature of the datasets, where genuine transactions
vastly outnumber fraudulent ones. Furthermore, the high number of dimensions
within the feature set gives rise to the ``curse of dimensionality"". In this
paper, we investigate subspace learning-based approaches centered on One-Class
Classification (OCC) algorithms, which excel in handling imbalanced data
distributions and possess the capability to anticipate and counter the
transactions carried out by yet-to-be-invented fraud techniques. The study
highlights the potential of subspace learning-based OCC algorithms by
investigating the limitations of current fraud detection strategies and the
specific challenges of credit card fraud detection. These algorithms integrate
subspace learning into the data description; hence, the models transform the
data into a lower-dimensional subspace optimized for OCC. Through rigorous
experimentation and analysis, the study validated that the proposed approach
helps tackle the curse of dimensionality and the imbalanced nature of credit
card data for automatic fraud detection to mitigate financial losses caused by
fraudulent activities.",http://arxiv.org/pdf/2309.14880v1
2309.14877v1,cs.HC,Explainable Sustainability for AI in the Arts,2023-09-26 12:20:18+00:00,"AI is becoming increasingly popular in artistic practices, but the tools for
informing practitioners about the environmental impact (and other
sustainability implications) of AI are adapted for other contexts than creative
practices -- making the tools and sustainability implications of AI not
accessible for artists and creative practitioners. In this position paper, I
describe two empirical studies that aim to develop environmental sustainability
reflection systems for AI Arts, and discuss and introduce Explainable
Sustainability in for AI Arts.",http://arxiv.org/pdf/2309.14877v1
2309.14859v1,cs.CV,Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation,2023-09-26 11:36:26+00:00,"Text-to-image generative models have garnered immense attention for their
ability to produce high-fidelity images from text prompts. Among these, Stable
Diffusion distinguishes itself as a leading open-source model in this
fast-growing field. However, the intricacies of fine-tuning these models pose
multiple challenges from new methodology integration to systematic evaluation.
Addressing these issues, this paper introduces LyCORIS (Lora beYond
Conventional methods, Other Rank adaptation Implementations for Stable
diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library
that offers a wide selection of fine-tuning methodologies for Stable Diffusion.
Furthermore, we present a thorough framework for the systematic assessment of
varied fine-tuning techniques. This framework employs a diverse suite of
metrics and delves into multiple facets of fine-tuning, including
hyperparameter adjustments and the evaluation with different prompt types
across various concept categories. Through this comprehensive approach, our
work provides essential insights into the nuanced effects of fine-tuning
parameters, bridging the gap between state-of-the-art research and practical
application.",http://arxiv.org/pdf/2309.14859v1
2309.14846v1,cs.SE,Supersonic: Learning to Generate Source Code Optimisations in C/C++,2023-09-26 11:21:46+00:00,"Software optimization refines programs for resource efficiency while
preserving functionality. Traditionally, it is a process done by developers and
compilers. This paper introduces a third option, automated optimization at the
source code level. We present Supersonic, a neural approach targeting minor
source code modifications for optimization. Using a seq2seq model, Supersonic
is trained on C/C++ program pairs ($x_{t}$, $x_{t+1}$), where $x_{t+1}$ is an
optimized version of $x_{t}$, and outputs a diff. Supersonic's performance is
benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming
tasks. The experiments show that Supersonic not only outperforms both models on
the code optimization task, but also minimizes the extent of change with a more
than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4.",http://arxiv.org/pdf/2309.14846v1
2309.14841v1,q-bio.NC,Towards a Neuronally Consistent Ontology for Robotic Agents,2023-09-26 11:13:02+00:00,"The Collaborative Research Center for Everyday Activity Science & Engineering
(CRC EASE) aims to enable robots to perform environmental interaction tasks
with close to human capacity. It therefore employs a shared ontology to model
the activity of both kinds of agents, empowering robots to learn from human
experiences. To properly describe these human experiences, the ontology will
strongly benefit from incorporating characteristics of neuronal information
processing which are not accessible from a behavioral perspective alone. We,
therefore, propose the analysis of human neuroimaging data for evaluation and
validation of concepts and events defined in the ontology model underlying most
of the CRC projects. In an exploratory analysis, we employed an Independent
Component Analysis (ICA) on functional Magnetic Resonance Imaging (fMRI) data
from participants who were presented with the same complex video stimuli of
activities as robotic and human agents in different environments and contexts.
We then correlated the activity patterns of brain networks represented by
derived components with timings of annotated event categories as defined by the
ontology model. The present results demonstrate a subset of common networks
with stable correlations and specificity towards particular event classes and
groups, associated with environmental and contextual factors. These neuronal
characteristics will open up avenues for adapting the ontology model to be more
consistent with human information processing.",http://arxiv.org/pdf/2309.14841v1
2309.14815v1,math.NA,Removing the mask -- reconstructing a scalar field on the sphere from a masked field,2023-09-26 10:30:33+00:00,"The paper analyses a spectral approach to reconstructing %the image of a
scalar field on the sphere, given only information about a masked version of
the field together with precise information about the (smooth) mask. The theory
is developed for a general mask, and later specialized to the case of an
axially symmetric mask. Numerical experiments are given for the case of an
axial mask motivated by the cosmic microwave background, assuming that the
underlying field is a realization of a Gaussian random field with an artificial
angular power spectrum of moderate degree ($\ell \le 100$). The recovery is
highly satisfactory in the absence of noise and even in the presence of
moderate noise.",http://arxiv.org/pdf/2309.14815v1
2309.14810v1,cs.NI,RAN Functional Splits in NTN: Architectures and Challenges,2023-09-26 10:16:41+00:00,"While 5G networks are already being deployed for commercial applications,
Academia and industry are focusing their effort on the development and
standardization of the next generations of mobile networks, i.e., 5G-Advance
and 6G. Beyond 5G networks will revolutionize communications systems providing
seamless connectivity, both in time and in space, to a unique ecosystem
consisting of the convergence of the digital, physical, and human domains. In
this scenario, NonTerrestrial Networks (NTN) will play a crucial role by
providing ubiquitous, secure, and resilient infrastructure fully integrated
into the overall system. The additional network complexity introduced by the
third dimension of the architecture requires the interoperability of different
network elements, enabled by the disaggregation and virtualization of network
components, their interconnection by standard interfaces and orchestration by
data-driven network artificial intelligence. The disaggregation paradigm
foresees the division of the radio access network in different virtualized
block of functions, introducing the concept of functional split. Wisely
selecting the RAN functional split is possible to better exploit the system
resources, obtaining costs saving, and to increase the system performances. In
this paper, we firstly provide a discussion of the current 6G NTN development
in terms of architectural solutions and then, we thoroughly analyze the impact
of the typical NTN channel impairments on the available functional splits.
Finally, the benefits of introducing the dynamic optimization of the functional
split in NTN are analyzed, together with the foreseen challenges.",http://arxiv.org/pdf/2309.14810v1
2309.14808v1,cs.LG,Revisiting Softmax Masking for Stability in Continual Learning,2023-09-26 10:06:28+00:00,"In continual learning, many classifiers use softmax function to learn
confidence. However, numerous studies have pointed out its inability to
accurately determine confidence distributions for outliers, often referred to
as epistemic uncertainty. This inherent limitation also curtails the accurate
decisions for selecting what to forget and keep in previously trained
confidence distributions over continual learning process. To address the issue,
we revisit the effects of masking softmax function. While this method is both
simple and prevalent in literature, its implication for retaining confidence
distribution during continual learning, also known as stability, has been
under-investigated. In this paper, we revisit the impact of softmax masking,
and introduce a methodology to utilize its confidence preservation effects. In
class- and task-incremental learning benchmarks with and without memory replay,
our approach significantly increases stability while maintaining sufficiently
large plasticity. In the end, our methodology shows better overall performance
than state-of-the-art methods, particularly in the use with zero or small
memory. This lays a simple and effective foundation of strongly stable
replay-based continual learning.",http://arxiv.org/pdf/2309.14808v1
2309.14807v1,cs.LG,Evaluating Soccer Match Prediction Models: A Deep Learning Approach and Feature Optimization for Gradient-Boosted Trees,2023-09-26 10:05:46+00:00,"Machine learning models have become increasingly popular for predicting the
results of soccer matches, however, the lack of publicly-available benchmark
datasets has made model evaluation challenging. The 2023 Soccer Prediction
Challenge required the prediction of match results first in terms of the exact
goals scored by each team, and second, in terms of the probabilities for a win,
draw, and loss. The original training set of matches and features, which was
provided for the competition, was augmented with additional matches that were
played between 4 April and 13 April 2023, representing the period after which
the training set ended, but prior to the first matches that were to be
predicted (upon which the performance was evaluated). A CatBoost model was
employed using pi-ratings as the features, which were initially identified as
the optimal choice for calculating the win/draw/loss probabilities. Notably,
deep learning models have frequently been disregarded in this particular task.
Therefore, in this study, we aimed to assess the performance of a deep learning
model and determine the optimal feature set for a gradient-boosted tree model.
The model was trained using the most recent five years of data, and three
training and validation sets were used in a hyperparameter grid search. The
results from the validation sets show that our model had strong performance and
stability compared to previously published models from the 2017 Soccer
Prediction Challenge for win/draw/loss prediction.",http://arxiv.org/pdf/2309.14807v1
2309.14805v1,cs.CL,Fine-tuning and aligning question answering models for complex information extraction tasks,2023-09-26 10:02:21+00:00,"The emergence of Large Language Models (LLMs) has boosted performance and
possibilities in various NLP tasks. While the usage of generative AI models
like ChatGPT opens up new opportunities for several business use cases, their
current tendency to hallucinate fake content strongly limits their
applicability to document analysis, such as information retrieval from
documents. In contrast, extractive language models like question answering (QA)
or passage retrieval models guarantee query results to be found within the
boundaries of an according context document, which makes them candidates for
more reliable information extraction in productive environments of companies.
In this work we propose an approach that uses and integrates extractive QA
models for improved feature extraction of German business documents such as
insurance reports or medical leaflets into a document analysis solution. We
further show that fine-tuning existing German QA models boosts performance for
tailored extraction tasks of complex linguistic features like damage cause
explanations or descriptions of medication appearance, even with using only a
small set of annotated data. Finally, we discuss the relevance of scoring
metrics for evaluating information extraction tasks and deduce a combined
metric from Levenshtein distance, F1-Score, Exact Match and ROUGE-L to mimic
the assessment criteria from human experts.",http://arxiv.org/pdf/2309.14805v1
2309.14796v1,cs.AI,Forgetting-aware Linear Bias for Attentive Knowledge Tracing,2023-09-26 09:48:30+00:00,"Knowledge Tracing (KT) aims to track proficiency based on a question-solving
history, allowing us to offer a streamlined curriculum. Recent studies actively
utilize attention-based mechanisms to capture the correlation between questions
and combine it with the learner's characteristics for responses. However, our
empirical study shows that existing attention-based KT models neglect the
learner's forgetting behavior, especially as the interaction history becomes
longer. This problem arises from the bias that overprioritizes the correlation
of questions while inadvertently ignoring the impact of forgetting behavior.
This paper proposes a simple-yet-effective solution, namely Forgetting-aware
Linear Bias (FoLiBi), to reflect forgetting behavior as a linear bias. Despite
its simplicity, FoLiBi is readily equipped with existing attentive KT models by
effectively decomposing question correlations with forgetting behavior. FoLiBi
plugged with several KT models yields a consistent improvement of up to 2.58%
in AUC over state-of-the-art KT models on four benchmark datasets.",http://arxiv.org/pdf/2309.14796v1
2309.14793v1,cs.CV,Semantic Map Learning of Traffic Light to Lane Assignment based on Motion Data,2023-09-26 09:42:21+00:00,"Understanding which traffic light controls which lane is crucial to navigate
intersections safely. Autonomous vehicles commonly rely on High Definition (HD)
maps that contain information about the assignment of traffic lights to lanes.
The manual provisioning of this information is tedious, expensive, and not
scalable. To remedy these issues, our novel approach derives the assignments
from traffic light states and the corresponding motion patterns of vehicle
traffic. This works in an automated way and independently of the geometric
arrangement. We show the effectiveness of basic statistical approaches for this
task by implementing and evaluating a pattern-based contribution method. In
addition, our novel rejection method includes accompanying safety
considerations by leveraging statistical hypothesis testing. Finally, we
propose a dataset transformation to re-purpose available motion prediction
datasets for semantic map learning. Our publicly available API for the Lyft
Level 5 dataset enables researchers to develop and evaluate their own
approaches.",http://arxiv.org/pdf/2309.14793v1
2309.14780v1,physics.ao-ph,Transferring climate change knowledge,2023-09-26 09:24:53+00:00,"Accurate climate projections are required for climate adaptation and
mitigation. Earth system model simulations, used to project climate change,
inherently make approximations in their representation of small-scale physical
processes, such as clouds, that are at the root of the uncertainties in global
mean temperature's response to increased greenhouse gas concentrations. Several
approaches have been developed to use historical observations to constrain
future projections and reduce uncertainties in climate projections and climate
feedbacks. Yet those methods cannot capture the non-linear complexity inherent
in the climate system. Using a Transfer Learning approach, we show that Machine
Learning, in particular Deep Neural Networks, can be used to optimally leverage
and merge the knowledge gained from Earth system model simulations and
historical observations to more accurately project global surface temperature
fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-4.5,
3-7.0 and 5-8.5, we refine regional estimates and the global projection of the
average global temperature in 2081-2098 (with respect to the period 1850-1900)
to 2.73{\deg}C (2.44-3.11{\deg}C), 3.92{\deg}C (3.5-4.47{\deg}C) and
4.53{\deg}C (3.69-5.5{\deg}C), respectively, compared to the unconstrained
2.7{\deg}C (1.65-3.8{\deg}C), 3.71{\deg}C (2.56-4.97{\deg}C) and 4.47{\deg}C
(2.95-6.02{\deg}C). Our findings show that the 1.5{\deg}C threshold of the
Paris' agreement will be crossed in 2031 (2028-2034) for SSP2-4.5, in 2029
(2027-2031) for SSP3-7.0 and in 2028 (2025-2031) for SSP5-8.5. Similarly, the
2{\deg}C threshold will be exceeded in 2051 (2045-2059), 2044 (2040-2047) and
2042 (2038-2047) respectively. Our new method provides more accurate climate
projections urgently required for climate adaptation.",http://arxiv.org/pdf/2309.14780v1
2309.14779v1,cs.CL,Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification,2023-09-26 09:24:46+00:00,"Domain-specific text classification faces the challenge of scarce labeled
data due to the high cost of manual labeling. Prompt-learning, known for its
efficiency in few-shot scenarios, is proposed as an alternative to traditional
fine-tuning methods. And besides, although large language models (LLMs) have
gained prominence, small language models (SLMs, with under 1B parameters) offer
significant customizability, adaptability, and cost-effectiveness for
domain-specific tasks, given industry constraints. In this study, we
investigate the potential of SLMs combined with prompt-learning paradigm for
domain-specific text classification, specifically within customer-agent
interactions in retail. Our evaluations show that, in few-shot settings when
prompt-based model fine-tuning is possible, T5-base, a typical SLM with 220M
parameters, achieve approximately 75% accuracy with limited labeled data (up to
15% of full data), which shows great potentials of SLMs with prompt-learning.
Based on this, We further validate the effectiveness of active few-shot
sampling and the ensemble strategy in the prompt-learning pipeline that
contribute to a remarkable performance gain. Besides, in zero-shot settings
with a fixed model, we underscore a pivotal observation that, although the
GPT-3.5-turbo equipped with around 154B parameters garners an accuracy of
55.16%, the power of well designed prompts becomes evident when the
FLAN-T5-large, a model with a mere 0.5% of GPT-3.5-turbo's parameters, achieves
an accuracy exceeding 31% with the optimized prompt, a leap from its sub-18%
performance with an unoptimized one. Our findings underscore the promise of
prompt-learning in classification tasks with SLMs, emphasizing the benefits of
active few-shot sampling, and ensemble strategies in few-shot settings, and the
importance of prompt engineering in zero-shot settings.",http://arxiv.org/pdf/2309.14779v1
2309.14771v1,cs.CL,Boosting In-Context Learning with Factual Knowledge,2023-09-26 09:06:39+00:00,"In-Context Learning (ICL) over Large language models (LLMs) aims at solving
previously unseen tasks by conditioning on a few training examples, eliminating
the need for parameter updates and achieving competitive performance. In this
paper, we demonstrate that factual knowledge is imperative for the performance
of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the
factual knowledge derived from the selected in-context examples, and the
knowledge biases in LLMs for output generation. To unleash the power of LLMs in
few-shot learning scenarios, we introduce a novel Knowledgeable In-Context
Tuning (KICT) framework to further improve the performance of ICL: 1) injecting
factual knowledge to LLMs during continual self-supervised pre-training, 2)
judiciously selecting the examples with high knowledge relevance, and 3)
calibrating the prediction results based on prior knowledge. We evaluate the
proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over
multiple text classification and question answering tasks. Experimental results
demonstrate that KICT substantially outperforms strong baselines, and improves
by more than 13% and 7% of accuracy on text classification and question
answering tasks, respectively.",http://arxiv.org/pdf/2309.14771v1
2309.14760v1,cs.CL,Program Repair with Minimal Edits Using CodeT5,2023-09-26 08:45:05+00:00,"Programmers often struggle to identify and fix bugs in their programs. In
recent years, many language models (LMs) have been proposed to fix erroneous
programs and support error recovery. However, the LMs tend to generate
solutions that differ from the original input programs. This leads to potential
comprehension difficulties for users. In this paper, we propose an approach to
suggest a correct program with minimal repair edits using CodeT5. We fine-tune
a pre-trained CodeT5 on code pairs of wrong and correct programs and evaluate
its performance with several baseline models. The experimental results show
that the fine-tuned CodeT5 achieves a pass@100 of 91.95% and an average edit
distance of the most similar correct program of 6.84, which indicates that at
least one correct program can be suggested by generating 100 candidate
programs. We demonstrate the effectiveness of LMs in suggesting program repair
with minimal edits for solving introductory programming problems.",http://arxiv.org/pdf/2309.14760v1
2309.14757v1,cs.LG,Age Minimization in Massive IoT via UAV Swarm: A Multi-agent Reinforcement Learning Approach,2023-09-26 08:37:21+00:00,"In many massive IoT communication scenarios, the IoT devices require coverage
from dynamic units that can move close to the IoT devices and reduce the uplink
energy consumption. A robust solution is to deploy a large number of UAVs (UAV
swarm) to provide coverage and a better line of sight (LoS) for the IoT
network. However, the study of these massive IoT scenarios with a massive
number of serving units leads to high dimensional problems with high
complexity. In this paper, we apply multi-agent deep reinforcement learning to
address the high-dimensional problem that results from deploying a swarm of
UAVs to collect fresh information from IoT devices. The target is to minimize
the overall age of information in the IoT network. The results reveal that both
cooperative and partially cooperative multi-agent deep reinforcement learning
approaches are able to outperform the high-complexity centralized deep
reinforcement learning approach, which stands helpless in large-scale networks.",http://arxiv.org/pdf/2309.14757v1
2309.14743v1,astro-ph.GA,The galactic tooth-fairy and a cosmic bullet: Amateur discoveries and a call for further research,2023-09-26 08:11:40+00:00,"There are countless digital sky surveys and automated scans of the night sky
which use computer algorithms to detect and categorize objects. With the advent
of Artificial Intelligence such surveys will become even more efficient in the
near future. Despite this some objects are missed by surveys or pose no initial
interest. At times such missed objects are unique in nature and of decent
angular sizes, demanding research, unlike the billions of tiny specs of
galaxies that would be too tedious to name and study. In this scenario the
amateur astronomer and their spirit for old school astronomical discovery steps
in, to manually comb the sky and catalogue unique objects as was done in the
early days of astronomy. In this paper two unique, previously uncatalogued
galaxy candidates, namely Shaheer I and Shaheer II are identified and studied.
Both galaxies lay at a distance of 6.67 arc-minutes from each other in the
constellation of Camelopardalis. One boasts an unusual morphological profile,
akin to a molar tooth, while the other seems to be shooting through space at
tremendous velocities. The objects were discovered during visual inspection of
digital surveys and then imaged from amateur telescopes at Taqwa observatory,
Pakistan's first and only dark sky observatory (bortle 1). We perform
photometry using PetroFit to discuss the potential nature of the galaxies and
implore further collaborative research to fully uncover their characteristics.",http://arxiv.org/pdf/2309.14743v1
2309.14735v1,cs.CL,Comparative Analysis of Artificial Intelligence for Indian Legal Question Answering (AILQA) Using Different Retrieval and QA Models,2023-09-26 07:56:55+00:00,"Legal question-answering (QA) systems have the potential to revolutionize the
way legal professionals interact with case law documents. This paper conducts a
comparative analysis of existing artificial intelligence models for their
utility in answering legal questions within the Indian legal system,
specifically focusing on Indian Legal Question Answering (AILQA) and our study
investigates the efficacy of different retrieval and QA algorithms currently
available. Utilizing the OpenAI GPT model as a benchmark, along with query
prompts, our investigation shows that existing AILQA systems can automatically
interpret natural language queries from users and generate highly accurate
responses. This research is particularly focused on applications within the
Indian criminal justice domain, which has its own set of challenges due to its
complexity and resource constraints. In order to rigorously assess the
performance of these models, empirical evaluations are complemented by feedback
from practicing legal professionals, thereby offering a multifaceted view on
the capabilities and limitations of AI in the context of Indian legal
question-answering.",http://arxiv.org/pdf/2309.14735v1
2309.14731v1,eess.SY,Exploring the impact of automated vehicles lane-changing behavior on urban network efficiency,2023-09-26 07:47:31+00:00,"While automated vehicle (AV) research has grown steadily in recent years, the
impact of automated lane changing behavior on transportation systems remains a
largely understudied topic. The present work aims to explore the effects of
automated lane changing behavior on urban network efficiency as the penetration
rate of AVs increases. To the best of the authors knowledge, this represents
the first attempt to do so by isolating the effects of the lane changing
behavior; this was obtained by considering AVs with automated lateral control,
yet retaining the same longitudinal control characteristics of conventional
vehicles (CV). An urban road network located in Hannover, Germany, was modeled
with the microsimulation software SUMO, and several scenarios were analyzed,
starting from a baseline with only CVs and then progressively increasing the AV
penetration rate with 10% increments. Results highlight a modest, but
statistically significant, decrease in system performance, with travel times
increasing, and average speed and network capacity decreasing, as penetration
rates increase. This was likely caused by a more prudent behavior of AVs, which
accepted larger gaps than CVs when performing lane changing maneuvers.",http://arxiv.org/pdf/2309.14731v1
2309.14727v1,eess.SY,Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization,2023-09-26 07:38:19+00:00,"In this paper, a novel Multi-agent Reinforcement Learning (MARL) approach,
Multi-Agent Continuous Dynamic Policy Gradient (MACDPP) was proposed to tackle
the issues of limited capability and sample efficiency in various scenarios
controlled by multiple agents. It alleviates the inconsistency of multiple
agents' policy updates by introducing the relative entropy regularization to
the Centralized Training with Decentralized Execution (CTDE) framework with the
Actor-Critic (AC) structure. Evaluated by multi-agent cooperation and
competition tasks and traditional control tasks including OpenAI benchmarks and
robot arm manipulation, MACDPP demonstrates significant superiority in learning
capability and sample efficiency compared with both related multi-agent and
widely implemented signal-agent baselines and therefore expands the potential
of MARL in effectively learning challenging control scenarios.",http://arxiv.org/pdf/2309.14727v1
2309.14726v1,cs.CV,PLMM: Personal Large Models on Mobile Devices,2023-09-26 07:36:20+00:00,"Inspired by Federated Learning, in this paper, we propose personal large
models that are distilled from traditional large language models but more
adaptive to local users' personal information such as education background and
hobbies. We classify the large language models into three levels: the personal
level, expert level and traditional level. The personal level models are
adaptive to users' personal information. They encrypt the users' input and
protect their privacy. The expert level models focus on merging specific
knowledge such as finance, IT and art. The traditional models focus on the
universal knowledge discovery and upgrading the expert models. In such
classifications, the personal models directly interact with the user. For the
whole system, the personal models have users' (encrypted) personal information.
Moreover, such models must be small enough to be performed on personal
computers or mobile devices. Finally, they also have to response in real-time
for better user experience and produce high quality results. The proposed
personal large models can be applied in a wide range of applications such as
language and vision tasks.",http://arxiv.org/pdf/2309.14726v1
2309.14718v1,cs.AI,Optimizing delegation between human and AI collaborative agents,2023-09-26 07:23:26+00:00,"In the context of humans operating with artificial or autonomous agents in a
hybrid team, it is essential to accurately identify when to authorize those
team members to perform actions. Given past examples where humans and
autonomous systems can either succeed or fail at tasks, we seek to train a
delegating manager agent to make delegation decisions with respect to these
potential performance deficiencies. Additionally, we cannot always expect the
various agents to operate within the same underlying model of the environment.
It is possible to encounter cases where the actions and transitions would vary
between agents. Therefore, our framework provides a manager model which learns
through observations of team performance without restricting agents to matching
dynamics. Our results show our manager learns to perform delegation decisions
with teams of agents operating under differing representations of the
environment, significantly outperforming alternative methods to manage the
team.",http://arxiv.org/pdf/2309.14718v1
2309.14691v1,cs.LG,On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks,2023-09-26 06:06:47+00:00,"Artificial neural networks (ANNs) with recurrence and self-attention have
been shown to be Turing-complete (TC). However, existing work has shown that
these ANNs require multiple turns or unbounded computation time, even with
unbounded precision in weights, in order to recognize TC grammars. However,
under constraints such as fixed or bounded precision neurons and time, ANNs
without memory are shown to struggle to recognize even context-free languages.
In this work, we extend the theoretical foundation for the $2^{nd}$-order
recurrent network ($2^{nd}$ RNN) and prove there exists a class of a $2^{nd}$
RNN that is Turing-complete with bounded time. This model is capable of
directly encoding a transition table into its recurrent weights, enabling
bounded time computation and is interpretable by design. We also demonstrate
that $2$nd order RNNs, without memory, under bounded weights and time
constraints, outperform modern-day models such as vanilla RNNs and gated
recurrent units in recognizing regular grammars. We provide an upper bound and
a stability analysis on the maximum number of neurons required by $2$nd order
RNNs to recognize any class of regular grammar. Extensive experiments on the
Tomita grammars support our findings, demonstrating the importance of tensor
connections in crafting computationally efficient RNNs. Finally, we show
$2^{nd}$ order RNNs are also interpretable by extraction and can extract state
machines with higher success rates as compared to first-order RNNs. Our results
extend the theoretical foundations of RNNs and offer promising avenues for
future explainable AI research.",http://arxiv.org/pdf/2309.14691v1
2309.14690v1,cs.CC,On the Tensor Representation and Algebraic Homomorphism of the Neural State Turing Machine,2023-09-26 06:00:12+00:00,"Recurrent neural networks (RNNs) and transformers have been shown to be
Turing-complete, but this result assumes infinite precision in their hidden
representations, positional encodings for transformers, and unbounded
computation time in general. In practical applications, however, it is crucial
to have real-time models that can recognize Turing complete grammars in a
single pass. To address this issue and to better understand the true
computational power of artificial neural networks (ANNs), we introduce a new
class of recurrent models called the neural state Turing machine (NSTM). The
NSTM has bounded weights and finite-precision connections and can simulate any
Turing Machine in real-time. In contrast to prior work that assumes unbounded
time and precision in weights, to demonstrate equivalence with TMs, we prove
that a $13$-neuron bounded tensor RNN, coupled with third-order synapses, can
model any TM class in real-time. Furthermore, under the Markov assumption, we
provide a new theoretical bound for a non-recurrent network augmented with
memory, showing that a tensor feedforward network with $25$th-order finite
precision weights is equivalent to a universal TM.",http://arxiv.org/pdf/2309.14690v1
2309.14681v1,cs.LG,Are Human-generated Demonstrations Necessary for In-context Learning?,2023-09-26 05:10:08+00:00,"Despite the promising few-shot ability of large language models (LLMs), the
standard paradigm of In-context Learning (ICL) suffers the disadvantages of
susceptibility to selected demonstrations and the intricacy to generate these
demonstrations. In this paper, we raise the fundamental question that whether
human-generated demonstrations are necessary for ICL. To answer this question,
we propose self-contemplation prompting strategy (SEC), a paradigm free from
human-crafted demonstrations. The key point of SEC is that, instead of using
hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create
demonstrations on their own, based on which the final output is generated. SEC
is a flexible framework and can be adapted to both the vanilla ICL and the
chain-of-thought (CoT), but with greater ease: as the manual-generation process
of both examples and rationale can be saved. Extensive experiments in
arithmetic reasoning, commonsense reasoning, multi-task language understanding,
and code generation benchmarks, show that SEC, which does not require
hand-crafted demonstrations, significantly outperforms the zero-shot learning
strategy, and achieves comparable results to ICL with hand-crafted
demonstrations. This demonstrates that, for many tasks, contemporary LLMs
possess a sufficient level of competence to exclusively depend on their own
capacity for decision making, removing the need for external training data.
Code is available at https://github.com/ruili33/SEC.",http://arxiv.org/pdf/2309.14681v1
2309.14677v1,cs.CR,XGV-BERT: Leveraging Contextualized Language Model and Graph Neural Network for Efficient Software Vulnerability Detection,2023-09-26 05:05:34+00:00,"With the advancement of deep learning (DL) in various fields, there are many
attempts to reveal software vulnerabilities by data-driven approach.
Nonetheless, such existing works lack the effective representation that can
retain the non-sequential semantic characteristics and contextual relationship
of source code attributes. Hence, in this work, we propose XGV-BERT, a
framework that combines the pre-trained CodeBERT model and Graph Neural Network
(GCN) to detect software vulnerabilities. By jointly training the CodeBERT and
GCN modules within XGV-BERT, the proposed model leverages the advantages of
large-scale pre-training, harnessing vast raw data, and transfer learning by
learning representations for training data through graph convolution. The
research results demonstrate that the XGV-BERT method significantly improves
vulnerability detection accuracy compared to two existing methods such as
VulDeePecker and SySeVR. For the VulDeePecker dataset, XGV-BERT achieves an
impressive F1-score of 97.5%, significantly outperforming VulDeePecker, which
achieved an F1-score of 78.3%. Again, with the SySeVR dataset, XGV-BERT
achieves an F1-score of 95.5%, surpassing the results of SySeVR with an
F1-score of 83.5%.",http://arxiv.org/pdf/2309.14677v1
2309.14674v1,cs.LG,Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST,2023-09-26 05:01:07+00:00,"Outbreaks of hand-foot-and-mouth disease(HFMD) have been associated with
significant morbidity and, in severe cases, mortality. Accurate forecasting of
daily admissions of pediatric HFMD patients is therefore crucial for aiding the
hospital in preparing for potential outbreaks and mitigating nosocomial
transmissions. To address this pressing need, we propose a novel
transformer-based model with a U-net shape, utilizing the patching strategy and
the joint prediction strategy that capitalizes on insights from herpangina, a
disease closely correlated with HFMD. This model also integrates representation
learning by introducing reconstruction loss as an auxiliary loss. The results
show that our U-net Patching Time Series Transformer (UPTST) model outperforms
existing approaches in both long- and short-arm prediction accuracy of HFMD at
hospital-level. Furthermore, the exploratory extension experiments show that
the model's capabilities extend beyond prediction of infectious disease,
suggesting broader applicability in various domains.",http://arxiv.org/pdf/2309.14674v1
2309.14673v1,cs.LG,ALEX: Towards Effective Graph Transfer Learning with Noisy Labels,2023-09-26 04:59:49+00:00,"Graph Neural Networks (GNNs) have garnered considerable interest due to their
exceptional performance in a wide range of graph machine learning tasks.
Nevertheless, the majority of GNN-based approaches have been examined using
well-annotated benchmark datasets, leading to suboptimal performance in
real-world graph learning scenarios. To bridge this gap, the present paper
investigates the problem of graph transfer learning in the presence of label
noise, which transfers knowledge from a noisy source graph to an unlabeled
target graph. We introduce a novel technique termed Balance Alignment and
Information-aware Examination (ALEX) to address this challenge. ALEX first
employs singular value decomposition to generate different views with crucial
structural semantics, which help provide robust node representations using
graph contrastive learning. To mitigate both label shift and domain shift, we
estimate a prior distribution to build subgraphs with balanced label
distributions. Building on this foundation, an adversarial domain discriminator
is incorporated for the implicit domain alignment of complex multi-modal
distributions. Furthermore, we project node representations into a different
space, optimizing the mutual information between the projected features and
labels. Subsequently, the inconsistency of similarity structures is evaluated
to identify noisy samples with potential overfitting. Comprehensive experiments
on various benchmark datasets substantiate the outstanding superiority of the
proposed ALEX in different settings.",http://arxiv.org/pdf/2309.14673v1
2309.14663v1,cs.AI,Learning Emergent Behavior in Robot Swarms with NEAT,2023-09-26 04:40:52+00:00,"When researching robot swarms, many studies observe complex group behavior
emerging from the individual agents' simple local actions. However, the task of
learning an individual policy to produce a desired emergent behavior remains a
challenging and largely unsolved problem. We present a method of training
distributed robotic swarm algorithms to produce emergent behavior. Inspired by
the biological evolution of emergent behavior in animals, we use an
evolutionary algorithm to train a 'population' of individual behaviors to
approximate a desired group behavior. We perform experiments using simulations
of the Georgia Tech Miniature Autonomous Blimps (GT-MABs) aerial robotics
platforms conducted in the CoppeliaSim simulator. Additionally, we test on
simulations of Anki Vector robots to display our algorithm's effectiveness on
various modes of actuation. We evaluate our algorithm on various tasks where a
somewhat complex group behavior is required for success. These tasks include an
Area Coverage task, a Surround Target task, and a Wall Climb task. We compare
behaviors evolved using our algorithm against 'designed policies', which we
create in order to exhibit the emergent behaviors we desire.",http://arxiv.org/pdf/2309.14663v1
2309.14660v1,cs.CV,CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration,2023-09-26 04:32:38+00:00,"Image-to-point cloud (I2P) registration is a fundamental task in the fields
of robot navigation and mobile mapping. Existing I2P registration works
estimate correspondences at the point-to-pixel level, neglecting the global
alignment. However, I2P matching without high-level guidance from global
constraints may converge to the local optimum easily. To solve the problem,
this paper proposes CoFiI2P, a novel I2P registration network that extracts
correspondences in a coarse-to-fine manner for the global optimal solution.
First, the image and point cloud are fed into a Siamese encoder-decoder network
for hierarchical feature extraction. Then, a coarse-to-fine matching module is
designed to exploit features and establish resilient feature correspondences.
Specifically, in the coarse matching block, a novel I2P transformer module is
employed to capture the homogeneous and heterogeneous global information from
image and point cloud. With the discriminate descriptors, coarse
super-point-to-super-pixel matching pairs are estimated. In the fine matching
module, point-to-pixel pairs are established with the
super-point-to-super-pixel correspondence supervision. Finally, based on
matching pairs, the transform matrix is estimated with the EPnP-RANSAC
algorithm. Extensive experiments conducted on the KITTI dataset have
demonstrated that CoFiI2P achieves a relative rotation error (RRE) of 2.25
degrees and a relative translation error (RTE) of 0.61 meters. These results
represent a significant improvement of 14% in RRE and 52% in RTE compared to
the current state-of-the-art (SOTA) method. The demo video for the experiments
is available at https://youtu.be/TG2GBrJTuW4. The source code will be public at
https://github.com/kang-1-2-3/CoFiI2P.",http://arxiv.org/pdf/2309.14660v1
2309.14636v1,cs.IT,Design of Energy-Efficient Artificial Noise for Physical Layer Security in Visible Light Communications,2023-09-26 03:34:04+00:00,"This paper studies the design of energy-efficient artificial noise (AN)
schemes in the context of physical layer security in visible light
communications (VLC). Two different transmission schemes termed
$\textit{selective AN-aided single-input single-output (SISO)}$ and
$\textit{AN-aided multiple-input single-output (MISO)}$ are examined and
compared in terms of secrecy energy efficiency (SEE). In the former, the
closest LED luminaire to the legitimate user (Bob) is the information-bearing
signal's transmitter. At the same time, the rest of the luminaries act as
jammers transmitting AN to degrade the channels of eavesdroppers (Eves). In the
latter, the information-bearing signal and AN are combined and transmitted by
all luminaries. When Eves' CSI is unknown, an indirect design to improve the
SEE is formulated by maximizing Bob's channel's energy efficiency. A
low-complexity design based on the zero-forcing criterion is also proposed. In
the case of known Eves' CSI, we study the design that maximizes the minimum SEE
among those corresponding to all eavesdroppers. At their respective optimal
SEEs, simulation results reveal that when Eves' CSI is unknown, the selective
AN-aided SISO transmission can archive twice better SEE as the AN-aided MISO
does. In contrast, when Eves' CSI is known, the AN-aided MISO outperforms by
30%.",http://arxiv.org/pdf/2309.14636v1
2309.14627v1,quant-ph,A First Principles Derivation of Energy Conserving Momentum Jumps in Surface Hopping Simulations,2023-09-26 02:41:11+00:00,"The fewest switches surface hopping (FSSH) method proposed by Tully in 1990
[J. C Tully, J. Chem. Phys. 93, 1061 (1990)] -- along with its many later
variations -- is basis for most practical simulations of molecular dynamics
with electronic transitions in realistic systems. Despite its popularity, a
rigorous formal derivation of the algorithm has yet to be achieved. In this
paper, we derive the energy conserving momentum jumps characterizing FSSH from
the perspective of quantum trajectory surface hopping (QTSH [C. C. Martens, J.
Phys. Chem. A 123, 1110 (2019)]. In the limit of localized nonadiabatic
transitions, simple mathematical and physical arguments allow the FSSH
algorithm to be derived from first principles. For general processes, the
quantum forces characterizing the QTSH method provides accurate results for
nonadiabatic dynamics with rigorous energy conservation at the ensemble level
within the consistency of the underlying stochastic surface hopping without
resorting to the artificial momentum rescaling of FSSH.",http://arxiv.org/pdf/2309.14627v1
2309.14622v1,cs.CV,Divide and Conquer in Video Anomaly Detection: A Comprehensive Review and New Approach,2023-09-26 02:21:23+00:00,"Video anomaly detection is a complex task, and the principle of ""divide and
conquer"" is often regarded as an effective approach to tackling intricate
issues. It's noteworthy that recent methods in video anomaly detection have
revealed the application of the divide and conquer philosophy (albeit with
distinct perspectives from traditional usage), yielding impressive outcomes.
This paper systematically reviews these literatures from six dimensions, aiming
to enhance the use of the divide and conquer strategy in video anomaly
detection. Furthermore, based on the insights gained from this review, a novel
approach is presented, which integrates human skeletal frameworks with video
data analysis techniques. This method achieves state-of-the-art performance on
the ShanghaiTech dataset, surpassing all existing advanced methods.",http://arxiv.org/pdf/2309.14622v1
2309.14617v1,cs.CY,Towards A Unified Utilitarian Ethics Framework for Healthcare Artificial Intelligence,2023-09-26 02:10:58+00:00,"Artificial Intelligence (AI) aims to elevate healthcare to a pinnacle by
aiding clinical decision support. Overcoming the challenges related to the
design of ethical AI will enable clinicians, physicians, healthcare
professionals, and other stakeholders to use and trust AI in healthcare
settings. This study attempts to identify the major ethical principles
influencing the utility performance of AI at different technological levels
such as data access, algorithms, and systems through a thematic analysis. We
observed that justice, privacy, bias, lack of regulations, risks, and
interpretability are the most important principles to consider for ethical AI.
This data-driven study has analyzed secondary survey data from the Pew Research
Center (2020) of 36 AI experts to categorize the top ethical principles of AI
design. To resolve the ethical issues identified by the meta-analysis and
domain experts, we propose a new utilitarian ethics-based theoretical framework
for designing ethical AI for the healthcare domain.",http://arxiv.org/pdf/2309.14617v1
2309.14610v1,cs.LG,Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas,2023-09-26 01:40:36+00:00,"Urban flood risk emerges from complex and nonlinear interactions among
multiple features related to flood hazard, flood exposure, and social and
physical vulnerabilities, along with the complex spatial flood dependence
relationships. Existing approaches for characterizing urban flood risk,
however, are primarily based on flood plain maps, focusing on a limited number
of features, primarily hazard and exposure features, without consideration of
feature interactions or the dependence relationships among spatial areas. To
address this gap, this study presents an integrated urban flood-risk rating
model based on a novel unsupervised graph deep learning model (called
FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among
areas and complex and nonlinear interactions among flood hazards and urban
features for specifying emergent flood risk. Using data from multiple
metropolitan statistical areas (MSAs) in the United States, the model
characterizes their flood risk into six distinct city-specific levels. The
model is interpretable and enables feature analysis of areas within each
flood-risk level, allowing for the identification of the three archetypes
shaping the highest flood risk within each MSA. Flood risk is found to be
spatially distributed in a hierarchical structure within each MSA, where the
core city disproportionately bears the highest flood risk. Multiple cities are
found to have high overall flood-risk levels and low spatial inequality,
indicating limited options for balancing urban development and flood-risk
reduction. Relevant flood-risk reduction strategies are discussed considering
ways that the highest flood risk and uneven spatial distribution of flood risk
are formed.",http://arxiv.org/pdf/2309.14610v1
2309.14606v1,eess.SP,Toward Energy Efficient Multiuser IRS-Assisted URLLC Systems: A Novel Rank Relaxation Method,2023-09-26 01:23:36+00:00,"This paper proposes an energy efficient resource allocation design algorithm
for an intelligent reflecting surface (IRS)-assisted downlink ultra-reliable
low-latency communication (URLLC) network. This setup features a multi-antenna
base station (BS) transmitting data traffic to a group of URLLC users with
short packet lengths. We maximize the total network's energy efficiency (EE)
through the optimization of active beamformers at the BS and passive
beamformers (a.k.a. phase shifts) at the IRS. The main non-convex problem is
divided into two sub-problems. An alternating optimization (AO) approach is
then used to solve the problem. Through the use of the successive convex
approximation (SCA) with a novel iterative rank relaxation method, we construct
a concave-convex objective function for each sub-problem. The first sub-problem
is a fractional program that is solved using the Dinkelbach method and a
penalty-based approach. The second sub-problem is then solved based on
semi-definite programming (SDP) and the penalty-based approach. The iterative
solution gradually approaches the rank-one for both the active beamforming and
unit modulus IRS phase-shift sub-problems. Our results demonstrate the efficacy
of the proposed solution compared to existing benchmarks.",http://arxiv.org/pdf/2309.14606v1
2309.14592v1,cs.LG,Efficient Post-training Quantization with FP8 Formats,2023-09-26 00:58:36+00:00,"Recent advances in deep learning methods such as LLMs and Diffusion models
have created a need for improved quantization methods that can meet the
computational demands of these modern architectures while maintaining accuracy.
Towards this goal, we study the advantages of FP8 data formats for
post-training quantization across 75 unique network architectures covering a
wide range of tasks, including machine translation, language modeling, text
generation, image classification, generation, and segmentation. We examine
three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects
of varying degrees of trade-off between dynamic range and precision on model
accuracy. Based on our extensive study, we developed a quantization workflow
that generalizes across different network architectures. Our empirical results
show that FP8 formats outperform INT8 in multiple aspects, including workload
coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader
range of operations. Furthermore, our findings suggest that E4M3 is better
suited for NLP models, whereas E3M4 performs marginally better than E4M3 on
computer vision tasks. The code is publicly available on Intel Neural
Compressor: https://github.com/intel/neural-compressor.",http://arxiv.org/pdf/2309.14592v1
2309.14587v1,cs.LG,Joint Communication and Computation Framework for Goal-Oriented Semantic Communication with Distortion Rate Resilience,2023-09-26 00:26:29+00:00,"Recent research efforts on semantic communication have mostly considered
accuracy as a main problem for optimizing goal-oriented communication systems.
However, these approaches introduce a paradox: the accuracy of artificial
intelligence (AI) tasks should naturally emerge through training rather than
being dictated by network constraints. Acknowledging this dilemma, this work
introduces an innovative approach that leverages the rate-distortion theory to
analyze distortions induced by communication and semantic compression, thereby
analyzing the learning process. Specifically, we examine the distribution shift
between the original data and the distorted data, thus assessing its impact on
the AI model's performance. Founding upon this analysis, we can preemptively
estimate the empirical accuracy of AI tasks, making the goal-oriented semantic
communication problem feasible. To achieve this objective, we present the
theoretical foundation of our approach, accompanied by simulations and
experiments that demonstrate its effectiveness. The experimental results
indicate that our proposed method enables accurate AI task performance while
adhering to network constraints, establishing it as a valuable contribution to
the field of signal processing. Furthermore, this work advances research in
goal-oriented semantic communication and highlights the significance of
data-driven approaches in optimizing the performance of intelligent systems.",http://arxiv.org/pdf/2309.14587v1
2309.14586v1,cs.SD,Speech Audio Synthesis from Tagged MRI and Non-Negative Matrix Factorization via Plastic Transformer,2023-09-26 00:21:17+00:00,"The tongue's intricate 3D structure, comprising localized functional units,
plays a crucial role in the production of speech. When measured using tagged
MRI, these functional units exhibit cohesive displacements and derived
quantities that facilitate the complex process of speech production.
Non-negative matrix factorization-based approaches have been shown to estimate
the functional units through motion features, yielding a set of building blocks
and a corresponding weighting map. Investigating the link between weighting
maps and speech acoustics can offer significant insights into the intricate
process of speech production. To this end, in this work, we utilize
two-dimensional spectrograms as a proxy representation, and develop an
end-to-end deep learning framework for translating weighting maps to their
corresponding audio waveforms. Our proposed plastic light transformer (PLT)
framework is based on directional product relative position bias and
single-level spatial pyramid pooling, thus enabling flexible processing of
weighting maps with variable size to fixed-size spectrograms, without input
information loss or dimension expansion. Additionally, our PLT framework
efficiently models the global correlation of wide matrix input. To improve the
realism of our generated spectrograms with relatively limited training samples,
we apply pair-wise utterance consistency with Maximum Mean Discrepancy
constraint and adversarial training. Experimental results on a dataset of 29
subjects speaking two utterances demonstrated that our framework is able to
synthesize speech audio waveforms from weighting maps, outperforming
conventional convolution and transformer models.",http://arxiv.org/pdf/2309.14586v1
2309.14580v1,cs.LG,CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss,2023-09-26 00:03:25+00:00,"This paper considers contrastive training for cross-modal 0-shot transfer
wherein a pre-trained model in one modality is used for representation learning
in another domain using pairwise data. The learnt models in the latter domain
can then be used for a diverse set of tasks in a zero-shot way, similar to
``Contrastive Language-Image Pre-training (CLIP)'' and ``Locked-image Tuning
(LiT)'' that have recently gained considerable attention. Most existing works
for cross-modal representation alignment (including CLIP and LiT) use the
standard contrastive training objective, which employs sets of positive and
negative examples to align similar and repel dissimilar training data samples.
However, similarity amongst training examples has a more continuous nature,
thus calling for a more `non-binary' treatment. To address this, we propose a
novel loss function called Continuously Weighted Contrastive Loss (CWCL) that
employs a continuous measure of similarity. With CWCL, we seek to align the
embedding space of one modality with another. Owing to the continuous nature of
similarity in the proposed loss function, these models outperform existing
methods for 0-shot transfer across multiple models, datasets and modalities.
Particularly, we consider the modality pairs of image-text and speech-text and
our models achieve 5-8% (absolute) improvement over previous state-of-the-art
methods in 0-shot image classification and 20-30% (absolute) improvement in
0-shot speech-to-intent classification and keyword classification.",http://arxiv.org/pdf/2309.14580v1
2309.14566v1,cs.RO,Integrating Higher-Order Dynamics and Roadway-Compliance into Constrained ILQR-based Trajectory Planning for Autonomous Vehicles,2023-09-25 22:30:18+00:00,"This paper addresses the advancements in on-road trajectory planning for
Autonomous Passenger Vehicles (APV). Trajectory planning aims to produce a
globally optimal route for APVs, considering various factors such as vehicle
dynamics, constraints, and detected obstacles. Traditional techniques involve a
combination of sampling methods followed by optimization algorithms, where the
former ensures global awareness and the latter refines for local optima.
Notably, the Constrained Iterative Linear Quadratic Regulator (CILQR)
optimization algorithm has recently emerged, adapted for APV systems,
emphasizing improved safety and comfort. However, existing implementations
utilizing the vehicle bicycle kinematic model may not guarantee controllable
trajectories. We augment this model by incorporating higher-order terms,
including the first and second-order derivatives of curvature and longitudinal
jerk. This inclusion facilitates a richer representation in our cost and
constraint design. We also address roadway compliance, emphasizing adherence to
lane boundaries and directions, which past work often overlooked. Lastly, we
adopt a relaxed logarithmic barrier function to address the CILQR's dependency
on feasible initial trajectories. The proposed methodology is then validated
through simulation and real-world experiment driving scenes in real time.",http://arxiv.org/pdf/2309.14566v1
2309.14564v1,cs.CV,Generative Escher Meshes,2023-09-25 22:24:02+00:00,"This paper proposes a fully-automatic, text-guided generative method for
producing periodic, repeating, tile-able 2D art, such as the one seen on
floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the
standard concept of a seamless texture, i.e., square images that are seamless
when tiled, our method generates non-square tilings which comprise solely of
repeating copies of the same object. It achieves this by optimizing both
geometry and color of a 2D mesh, in order to generate a non-square tile in the
shape and appearance of the desired object, with close to no additional
background details. We enable geometric optimization of tilings by our key
technical contribution: an unconstrained, differentiable parameterization of
the space of all possible tileable shapes for a given symmetry group. Namely,
we prove that modifying the laplacian used in a 2D mesh-mapping technique -
Orbifold Tutte Embedding - can achieve all possible tiling configurations for a
chosen planar symmetry group. We thus consider both the mesh's tile-shape and
its texture as optimizable parameters, rendering the textured mesh via a
differentiable renderer. We leverage a trained image diffusion model to define
a loss on the resulting image, thereby updating the mesh's parameters based on
its appearance matching the text prompt. We show our method is able to produce
plausible, appealing results, with non-trivial tiles, for a variety of
different periodic tiling patterns.",http://arxiv.org/pdf/2309.14564v1
2309.14556v1,cs.CL,Art or Artifice? Large Language Models and the False Promise of Creativity,2023-09-25 22:02:46+00:00,"Researchers have argued that large language models (LLMs) exhibit
high-quality writing capabilities from blogs to stories. However, evaluating
objectively the creativity of a piece of writing is challenging. Inspired by
the Torrance Test of Creative Thinking (TTCT), which measures creativity as a
process, we use the Consensual Assessment Technique [3] and propose the
Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product.
TTCW consists of 14 binary tests organized into the original dimensions of
Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative
writers and implement a human assessment of 48 stories written either by
professional authors or LLMs using TTCW. Our analysis shows that LLM-generated
stories pass 3-10X less TTCW tests than stories written by professionals. In
addition, we explore the use of LLMs as assessors to automate the TTCW
evaluation, revealing that none of the LLMs positively correlate with the
expert assessments.",http://arxiv.org/pdf/2309.14556v1
2309.14552v1,cs.RO,Tactile Estimation of Extrinsic Contact Patch for Stable Placement,2023-09-25 21:51:48+00:00,"Precise perception of contact interactions is essential for the fine-grained
manipulation skills for robots. In this paper, we present the design of
feedback skills for robots that must learn to stack complex-shaped objects on
top of each other. To design such a system, a robot should be able to reason
about the stability of placement from very gentle contact interactions. Our
results demonstrate that it is possible to infer the stability of object
placement based on tactile readings during contact formation between the object
and its environment. In particular, we estimate the contact patch between a
grasped object and its environment using force and tactile observations to
estimate the stability of the object during a contact formation. The contact
patch could be used to estimate the stability of the object upon the release of
the grasp. The proposed method is demonstrated on various pairs of objects that
are used in a very popular board game.",http://arxiv.org/pdf/2309.14552v1
2309.14548v1,cs.AI,Algorithmic Collusion or Competition: the Role of Platforms' Recommender Systems,2023-09-25 21:45:30+00:00,"Recent academic research has extensively examined algorithmic collusion
resulting from the utilization of artificial intelligence (AI)-based dynamic
pricing algorithms. Nevertheless, e-commerce platforms employ recommendation
algorithms to allocate exposure to various products, and this important aspect
has been largely overlooked in previous studies on algorithmic collusion. Our
study bridges this important gap in the literature and examines how
recommendation algorithms can determine the competitive or collusive dynamics
of AI-based pricing algorithms. Specifically, two commonly deployed
recommendation algorithms are examined: (i) a recommender system that aims to
maximize the sellers' total profit (profit-based recommender system) and (ii) a
recommender system that aims to maximize the demand for products sold on the
platform (demand-based recommender system). We construct a repeated game
framework that incorporates both pricing algorithms adopted by sellers and the
platform's recommender system. Subsequently, we conduct experiments to observe
price dynamics and ascertain the final equilibrium. Experimental results reveal
that a profit-based recommender system intensifies algorithmic collusion among
sellers due to its congruence with sellers' profit-maximizing objectives.
Conversely, a demand-based recommender system fosters price competition among
sellers and results in a lower price, owing to its misalignment with sellers'
goals. Extended analyses suggest the robustness of our findings in various
market scenarios. Overall, we highlight the importance of platforms'
recommender systems in delineating the competitive structure of the digital
marketplace, providing important insights for market participants and
corresponding policymakers.",http://arxiv.org/pdf/2309.14548v1
2309.14540v1,cs.LG,Effect of roundabout design on the behavior of road users: A case study of roundabouts with application of Unsupervised Machine Learning,2023-09-25 21:28:52+00:00,"This research aims to evaluate the performance of the rotors and study the
behavior of the human driver in interacting with the rotors. In recent years,
rotors have been increasingly used between countries due to their safety,
capacity, and environmental advantages, and because they provide safe and fluid
flows of vehicles for transit and integration. It turns out that roundabouts
can significantly reduce speed at twisting intersections, entry speed and the
resulting effect on speed depends on the rating of road users. In our research,
(bus, car, truck) drivers were given special attention and their behavior was
categorized into (conservative, normal, aggressive). Anticipating and
recognizing driver behavior is an important challenge. Therefore, the aim of
this research is to study the effect of roundabouts on these classifiers and to
develop a method for predicting the behavior of road users at roundabout
intersections. Safety is primarily due to two inherent features of the rotor.
First, by comparing the data collected and processed in order to classify and
evaluate drivers' behavior, and comparing the speeds of the drivers (bus, car
and truck), the speed of motorists at crossing the roundabout was more fit than
that of buses and trucks. We looked because the car is smaller and all parts of
the rotor are visible to it. So drivers coming from all directions have to slow
down, giving them more time to react and mitigating the consequences in the
event of an accident. Second, with fewer conflicting flows (and points of
conflict), drivers only need to look to their left (in right-hand traffic) for
other vehicles, making their job of crossing the roundabout easier as there is
less need to split attention between different directions.",http://arxiv.org/pdf/2309.14540v1
2309.14523v1,q-bio.NC,Smooth Exact Gradient Descent Learning in Spiking Neural Networks,2023-09-25 20:51:00+00:00,"Artificial neural networks are highly successfully trained with
backpropagation. For spiking neural networks, however, a similar gradient
descent scheme seems prohibitive due to the sudden, disruptive (dis-)appearance
of spikes. Here, we demonstrate exact gradient descent learning based on
spiking dynamics that change only continuously. These are generated by neuron
models whose spikes vanish and appear at the end of a trial, where they do not
influence other neurons anymore. This also enables gradient-based spike
addition and removal. We apply our learning scheme to induce and continuously
move spikes to desired times, in single neurons and recurrent networks.
Further, it achieves competitive performance in a benchmark task using deep,
initially silent networks. Our results show how non-disruptive learning is
possible despite discrete spikes.",http://arxiv.org/pdf/2309.14523v1
2309.14517v1,cs.HC,Watch Your Language: Large Language Models and Content Moderation,2023-09-25 20:23:51+00:00,"Large language models (LLMs) have exploded in popularity due to their ability
to perform a wide array of natural language tasks. Text-based content
moderation is one LLM use case that has received recent enthusiasm, however,
there is little research investigating how LLMs perform in content moderation
settings. In this work, we evaluate a suite of modern, commercial LLMs (GPT-3,
GPT-3.5, GPT-4) on two common content moderation tasks: rule-based community
moderation and toxic content detection. For rule-based community moderation, we
construct 95 LLM moderation-engines prompted with rules from 95 Reddit
subcommunities and find that LLMs can be effective at rule-based moderation for
many communities, achieving a median accuracy of 64% and a median precision of
83%. For toxicity detection, we find that LLMs significantly outperform
existing commercially available toxicity classifiers. However, we also find
that recent increases in model size add only marginal benefit to toxicity
detection, suggesting a potential performance plateau for LLMs on toxicity
detection tasks. We conclude by outlining avenues for future work in studying
LLMs and content moderation.",http://arxiv.org/pdf/2309.14517v1
2309.14514v1,cs.CV,Accurate and Interactive Visual-Inertial Sensor Calibration with Next-Best-View and Next-Best-Trajectory Suggestion,2023-09-25 20:22:16+00:00,"Visual-Inertial (VI) sensors are popular in robotics, self-driving vehicles,
and augmented and virtual reality applications. In order to use them for any
computer vision or state-estimation task, a good calibration is essential.
However, collecting informative calibration data in order to render the
calibration parameters observable is not trivial for a non-expert. In this
work, we introduce a novel VI calibration pipeline that guides a non-expert
with the use of a graphical user interface and information theory in collecting
informative calibration data with Next-Best-View and Next-Best-Trajectory
suggestions to calibrate the intrinsics, extrinsics, and temporal misalignment
of a VI sensor. We show through experiments that our method is faster, more
accurate, and more consistent than state-of-the-art alternatives. Specifically,
we show how calibrations with our proposed method achieve higher accuracy
estimation results when used by state-of-the-art VI Odometry as well as VI-SLAM
approaches. The source code of our software can be found on:
https://github.com/chutsu/yac.",http://arxiv.org/pdf/2309.14514v1
2309.14510v1,cs.HC,"An Empathy-Based Sandbox Approach to Bridge Attitudes, Goals, Knowledge, and Behaviors in the Privacy Paradox",2023-09-25 20:16:54+00:00,"The ""privacy paradox"" describes the discrepancy between users' privacy
attitudes and their actual behaviors. Mitigating this discrepancy requires
solutions that account for both system opaqueness and users' hesitations in
testing different privacy settings due to fears of unintended data exposure. We
introduce an empathy-based approach that allows users to experience how privacy
behaviors may alter system outcomes in a risk-free sandbox environment from the
perspective of artificially generated personas. To generate realistic personas,
we introduce a novel pipeline that augments the outputs of large language
models using few-shot learning, contextualization, and chain of thoughts. Our
empirical studies demonstrated the adequate quality of generated personas and
highlighted the changes in privacy-related applications (e.g., online
advertising) caused by different personas. Furthermore, users demonstrated
cognitive and emotional empathy towards the personas when interacting with our
sandbox. We offered design implications for downstream applications in
improving user privacy literacy and promoting behavior changes.",http://arxiv.org/pdf/2309.14510v1
2309.14499v1,cs.RO,FurNav: Development and Preliminary Study of a Robot Direction Giver,2023-09-25 19:50:22+00:00,"When giving directions to a lost-looking tourist, would you first reference
the street-names, cardinal directions, landmarks, or simply tell them to walk
five hundred metres in one direction then turn left? Depending on the
circumstances, one could reasonably make use of any of these direction giving
styles. However, research on direction giving with a robot does not often look
at how these different direction styles impact perceptions of the robots
intelligence, nor does it take into account how users prior dispositions may
impact ratings. In this work, we look at generating natural language for two
navigation styles using a created system for a Furhat robot, before measuring
perceived intelligence and animacy alongside users prior dispositions to robots
in a small preliminary study (N=7). Our results confirm findings by previous
work that prior negative attitudes towards robots correlates negatively with
propensity to trust robots, and also suggests avenues for future research. For
example, more data is needed to explore the link between perceived intelligence
and direction style. We end by discussing our plan to run a larger scale
experiment, and how to improve our existing study design.",http://arxiv.org/pdf/2309.14499v1
2309.14497v1,cs.AI,Interaction-Aware Decision-Making for Autonomous Vehicles in Forced Merging Scenario Leveraging Social Psychology Factors,2023-09-25 19:49:14+00:00,"Understanding the intention of vehicles in the surrounding traffic is crucial
for an autonomous vehicle to successfully accomplish its driving tasks in
complex traffic scenarios such as highway forced merging. In this paper, we
consider a behavioral model that incorporates both social behaviors and
personal objectives of the interacting drivers. Leveraging this model, we
develop a receding-horizon control-based decision-making strategy, that
estimates online the other drivers' intentions using Bayesian filtering and
incorporates predictions of nearby vehicles' behaviors under uncertain
intentions. The effectiveness of the proposed decision-making strategy is
demonstrated and evaluated based on simulation studies in comparison with a
game theoretic controller and a real-world traffic dataset.",http://arxiv.org/pdf/2309.14497v1
2309.14496v1,cs.LG,Era Splitting,2023-09-25 19:45:45+00:00,"Real life machine learning problems exhibit distributional shifts in the data
from one time to another or from on place to another. This behavior is beyond
the scope of the traditional empirical risk minimization paradigm, which
assumes i.i.d. distribution of data over time and across locations. The
emerging field of out-of-distribution (OOD) generalization addresses this
reality with new theory and algorithms which incorporate environmental, or
era-wise information into the algorithms. So far, most research has been
focused on linear models and/or neural networks. In this research we develop
two new splitting criteria for decision trees, which allow us to apply ideas
from OOD generalization research to decision tree models, including random
forest and gradient-boosting decision trees. The new splitting criteria use
era-wise information associated with each data point to allow tree-based models
to find split points that are optimal across all disjoint eras in the data,
instead of optimal over the entire data set pooled together, which is the
default setting. We describe the new splitting criteria in detail and develop
unique experiments to showcase the benefits of these new criteria, which
improve metrics in our experiments out-of-sample. The new criteria are
incorporated into the a state-of-the-art gradient boosted decision tree model
in the Scikit-Learn code base, which is made freely available.",http://arxiv.org/pdf/2309.14496v1
2309.14488v1,cs.CL,When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs,2023-09-25 19:32:18+00:00,"The use of machine learning (ML) models to assess and score textual data has
become increasingly pervasive in an array of contexts including natural
language processing, information retrieval, search and recommendation, and
credibility assessment of online content. A significant disruption at the
intersection of ML and text are text-generating large-language models such as
generative pre-trained transformers (GPTs). We empirically assess the
differences in how ML-based scoring models trained on human content assess the
quality of content generated by humans versus GPTs. To do so, we propose an
analysis framework that encompasses essay scoring ML-models, human and
ML-generated essays, and a statistical model that parsimoniously considers the
impact of type of respondent, prompt genre, and the ML model used for
assessment model. A rich testbed is utilized that encompasses 18,460
human-generated and GPT-based essays. Results of our benchmark analysis reveal
that transformer pretrained language models (PLMs) more accurately score human
essay quality as compared to CNN/RNN and feature-based ML methods.
Interestingly, we find that the transformer PLMs tend to score GPT-generated
text 10-15\% higher on average, relative to human-authored documents.
Conversely, traditional deep learning and feature-based ML models score human
text considerably higher. Further analysis reveals that although the
transformer PLMs are exclusively fine-tuned on human text, they more
prominently attend to certain tokens appearing only in GPT-generated text,
possibly due to familiarity/overlap in pre-training. Our framework and results
have implications for text classification settings where automated scoring of
text is likely to be disrupted by generative AI.",http://arxiv.org/pdf/2309.14488v1
2309.14478v1,cs.CV,Incorporating Ensemble and Transfer Learning For An End-To-End Auto-Colorized Image Detection Model,2023-09-25 19:22:57+00:00,"Image colorization is the process of colorizing grayscale images or
recoloring an already-color image. This image manipulation can be used for
grayscale satellite, medical and historical images making them more expressive.
With the help of the increasing computation power of deep learning techniques,
the colorization algorithms results are becoming more realistic in such a way
that human eyes cannot differentiate between natural and colorized images.
However, this poses a potential security concern, as forged or illegally
manipulated images can be used illegally. There is a growing need for effective
detection methods to distinguish between natural color and computer-colorized
images. This paper presents a novel approach that combines the advantages of
transfer and ensemble learning approaches to help reduce training time and
resource requirements while proposing a model to classify natural color and
computer-colorized images. The proposed model uses pre-trained branches VGG16
and Resnet50, along with Mobile Net v2 or Efficientnet feature vectors. The
proposed model showed promising results, with accuracy ranging from 94.55% to
99.13% and very low Half Total Error Rate values. The proposed model
outperformed existing state-of-the-art models regarding classification
performance and generalization capabilities.",http://arxiv.org/pdf/2309.14478v1
2309.14474v1,eess.IV,Gastro-Intestinal Tract Segmentation Using an Explainable 3D Unet,2023-09-25 19:16:19+00:00,"In treating gastrointestinal cancer using radiotherapy, the role of the
radiation oncologist is to administer high doses of radiation, through x-ray
beams, toward the tumor while avoiding the stomach and intestines. With the
advent of precise radiation treatment technology such as the MR-Linac,
oncologists can visualize the daily positions of the tumors and intestines,
which may vary day to day. Before delivering radiation, radio oncologists must
manually outline the position of the gastrointestinal organs in order to
determine position and direction of the x-ray beam. This is a time consuming
and labor intensive process that may substantially prolong a patient's
treatment. A deep learning (DL) method can automate and expedite the process.
However, many deep neural networks approaches currently in use are black-boxes
which lack interpretability which render them untrustworthy and impractical in
a healthcare setting. To address this, an emergent field of AI known as
Explainable AI (XAI) may be incorporated to improve the transparency and
viability of a model. This paper proposes a deep learning pipeline that
incorporates XAI to address the challenges of organ segmentation.",http://arxiv.org/pdf/2309.14474v1
2309.14471v1,cs.LG,Adapting Double Q-Learning for Continuous Reinforcement Learning,2023-09-25 19:09:54+00:00,"Majority of off-policy reinforcement learning algorithms use overestimation
bias control techniques. Most of these techniques rooted in heuristics,
primarily addressing the consequences of overestimation rather than its
fundamental origins. In this work we present a novel approach to the bias
correction, similar in spirit to Double Q-Learning. We propose using a policy
in form of a mixture with two components. Each policy component is maximized
and assessed by separate networks, which removes any basis for the
overestimation bias. Our approach shows promising near-SOTA results on a small
set of MuJoCo environments.",http://arxiv.org/pdf/2309.14471v1
2309.14463v1,cs.RO,DefGoalNet: Contextual Goal Learning from Demonstrations For Deformable Object Manipulation,2023-09-25 18:54:32+00:00,"Shape servoing, a robotic task dedicated to controlling objects to desired
goal shapes, is a promising approach to deformable object manipulation. An
issue arises, however, with the reliance on the specification of a goal shape.
This goal has been obtained either by a laborious domain knowledge engineering
process or by manually manipulating the object into the desired shape and
capturing the goal shape at that specific moment, both of which are impractical
in various robotic applications. In this paper, we solve this problem by
developing a novel neural network DefGoalNet, which learns deformable object
goal shapes directly from a small number of human demonstrations. We
demonstrate our method's effectiveness on various robotic tasks, both in
simulation and on a physical robot. Notably, in the surgical retraction task,
even when trained with as few as 10 demonstrations, our method achieves a
median success percentage of nearly 90%. These results mark a substantial
advancement in enabling shape servoing methods to bring deformable object
manipulation closer to practical, real-world applications.",http://arxiv.org/pdf/2309.14463v1
2309.14460v1,eess.AS,Online Active Learning For Sound Event Detection,2023-09-25 18:48:36+00:00,"Data collection and annotation is a laborious, time-consuming prerequisite
for supervised machine learning tasks. Online Active Learning (OAL) is a
paradigm that addresses this issue by simultaneously minimizing the amount of
annotation required to train a classifier and adapting to changes in the data
over the duration of the data collection process. Prior work has indicated that
fluctuating class distributions and data drift are still common problems for
OAL. This work presents new loss functions that address these challenges when
OAL is applied to Sound Event Detection (SED). Experimental results from the
SONYC dataset and two Voice-Type Discrimination (VTD) corpora indicate that OAL
can reduce the time and effort required to train SED classifiers by a factor of
5 for SONYC, and that the new methods presented here successfully resolve
issues present in existing OAL methods.",http://arxiv.org/pdf/2309.14460v1
2309.14425v1,cs.RO,Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery,2023-09-25 18:00:03+00:00,"A general-purpose service robot (GPSR), which can execute diverse tasks in
various environments, requires a system with high generalizability and
adaptability to tasks and environments. In this paper, we first developed a
top-level GPSR system for worldwide competition (RoboCup@Home 2023) based on
multiple foundation models. This system is both generalizable to variations and
adaptive by prompting each model. Then, by analyzing the performance of the
developed system, we found three types of failure in more realistic GPSR
application settings: insufficient information, incorrect plan generation, and
plan execution failure. We then propose the self-recovery prompting pipeline,
which explores the necessary information and modifies its prompts to recover
from failure. We experimentally confirm that the system with the self-recovery
mechanism can accomplish tasks by resolving various failure cases.
Supplementary videos are available at https://sites.google.com/view/srgpsr .",http://arxiv.org/pdf/2309.14425v1
2309.14341v1,cs.RO,Extreme Parkour with Legged Robots,2023-09-25 17:59:55+00:00,"Humans can perform parkour by traversing obstacles in a highly dynamic
fashion requiring precise eye-muscle coordination and movement. Getting robots
to do the same task requires overcoming similar challenges. Classically, this
is done by independently engineering perception, actuation, and control systems
to very low tolerances. This restricts them to tightly controlled settings such
as a predetermined obstacle course in labs. In contrast, humans are able to
learn parkour through practice without significantly changing their underlying
biology. In this paper, we take a similar approach to developing robot parkour
on a small low-cost robot with imprecise actuation and a single front-facing
depth camera for perception which is low-frequency, jittery, and prone to
artifacts. We show how a single neural net policy operating directly from a
camera image, trained in simulation with large-scale RL, can overcome imprecise
sensing and actuation to output highly precise control behavior end-to-end. We
show our robot can perform a high jump on obstacles 2x its height, long jump
across gaps 2x its length, do a handstand and run across tilted ramps, and
generalize to novel obstacle courses with different physical properties.
Parkour videos at https://extreme-parkour.github.io/",http://arxiv.org/pdf/2309.14341v1
2309.14405v1,cs.SD,Joint Audio and Speech Understanding,2023-09-25 17:59:05+00:00,"Humans are surrounded by audio signals that include both speech and
non-speech sounds. The recognition and understanding of speech and non-speech
audio events, along with a profound comprehension of the relationship between
them, constitute fundamental cognitive capabilities. For the first time, we
build a machine learning model, called LTU-AS, that has a conceptually similar
universal audio perception and advanced reasoning ability. Specifically, by
integrating Whisper as a perception module and LLaMA as a reasoning module,
LTU-AS can simultaneously recognize and jointly understand spoken text, speech
paralinguistics, and non-speech audio events - almost everything perceivable
from audio signals.",http://arxiv.org/pdf/2309.14405v1
2309.14335v1,cs.CV,UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation,2023-09-25 17:58:46+00:00,"Human generation has achieved significant progress. Nonetheless, existing
methods still struggle to synthesize specific regions such as faces and hands.
We argue that the main reason is rooted in the training data. A holistic human
dataset inevitably has insufficient and low-resolution information on local
parts. Therefore, we propose to use multi-source datasets with various
resolution images to jointly learn a high-resolution human generative model.
However, multi-source data inherently a) contains different parts that do not
spatially align into a coherent human, and b) comes with different scales. To
tackle these challenges, we propose an end-to-end framework, UnitedHuman, that
empowers continuous GAN with the ability to effectively utilize multi-source
data for high-resolution human generation. Specifically, 1) we design a
Multi-Source Spatial Transformer that spatially aligns multi-source images to
full-body space with a human parametric model. 2) Next, a continuous GAN is
proposed with global-structural guidance and CutMix consistency. Patches from
different datasets are then sampled and transformed to supervise the training
of this scale-invariant generative model. Extensive experiments demonstrate
that our model jointly learned from multi-source data achieves superior quality
than those learned from a holistic dataset.",http://arxiv.org/pdf/2309.14335v1
2309.14404v1,q-bio.QM,pLMFPPred: a novel approach for accurate prediction of functional peptides integrating embedding from pre-trained protein language model and imbalanced learning,2023-09-25 17:57:39+00:00,"Functional peptides have the potential to treat a variety of diseases. Their
good therapeutic efficacy and low toxicity make them ideal therapeutic agents.
Artificial intelligence-based computational strategies can help quickly
identify new functional peptides from collections of protein sequences and
discover their different functions.Using protein language model-based
embeddings (ESM-2), we developed a tool called pLMFPPred (Protein Language
Model-based Functional Peptide Predictor) for predicting functional peptides
and identifying toxic peptides. We also introduced SMOTE-TOMEK data synthesis
sampling and Shapley value-based feature selection techniques to relieve data
imbalance issues and reduce computational costs. On a validated independent
test set, pLMFPPred achieved accuracy, Area under the curve - Receiver
Operating Characteristics, and F1-Score values of 0.974, 0.99, and 0.974,
respectively. Comparative experiments show that pLMFPPred outperforms current
methods for predicting functional peptides.The experimental results suggest
that the proposed method (pLMFPPred) can provide better performance in terms of
Accuracy, Area under the curve - Receiver Operating Characteristics, and
F1-Score than existing methods. pLMFPPred has achieved good performance in
predicting functional peptides and represents a new computational method for
predicting functional peptides.",http://arxiv.org/pdf/2309.14404v1
2309.14331v1,cs.LG,LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference,2023-09-25 17:56:54+00:00,"The growth of Graph Convolution Network (GCN) model sizes has revolutionized
numerous applications, surpassing human performance in areas such as personal
healthcare and financial systems. The deployment of GCNs in the cloud raises
privacy concerns due to potential adversarial attacks on client data. To
address security concerns, Privacy-Preserving Machine Learning (PPML) using
Homomorphic Encryption (HE) secures sensitive client data. However, it
introduces substantial computational overhead in practical applications. To
tackle those challenges, we present LinGCN, a framework designed to reduce
multiplication depth and optimize the performance of HE based GCN inference.
LinGCN is structured around three key elements: (1) A differentiable structural
linearization algorithm, complemented by a parameterized discrete indicator
function, co-trained with model weights to meet the optimization goal. This
strategy promotes fine-grained node-level non-linear location selection,
resulting in a model with minimized multiplication depth. (2) A compact
node-wise polynomial replacement policy with a second-order trainable
activation function, steered towards superior convergence by a two-level
distillation approach from an all-ReLU based teacher model. (3) an enhanced HE
solution that enables finer-grained operator fusion for node-wise activation
functions, further reducing multiplication level consumption in HE-based
inference. Our experiments on the NTU-XVIEW skeleton joint dataset reveal that
LinGCN excels in latency, accuracy, and scalability for homomorphically
encrypted inference, outperforming solutions such as CryptoGCN. Remarkably,
LinGCN achieves a 14.2x latency speedup relative to CryptoGCN, while preserving
an inference accuracy of 75% and notably reducing multiplication depth.",http://arxiv.org/pdf/2309.14331v1
2309.14329v1,cs.HC,Innovative Digital Storytelling with AIGC: Exploration and Discussion of Recent Advances,2023-09-25 17:54:29+00:00,"Digital storytelling, as an art form, has struggled with cost-quality
balance. The emergence of AI-generated Content (AIGC) is considered as a
potential solution for efficient digital storytelling production. However, the
specific form, effects, and impacts of this fusion remain unclear, leaving the
boundaries of AIGC combined with storytelling undefined. This work explores the
current integration state of AIGC and digital storytelling, investigates the
artistic value of their fusion in a sample project, and addresses common issues
through interviews. Through our study, we conclude that AIGC, while proficient
in image creation, voiceover production, and music composition, falls short of
replacing humans due to the irreplaceable elements of human creativity and
aesthetic sensibilities at present, especially in complex character animations,
facial expressions, and sound effects. The research objective is to increase
public awareness of the current state, limitations, and challenges arising from
combining AIGC and digital storytelling.",http://arxiv.org/pdf/2309.14329v1
2309.14316v1,cs.CL,"Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",2023-09-25 17:37:20+00:00,"Large language models can store extensive world knowledge, often extractable
through question-answering (e.g., ""What is Abraham Lincoln's birthday?"").
However, it's unclear whether the model answers questions based on exposure to
exact/similar questions during training, or if it genuinely extracts knowledge
from the source (e.g., Wikipedia biographies).
  In this paper, we conduct an in-depth study of this problem using a
controlled set of semi-synthetic biography data. We uncover a relationship
between the model's knowledge extraction ability and different diversity
measures of the training data. We conduct (nearly) linear probing, revealing a
strong correlation between this relationship and whether the model (nearly)
linearly encodes the knowledge attributes at the hidden embedding of the entity
names, or across the embeddings of other tokens in the training text.",http://arxiv.org/pdf/2309.14316v1
2309.14309v1,cs.CV,Multiple Different Explanations for Image Classifiers,2023-09-25 17:28:28+00:00,"Existing explanation tools for image classifiers usually give only one single
explanation for an image. For many images, however, both humans and image
classifiers accept more than one explanation for the image label. Thus,
restricting the number of explanations to just one severely limits the insight
into the behavior of the classifier. In this paper, we describe an algorithm
and a tool, REX, for computing multiple explanations of the output of a
black-box image classifier for a given image. Our algorithm uses a principled
approach based on causal theory. We analyse its theoretical complexity and
provide experimental results showing that REX finds multiple explanations on 7
times more images than the previous work on the ImageNet-mini benchmark.",http://arxiv.org/pdf/2309.14309v1
2309.14304v1,cs.CV,Overview of Class Activation Maps for Visualization Explainability,2023-09-25 17:20:51+00:00,"Recent research in deep learning methodology has led to a variety of complex
modelling techniques in computer vision (CV) that reach or even outperform
human performance. Although these black-box deep learning models have obtained
astounding results, they are limited in their interpretability and transparency
which are critical to take learning machines to the next step to include them
in sensitive decision-support systems involving human supervision. Hence, the
development of explainable techniques for computer vision (XCV) has recently
attracted increasing attention. In the realm of XCV, Class Activation Maps
(CAMs) have become widely recognized and utilized for enhancing
interpretability and insights into the decision-making process of deep learning
models. This work presents a comprehensive overview of the evolution of Class
Activation Map methods over time. It also explores the metrics used for
evaluating CAMs and introduces auxiliary techniques to improve the saliency of
these methods. The overview concludes by proposing potential avenues for future
research in this evolving field.",http://arxiv.org/pdf/2309.14304v1
2309.14293v1,cs.CV,NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields,2023-09-25 17:04:30+00:00,"Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their prohibitively high computational complexity limits deployability,
especially on resource-constrained platforms. To enable practical usage of
NeRFs, quality tuning is essential to reduce computational complexity, akin to
adjustable graphics settings in video games. However while existing solutions
strive for efficiency, they use one-size-fits-all architectures regardless of
scene complexity, although the same architecture may be unnecessarily large for
simple scenes but insufficient for complex ones. Thus as NeRFs become more
widely used for 3D visualization, there is a need to dynamically optimize the
neural network component of NeRFs to achieve a balance between computational
complexity and specific targets for synthesis quality. Addressing this gap, we
introduce NAS-NeRF: a generative neural architecture search strategy uniquely
tailored to generate NeRF architectures on a per-scene basis by optimizing the
trade-off between complexity and performance, while adhering to constraints on
computational budget and minimum synthesis quality. Our experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3\% average SSIM drop. The source code for our
work is also made publicly available at
https://saeejithnair.github.io/NAS-NeRF.",http://arxiv.org/pdf/2309.14293v1
2309.14288v1,math.AP,Comparison of One- Two- and Three- Dimensional CNN models for Drawing-Test-Based Diagnostics of the Parkinson's Disease,2023-09-25 16:52:57+00:00,"Subject: In this article, convolutional networks of one, two, and three
dimensions are compared with respect to their ability to distinguish between
the drawing tests produced by Parkinson's disease patients and healthy control
subjects.
  Motivation: The application of deep learning techniques for the analysis of
drawing tests to support the diagnosis of Parkinson's disease has become a
growing trend in the area of Artificial Intelligence.
  Method: The dynamic features of the handwriting signal are embedded in the
static test data to generate one-dimensional time series, two-dimensional RGB
images and three-dimensional voxelized point clouds, and then one-, two-, and
three-dimensional CNN can be used to automatically extract features for
effective diagnosis.
  Novelty: While there are many results that describe the application of
two-dimensional convolutional models to the problem, to the best knowledge of
the authors, there are no results based on the application of three-dimensional
models and very few using one-dimensional models.
  Main result: The accuracy of the one-, two- and three-dimensional CNN models
was 62.50%, 77.78% and 83.34% in the DraWritePD dataset (acquired by the
authors) and 73.33%, 80.00% and 86.67% in the PaHaW dataset (well known from
the literature), respectively. For these two data sets, the proposed
three-dimensional convolutional classification method exhibits the best
diagnostic performance.",http://arxiv.org/pdf/2309.14288v1
2309.14280v1,eess.SP,Joint RIS Phase Profile Design and Power Allocation for Parameter Estimation in Presence of Eavesdropping,2023-09-25 16:42:27+00:00,"We consider secure transmission of a deterministic complex-valued parameter
vector from a transmitter to an intended receiver in the presence of an
eavesdropper in a reconfigurable intelligent surface (RIS)-integrated
environment. We aim to jointly optimize the RIS phase profile and the power
allocation matrix at the transmitter to enhance the estimation accuracy at the
intended receiver while limiting that at the eavesdropper. We utilize the trace
of the Fisher information matrix (FIM), equivalently, the average Fisher
information, as the estimation accuracy metric, and obtain its closed form
expression for the intended receiver and the eavesdropper. Accordingly, the
joint RIS phase profile and power allocation problem is formulated, and it is
solved via alternating optimization. When the power allocation matrix is fixed
during alternating optimization, the optimal RIS phase profile design problem
is formulated as a non-convex problem and it is solved via semidefinite
relaxation and rank reduction. When the RIS phase profile is fixed, a linear
programming formulation is obtained for optimal power allocation. Via
simulations, the effects of RIS phase design and power allocation are
illustrated individually and jointly. Moreover, extensions are provided by
considering the presence of line of sight paths in the environment and the
availability of RIS elements with adjustable magnitudes.",http://arxiv.org/pdf/2309.14280v1
2309.14272v1,cs.RO,Perception-and-Energy-aware Motion Planning for UAV using Learning-based Model under Heteroscedastic Uncertainty,2023-09-25 16:34:54+00:00,"Global navigation satellite systems (GNSS) denied environments/conditions
require unmanned aerial vehicles (UAVs) to energy-efficiently and reliably fly.
To this end, this study presents perception-and-energy-aware motion planning
for UAVs in GNSS-denied environments. The proposed planner solves the
trajectory planning problem by optimizing a cost function consisting of two
indices: the total energy consumption of a UAV and the perception quality of
light detection and ranging (LiDAR) sensor mounted on the UAV. Before online
navigation, a high-fidelity simulator acquires a flight dataset to learn energy
consumption for the UAV and heteroscedastic uncertainty associated with LiDAR
measurements, both as functions of the horizontal velocity of the UAV. The
learned models enable the online planner to estimate energy consumption and
perception quality, reducing UAV battery usage and localization errors.
Simulation experiments in a photorealistic environment confirm that the
proposed planner can address the trade-off between energy efficiency and
perception quality under heteroscedastic uncertainty. The open-source code is
released at https://gitlab.com/ReI08/perception-energy-planner.",http://arxiv.org/pdf/2309.14272v1
2309.14269v1,cs.CV,Unsupervised correspondence with combined geometric learning and imaging for radiotherapy applications,2023-09-25 16:29:18+00:00,"The aim of this study was to develop a model to accurately identify
corresponding points between organ segmentations of different patients for
radiotherapy applications. A model for simultaneous correspondence and
interpolation estimation in 3D shapes was trained with head and neck organ
segmentations from planning CT scans. We then extended the original model to
incorporate imaging information using two approaches: 1) extracting features
directly from image patches, and 2) including the mean square error between
patches as part of the loss function. The correspondence and interpolation
performance were evaluated using the geodesic error, chamfer distance and
conformal distortion metrics, as well as distances between anatomical
landmarks. Each of the models produced significantly better correspondences
than the baseline non-rigid registration approach. The original model performed
similarly to the model with direct inclusion of image features. The best
performing model configuration incorporated imaging information as part of the
loss function which produced more anatomically plausible correspondences. We
will use the best performing model to identify corresponding anatomical points
on organs to improve spatial normalisation, an important step in outcome
modelling, or as an initialisation for anatomically informed registrations. All
our code is publicly available at
https://github.com/rrr-uom-projects/Unsup-RT-Corr-Net",http://arxiv.org/pdf/2309.14269v1
2309.14399v1,physics.med-ph,Date-Driven Approach for Identifying State of Hemodialysis Fistulas: Entropy-Complexity and Formal Concept Analysis,2023-09-25 16:15:27+00:00,"The paper explores mathematical methods that differentiate regular and
chaotic time series, specifically for identifying pathological fistulas. It
proposes a noise-resistant method for classifying responding rows of normally
and pathologically functioning fistulas. This approach is grounded in the
hypothesis that laminar blood flow signifies normal function, while turbulent
flow indicates pathology. The study explores two distinct methods for
distinguishing chaotic from regular time series. The first method involves
mapping the time series onto the entropy-complexity plane and subsequently
comparing it to established clusters. The second method, introduced by the
authors, constructs a concepts-objects graph using formal concept analysis.
Both of these methods exhibit high efficiency in determining the state of the
fistula.",http://arxiv.org/pdf/2309.14399v1
2309.14258v1,cs.CL,"OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding",2023-09-25 16:15:09+00:00,"Event understanding aims at understanding the content and relationship of
events within texts, which covers multiple complicated information extraction
tasks: event detection, event argument extraction, and event relation
extraction. To facilitate related research and application, we present an event
understanding toolkit OmniEvent, which features three desiderata: (1)
Comprehensive. OmniEvent supports mainstream modeling paradigms of all the
event understanding tasks and the processing of 15 widely-used English and
Chinese datasets. (2) Fair. OmniEvent carefully handles the inconspicuous
evaluation pitfalls reported in Peng et al. (2023), which ensures fair
comparisons between different models. (3) Easy-to-use. OmniEvent is designed to
be easily used by users with varying needs. We provide off-the-shelf models
that can be directly deployed as web services. The modular framework also
enables users to easily implement and evaluate new event understanding models
with OmniEvent. The toolkit (https://github.com/THU-KEG/OmniEvent) is publicly
released along with the demonstration website and video
(https://omnievent.xlore.cn/).",http://arxiv.org/pdf/2309.14258v1
2309.14256v1,stat.ME,A Weighted Prognostic Covariate Adjustment Method for Efficient and Powerful Treatment Effect Inferences in Randomized Controlled Trials,2023-09-25 16:14:13+00:00,"A crucial task for a randomized controlled trial (RCT) is to specify a
statistical method that can yield an efficient estimator and powerful test for
the treatment effect. A novel and effective strategy to obtain efficient and
powerful treatment effect inferences is to incorporate predictions from
generative artificial intelligence (AI) algorithms into covariate adjustment
for the regression analysis of a RCT. Training a generative AI algorithm on
historical control data enables one to construct a digital twin generator (DTG)
for RCT participants, which utilizes a participant's baseline covariates to
generate a probability distribution for their potential control outcome.
Summaries of the probability distribution from the DTG are highly predictive of
the trial outcome, and adjusting for these features via regression can thus
improve the quality of treatment effect inferences, while satisfying regulatory
guidelines on statistical analyses, for a RCT. However, a critical assumption
in this strategy is homoskedasticity, or constant variance of the outcome
conditional on the covariates. In the case of heteroskedasticity, existing
covariate adjustment methods yield inefficient estimators and underpowered
tests. We propose to address heteroskedasticity via a weighted prognostic
covariate adjustment methodology (Weighted PROCOVA) that adjusts for both the
mean and variance of the regression model using information obtained from the
DTG. We prove that our method yields unbiased treatment effect estimators, and
demonstrate via comprehensive simulation studies and case studies from
Alzheimer's disease that it can reduce the variance of the treatment effect
estimator, maintain the Type I error rate, and increase the power of the test
for the treatment effect from 80% to 85%~90% when the variances from the DTG
can explain 5%~10% of the variation in the RCT participants' outcomes.",http://arxiv.org/pdf/2309.14256v1
2309.14250v1,stat.AP,Prediction Model For Wordle Game Results With High Robustness,2023-09-25 16:10:35+00:00,"In this study, we delve into the dynamics of Wordle using data analysis and
machine learning. Our analysis initially focused on the correlation between the
date and the number of submitted results. Due to initial popularity bias, we
modeled stable data using an ARIMAX model with coefficient values of 9, 0, 2,
and weekdays/weekends as the exogenous variable. We found no significant
relationship between word attributes and hard mode results.
  To predict word difficulty, we employed a Backpropagation Neural Network,
overcoming overfitting via feature engineering. We also used K-means
clustering, optimized at five clusters, to categorize word difficulty
numerically. Our findings indicate that on March 1st, 2023, around 12,884
results will be submitted and the word ""eerie"" averages 4.8 attempts, falling
into the hardest difficulty cluster.
  We further examined the percentage of loyal players and their propensity to
undertake daily challenges. Our models underwent rigorous sensitivity analyses,
including ADF, ACF, PACF tests, and cross-validation, confirming their
robustness. Overall, our study provides a predictive framework for Wordle
gameplay based on date or a given five-letter word. Results have been
summarized and submitted to the Puzzle Editor of the New York Times.",http://arxiv.org/pdf/2309.14250v1
2309.14247v1,cs.NI,Rethinking Internet Communication Through LLMs: How Close Are We?,2023-09-25 16:07:07+00:00,"In this paper, we rethink the way that communication among users over the
Internet, one of the fundamental outcomes of the Internet evolution, takes
place. Instead of users communicating directly over the Internet, we explore an
architecture that enables users to communicate with (query) Large Language
Models (LLMs) that capture the cognition of users on the other end of the
communication channel. We present an architecture to achieve such LLM-based
communication and we perform a reality check to assess how close we are today
to realizing such a communication architecture from a technical point of view.
Finally, we discuss several research challenges and identify interesting
directions for future research.",http://arxiv.org/pdf/2309.14247v1
2309.14243v1,cs.LG,Enhancing data efficiency in reinforcement learning: a novel imagination mechanism based on mesh information propagation,2023-09-25 16:03:08+00:00,"Reinforcement learning (RL) algorithms face the challenge of limited data
efficiency, particularly when dealing with high-dimensional state spaces and
large-scale problems. Most RL methods often rely solely on state transition
information within the same episode when updating the agent's Critic, which can
lead to low data efficiency and sub-optimal training time consumption. Inspired
by human-like analogical reasoning abilities, we introduce a novel mesh
information propagation mechanism, termed the 'Imagination Mechanism (IM)',
designed to significantly enhance the data efficiency of RL algorithms.
Specifically, IM enables information generated by a single sample to be
effectively broadcasted to different states, instead of simply transmitting in
the same episode and it allows the model to better understand the
interdependencies between states and learn scarce sample information more
efficiently. To promote versatility, we extend the imagination mechanism to
function as a plug-and-play module that can be seamlessly and fluidly
integrated into other widely adopted RL models. Our experiments demonstrate
that Imagination mechanism consistently boosts four mainstream SOTA
RL-algorithms, such as SAC, PPO, DDPG, and DQN, by a considerable margin,
ultimately leading to superior performance than before across various tasks.
For access to our code and data, please visit
https://github.com/Zero-coder/FECAM.",http://arxiv.org/pdf/2309.14243v1
2309.14398v1,cs.LG,Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion,2023-09-25 16:00:06+00:00,"Motivational Interviewing (MI) is an approach to therapy that emphasizes
collaboration and encourages behavioral change. To evaluate the quality of an
MI conversation, client utterances can be classified using the MISC code as
either change talk, sustain talk, or follow/neutral talk. The proportion of
change talk in a MI conversation is positively correlated with therapy
outcomes, making accurate classification of client utterances essential. In
this paper, we present a classifier that accurately distinguishes between the
three MISC classes (change talk, sustain talk, and follow/neutral talk)
leveraging multimodal features such as text, prosody, facial expressivity, and
body expressivity. To train our model, we perform annotations on the publicly
available AnnoMI dataset to collect multimodal information, including text,
audio, facial expressivity, and body expressivity. Furthermore, we identify the
most important modalities in the decision-making process, providing valuable
insights into the interplay of different modalities during a MI conversation.",http://arxiv.org/pdf/2309.14398v1
2309.14236v1,cs.RO,MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation,2023-09-25 15:51:29+00:00,"Robotic systems that aspire to operate in uninstrumented real-world
environments must perceive the world directly via onboard sensing. Vision-based
learning systems aim to eliminate the need for environment instrumentation by
building an implicit understanding of the world based on raw pixels, but
navigating the contact-rich high-dimensional search space from solely sparse
visual reward signals significantly exacerbates the challenge of exploration.
The applicability of such systems is thus typically restricted to simulated or
heavily engineered environments since agent exploration in the real-world
without the guidance of explicit state estimation and dense rewards can lead to
unsafe behavior and safety faults that are catastrophic. In this study, we
isolate the root causes behind these limitations to develop a system, called
MoDem-V2, capable of learning contact-rich manipulation directly in the
uninstrumented real world. Building on the latest algorithmic advancements in
model-based reinforcement learning (MBRL), demo-bootstrapping, and effective
exploration, MoDem-V2 can acquire contact-rich dexterous manipulation skills
directly in the real world. We identify key ingredients for leveraging
demonstrations in model learning while respecting real-world safety
considerations -- exploration centering, agency handover, and actor-critic
ensembles. We empirically demonstrate the contribution of these ingredients in
four complex visuo-motor manipulation problems in both simulation and the real
world. To the best of our knowledge, our work presents the first successful
system for demonstration-augmented visual MBRL trained directly in the real
world. Visit https://sites.google.com/view/modem-v2 for videos and more
details.",http://arxiv.org/pdf/2309.14236v1
2309.14235v1,cs.LG,Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving,2023-09-25 15:47:07+00:00,"The deployment of autonomous vehicles (AVs) has faced hurdles due to the
dominance of rare but critical corner cases within the long-tail distribution
of driving scenarios, which negatively affects their overall performance. To
address this challenge, adversarial generation methods have emerged as a class
of efficient approaches to synthesize safety-critical scenarios for AV testing.
However, these generated scenarios are often underutilized for AV training,
resulting in the potential for continual AV policy improvement remaining
untapped, along with a deficiency in the closed-loop design needed to achieve
it. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately
characterize the hierarchical nature of vehicle interaction dynamics,
facilitating iterative improvement by engaging background vehicles (BVs) and AV
in a sequential game-like interaction paradigm. With AV acting as the leader
and BVs as followers, this leader-follower modeling ensures that AV would
consistently refine its policy, always taking into account the additional
information that BVs play the best response to challenge AV. Extensive
experiments have shown that our algorithm exhibits superior performance
compared to several baselines especially in higher dimensional scenarios,
leading to substantial advancements in AV capabilities while continually
generating progressively challenging scenarios.",http://arxiv.org/pdf/2309.14235v1
2309.14231v1,math.OC,Combined sizing and layout optimization of truss structures via update Monte Carlo tree search (UMCTS) algorithm,2023-09-25 15:42:52+00:00,"The main concern of this study is to find the optimal design of truss
structures considering sizing and layout variables simultaneously. As compared
to purely sizing optimization problems, this problem is more challenging since
the two types of variables involved are fundamentally different in nature. In
this paper, a reinforcement learning method combining the update process and
Monte Carlo tree search called the update Monte Carlo tree search (UMCTS) for
sizing optimization problems is applied to solve combined sizing and layout
optimization for truss structures. This study proposes a novel update process
for nodal coordinates with two features. (1) The allowed range of each
coordinate varies in each round. (2) Accelerators for the number of entries in
the allowed range and iteration numbers are introduced to reduce the
computation time. Furthermore, nodal coordinates and member areas are
determined at the same time with only one search tree in each round. The
validation and efficiency of the UMCTS are tested on benchmark problems of
planar and spatial trusses with discrete sizing variables and continuous layout
variables. It is shown that the CPU time of the UMCTS is two times faster than
the branch and bound method. The numerical results demonstrate that the
proposed method stably achieves a better solution than other traditional
methods.",http://arxiv.org/pdf/2309.14231v1
2309.14395v1,cs.LG,Implicit Sensing in Traffic Optimization: Advanced Deep Reinforcement Learning Techniques,2023-09-25 15:33:08+00:00,"A sudden roadblock on highways due to many reasons such as road maintenance,
accidents, and car repair is a common situation we encounter almost daily.
Autonomous Vehicles (AVs) equipped with sensors that can acquire vehicle
dynamics such as speed, acceleration, and location can make intelligent
decisions to change lanes before reaching a roadblock. A number of literature
studies have examined car-following models and lane-changing models. However,
only a few studies proposed an integrated car-following and lane-changing
model, which has the potential to model practical driving maneuvers. Hence, in
this paper, we present an integrated car-following and lane-changing
decision-control system based on Deep Reinforcement Learning (DRL) to address
this issue. Specifically, we consider a scenario where sudden construction work
will be carried out along a highway. We model the scenario as a Markov Decision
Process (MDP) and employ the well-known DQN algorithm to train the RL agent to
make the appropriate decision accordingly (i.e., either stay in the same lane
or change lanes). To overcome the delay and computational requirement of DRL
algorithms, we adopt an MEC-assisted architecture where the RL agents are
trained on MEC servers. We utilize the highly reputable SUMO simulator and
OPENAI GYM to evaluate the performance of the proposed model under two
policies; {\epsilon}-greedy policy and Boltzmann policy. The results
unequivocally demonstrate that the DQN agent trained using the
{\epsilon}-greedy policy significantly outperforms the one trained with the
Boltzmann policy.",http://arxiv.org/pdf/2309.14395v1
2309.14394v1,cs.CL,Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation,2023-09-25 15:31:16+00:00,"Domain-to-domain translation involves generating a target domain sample given
a condition in the source domain. Most existing methods focus on fixed input
and output domains, i.e. they only work for specific configurations (i.e. for
two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper
proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for
multi-domain translation in a semi-supervised context. Unlike previous methods,
MDD does not require defining input and output domains, allowing translation
between any partition of domains within a set (such as $(D_1,
D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$,
etc. for 3 domains), without the need to train separate models for each domain
configuration. The key idea behind MDD is to leverage the noise formulation of
diffusion models by incorporating one noise level per domain, which allows
missing domains to be modeled with noise in a natural way. This transforms the
training task from a simple reconstruction task to a domain translation task,
where the model relies on less noisy domains to reconstruct more noisy domains.
We present results on a multi-domain (with more than two domains) synthetic
image translation dataset with challenging semantic domain inversion.",http://arxiv.org/pdf/2309.14394v1
2309.14221v1,cs.LG,Accelerating Machine Learning Algorithms with Adaptive Sampling,2023-09-25 15:25:59+00:00,"The era of huge data necessitates highly efficient machine learning
algorithms. Many common machine learning algorithms, however, rely on
computationally intensive subroutines that are prohibitively expensive on large
datasets. Oftentimes, existing techniques subsample the data or use other
methods to improve computational efficiency, at the expense of incurring some
approximation error. This thesis demonstrates that it is often sufficient,
instead, to substitute computationally intensive subroutines with a special
kind of randomized counterparts that results in almost no degradation in
quality.",http://arxiv.org/pdf/2309.14221v1
2309.14216v1,cs.LG,MemDA: Forecasting Urban Time Series with Memory-based Drift Adaptation,2023-09-25 15:22:28+00:00,"Urban time series data forecasting featuring significant contributions to
sustainable development is widely studied as an essential task of the smart
city. However, with the dramatic and rapid changes in the world environment,
the assumption that data obey Independent Identically Distribution is
undermined by the subsequent changes in data distribution, known as concept
drift, leading to weak replicability and transferability of the model over
unseen data. To address the issue, previous approaches typically retrain the
model, forcing it to fit the most recent observed data. However, retraining is
problematic in that it leads to model lag, consumption of resources, and model
re-invalidation, causing the drift problem to be not well solved in realistic
scenarios. In this study, we propose a new urban time series prediction model
for the concept drift problem, which encodes the drift by considering the
periodicity in the data and makes on-the-fly adjustments to the model based on
the drift using a meta-dynamic network. Experiments on real-world datasets show
that our design significantly outperforms state-of-the-art methods and can be
well generalized to existing prediction backbones by reducing their sensitivity
to distribution changes.",http://arxiv.org/pdf/2309.14216v1
2309.14213v1,cs.CE,"Autonomous Vehicles an overview on system, cyber security, risks, issues, and a way forward",2023-09-25 15:19:09+00:00,"This chapter explores the complex realm of autonomous cars, analyzing their
fundamental components and operational characteristics. The initial phase of
the discussion is elucidating the internal mechanics of these automobiles,
encompassing the crucial involvement of sensors, artificial intelligence (AI)
identification systems, control mechanisms, and their integration with
cloud-based servers within the framework of the Internet of Things (IoT). It
delves into practical implementations of autonomous cars, emphasizing their
utilization in forecasting traffic patterns and transforming the dynamics of
transportation. The text also explores the topic of Robotic Process Automation
(RPA), illustrating the impact of autonomous cars on different businesses
through the automation of tasks. The primary focus of this investigation lies
in the realm of cybersecurity, specifically in the context of autonomous
vehicles. A comprehensive analysis will be conducted to explore various risk
management solutions aimed at protecting these vehicles from potential threats
including ethical, environmental, legal, professional, and social dimensions,
offering a comprehensive perspective on their societal implications. A
strategic plan for addressing the challenges and proposing strategies for
effectively traversing the complex terrain of autonomous car systems,
cybersecurity, hazards, and other concerns are some resources for acquiring an
understanding of the intricate realm of autonomous cars and their ramifications
in contemporary society, supported by a comprehensive compilation of resources
for additional investigation.
  Keywords: RPA, Cyber Security, AV, Risk, Smart Cars",http://arxiv.org/pdf/2309.14213v1
2309.14209v1,cs.LG,Continual Driving Policy Optimization with Closed-Loop Individualized Curricula,2023-09-25 15:14:54+00:00,"The safety of autonomous vehicles (AV) has been a long-standing top concern,
stemming from the absence of rare and safety-critical scenarios in the
long-tail naturalistic driving distribution. To tackle this challenge, a surge
of research in scenario-based autonomous driving has emerged, with a focus on
generating high-risk driving scenarios and applying them to conduct
safety-critical testing of AV models. However, limited work has been explored
on the reuse of these extensive scenarios to iteratively improve AV models.
Moreover, it remains intractable and challenging to filter through gigantic
scenario libraries collected from other AV models with distinct behaviors,
attempting to extract transferable information for current AV improvement.
Therefore, we develop a continual driving policy optimization framework
featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into
a set of standardized sub-modules for flexible implementation choices: AV
Evaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a
collision prediction task, where it estimates the chance of AV failures in
these scenarios at each iteration. Subsequently, by re-sampling from historical
scenarios based on these failure probabilities, CLIC tailors individualized
curricula for downstream training, aligning them with the evaluated capability
of AV. Accordingly, CLIC not only maximizes the utilization of the vast
pre-collected scenario library for closed-loop driving policy optimization but
also facilitates AV improvement by individualizing its training with more
challenging cases out of those poorly organized scenarios. Experimental results
clearly indicate that CLIC surpasses other curriculum-based training
strategies, showing substantial improvement in managing risky scenarios, while
still maintaining proficiency in handling simpler cases.",http://arxiv.org/pdf/2309.14209v1
2309.14208v1,cs.CY,Framework based on complex networks to model and mine patient pathways,2023-09-25 15:11:52+00:00,"The automatic discovery of a model to represent the history of encounters of
a group of patients with the healthcare system -- the so-called ``pathway of
patients'' -- is a new field of research that supports clinical and
organisational decisions to improve the quality and efficiency of the treatment
provided. The pathways of patients with chronic conditions tend to vary
significantly from one person to another, have repetitive tasks, and demand the
analysis of multiple perspectives (interventions, diagnoses, medical
specialities, among others) influencing the results. Therefore, modelling and
mining those pathways is still a challenging task. In this work, we propose a
framework comprising: (i) a pathway model based on a multi-aspect graph, (ii) a
novel dissimilarity measurement to compare pathways taking the elapsed time
into account, and (iii) a mining method based on traditional centrality
measures to discover the most relevant steps of the pathways. We evaluated the
framework using the study cases of pregnancy and diabetes, which revealed its
usefulness in finding clusters of similar pathways, representing them in an
easy-to-interpret way, and highlighting the most significant patterns according
to multiple perspectives.",http://arxiv.org/pdf/2309.14208v1
2309.14393v1,cs.CL,LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models,2023-09-25 14:50:04+00:00,"The carbon footprint associated with large language models (LLMs) is a
significant concern, encompassing emissions from their training, inference,
experimentation, and storage processes, including operational and embodied
carbon emissions. An essential aspect is accurately estimating the carbon
impact of emerging LLMs even before their training, which heavily relies on GPU
usage. Existing studies have reported the carbon footprint of LLM training, but
only one tool, mlco2, can predict the carbon footprint of new neural networks
prior to physical training. However, mlco2 has several serious limitations. It
cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs,
disregards critical architectural parameters, focuses solely on GPUs, and
cannot model embodied carbon footprints. Addressing these gaps, we introduce
\textit{LLMCarbon}, an end-to-end carbon footprint projection model designed
for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly
enhances the accuracy of carbon footprint estimations for various LLMs.",http://arxiv.org/pdf/2309.14393v1
2309.14183v2,cs.CV,Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition,2023-09-25 14:46:01+00:00,"The development of foundation vision models has pushed the general visual
recognition to a high level, but cannot well address the fine-grained
recognition in specialized domain such as invasive species classification.
Identifying and managing invasive species has strong social and ecological
value. Currently, most invasive species datasets are limited in scale and cover
a narrow range of species, which restricts the development of deep-learning
based invasion biometrics systems. To fill the gap of this area, we introduced
Species196, a large-scale semi-supervised dataset of 196-category invasive
species. It collects over 19K images with expert-level accurate annotations
Species196-L, and 1.2M unlabeled images of invasive species Species196-U. The
dataset provides four experimental settings for benchmarking the existing
models and algorithms, namely, supervised learning, semi-supervised learning,
self-supervised pretraining and zero-shot inference ability of large
multi-modal models. To facilitate future research on these four learning
paradigms, we conduct an empirical study of the representative methods on the
introduced dataset. The dataset is publicly available at
https://species-dataset.github.io/.",http://arxiv.org/pdf/2309.14183v2
2309.14181v1,cs.CV,Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision,2023-09-25 14:43:43+00:00,"The rapid evolution of Multi-modality Large Language Models (MLLMs) has
catalyzed a shift in computer vision from specialized models to general-purpose
foundation models. Nevertheless, there is still an inadequacy in assessing the
abilities of MLLMs on low-level visual perception and understanding. To address
this gap, we present Q-Bench, a holistic benchmark crafted to systematically
evaluate potential abilities of MLLMs on three realms: low-level visual
perception, low-level visual description, and overall visual quality
assessment. a) To evaluate the low-level perception ability, we construct the
LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped
with a human-asked question focusing on its low-level attributes. We then
measure the correctness of MLLMs on answering these questions. b) To examine
the description ability of MLLMs on low-level information, we propose the
LLDescribe dataset consisting of long expert-labelled golden low-level text
descriptions on 499 images, and a GPT-involved comparison pipeline between
outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we
further measure their visual quality assessment ability to align with human
opinion scores. Specifically, we design a softmax-based strategy that enables
MLLMs to predict quantifiable quality scores, and evaluate them on various
existing image quality assessment (IQA) datasets. Our evaluation across the
three abilities confirms that MLLMs possess fundamental low-level visual
skills. However, these skills are still unstable and relatively imprecise,
indicating the need for specific enhancements on MLLMs towards these abilities.
We hope that our benchmark can encourage the research community to delve deeper
to discover and enhance these untapped potentials of MLLMs.",http://arxiv.org/pdf/2309.14181v1
2309.14177v1,physics.optics,\textit{Anoplophora graafi} Longhorn Beetle Coloration is due to Disordered Diamond-like Packed Spheres,2023-09-25 14:40:51+00:00,"While artificially photonic materials are typically highly ordered, photonic
structures in many species of birds and insects do not possess a long-range
order. Studying their order-disorder interplay sheds light on the origin of the
photonic band gap. Here, we investigated the scale morphology of the
\textit{Anoplophora graafi} longhorn beetle. Combining small-angle X-ray
scattering and slice-and-view FIB-SEM tomography with molecular dynamics and
optical simulations, we characterised the chitin sphere assemblies within blue
and green \textit{A.\ graafi} scales. The low volume fraction of spheres and
the number of their nearest neighbours are incompatible with any known
close-packed sphere morphology. A short-range diamond lattice with long-range
disorder best describes the sphere assembly, which will inspire the development
of new colloid-based photonic materials.",http://arxiv.org/pdf/2309.14177v1
2309.14162v1,cs.CV,Data Upcycling Knowledge Distillation for Image Super-Resolution,2023-09-25 14:13:26+00:00,"Knowledge distillation (KD) emerges as a challenging yet promising technique
for compressing deep learning models, characterized by the transmission of
extensive learning representations from proficient and computationally
intensive teacher models to compact student models. However, only a handful of
studies have endeavored to compress the models for single image
super-resolution (SISR) through KD, with their effects on student model
enhancement remaining marginal. In this paper, we put forth an approach from
the perspective of efficient data utilization, namely, the Data Upcycling
Knowledge Distillation (DUKD) which facilitates the student model by the prior
knowledge teacher provided via upcycled in-domain data derived from their
inputs. This upcycling process is realized through two efficient image zooming
operations and invertible data augmentations which introduce the label
consistency regularization to the field of KD for SISR and substantially boosts
student model's generalization. The DUKD, due to its versatility, can be
applied across a broad spectrum of teacher-student architectures. Comprehensive
experiments across diverse benchmarks demonstrate that our proposed DUKD method
significantly outperforms previous art, exemplified by an increase of up to
0.5dB in PSNR over baselines methods, and a 67% parameters reduced RCAN model's
performance remaining on par with that of the RCAN teacher model.",http://arxiv.org/pdf/2309.14162v1
2309.14153v1,cs.ET,An optimized quantum minimum searching algorithm with sure-success probability and its experiment simulation with Cirq,2023-09-25 14:07:27+00:00,"Finding a minimum is an essential part of mathematical models, and it plays
an important role in some optimization problems. Durr and Hoyer proposed a
quantum searching algorithm (DHA), with a certain probability of success, to
achieve quadratic speed than classical ones. In this paper, we propose an
optimized quantum minimum searching algorithm with sure-success probability,
which utilizes Grover-Long searching to implement the optimal exact searching,
and the dynamic strategy to reduce the iterations of our algorithm. Besides, we
optimize the oracle circuit to reduce the number of gates by the simplified
rules. The performance evaluation including the theoretical success rate and
computational complexity shows that our algorithm has higher accuracy and
efficiency than DHA algorithm. Finally, a simulation experiment based on Cirq
is performed to verify its feasibility.",http://arxiv.org/pdf/2309.14153v1
2309.14148v1,cs.DC,SPIRT: A Fault-Tolerant and Reliable Peer-to-Peer Serverless ML Training Architecture,2023-09-25 14:01:35+00:00,"The advent of serverless computing has ushered in notable advancements in
distributed machine learning, particularly within parameter server-based
architectures. Yet, the integration of serverless features within peer-to-peer
(P2P) distributed networks remains largely uncharted. In this paper, we
introduce SPIRT, a fault-tolerant, reliable, and secure serverless P2P ML
training architecture. designed to bridge this existing gap.
  Capitalizing on the inherent robustness and reliability innate to P2P
systems, SPIRT employs RedisAI for in-database operations, leading to an 82\%
reduction in the time required for model updates and gradient averaging across
a variety of models and batch sizes. This architecture showcases resilience
against peer failures and adeptly manages the integration of new peers, thereby
highlighting its fault-tolerant characteristics and scalability. Furthermore,
SPIRT ensures secure communication between peers, enhancing the reliability of
distributed machine learning tasks. Even in the face of Byzantine attacks, the
system's robust aggregation algorithms maintain high levels of accuracy. These
findings illuminate the promising potential of serverless architectures in P2P
distributed machine learning, offering a significant stride towards the
development of more efficient, scalable, and resilient applications.",http://arxiv.org/pdf/2309.14148v1
2309.14139v1,cs.DC,Exploring the Impact of Serverless Computing on Peer To Peer Training Machine Learning,2023-09-25 13:51:07+00:00,"The increasing demand for computational power in big data and machine
learning has driven the development of distributed training methodologies.
Among these, peer-to-peer (P2P) networks provide advantages such as enhanced
scalability and fault tolerance. However, they also encounter challenges
related to resource consumption, costs, and communication overhead as the
number of participating peers grows. In this paper, we introduce a novel
architecture that combines serverless computing with P2P networks for
distributed training and present a method for efficient parallel gradient
computation under resource constraints.
  Our findings show a significant enhancement in gradient computation time,
with up to a 97.34\% improvement compared to conventional P2P distributed
training methods. As for costs, our examination confirmed that the serverless
architecture could incur higher expenses, reaching up to 5.4 times more than
instance-based architectures. It is essential to consider that these higher
costs are associated with marked improvements in computation time, particularly
under resource-constrained scenarios. Despite the cost-time trade-off, the
serverless approach still holds promise due to its pay-as-you-go model.
Utilizing dynamic resource allocation, it enables faster training times and
optimized resource utilization, making it a promising candidate for a wide
range of machine learning applications.",http://arxiv.org/pdf/2309.14139v1
2309.14134v1,cs.LG,One-Class Classification for Intrusion Detection on Vehicular Networks,2023-09-25 13:42:22+00:00,"Controller Area Network bus systems within vehicular networks are not
equipped with the tools necessary to ward off and protect themselves from
modern cyber-security threats. Work has been done on using machine learning
methods to detect and report these attacks, but common methods are not robust
towards unknown attacks. These methods usually rely on there being a sufficient
representation of attack data, which may not be available due to there either
not being enough data present to adequately represent its distribution or the
distribution itself is too diverse in nature for there to be a sufficient
representation of it. With the use of one-class classification methods, this
issue can be mitigated as only normal data is required to train a model for the
detection of anomalous instances. Research has been done on the efficacy of
these methods, most notably One-Class Support Vector Machine and Support Vector
Data Description, but many new extensions of these works have been proposed and
have yet to be tested for injection attacks in vehicular networks. In this
paper, we investigate the performance of various state-of-the-art one-class
classification methods for detecting injection attacks on Controller Area
Network bus traffic. We investigate the effectiveness of these techniques on
attacks launched on Controller Area Network buses from two different vehicles
during normal operation and while being attacked. We observe that the Subspace
Support Vector Data Description method outperformed all other tested methods
with a Gmean of about 85%.",http://arxiv.org/pdf/2309.14134v1
2309.14123v1,eess.SY,Harnessing Supervised Learning for Adaptive Beamforming in Multibeam Satellite Systems,2023-09-25 13:23:22+00:00,"In today's ever-connected world, the demand for fast and widespread
connectivity is insatiable, making multibeam satellite systems an indispensable
pillar of modern telecommunications infrastructure. However, the evolving
communication landscape necessitates a high degree of adaptability. This
adaptability is particularly crucial for beamforming, as it enables the
adjustment of peak throughput and beamwidth to meet fluctuating traffic demands
by varying the beamwidth, side lobe level (SLL), and effective isotropic
radiated power (EIRP). This paper introduces an innovative approach rooted in
supervised learning to efficiently derive the requisite beamforming matrix,
aligning it with system requirements. Significantly reducing computation time,
this method is uniquely tailored for real-time adaptation, enhancing the
agility and responsiveness of satellite multibeam systems. Exploiting the power
of supervised learning, this research enables multibeam satellites to respond
quickly and intelligently to changing communication needs, ultimately ensuring
uninterrupted and optimized connectivity in a dynamic world.",http://arxiv.org/pdf/2309.14123v1
2309.14117v1,cs.CV,Small Objects Matters in Weakly-supervised Semantic Segmentation,2023-09-25 13:15:57+00:00,"Weakly-supervised semantic segmentation (WSSS) performs pixel-wise
classification given only image-level labels for training. Despite the
difficulty of this task, the research community has achieved promising results
over the last five years. Still, current WSSS literature misses the detailed
sense of how well the methods perform on different sizes of objects. Thus we
propose a novel evaluation metric to provide a comprehensive assessment across
different object sizes and collect a size-balanced evaluation set to complement
PASCAL VOC. With these two gadgets, we reveal that the existing WSSS methods
struggle in capturing small objects. Furthermore, we propose a size-balanced
cross-entropy loss coupled with a proper training strategy. It generally
improves existing WSSS methods as validated upon ten baselines on three
different datasets.",http://arxiv.org/pdf/2309.14117v1
2309.14112v1,cs.AI,Semi-Abstract Value-Based Argumentation Framework,2023-09-25 13:10:56+00:00,"In his seminal paper, Phan Minh Dung (1995) proposed abstract argumentation
framework, which models argumentation using directed graphs where structureless
arguments are the nodes and attacks among the arguments are the edges. In the
following years, many extensions of this framework were introduced. These
extensions typically add a certain form of structure to the arguments. This
thesis showcases two such extensions -- value-based argumentation framework by
Trevor Bench-Capon (2002) and semi-abstract argumentation framework by Esther
Anna Corsi and Christian Ferm\""uller (2017). The former introduces a mapping
function that links individual arguments to a set of ordered values, enabling a
distinction between objectively and subjectively acceptable arguments. The
latter links claims of individual arguments to propositional formulae and then
applies newly-introduced attack principles in order to make implicit attacks
explicit and to enable a definition of a consequence relation that relies on
neither the truth values nor the interpretations in the usual sense.
  The contribution of this thesis is two-fold. Firstly, the new semi-abstract
value-based argumentation framework is introduced. This framework maps
propositional formulae associated with individual arguments to a set of ordered
values. Secondly, a complex moral dilemma is formulated using the original and
the value-based argumentation frameworks showcasing the expressivity of these
formalisms.",http://arxiv.org/pdf/2309.14112v1
2309.14092v1,cs.DB,From OCEL to DOCEL -- Datasets and Automated Transformation,2023-09-25 12:31:50+00:00,"Object-centric event data represent processes from the point of view of all
the involved object types. This perspective has gained interest in recent years
as it supports the analysis of processes that previously could not be
adequately captured, due to the lack of a clear case notion as well as an
increasing amount of output data that needs to be stored. Although publicly
available event logs are crucial artifacts for researchers to develop and
evaluate novel process mining techniques, the currently available
object-centric event logs have limitations in this regard. Specifically, they
mainly focus on control-flow and rarely contain objects with attributes that
change over time, even though this is not realistic, as the attribute values of
objects can be altered during their lifecycle. This paper addresses this gap by
providing two means of establishing object-centric datasets with dynamically
evolving attributes. First, we provide event log generators, which allow
researchers to generate customized, artificial logs with dynamic attributes in
the recently proposed DOCEL format. Second, we propose and evaluate an
algorithm to convert OCEL logs into DOCEL logs, which involves the detection of
event attributes that capture evolving object information and the creation of
dynamic attributes from these. Through these contributions, this paper supports
the advancement of object-centric process analysis by providing researchers
with new means to obtain relevant data to use during the development of new
techniques.",http://arxiv.org/pdf/2309.14092v1
2309.14087v1,eess.SP,Adaptive Three Layer Hybrid Reconfigurable Intelligent Surface for 6G Wireless Communication: Trade-offs and Performance,2023-09-25 12:30:03+00:00,"A potential candidate technology for the development of future 6G networks
has been recognized as Reconfigurable Intelligent Surface (RIS). However, due
to the variation in radio link quality, traditional passive RISs only
accomplish a minimal signal gain in situations with strong direct links between
user equipment (UE) and base station (BS). In order to get over this
fundamental restriction of smaller gain, the idea of active RISs might be a
suitable solution. In contrast to current passive RIS, which simply reflects
and directs signals without any additional amplification, active RISs have the
ability to enhance reflected signals by the incorporation of amplifiers inside
its elements. However, with additional amplifiers, apart from the relatively
complex attributes of RIS-assisted arrangements, the additional energy
consumption of such technologies is often disregarded. So, there might be a
tradeoff between the additional energy consumption for the RIS technologies and
the overall gain acquired by deploying this potential advancement. The
objective of this work is to provide a primary idea of a three-layer hybrid
RIS-assisted configuration that is responsive to both active and passive RIS,
as well as an additional dormant or inactive state. The single RIS structure
should be capable of adjusting its overall configuration in response to
fluctuations in transmit power and radio link quality. Furthermore, our
fabricated passive RIS-assisted structure verifies a portion of the proposed
idea, with simulations highlighting its advantages over standalone passive or
active RIS-assisted technologies.",http://arxiv.org/pdf/2309.14087v1
2309.14084v1,cs.CL,"Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges",2023-09-25 12:23:37+00:00,"In the domain of Natural Language Processing (NLP), Named Entity Recognition
(NER) stands out as a pivotal mechanism for extracting structured insights from
unstructured text. This manuscript offers an exhaustive exploration into the
evolving landscape of NER methodologies, blending foundational principles with
contemporary AI advancements. Beginning with the rudimentary concepts of NER,
the study spans a spectrum of techniques from traditional rule-based strategies
to the contemporary marvels of transformer architectures, particularly
highlighting integrations such as BERT with LSTM and CNN. The narrative
accentuates domain-specific NER models, tailored for intricate areas like
finance, legal, and healthcare, emphasizing their specialized adaptability.
Additionally, the research delves into cutting-edge paradigms including
reinforcement learning, innovative constructs like E-NER, and the interplay of
Optical Character Recognition (OCR) in augmenting NER capabilities. Grounding
its insights in practical realms, the paper sheds light on the indispensable
role of NER in sectors like finance and biomedicine, addressing the unique
challenges they present. The conclusion outlines open challenges and avenues,
marking this work as a comprehensive guide for those delving into NER research
and applications.",http://arxiv.org/pdf/2309.14084v1
2309.14078v1,cs.LG,ODE-based Recurrent Model-free Reinforcement Learning for POMDPs,2023-09-25 12:13:56+00:00,"Neural ordinary differential equations (ODEs) are widely recognized as the
standard for modeling physical mechanisms, which help to perform approximate
inference in unknown physical or biological environments. In partially
observable (PO) environments, how to infer unseen information from raw
observations puzzled the agents. By using a recurrent policy with a compact
context, context-based reinforcement learning provides a flexible way to
extract unobservable information from historical transitions. To help the agent
extract more dynamics-related information, we present a novel ODE-based
recurrent model combines with model-free reinforcement learning (RL) framework
to solve partially observable Markov decision processes (POMDPs). We
experimentally demonstrate the efficacy of our methods across various PO
continuous control and meta-RL tasks. Furthermore, our experiments illustrate
that our method is robust against irregular observations, owing to the ability
of ODEs to model irregularly-sampled time series.",http://arxiv.org/pdf/2309.14078v1
2309.14073v1,stat.ML,Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach,2023-09-25 12:07:00+00:00,"We propose a graphical structure for structural equation models that is
stable under marginalization under linearity and Gaussianity assumptions. We
show that computing the maximum likelihood estimation of this model is
equivalent to training a neural network. We implement a GPU-based algorithm
that computes the maximum likelihood estimation of these models.",http://arxiv.org/pdf/2309.14073v1
2309.14065v2,cs.CV,AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation,2023-09-25 11:57:16+00:00,"In the realm of robotic intelligence, achieving efficient and precise RGB-D
semantic segmentation is a key cornerstone. State-of-the-art multimodal
semantic segmentation methods, primarily rooted in symmetrical skeleton
networks, find it challenging to harmonize computational efficiency and
precision. In this work, we propose AsymFormer, a novel network for real-time
RGB-D semantic segmentation, which targets the minimization of superfluous
parameters by optimizing the distribution of computational resources and
introduces an asymmetrical backbone to allow for the effective fusion of
multimodal features. Furthermore, we explore techniques to bolster network
accuracy by redefining feature selection and extracting multi-modal
self-similarity features without a substantial increase in the parameter count,
thereby ensuring real-time execution on robotic platforms. Additionally, a
Local Attention-Guided Feature Selection (LAFS) module is used to selectively
fuse features from different modalities by leveraging their dependencies.
Subsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding
(CMA) module is introduced to further extract cross-modal representations. This
method is evaluated on NYUv2 and SUNRGBD datasets, with AsymFormer
demonstrating competitive results with 52.0% mIoU on NYUv2 and 49.1% mIoU on
SUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS and after
implementing mixed precision quantization, it attains an impressive inference
speed of 79 FPS on RTX3090. This significantly outperforms existing multi-modal
methods, thereby demonstrating that AsymFormer can strike a balance between
high accuracy and efficiency for RGB-D semantic segmentation.",http://arxiv.org/pdf/2309.14065v2
2309.14054v1,cs.LG,Adapt then Unlearn: Exploiting Parameter Space Semantics for Unlearning in Generative Adversarial Networks,2023-09-25 11:36:20+00:00,"The increased attention to regulating the outputs of deep generative models,
driven by growing concerns about privacy and regulatory compliance, has
highlighted the need for effective control over these models. This necessity
arises from instances where generative models produce outputs containing
undesirable, offensive, or potentially harmful content. To tackle this
challenge, the concept of machine unlearning has emerged, aiming to forget
specific learned information or to erase the influence of undesired data
subsets from a trained model. The objective of this work is to prevent the
generation of outputs containing undesired features from a pre-trained GAN
where the underlying training data set is inaccessible. Our approach is
inspired by a crucial observation: the parameter space of GANs exhibits
meaningful directions that can be leveraged to suppress specific undesired
features. However, such directions usually result in the degradation of the
quality of generated samples. Our proposed method, known as
'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also
maintaining the quality of generated samples. This method unfolds in two
stages: in the initial stage, we adapt the pre-trained GAN using negative
samples provided by the user, while in the subsequent stage, we focus on
unlearning the undesired feature. During the latter phase, we train the
pre-trained GAN using positive samples, incorporating a repulsion regularizer.
This regularizer encourages the model's parameters to be away from the
parameters associated with the adapted model from the first stage while also
maintaining the quality of generated samples. To the best of our knowledge, our
approach stands as first method addressing unlearning in GANs. We validate the
effectiveness of our method through comprehensive experiments.",http://arxiv.org/pdf/2309.14054v1
2309.14053v1,cs.LG,Revisiting LARS for Large Batch Training Generalization of Neural Networks,2023-09-25 11:35:10+00:00,"LARS and LAMB have emerged as prominent techniques in Large Batch Learning
(LBL), ensuring the stability of AI training. One of the primary challenges in
LBL is convergence stability, where the AI agent usually gets trapped into the
sharp minimizer. Addressing this challenge, a relatively recent technique,
known as warm-up, has been employed. However, warm-up lacks a strong
theoretical foundation, leaving the door open for further exploration of more
efficacious algorithms. In light of this situation, we conduct empirical
experiments to analyze the behaviors of the two most popular optimizers in the
LARS family: LARS and LAMB, with and without a warm-up strategy. Our analyses
give us a comprehension of the novel LARS, LAMB, and the necessity of a warm-up
technique in LBL. Building upon these insights, we propose a novel algorithm
called Time Varying LARS (TVLARS), which facilitates robust training in the
initial phase without the need for warm-up. Experimental evaluation
demonstrates that TVLARS achieves competitive results with LARS and LAMB when
warm-up is utilized while surpassing their performance without the warm-up
technique.",http://arxiv.org/pdf/2309.14053v1
2309.14042v1,cond-mat.mes-hall,Making topologically trivial non-Hermitian systems non-trivial via gauge fields,2023-09-25 11:19:15+00:00,"Non-Hermiticity significantly enriches the concepts of symmetry and topology
in physics. Particularly, non-Hermiticity gives rise to the ramified
symmetries, where the non-Hermitian Hamiltonian $H$ is transformed to
$H^\dagger$. For time-reversal ($T$) and sublattice symmetries, there are six
ramified symmetry classes leading to novel topological classifications with
various non-Hermitian skin effects. As artificial crystals are the main
experimental platforms for non-Hermitian physics, there exists the symmetry
barrier for realizing topological physics in the six ramified symmetry classes:
While artificial crystals are in spinless classes with $T^2=1$, nontrivial
classifications dominantly appear in spinful classes with $T^2=-1$. Here, we
present a general mechanism to cross the symmetry barrier. With an internal
parity symmetry $P$, the square of the combination $\tilde{T}=PT$ can be
modified by appropriate gauge fluxes. Using the general mechanism, we
systematically construct spinless models for all non-Hermitian spinful
topological phases in one and two dimensions, which are experimentally
realizable. Our work suggests that gauge structures may significantly enrich
non-Hermitian physics at the fundamental level.",http://arxiv.org/pdf/2309.14042v1
2309.14037v1,cs.NE,An automatic selection of optimal recurrent neural network architecture for processes dynamics modelling purposes,2023-09-25 11:06:35+00:00,"A problem related to the development of algorithms designed to find the
structure of artificial neural network used for behavioural (black-box)
modelling of selected dynamic processes has been addressed in this paper. The
research has included four original proposals of algorithms dedicated to neural
network architecture search. Algorithms have been based on well-known
optimisation techniques such as evolutionary algorithms and gradient descent
methods. In the presented research an artificial neural network of recurrent
type has been used, whose architecture has been selected in an optimised way
based on the above-mentioned algorithms. The optimality has been understood as
achieving a trade-off between the size of the neural network and its accuracy
in capturing the response of the mathematical model under which it has been
learnt. During the optimisation, original specialised evolutionary operators
have been proposed. The research involved an extended validation study based on
data generated from a mathematical model of the fast processes occurring in a
pressurised water nuclear reactor.",http://arxiv.org/pdf/2309.14037v1
2309.14032v1,cs.NE,DeepACO: Neural-enhanced Ant Systems for Combinatorial Optimization,2023-09-25 10:56:38+00:00,"Ant Colony Optimization (ACO) is a meta-heuristic algorithm that has been
successfully applied to various Combinatorial Optimization Problems (COPs).
Traditionally, customizing ACO for a specific problem requires the expert
design of knowledge-driven heuristics. In this paper, we propose DeepACO, a
generic framework that leverages deep reinforcement learning to automate
heuristic designs. DeepACO serves to strengthen the heuristic measures of
existing ACO algorithms and dispense with laborious manual design in future ACO
applications. As a neural-enhanced meta-heuristic, DeepACO consistently
outperforms its ACO counterparts on eight COPs using a single neural model and
a single set of hyperparameters. As a Neural Combinatorial Optimization method,
DeepACO performs better than or on par with problem-specific methods on
canonical routing problems. Our code is publicly available at
https://github.com/henry-yeh/DeepACO.",http://arxiv.org/pdf/2309.14032v1
2309.14029v1,cs.LG,Diffeomorphic Transformations for Time Series Analysis: An Efficient Approach to Nonlinear Warping,2023-09-25 10:51:47+00:00,"The proliferation and ubiquity of temporal data across many disciplines has
sparked interest for similarity, classification and clustering methods
specifically designed to handle time series data. A core issue when dealing
with time series is determining their pairwise similarity, i.e., the degree to
which a given time series resembles another. Traditional distance measures such
as the Euclidean are not well-suited due to the time-dependent nature of the
data. Elastic metrics such as dynamic time warping (DTW) offer a promising
approach, but are limited by their computational complexity,
non-differentiability and sensitivity to noise and outliers. This thesis
proposes novel elastic alignment methods that use parametric \& diffeomorphic
warping transformations as a means of overcoming the shortcomings of DTW-based
metrics. The proposed method is differentiable \& invertible, well-suited for
deep learning architectures, robust to noise and outliers, computationally
efficient, and is expressive and flexible enough to capture complex patterns.
Furthermore, a closed-form solution was developed for the gradient of these
diffeomorphic transformations, which allows an efficient search in the
parameter space, leading to better solutions at convergence. Leveraging the
benefits of these closed-form diffeomorphic transformations, this thesis
proposes a suite of advancements that include: (a) an enhanced temporal
transformer network for time series alignment and averaging, (b) a
deep-learning based time series classification model to simultaneously align
and classify signals with high accuracy, (c) an incremental time series
clustering algorithm that is warping-invariant, scalable and can operate under
limited computational and time resources, and finally, (d) a normalizing flow
model that enhances the flexibility of affine transformations in coupling and
autoregressive layers.",http://arxiv.org/pdf/2309.14029v1
2309.14025v1,physics.flu-dyn,Optimum control strategies for maximum thrust production in underwater undulatory swimming,2023-09-25 10:42:58+00:00,"Fish, cetaceans and many other aquatic vertebrates undulate their bodies to
propel themselves through water. Numerous studies on natural, artificial or
analogous swimmers are dedicated to revealing the links between the kinematics
of body oscillation and the production of thrust for swimming. One of the most
open and difficult questions concerns the best kinematics to maximize this
later quantity for given constraints and how a system strategizes and adjusts
its internal parameters to reach this maximum. To address this challenge, we
exploit a biomimetic robotic swimmer to determine the control signal that
produces the highest thrust. Using machine learning techniques and intuitive
models, we find that this optimal control consists of a square wave function,
whose frequency is fixed by the interplay between the internal dynamics of the
swimmer and the fluid-structure interaction with the surrounding fluid. We then
propose a simple implementation for autonomous robotic swimmers that requires
no prior knowledge of systems or equations. This application to aquatic
locomotion is validated by 2D numerical simulations.",http://arxiv.org/pdf/2309.14025v1
2309.14021v1,cs.CL,LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression,2023-09-25 10:35:17+00:00,"Low Rank Decomposition of matrix - splitting a large matrix into a product of
two smaller matrix offers a means for compression that reduces the parameters
of a model without sparsification, and hence delivering more speedup on modern
hardware. Moreover, unlike quantization, the compressed linear layers remain
fully differentiable and all the parameters trainable, while being able to
leverage the existing highly efficient kernels over floating point matrices. We
study the potential to compress Large Language Models (LLMs) for monolingual
Code generation via Low Rank Decomposition (LoRD) and observe that ranks for
the linear layers in these models can be reduced by upto 39.58% with less than
1% increase in perplexity. We then use Low Rank Decomposition (LoRD) to
compress StarCoder 16B to 13.2B parameter with no drop and to 12.3B with
minimal drop in HumanEval Pass@1 score, in less than 10 minutes on a single
A100. The compressed models speeds up inference by up to 22.35% with just a
single line of change in code over huggingface's implementation with pytorch
backend. Low Rank Decomposition (LoRD) models remain compatible with state of
the art near-lossless quantization method such as SpQR, which allows leveraging
further compression gains of quantization. Lastly, QLoRA over Low Rank
Decomposition (LoRD) model further reduces memory requirements by as much as
21.2% over vanilla QLoRA while offering similar gains from parameter efficient
fine tuning. Our work shows Low Rank Decomposition (LoRD) as a promising new
paradigm for LLM compression.",http://arxiv.org/pdf/2309.14021v1
2309.14008v1,eess.SP,Carrier Aggregation Enabled Integrated Sensing and Communication Signal Design and Processing,2023-09-25 10:20:13+00:00,"The future mobile communication systems will support intelligent applications
such as Internet of Vehicles (IoV) and Extended Reality (XR). Integrated
Sensing and Communication (ISAC) is regarded as one of the key technologies
satisfying the high data rate communication and highly accurate sensing for
these intelligent applications in future mobile communication systems. With the
explosive growth of wireless devices and services, the shortage of spectrum
resources leads to the fragmentation of available frequency bands for ISAC
systems, which degrades sensing performance. Facing the above challenges, this
paper proposes a Carrier Aggregation (CA)-based ISAC signal aggregating high
and low-frequency bands to improve the sensing performance, where the CA-based
ISAC signal can use four different aggregated pilot structures for sensing.
Then, an ISAC signal processing algorithm with Compressed Sensing (CS) is
proposed and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) is
used to solve the reconfiguration convex optimization problem. Finally, the
Cram'er-Rao Lower Bounds (CRLBs) are derived for the CA-based ISAC signal.
Simulation results show that CA efficiently improves the accuracy of range and
velocity estimation.",http://arxiv.org/pdf/2309.14008v1
2309.13979v1,cs.OH,"Morphological Computing as Logic Underlying Cognition in Human, Animal, and Intelligent Machine",2023-09-25 09:31:25+00:00,"This work examines the interconnections between logic, epistemology, and
sciences within the Naturalist tradition. It presents a scheme that connects
logic, mathematics, physics, chemistry, biology, and cognition, emphasizing
scale-invariant, self-organizing dynamics across organizational tiers of
nature. The inherent logic of agency exists in natural processes at various
levels, under information exchanges. It applies to humans, animals, and
artifactual agents. The common human-centric, natural language-based logic is
an example of complex logic evolved by living organisms that already appears in
the simplest form at the level of basal cognition of unicellular organisms.
Thus, cognitive logic stems from the evolution of physical, chemical, and
biological logic. In a computing nature framework with a self-organizing
agency, innovative computational frameworks grounded in
morphological/physical/natural computation can be used to explain the genesis
of human-centered logic through the steps of naturalized logical processes at
lower levels of organization. The Extended Evolutionary Synthesis of living
agents is essential for understanding the emergence of human-level logic and
the relationship between logic and information processing/computational
epistemology. We conclude that more research is needed to elucidate the details
of the mechanisms linking natural phenomena with the logic of agency in nature.",http://arxiv.org/pdf/2309.13979v1
2309.14974v1,cs.CL,Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts,2023-09-25 09:21:25+00:00,"In this study, we propose to evaluate the use of deep learning methods for
semantic classification at the sentence level to accelerate the process of
corpus building in the field of humanities and linguistics, a traditional and
time-consuming task. We introduce a novel corpus comprising around 2500
sentences spanning from 300 BCE to 900 CE including sexual semantics (medical,
erotica, etc.). We evaluate various sentence classification approaches and
different input embedding layers, and show that all consistently outperform
simple token-based searches. We explore the integration of idiolectal and
sociolectal metadata embeddings (centuries, author, type of writing), but find
that it leads to overfitting. Our results demonstrate the effectiveness of this
approach, achieving high precision and true positive rates (TPR) of
respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset
size on the model performances (420 instead of 2013), and show that, while our
models perform worse, they still offer a high enough precision and TPR, even
without MLM, respectively 69% and 51%. Given the result, we provide an analysis
of the attention mechanism as a supporting added value for humanists in order
to produce more data.",http://arxiv.org/pdf/2309.14974v1
2309.13972v1,cs.SD,Audio classification with Dilated Convolution with Learnable Spacings,2023-09-25 09:09:54+00:00,"Dilated convolution with learnable spacings (DCLS) is a recent convolution
method in which the positions of the kernel elements are learned throughout
training by backpropagation. Its interest has recently been demonstrated in
computer vision (ImageNet classification and downstream tasks). Here we show
that DCLS is also useful for audio tagging using the AudioSet classification
benchmark. We took two state-of-the-art convolutional architectures using
depthwise separable convolutions (DSC), ConvNeXt and ConvFormer, and a hybrid
one using attention in addition, FastViT, and drop-in replaced all the DSC
layers by DCLS ones. This significantly improved the mean average precision
(mAP) with the three architectures without increasing the number of parameters
and with only a low cost on the throughput. The method code is based on PyTorch
and is available at https://github.com/K-H-Ismail/DCLS-Audio",http://arxiv.org/pdf/2309.13972v1
2309.13970v1,cs.HC,A Cyberpunk 2077 perspective on the prediction and understanding of future technology,2023-09-25 09:08:30+00:00,"Science fiction and video games have long served as valuable tools for
envisioning and inspiring future technological advancements. This position
paper investigates the potential of Cyberpunk 2077, a popular science fiction
video game, to shed light on the future of technology, particularly in the
areas of artificial intelligence, edge computing, augmented humans, and
biotechnology. By analyzing the game's portrayal of these technologies and
their implications, we aim to understand the possibilities and challenges that
lie ahead. We discuss key themes such as neurolink and brain-computer
interfaces, multimodal recording systems, virtual and simulated reality,
digital representation of the physical world, augmented and AI-based home
appliances, smart clothing, and autonomous vehicles. The paper highlights the
importance of designing technologies that can coexist with existing preferences
and systems, considering the uneven adoption of new technologies. Through this
exploration, we emphasize the potential of science fiction and video games like
Cyberpunk 2077 as tools for guiding future technological advancements and
shaping public perception of emerging innovations.",http://arxiv.org/pdf/2309.13970v1
2309.14391v1,cs.LG,An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-oriented Systems,2023-09-25 09:05:36+00:00,"Deep Reinforcement Learning (Deep RL) is increasingly used to cope with the
open-world assumption in service-oriented systems. Deep RL was successfully
applied to problems such as dynamic service composition, job scheduling, and
offloading, as well as service adaptation. While Deep RL offers many benefits,
understanding the decision-making of Deep RL is challenging because its learned
decision-making policy essentially appears as a black box. Yet, understanding
the decision-making of Deep RL is key to help service developers perform
debugging, support service providers to comply with relevant legal frameworks,
and facilitate service users to build trust. We introduce Chat4XAI to
facilitate the understanding of the decision-making of Deep RL by providing
natural-language explanations. Compared with visual explanations, the reported
benefits of natural-language explanations include better understandability for
non-technical users, increased user acceptance and trust, as well as more
efficient explanations. Chat4XAI leverages modern AI chatbot technology and
dedicated prompt engineering. Compared to earlier work on natural-language
explanations using classical software-based dialogue systems, using an AI
chatbot eliminates the need for eliciting and defining potential questions and
answers up-front. We prototypically realize Chat4XAI using OpenAI's ChatGPT API
and evaluate the fidelity and stability of its explanations using an adaptive
service exemplar.",http://arxiv.org/pdf/2309.14391v1
2309.13965v1,cs.HC,May I Ask a Follow-up Question? Understanding the Benefits of Conversations in Neural Network Explainability,2023-09-25 09:00:38+00:00,"Research in explainable AI (XAI) aims to provide insights into the
decision-making process of opaque AI models. To date, most XAI methods offer
one-off and static explanations, which cannot cater to the diverse backgrounds
and understanding levels of users. With this paper, we investigate if free-form
conversations can enhance users' comprehension of static explanations, improve
acceptance and trust in the explanation methods, and facilitate human-AI
collaboration. Participants are presented with static explanations, followed by
a conversation with a human expert regarding the explanations. We measure the
effect of the conversation on participants' ability to choose, from three
machine learning models, the most accurate one based on explanations and their
self-reported comprehension, acceptance, and trust. Empirical results show that
conversations significantly improve comprehension, acceptance, trust, and
collaboration. Our findings highlight the importance of customized model
explanations in the format of free-form conversations and provide insights for
the future design of conversational explanations.",http://arxiv.org/pdf/2309.13965v1
2309.13960v1,cs.LG,Newton Method-based Subspace Support Vector Data Description,2023-09-25 08:49:41+00:00,"In this paper, we present an adaptation of Newton's method for the
optimization of Subspace Support Vector Data Description (S-SVDD). The
objective of S-SVDD is to map the original data to a subspace optimized for
one-class classification, and the iterative optimization process of data
mapping and description in S-SVDD relies on gradient descent. However, gradient
descent only utilizes first-order information, which may lead to suboptimal
results. To address this limitation, we leverage Newton's method to enhance
data mapping and data description for an improved optimization of subspace
learning-based one-class classification. By incorporating this auxiliary
information, Newton's method offers a more efficient strategy for subspace
learning in one-class classification as compared to gradient-based
optimization. The paper discusses the limitations of gradient descent and the
advantages of using Newton's method in subspace learning for one-class
classification tasks. We provide both linear and nonlinear formulations of
Newton's method-based optimization for S-SVDD. In our experiments, we explored
both the minimization and maximization strategies of the objective. The results
demonstrate that the proposed optimization strategy outperforms the
gradient-based S-SVDD in most cases.",http://arxiv.org/pdf/2309.13960v1
2309.14390v1,cs.LG,Early Churn Prediction from Large Scale User-Product Interaction Time Series,2023-09-25 08:44:32+00:00,"User churn, characterized by customers ending their relationship with a
business, has profound economic consequences across various
Business-to-Customer scenarios. For numerous system-to-user actions, such as
promotional discounts and retention campaigns, predicting potential churners
stands as a primary objective. In volatile sectors like fantasy sports,
unpredictable factors such as international sports events can influence even
regular spending habits. Consequently, while transaction history and
user-product interaction are valuable in predicting churn, they demand deep
domain knowledge and intricate feature engineering. Additionally, feature
development for churn prediction systems can be resource-intensive,
particularly in production settings serving 200m+ users, where inference
pipelines largely focus on feature engineering. This paper conducts an
exhaustive study on predicting user churn using historical data. We aim to
create a model forecasting customer churn likelihood, facilitating businesses
in comprehending attrition trends and formulating effective retention plans.
Our approach treats churn prediction as multivariate time series
classification, demonstrating that combining user activity and deep neural
networks yields remarkable results for churn prediction in complex
business-to-customer contexts.",http://arxiv.org/pdf/2309.14390v1
2309.13952v1,cs.CV,VidChapters-7M: Video Chapters at Scale,2023-09-25 08:38:11+00:00,"Segmenting long videos into chapters enables users to quickly navigate to the
information of their interest. This important topic has been understudied due
to the lack of publicly released datasets. To address this issue, we present
VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters
in total. VidChapters-7M is automatically created from videos online in a
scalable manner by scraping user-annotated chapters and hence without any
additional manual annotation. We introduce the following three tasks based on
this data. First, the video chapter generation task consists of temporally
segmenting the video and generating a chapter title for each segment. To
further dissect the problem, we also define two variants of this task: video
chapter generation given ground-truth boundaries, which requires generating a
chapter title given an annotated video segment, and video chapter grounding,
which requires temporally localizing a chapter given its annotated title. We
benchmark both simple baselines and state-of-the-art video-language models for
these three tasks. We also show that pretraining on VidChapters-7M transfers
well to dense video captioning tasks in both zero-shot and finetuning settings,
largely improving the state of the art on the YouCook2 and ViTT benchmarks.
Finally, our experiments reveal that downstream performance scales well with
the size of the pretraining dataset. Our dataset, code, and models are publicly
available at https://antoyang.github.io/vidchapters.html.",http://arxiv.org/pdf/2309.13952v1
2309.13948v1,cs.RO,Co-Design Optimisation of Morphing Topology and Control of Winged Drones,2023-09-25 08:27:35+00:00,"The design and control of winged aircraft and drones is an iterative process
aimed at identifying a compromise of mission-specific costs and constraints.
When agility is required, shape-shifting (morphing) drones represent an
efficient solution. However, morphing drones require the addition of actuated
joints that increase the topology and control coupling, making the design
process more complex. We propose a co-design optimisation method that assists
the engineers by proposing a morphing drone's conceptual design that includes
topology, actuation, morphing strategy, and controller parameters. The method
consists of applying multi-objective constraint-based optimisation to a
multi-body winged drone with trajectory optimisation to solve the motion
intelligence problem under diverse flight mission requirements. We show that
co-designed morphing drones outperform fixed-winged drones in terms of energy
efficiency and agility, suggesting that the proposed co-design method could be
a useful addition to the aircraft engineering toolbox.",http://arxiv.org/pdf/2309.13948v1
2309.13939v1,cs.AI,"The Time Traveler's Guide to Semantic Web Research: Analyzing Fictitious Research Themes in the ESWC ""Next 20 Years"" Track",2023-09-25 08:20:06+00:00,"What will Semantic Web research focus on in 20 years from now? We asked this
question to the community and collected their visions in the ""Next 20 years""
track of ESWC 2023. We challenged the participants to submit ""future"" research
papers, as if they were submitting to the 2043 edition of the conference. The
submissions - entirely fictitious - were expected to be full scientific papers,
with research questions, state of the art references, experimental results and
future work, with the goal to get an idea of the research agenda for the late
2040s and early 2050s. We received ten submissions, eight of which were
accepted for presentation at the conference, that mixed serious ideas of
potential future research themes and discussion topics with some fun and irony.
  In this paper, we intend to provide a survey of those ""science fiction""
papers, considering the emerging research themes and topics, analysing the
research methods applied by the authors in these very special submissions, and
investigating also the most fictitious parts (e.g., neologisms, fabricated
references). Our goal is twofold: on the one hand, we investigate what this
special track tells us about the Semantic Web community and, on the other hand,
we aim at getting some insights on future research practices and directions.",http://arxiv.org/pdf/2309.13939v1
2309.13937v1,cs.RO,SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems,2023-09-25 08:13:49+00:00,"Pick-and-place is one of the fundamental tasks in robotics research. However,
the attention has been mostly focused on the ``pick'' task, leaving the
``place'' task relatively unexplored. In this paper, we address the problem of
placing objects in the context of a teleoperation framework. Particularly, we
focus on two aspects of the place task: stability robustness and contextual
reasonableness of object placements. Our proposed method combines
simulation-driven physical stability verification via real-to-sim and the
semantic reasoning capability of large language models. In other words, given
place context information (e.g., user preferences, object to place, and current
scene information), our proposed method outputs a probability distribution over
the possible placement candidates, considering the robustness and
reasonableness of the place task. Our proposed method is extensively evaluated
in two simulation and one real world environments and we show that our method
can greatly increase the physical plausibility of the placement as well as
contextual soundness while considering user preferences.",http://arxiv.org/pdf/2309.13937v1
2309.13933v1,cs.CY,Fairness and Bias in Algorithmic Hiring,2023-09-25 08:04:18+00:00,"Employers are adopting algorithmic hiring technology throughout the
recruitment pipeline. Algorithmic fairness is especially applicable in this
domain due to its high stakes and structural inequalities. Unfortunately, most
work in this space provides partial treatment, often constrained by two
competing narratives, optimistically focused on replacing biased recruiter
decisions or pessimistically pointing to the automation of discrimination.
Whether, and more importantly what types of, algorithmic hiring can be less
biased and more beneficial to society than low-tech alternatives currently
remains unanswered, to the detriment of trustworthiness. This multidisciplinary
survey caters to practitioners and researchers with a balanced and integrated
coverage of systems, biases, measures, mitigation strategies, datasets, and
legal aspects of algorithmic hiring and fairness. Our work supports a
contextualized understanding and governance of this technology by highlighting
current opportunities and limitations, providing recommendations for future
work to ensure shared benefits for all stakeholders.",http://arxiv.org/pdf/2309.13933v1
2309.13926v2,cs.LG,Pseudo Label Selection is a Decision Problem,2023-09-25 07:48:02+00:00,"Pseudo-Labeling is a simple and effective approach to semi-supervised
learning. It requires criteria that guide the selection of pseudo-labeled data.
The latter have been shown to crucially affect pseudo-labeling's generalization
performance. Several such criteria exist and were proven to work reasonably
well in practice. However, their performance often depends on the initial model
fit on labeled data. Early overfitting can be propagated to the final model by
choosing instances with overconfident but wrong predictions, often called
confirmation bias. In two recent works, we demonstrate that pseudo-label
selection (PLS) can be naturally embedded into decision theory. This paves the
way for BPLS, a Bayesian framework for PLS that mitigates the issue of
confirmation bias. At its heart is a novel selection criterion: an analytical
approximation of the posterior predictive of pseudo-samples and labeled data.
We derive this selection criterion by proving Bayes-optimality of this ""pseudo
posterior predictive"". We empirically assess BPLS for generalized linear,
non-parametric generalized additive models and Bayesian neural networks on
simulated and real-world data. When faced with data prone to overfitting and
thus a high chance of confirmation bias, BPLS outperforms traditional PLS
methods. The decision-theoretic embedding further allows us to render PLS more
robust towards the involved modeling assumptions. To achieve this goal, we
introduce a multi-objective utility function. We demonstrate that the latter
can be constructed to account for different sources of uncertainty and explore
three examples: model selection, accumulation of errors and covariate shift.",http://arxiv.org/pdf/2309.13926v2
2309.13925v1,cs.CV,UCF-Crime Annotation: A Benchmark for Surveillance Video-and-Language Understanding,2023-09-25 07:46:56+00:00,"Surveillance videos are an essential component of daily life with various
critical applications, particularly in public security. However, current
surveillance video tasks mainly focus on classifying and localizing anomalous
events. Existing methods are limited to detecting and classifying the
predefined events with unsatisfactory generalization ability and semantic
understanding, although they have obtained considerable performance. To address
this issue, we propose constructing the first multimodal surveillance video
dataset by manually annotating the real-world surveillance dataset UCF-Crime
with fine-grained event content and timing. Our newly annotated dataset, UCA
(UCF-Crime Annotation), provides a novel benchmark for multimodal surveillance
video analysis. It not only describes events in detailed descriptions but also
provides precise temporal grounding of the events in 0.1-second intervals. UCA
contains 20,822 sentences, with an average length of 23 words, and its
annotated videos are as long as 102 hours. Furthermore, we benchmark the
state-of-the-art models of multiple multimodal tasks on this newly created
dataset, including temporal sentence grounding in videos, video captioning, and
dense video captioning. Through our experiments, we found that mainstream
models used in previously publicly available datasets perform poorly on
multimodal surveillance video scenarios, which highlights the necessity of
constructing this dataset. The link to our dataset and code is provided at:
https://github.com/Xuange923/UCA-dataset.",http://arxiv.org/pdf/2309.13925v1
2309.13908v1,cs.RO,A comparison of controller architectures and learning mechanisms for arbitrary robot morphologies,2023-09-25 07:11:43+00:00,"The main question this paper addresses is: What combination of a robot
controller and a learning method should be used, if the morphology of the
learning robot is not known in advance? Our interest is rooted in the context
of morphologically evolving modular robots, but the question is also relevant
in general, for system designers interested in widely applicable solutions. We
perform an experimental comparison of three controller-and-learner
combinations: one approach where controllers are based on modelling animal
locomotion (Central Pattern Generators, CPG) and the learner is an evolutionary
algorithm, a completely different method using Reinforcement Learning (RL) with
a neural network controller architecture, and a combination `in-between' where
controllers are neural networks and the learner is an evolutionary algorithm.
We apply these three combinations to a test suite of modular robots and compare
their efficacy, efficiency, and robustness. Surprisingly, the usual CPG-based
and RL-based options are outperformed by the in-between combination that is
more robust and efficient than the other two setups.",http://arxiv.org/pdf/2309.13908v1
2309.14389v1,cs.CV,Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering,2023-09-25 07:01:16+00:00,"Recent document question answering models consist of two key components: the
vision encoder, which captures layout and visual elements in images, and a
Large Language Model (LLM) that helps contextualize questions to the image and
supplements them with external world knowledge to generate accurate answers.
However, the relative contributions of the vision encoder and the language
model in these tasks remain unclear. This is especially interesting given the
effectiveness of instruction-tuned LLMs, which exhibit remarkable adaptability
to new tasks. To this end, we explore the following aspects in this work: (1)
The efficacy of an LLM-only approach on document question answering tasks (2)
strategies for serializing textual information within document images and
feeding it directly to an instruction-tuned LLM, thus bypassing the need for an
explicit vision encoder (3) thorough quantitative analysis on the feasibility
of such an approach. Our comprehensive analysis encompasses six diverse
benchmark datasets, utilizing LLMs of varying scales. Our findings reveal that
a strategy exclusively reliant on the LLM yields results that are on par with
or closely approach state-of-the-art performance across a range of datasets. We
posit that this evaluation framework will serve as a guiding resource for
selecting appropriate datasets for future research endeavors that emphasize the
fundamental importance of layout and image content information.",http://arxiv.org/pdf/2309.14389v1
2309.13902v1,eess.SP,NoncovANM: Gridless DOA Estimation for LPDF System,2023-09-25 06:51:05+00:00,"Direction of arrival (DOA) estimation is an important research in the area of
array signal processing, and has been studied for decades. High resolution DOA
estimation requires large array aperture, which leads to the increase of
hardware cost. Besides, high accuracy DOA estimation methods usually have high
computational complexity. In this paper, the problem of decreasing the hardware
cost and algorithm complexity is addressed. First, considering the ability of
flexible controlling the electromagnetic waves and low-cost, an intelligent
reconfigurable surface (IRS)-aided low-cost passive direction finding (LPDF)
system is developed, where only one fully functional receiving channel is
adopted. Then, the sparsity of targets direction in the spatial domain is
exploited by formulating an atomic norm minimization (ANM) problem to estimate
the DOA. Traditionally, solving ANM problem is complex and cannot be realized
efficiently. Hence, a novel nonconvex-based ANM (NC-ANM) method is proposed by
gradient threshold iteration, where a perturbation is introduced to avoid
falling into saddle points. The theoretical analysis for the convergence of the
NC-ANM method is also given. Moreover, the corresponding Cram\'er-Rao lower
bound (CRLB) in the LPDF system is derived, and taken as the referred bound of
the DOA estimation. Simulation results show that the proposed method
outperforms the compared methods in the DOA estimation with lower computational
complexity in the LPDF system.",http://arxiv.org/pdf/2309.13902v1
2309.14387v1,cs.RO,Exploring Robot Morphology Spaces through Breadth-First Search and Random Query,2023-09-25 06:46:19+00:00,"Evolutionary robotics offers a powerful framework for designing and evolving
robot morphologies, particularly in the context of modular robots. However, the
role of query mechanisms during the genotype-to-phenotype mapping process has
been largely overlooked. This research addresses this gap by conducting a
comparative analysis of query mechanisms in the brain-body co-evolution of
modular robots. Using two different query mechanisms, Breadth-First Search
(BFS) and Random Query, within the context of evolving robot morphologies using
CPPNs and robot controllers using tensors, and testing them in two evolutionary
frameworks, Lamarckian and Darwinian systems, this study investigates their
influence on evolutionary outcomes and performance. The findings demonstrate
the impact of the two query mechanisms on the evolution and performance of
modular robot bodies, including morphological intelligence, diversity, and
morphological traits. This study suggests that BFS is both more effective and
efficient in producing highly performing robots. It also reveals that
initially, robot diversity was higher with BFS compared to Random Query, but in
the Lamarckian system, it declines faster, converging to superior designs,
while in the Darwinian system, BFS led to higher end-process diversity.",http://arxiv.org/pdf/2309.14387v1
2309.13893v1,cs.RO,Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments,2023-09-25 06:16:09+00:00,"Navigating complex and dynamic environments requires autonomous vehicles
(AVs) to reason about both visible and occluded regions. This involves
predicting the future motion of observed agents, inferring occluded ones, and
modeling their interactions based on vectorized scene representations of the
partially observable environment. However, prior work on occlusion inference
and trajectory prediction have developed in isolation, with the former based on
simplified rasterized methods and the latter assuming full environment
observability. We introduce the Scene Informer, a unified approach for
predicting both observed agent trajectories and inferring occlusions in a
partially observable setting. It uses a transformer to aggregate various input
modalities and facilitate selective queries on occlusions that might intersect
with the AV's planned path. The framework estimates occupancy probabilities and
likely trajectories for occlusions, as well as forecast motion for observed
agents. We explore common observability assumptions in both domains and their
performance impact. Our approach outperforms existing methods in both occupancy
prediction and trajectory prediction in partially observable setting on the
Waymo Open Motion Dataset.",http://arxiv.org/pdf/2309.13893v1
2309.13885v1,cs.LG,TouchUp-G: Improving Feature Representation through Graph-Centric Finetuning,2023-09-25 05:44:40+00:00,"How can we enhance the node features acquired from Pretrained Models (PMs) to
better suit downstream graph learning tasks? Graph Neural Networks (GNNs) have
become the state-of-the-art approach for many high-impact, real-world graph
applications. For feature-rich graphs, a prevalent practice involves utilizing
a PM directly to generate features, without incorporating any domain adaptation
techniques. Nevertheless, this practice is suboptimal because the node features
extracted from PM are graph-agnostic and prevent GNNs from fully utilizing the
potential correlations between the graph structure and node features, leading
to a decline in GNNs performance. In this work, we seek to improve the node
features obtained from a PM for downstream graph tasks and introduce TOUCHUP-G,
which has several advantages. It is (a) General: applicable to any downstream
graph task, including link prediction which is often employed in recommender
systems; (b) Multi-modal: able to improve raw features of any modality (e.g.
images, texts, audio); (c) Principled: it is closely related to a novel metric,
feature homophily, which we propose to quantify the potential correlations
between the graph structure and node features and we show that TOUCHUP-G can
effectively shrink the discrepancy between the graph structure and node
features; (d) Effective: achieving state-of-the-art results on four real-world
datasets spanning different tasks and modalities.",http://arxiv.org/pdf/2309.13885v1
2309.13860v1,cs.CL,Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning,2023-09-25 04:07:34+00:00,"Recent years have witnessed significant advancements in self-supervised
learning (SSL) methods for speech-processing tasks. Various speech-based SSL
models have been developed and present promising performance on a range of
downstream tasks including speech recognition. However, existing speech-based
SSL models face a common dilemma in terms of computational cost, which might
hinder their potential application and in-depth academic research. To address
this issue, we first analyze the computational cost of different modules during
HuBERT pre-training and then introduce a stack of efficiency optimizations,
which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be
trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without
performance degradation, resulting in a 5.2x speedup, compared to the original
implementation. Moreover, we explore two well-studied techniques in the
Fast-HuBERT and demonstrate consistent improvements as reported in previous
work.",http://arxiv.org/pdf/2309.13860v1
2309.13856v1,eess.SP,DNN-DANM: A High-Accuracy Two-Dimensional DOA Estimation Method Using Practical RIS,2023-09-25 03:47:41+00:00,"Reconfigurable intelligent surface (RIS) or intelligent reflecting surface
(IRS) has been an attractive technology for future wireless communication and
sensing systems. However, in the practical RIS, the mutual coupling effect
among RIS elements, the reflection phase shift, and amplitude errors will
degrade the RIS performance significantly. This paper investigates the
two-dimensional direction-of-arrival (DOA) estimation problem in the scenario
using a practical RIS. After formulating the system model with the mutual
coupling effect and the reflection phase/amplitude errors of the RIS, a novel
DNNDANM method is proposed for the DOA estimation by combining the deep neural
network (DNN) and the decoupling atomic norm minimization (DANM). The DNN step
reconstructs the received signal from the one with RIS impairments, and the
DANM step exploits the signal sparsity in the two-dimensional spatial domain.
Additionally, a semi-definite programming (SDP) method with low computational
complexity is proposed to solve the atomic minimization problem. Finally, both
simulation and prototype are carried out to show estimation performance, and
the proposed method outperforms the existing methods in the two-dimensional DOA
estimation with low complexity in the scenario with practical RIS.",http://arxiv.org/pdf/2309.13856v1
2309.13851v1,cs.CV,DISeR: Designing Imaging Systems with Reinforcement Learning,2023-09-25 03:35:51+00:00,"Imaging systems consist of cameras to encode visual information about the
world and perception models to interpret this encoding. Cameras contain (1)
illumination sources, (2) optical elements, and (3) sensors, while perception
models use (4) algorithms. Directly searching over all combinations of these
four building blocks to design an imaging system is challenging due to the size
of the search space. Moreover, cameras and perception models are often designed
independently, leading to sub-optimal task performance. In this paper, we
formulate these four building blocks of imaging systems as a context-free
grammar (CFG), which can be automatically searched over with a learned camera
designer to jointly optimize the imaging system with task-specific perception
models. By transforming the CFG to a state-action space, we then show how the
camera designer can be implemented with reinforcement learning to intelligently
search over the combinatorial space of possible imaging system configurations.
We demonstrate our approach on two tasks, depth estimation and camera rig
design for autonomous vehicles, showing that our method yields rigs that
outperform industry-wide standards. We believe that our proposed approach is an
important step towards automating imaging system design.",http://arxiv.org/pdf/2309.13851v1
2309.14385v1,cs.LG,Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence,2023-09-25 02:46:19+00:00,"Explainable Artificial Intelligence (XAI) models have recently attracted a
great deal of interest from a variety of application sectors. Despite
significant developments in this area, there are still no standardized methods
or approaches for understanding AI model outputs. A systematic and cohesive
framework is also increasingly necessary to incorporate new techniques like
discriminative and generative models to close the gap. This paper contributes
to the discourse on XAI by presenting an empirical evaluation based on a novel
framework: Sampling - Variational Auto Encoder (VAE) - Ensemble Anomaly
Detection (SVEAD). It is a hybrid architecture where VAE combined with ensemble
stacking and SHapley Additive exPlanations are used for imbalanced
classification. The finding reveals that combining ensemble stacking, VAE, and
SHAP can. not only lead to better model performance but also provide an easily
explainable framework. This work has used SHAP combined with Permutation
Importance and Individual Conditional Expectations to create a powerful
interpretability of the model. The finding has an important implication in the
real world, where the need for XAI is paramount to boost confidence in AI
applications.",http://arxiv.org/pdf/2309.14385v1
2309.13834v1,cs.AI,Prior Bilinear Based Models for Knowledge Graph Completion,2023-09-25 02:44:33+00:00,"Bilinear based models are powerful and widely used approaches for Knowledge
Graphs Completion (KGC). Although bilinear based models have achieved
significant advances, these studies mainly concentrate on posterior properties
(based on evidence, e.g. symmetry pattern) while neglecting the prior
properties. In this paper, we find a prior property named ""the law of identity""
that cannot be captured by bilinear based models, which hinders them from
comprehensively modeling the characteristics of KGs. To address this issue, we
introduce a solution called Unit Ball Bilinear Model (UniBi). This model not
only achieves theoretical superiority but also offers enhanced interpretability
and performance by minimizing ineffective learning through minimal constraints.
Experiments demonstrate that UniBi models the prior property and verify its
interpretability and performance.",http://arxiv.org/pdf/2309.13834v1
2309.13833v1,cs.CV,Dual Feature Augmentation Network for Generalized Zero-shot Learning,2023-09-25 02:37:52+00:00,"Zero-shot learning (ZSL) aims to infer novel classes without training samples
by transferring knowledge from seen classes. Existing embedding-based
approaches for ZSL typically employ attention mechanisms to locate attributes
on an image. However, these methods often ignore the complex entanglement among
different attributes' visual features in the embedding space. Additionally,
these methods employ a direct attribute prediction scheme for classification,
which does not account for the diversity of attributes in images of the same
category. To address these issues, we propose a novel Dual Feature Augmentation
Network (DFAN), which comprises two feature augmentation modules, one for
visual features and the other for semantic features. The visual feature
augmentation module explicitly learns attribute features and employs cosine
distance to separate them, thus enhancing attribute representation. In the
semantic feature augmentation module, we propose a bias learner to capture the
offset that bridges the gap between actual and predicted attribute values from
a dataset's perspective. Furthermore, we introduce two predictors to reconcile
the conflicts between local and global features. Experimental results on three
benchmarks demonstrate the marked advancement of our method compared to
state-of-the-art approaches. Our code is available at
https://github.com/Sion1/DFAN.",http://arxiv.org/pdf/2309.13833v1
2309.13803v1,cs.CR,Privacy-preserving Linear Computations in Spiking Neural P Systems,2023-09-25 01:16:18+00:00,"Spiking Neural P systems are a class of membrane computing models inspired
directly by biological neurons. Besides the theoretical progress made in this
new computational model, there are also numerous applications of P systems in
fields like formal verification, artificial intelligence, or cryptography.
Motivated by all the use cases of SN P systems, in this paper, we present a new
privacy-preserving protocol that enables a client to compute a linear function
using an SN P system hosted on a remote server. Our protocol allows the client
to use the server to evaluate functions of the form t_1k + t_2 without
revealing t_1, t_2 or k and without the server knowing the result. We also
present an SN P system to implement any linear function over natural numbers
and some security considerations of our protocol in the honest-but-curious
security model.",http://arxiv.org/pdf/2309.13803v1
2309.13788v1,cs.CL,Can LLM-Generated Misinformation Be Detected?,2023-09-25 00:45:07+00:00,"The advent of Large Language Models (LLMs) has made a transformative impact.
However, the potential that LLMs such as ChatGPT can be exploited to generate
misinformation has posed a serious concern to online safety and public trust. A
fundamental research question is: will LLM-generated misinformation cause more
harm than human-written misinformation? We propose to tackle this question from
the perspective of detection difficulty. We first build a taxonomy of
LLM-generated misinformation. Then we categorize and validate the potential
real-world methods for generating misinformation with LLMs. Then, through
extensive empirical investigation, we discover that LLM-generated
misinformation can be harder to detect for humans and detectors compared to
human-written misinformation with the same semantics, which suggests it can
have more deceptive styles and potentially cause more harm. We also discuss the
implications of our discovery on combating misinformation in the age of LLMs
and the countermeasures.",http://arxiv.org/pdf/2309.13788v1
2309.13782v1,cs.LG,On the Computational Benefit of Multimodal Learning,2023-09-25 00:20:50+00:00,"Human perception inherently operates in a multimodal manner. Similarly, as
machines interpret the empirical world, their learning processes ought to be
multimodal. The recent, remarkable successes in empirical multimodal learning
underscore the significance of understanding this paradigm. Yet, a solid
theoretical foundation for multimodal learning has eluded the field for some
time. While a recent study by Lu (2023) has shown the superior sample
complexity of multimodal learning compared to its unimodal counterpart, another
basic question remains: does multimodal learning also offer computational
advantages over unimodal learning? This work initiates a study on the
computational benefit of multimodal learning. We demonstrate that, under
certain conditions, multimodal learning can outpace unimodal learning
exponentially in terms of computation. Specifically, we present a learning task
that is NP-hard for unimodal learning but is solvable in polynomial time by a
multimodal algorithm. Our construction is based on a novel modification to the
intersection of two half-spaces problem.",http://arxiv.org/pdf/2309.13782v1
2309.13781v2,cs.LG,Explainable Machine Learning for ICU Readmission Prediction,2023-09-25 00:16:43+00:00,"The intensive care unit (ICU) comprises a complex hospital environment, where
decisions made by clinicians have a high level of risk for the patients' lives.
A comprehensive care pathway must then be followed to reduce p complications.
Uncertain, competing and unplanned aspects within this environment increase the
difficulty in uniformly implementing the care pathway. Readmission contributes
to this pathway's difficulty, occurring when patients are admitted again to the
ICU in a short timeframe, resulting in high mortality rates and high resource
utilisation. Several works have tried to predict readmission through patients'
medical information. Although they have some level of success while predicting
readmission, those works do not properly assess, characterise and understand
readmission prediction. This work proposes a standardised and explainable
machine learning pipeline to model patient readmission on a multicentric
database (i.e., the eICU cohort with 166,355 patients, 200,859 admissions and
6,021 readmissions) while validating it on monocentric (i.e., the MIMIC IV
cohort with 382,278 patients, 523,740 admissions and 5,984 readmissions) and
multicentric settings. Our machine learning pipeline achieved predictive
performance in terms of the area of the receiver operating characteristic curve
(AUC) up to 0.7 with a Random Forest classification model, yielding an overall
good calibration and consistency on validation sets. From explanations provided
by the constructed models, we could also derive a set of insightful
conclusions, primarily on variables related to vital signs and blood tests
(e.g., albumin, blood urea nitrogen and hemoglobin levels), demographics (e.g.,
age, and admission height and weight), and ICU-associated variables (e.g., unit
type). These insights provide an invaluable source of information during
clinicians' decision-making while discharging ICU patients.",http://arxiv.org/pdf/2309.13781v2
2309.13773v1,cs.LG,GHN-QAT: Training Graph Hypernetworks to Predict Quantization-Robust Parameters of Unseen Limited Precision Neural Networks,2023-09-24 23:01:00+00:00,"Graph Hypernetworks (GHN) can predict the parameters of varying unseen CNN
architectures with surprisingly good accuracy at a fraction of the cost of
iterative optimization. Following these successes, preliminary research has
explored the use of GHNs to predict quantization-robust parameters for 8-bit
and 4-bit quantized CNNs. However, this early work leveraged full-precision
float32 training and only quantized for testing. We explore the impact of
quantization-aware training and/or other quantization-based training strategies
on quantized robustness and performance of GHN predicted parameters for
low-precision CNNs. We show that quantization-aware training can significantly
improve quantized accuracy for GHN predicted parameters of 4-bit quantized CNNs
and even lead to greater-than-random accuracy for 2-bit quantized CNNs. These
promising results open the door for future explorations such as investigating
the use of GHN predicted parameters as initialization for further quantized
training of individual CNNs, further exploration of ""extreme bitwidth""
quantization, and mixed precision quantization schemes.",http://arxiv.org/pdf/2309.13773v1
2309.15111v1,cs.LG,SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem,2023-09-26 17:57:44+00:00,"In this work, we consider the optimization process of minibatch stochastic
gradient descent (SGD) on a 2-layer neural network with data separated by a
quadratic ground truth function. We prove that with data drawn from the
$d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y
= -x_ix_j$, it is possible to train to a population error $o(1)$ with $d
\:\text{polylog}(d)$ samples. Our result considers simultaneously training both
layers of the two-layer-neural network with ReLU activations via standard
minibatch SGD on the logistic loss. To our knowledge, this work is the first to
give a sample complexity of $\tilde{O}(d)$ for efficiently learning the XOR
function on isotropic data on a standard neural network with standard training.
Our main technique is showing that the network evolves in two phases: a
$\textit{signal-finding}$ phase where the network is small and many of the
neurons evolve independently to find features, and a $\textit{signal-heavy}$
phase, where SGD maintains and balances the features. We leverage the
simultaneous training of the layers to show that it is sufficient for only a
small fraction of the neurons to learn features, since those neurons will be
amplified by the simultaneous growth of their second layer weights.",http://arxiv.org/pdf/2309.15111v1
2309.15103v1,cs.CV,LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models,2023-09-26 17:52:03+00:00,"This work aims to learn a high-quality text-to-video (T2V) generative model
by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a
highly desirable yet challenging task to simultaneously a) accomplish the
synthesis of visually realistic and temporally coherent videos while b)
preserving the strong creative generation nature of the pre-trained T2I model.
To this end, we propose LaVie, an integrated video generation framework that
operates on cascaded video latent diffusion models, comprising a base T2V
model, a temporal interpolation model, and a video super-resolution model. Our
key insights are two-fold: 1) We reveal that the incorporation of simple
temporal self-attentions, coupled with rotary positional encoding, adequately
captures the temporal correlations inherent in video data. 2) Additionally, we
validate that the process of joint image-video fine-tuning plays a pivotal role
in producing high-quality and creative outcomes. To enhance the performance of
LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M,
consisting of 25 million text-video pairs that prioritize quality, diversity,
and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves
state-of-the-art performance both quantitatively and qualitatively.
Furthermore, we showcase the versatility of pre-trained LaVie models in various
long video generation and personalized video synthesis applications.",http://arxiv.org/pdf/2309.15103v1
2309.15102v1,math.QA,Quantum geodesic flow on the integer lattice line,2023-09-26 17:51:39+00:00,"We use a recent formalism of quantum geodesics in noncommutative geometry to
construct geodesic flow on the infinite chain
$\cdots\bullet$--$\bullet$--$\bullet\cdots$. We find that noncommutative
effects due to the discretisation of the line cause an initially real geodesic
flow amplitude $\psi$ (for which the density is $|\psi|^2$) to become complex.
This has been noted also for other quantum geometries and suggests that the
complex nature of the wave function in quantum mechanics (and the interference
effects that follow) may have its origin in a quantum/discrete nature of
spacetime at the Planck scale.",http://arxiv.org/pdf/2309.15102v1
2309.15101v1,cs.GR,Local Positional Encoding for Multi-Layer Perceptrons,2023-09-26 17:50:37+00:00,"A multi-layer perceptron (MLP) is a type of neural networks which has a long
history of research and has been studied actively recently in computer vision
and graphics fields. One of the well-known problems of an MLP is the capability
of expressing high-frequency signals from low-dimensional inputs. There are
several studies for input encodings to improve the reconstruction quality of an
MLP by applying pre-processing against the input data. This paper proposes a
novel input encoding method, local positional encoding, which is an extension
of positional and grid encodings. Our proposed method combines these two
encoding techniques so that a small MLP learns high-frequency signals by using
positional encoding with fewer frequencies under the lower resolution of the
grid to consider the local position and scale in each grid cell. We demonstrate
the effectiveness of our proposed method by applying it to common 2D and 3D
regression tasks where it shows higher-quality results compared to positional
and grid encodings, and comparable results to hierarchical variants of grid
encoding such as multi-resolution grid encoding with equivalent memory
footprint.",http://arxiv.org/pdf/2309.15101v1
2309.15100v1,physics.app-ph,SuperGaN: Synthesis of NbTiN/GaN/NbTiN Tunnel Junctions,2023-09-26 17:50:17+00:00,"Nb-based circuits have broad applications in quantum-limited photon
detectors, low-noise parametric amplifiers, superconducting digital logic
circuits, and low-loss circuits for quantum computing. The current
state-of-the-art approach for superconductor-insulator-superconductor (SIS)
junction material is the Gurvitch trilayer process based on magnetron
sputtering of Nb electrodes with Al-Oxide or AlN tunnel barriers grown on an Al
overlayer. However, a current limitation of elemental Nb-based circuits is the
low-loss operation of THz circuits operating above the 670 GHz gap frequency of
Nb and operation at higher temperatures for projects with a strict power
budget, such as space-based applications.
  NbTiN is an alternative higher energy gap material and we have previously
reported on the first NbTiN/AlN/NbTiN
superconducting-insulating-superconducting (SIS) junctions with an epitaxially
grown AlN tunnel barrier. One drawback of a directly grown tunnel barrier
compared to thermal oxidation or plasma nitridation is control of the barrier
thickness and uniformity across a substrate, leading to variations in current
density (Jc). Semiconductor barriers with smaller barrier heights enable
thicker tunnel barriers for a given Jc. GaN is an alternative semiconductor
material with a closed-packed Wurtzite crystal structure similar to AlN and it
can be epitaxially grown as a tunnel barrier using the Reactive Bias Target Ion
Beam Deposition (RBTIBD) technique. This work presents the preliminary results
of the first reported high-quality NbTiN/GaN/NbTiN heterojunctions with
underdamped SIS I(V) characteristics.",http://arxiv.org/pdf/2309.15100v1
2309.15094v1,cs.LG,Identifying Simulation Model Through Alternative Techniques for a Medical Device Assembly Process,2023-09-26 17:40:29+00:00,"This scientific paper explores two distinct approaches for identifying and
approximating the simulation model, particularly in the context of the snap
process crucial to medical device assembly. Simulation models play a pivotal
role in providing engineers with insights into industrial processes, enabling
experimentation and troubleshooting before physical assembly. However, their
complexity often results in time-consuming computations.
  To mitigate this complexity, we present two distinct methods for identifying
simulation models: one utilizing Spline functions and the other harnessing
Machine Learning (ML) models. Our goal is to create adaptable models that
accurately represent the snap process and can accommodate diverse scenarios.
Such models hold promise for enhancing process understanding and aiding in
decision-making, especially when data availability is limited.",http://arxiv.org/pdf/2309.15094v1
2309.15088v1,cs.IR,RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models,2023-09-26 17:31:57+00:00,"Researchers have successfully applied large language models (LLMs) such as
ChatGPT to reranking in an information retrieval context, but to date, such
work has mostly been built on proprietary models hidden behind opaque API
endpoints. This approach yields experimental results that are not reproducible
and non-deterministic, threatening the veracity of outcomes that build on such
shaky foundations. To address this significant shortcoming, we present
RankVicuna, the first fully open-source LLM capable of performing high-quality
listwise reranking in a zero-shot setting. Experimental results on the TREC
2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness
comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter
model, although our effectiveness remains slightly behind reranking with GPT-4.
We hope our work provides the foundation for future research on reranking with
modern LLMs. All the code necessary to reproduce our results is available at
https://github.com/castorini/rank_llm.",http://arxiv.org/pdf/2309.15088v1
2309.15087v1,cs.CR,Privacy-preserving and Privacy-attacking Approaches for Speech and Audio -- A Survey,2023-09-26 17:31:35+00:00,"In contemporary society, voice-controlled devices, such as smartphones and
home assistants, have become pervasive due to their advanced capabilities and
functionality. The always-on nature of their microphones offers users the
convenience of readily accessing these devices. However, recent research and
events have revealed that such voice-controlled devices are prone to various
forms of malicious attacks, hence making it a growing concern for both users
and researchers to safeguard against such attacks. Despite the numerous studies
that have investigated adversarial attacks and privacy preservation for images,
a conclusive study of this nature has not been conducted for the audio domain.
Therefore, this paper aims to examine existing approaches for
privacy-preserving and privacy-attacking strategies for audio and speech. To
achieve this goal, we classify the attack and defense scenarios into several
categories and provide detailed analysis of each approach. We also interpret
the dissimilarities between the various approaches, highlight their
contributions, and examine their limitations. Our investigation reveals that
voice-controlled devices based on neural networks are inherently susceptible to
specific types of attacks. Although it is possible to enhance the robustness of
such models to certain forms of attack, more sophisticated approaches are
required to comprehensively safeguard user privacy.",http://arxiv.org/pdf/2309.15087v1
2309.15084v1,cs.CV,The Surveillance AI Pipeline,2023-09-26 17:27:22+00:00,"A rapidly growing number of voices have argued that AI research, and computer
vision in particular, is closely tied to mass surveillance. Yet the direct path
from computer vision research to surveillance has remained obscured and
difficult to assess. This study reveals the Surveillance AI pipeline. We obtain
three decades of computer vision research papers and downstream patents (more
than 20,000 documents) and present a rich qualitative and quantitative
analysis. This analysis exposes the nature and extent of the Surveillance AI
pipeline, its institutional roots and evolution, and ongoing patterns of
obfuscation. We first perform an in-depth content analysis of computer vision
papers and downstream patents, identifying and quantifying key features and the
many, often subtly expressed, forms of surveillance that appear. On the basis
of this analysis, we present a topology of Surveillance AI that characterizes
the prevalent targeting of human data, practices of data transferal, and
institutional data use. We find stark evidence of close ties between computer
vision and surveillance. The majority (68%) of annotated computer vision papers
and patents self-report their technology enables data extraction about human
bodies and body parts and even more (90%) enable data extraction about humans
in general.",http://arxiv.org/pdf/2309.15084v1
2309.15081v1,eess.IV,Challenges of building medical image datasets for development of deep learning software in stroke,2023-09-26 17:23:29+00:00,"Despite the large amount of brain CT data generated in clinical practice, the
availability of CT datasets for deep learning (DL) research is currently
limited. Furthermore, the data can be insufficiently or improperly prepared for
machine learning and thus lead to spurious and irreproducible analyses. This
lack of access to comprehensive and diverse datasets poses a significant
challenge for the development of DL algorithms. In this work, we propose a
complete semi-automatic pipeline to address the challenges of preparing a
clinical brain CT dataset for DL analysis and describe the process of
standardising this heterogeneous dataset. Challenges include handling image
sets with different orientations (axial, sagittal, coronal), different image
types (to view soft tissues or bones) and dimensions, and removing redundant
background. The final pipeline was able to process 5,868/10,659 (45%) CT image
datasets. Reasons for rejection include non-axial data (n=1,920), bone
reformats (n=687), separated skull base/vault images (n=1,226), and
registration failures (n=465). Further format adjustments, including image
cropping, resizing and scaling are also needed for DL processing. Of the axial
scans that were not localisers, bone reformats or split brains, 5,868/6,333
(93%) were accepted, while the remaining 465 failed the registration process.
Appropriate preparation of medical imaging datasets for DL is a costly and
time-intensive process.",http://arxiv.org/pdf/2309.15081v1
2309.15078v1,cond-mat.supr-con,"Structural transitions, octahedral rotations, and electronic properties of $A_3$Ni$_2$O$_7$ rare-earth nickelates under high pressure",2023-09-26 17:18:46+00:00,"Motivated by the recent observation of superconductivity with $T_c \sim 80$ K
in pressurized La3Ni2O7 [Nature 621, 493 (2023)], we explore the structural and
electronic properties in A3Ni2O7 bilayer nickelates (A=La-Lu, Y, Sc) as a
function of hydrostatic pressure (0-150 GPa) from first principles including a
Coulomb repulsion term. At $\sim 20$ GPa, we observe an
orthorhombic-to-tetragonal transition in La$_3$Ni$_2$O$_7$ at variance with
recent x-ray diffraction data, which points to so-far unresolved complexities
at the onset of superconductivity, e.g., charge doping by variations in the
oxygen stoichiometry. We compile a structural phase diagram with particular
emphasis on the $b/a$ ratio, octahedral anisotropy, and octahedral rotations.
Intriguingly, chemical and external pressure emerge as two distinct and
counteracting control parameters. We find unexpected correlations between $T_c$
and the in-plane Ni-O-Ni bond angles for La$_3$Ni$_2$O$_7$. Moreover, two novel
structural phases with significant $c^+$ octahedral rotations and in-plane bond
disproportionations are uncovered for A=Nd-Lu, Y, Sc that exhibit a surprising
pressure-driven electronic reconstruction in the Ni $e_g$ manifold. By
disentangling the involvement of basal versus apical oxygen states at the Fermi
surface, we identify Tb$_3$Ni$_2$O$_7$ as an interesting candidate for
superconductivity at ambient pressure. These results suggest a profound
tunability of the structural and electronic phases in this novel materials
class and are key for a fundamental understanding of the superconductivity
mechanism.",http://arxiv.org/pdf/2309.15078v1
2309.15076v1,stat.ME,Mixture polarization in inter-rater agreement analysis: a Bayesian nonparametric index,2023-09-26 17:15:43+00:00,"In several observational contexts where different raters evaluate a set of
items, it is common to assume that all raters draw their scores from the same
underlying distribution. However, a plenty of scientific works have evidenced
the relevance of individual variability in different type of rating tasks. To
address this issue the intra-class correlation coefficient (ICC) has been used
as a measure of variability among raters within the Hierarchical Linear Models
approach. A common distributional assumption in this setting is to specify
hierarchical effects as independent and identically distributed from a normal
with the mean parameter fixed to zero and unknown variance. The present work
aims to overcome this strong assumption in the inter-rater agreement estimation
by placing a Dirichlet Process Mixture over the hierarchical effects' prior
distribution. A new nonparametric index $\lambda$ is proposed to quantify
raters polarization in presence of group heterogeneity. The model is applied on
a set of simulated experiments and real world data. Possible future directions
are discussed.",http://arxiv.org/pdf/2309.15076v1
2309.15070v1,physics.soc-ph,Temporal criticality,2023-09-26 17:03:19+00:00,"In complex systems, external parameters often determine the phase in which
the system operates, i.e., its macroscopic behavior. For nearly a century,
statistical physics has extensively studied systems' transitions across phases,
(universal) critical exponents, and related dynamical properties. Here we
consider the functionality of systems, notably operations in socio-technical
ones, production in economic ones and possibly information-processing in
biological ones, where timing is of crucial importance. We introduce a stylized
model on temporal networks with the magnitude of delay-mitigating buffers as
the control parameter. The model exhibits {\it temporal criticality}, a novel
form of critical behavior {\it in time}. We characterize fluctuations near
criticality, commonly referred to as ``avalanches'', and identify the
corresponding critical exponents. We show that real-world temporal networks,
too, exhibit temporal criticality. We also explore potential connections with
the Mode-Coupling Theory of glasses and the directed polymer problem.",http://arxiv.org/pdf/2309.15070v1
2309.15068v1,hep-ph,Gluon-gluon fusion contribution to the productions of three gauge bosons at the LHC,2023-09-26 17:00:26+00:00,"Productions of multiple gauge bosons at the LHC are sensitive to triple or
quartic gauge couplings and thus provide a sensitive test for the electroweak
sector of the Standard Model and allow for a probe of new physics. In this work
we calculate the gluon-gluon initiate state contribution to the productions of
three gauge bosons ($Z\gamma\gamma$, $ZZ\gamma$ and $W^+W^-\gamma$) at the LHC,
which is formally part of NNLO effects compared to the LO quark-antiquark
channels corrections. For each process we present the ratio between the
gluon-gluon channels contribution and the quark-antiquark channels
contribution. We found that such a ratio for $Z\gamma\gamma$ ($ZZ\gamma$) is of
the order of $10^{-3}$ ($10^{-4}$), much smaller than the corresponding ratio
for the diboson production due to the decrease of gluon PDF when more particles
appear in the final states. These small ratios imply that gluon-gluon fusion
contribution is phenomenological negligible for the productions of
$Z\gamma\gamma$ and $ZZ\gamma$. However, for $W^+W^-\gamma$ production, the
ratio is about 5\%, which is of the same order of magnitude as the ratio for
$W^+W^-$ production due to the big cancellation between the amplitudes of
quark-antiquark channels. While such an effect can be neglected currently at
the LHC, it may be accessible at the HL-LHC.",http://arxiv.org/pdf/2309.15068v1
2309.15065v1,cs.RO,Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-time Visual Scene Understanding,2023-09-26 16:50:20+00:00,"Versatile and adaptive semantic understanding would enable autonomous systems
to comprehend and interact with their surroundings. Existing fixed-class models
limit the adaptability of indoor mobile and assistive autonomous systems. In
this work, we introduce LEXIS, a real-time indoor Simultaneous Localization and
Mapping (SLAM) system that harnesses the open-vocabulary nature of Large
Language Models (LLMs) to create a unified approach to scene understanding and
place recognition. The approach first builds a topological SLAM graph of the
environment (using visual-inertial odometry) and embeds Contrastive
Language-Image Pretraining (CLIP) features in the graph nodes. We use this
representation for flexible room classification and segmentation, serving as a
basis for room-centric place recognition. This allows loop closure searches to
be directed towards semantically relevant places. Our proposed system is
evaluated using both public, simulated data and real-world data, covering
office and home environments. It successfully categorizes rooms with varying
layouts and dimensions and outperforms the state-of-the-art (SOTA). For place
recognition and trajectory estimation tasks we achieve equivalent performance
to the SOTA, all also utilizing the same pre-trained model. Lastly, we
demonstrate the system's potential for planning.",http://arxiv.org/pdf/2309.15065v1
2309.15064v1,eess.AS,Simultaneously Learning Speaker's Direction and Head Orientation from Binaural Recordings,2023-09-26 16:49:21+00:00,"Estimation of a speaker's direction and head orientation with binaural
recordings can be a critical piece of information in many real-world
applications with emerging `earable' devices, including smart headphones and
AR/VR headsets. However, it requires predicting the mutual head orientations of
both the speaker and the listener, which is challenging in practice. This paper
presents a system for jointly predicting speaker-listener head orientations by
leveraging inherent human voice directivity and listener's head-related
transfer function (HRTF) as perceived by the ear-mounted microphones on the
listener. We propose a convolution neural network model that, given binaural
speech recording, can predict the orientation of both speaker and listener with
respect to the line joining the two. The system builds on the core observation
that the recordings from the left and right ears are differentially affected by
the voice directivity as well as the HRTF. We also incorporate the fact that
voice is more directional at higher frequencies compared to lower frequencies.",http://arxiv.org/pdf/2309.15064v1
2309.15063v1,physics.ao-ph,PARMESAN: Meteorological Timeseries and Turbulence Analysis Backed by Symbolic Mathematics,2023-09-26 16:45:39+00:00,"PARMESAN (the Python Atmospheric Research Package for MEteorological
TimeSeries and Turbulence ANalysis) is a Python package providing common
functionality for atmospheric scientists doing time series or turbulence
analysis. Several meteorological quantities such as potential temperature,
various humidity measures, gas concentrations, wind speed and direction,
turbulence and stability parameters can be calculated. Furthermore, signal
processing functionality such as properly normed variance spectra for frequency
analysis is available. In contrast to existing packages with similar goals, its
routines for physical quantities are derived from symbolic mathematical
expressions, enabling inspection, automatic rearrangement, reuse and
recombination of the underlying equations. Building on this, PARMESAN's
functions as well as their comprehensive parameter documentation are mostly
auto-generated, minimizing human error and effort. In addition,
sensitivity/error propagation analysis is possible as mathematical operations
like derivations can be applied to the underlying equations. Physical
consistency in terms of units and value domains are transparently ensured for
PARMESAN functions. PARMESAN's approach can be reused to simplify
implementation of robust routines in other fields of physics.",http://arxiv.org/pdf/2309.15063v1
2309.15062v1,cond-mat.mes-hall,Fine structure splitting cancellation in highly asymmetric InAs/InP droplet epitaxy quantum dots,2023-09-26 16:45:03+00:00,"We find the single exciton's fine structure splitting (FSS), which splits its
degenerate ground state manifold into singlets, nearly vanishes in highly
asymmetric quantum dots due to the cancellation of splitting effects with
markedly different origin. The dots simulated are those that emerge on top of
etch pits through the droplet epitaxy growth process; these etch pit dots break
square ($C_{4v}$) spatial symmetry, which has been previously associated with
small FSS. Configuration interaction calculations predict a vanishing FSS at a
specific finite etch pit displacement from the center of the dot, for a
structure far from square symmetry. We thus predict that highly asymmetric
quantum dots may still display negligible fine structure splitting, providing
new avenues for high-fidelity generation of indistinguishable, polarization
entangled photon pairs on demand.",http://arxiv.org/pdf/2309.15062v1
2309.15058v1,math.AT,Monoidal Structures in Orthogonal Calculus,2023-09-26 16:39:02+00:00,"Orthogonal Calculus, first developed by Weiss in 1991, provides a calculus of
functors for functors from real inner product spaces to spaces. Many of the
functors to which Orthogonal Calculus has been applied since carry an
additional lax symmetric monoidal structure which has so far been ignored. For
instance, the functor $V \mapsto \text{BO}(V)$ admits maps $$\text{BO}(V)
\times \text{BO}(W) \to \text{BO}(V \oplus W)$$ which determine a lax symmetric
monoidal structure.
  Our first main result, Corollary 4.2.0.2, states that the Taylor
approximations of a lax symmetric monoidal functor are themselves lax symmetric
monoidal. We also study the derivative spectra of lax symmetric monoidal
functors, and prove in Corollary 5.4.0.1 that they admit $O(n)$-equivariant
structure maps of the form $$\Theta^nF \otimes \Theta^nF \to D_{O(n)} \otimes
\Theta^nF$$ where $D_{O(n)} \simeq S^{\text{Ad}_n}$ is the Klein-Spivak
dualising spectrum of the topological group $O(n)$.
  As our proof methods are largely abstract and $\infty$-categorical, we also
formulate Orthogonal Calculus in that language before proving our results.",http://arxiv.org/pdf/2309.15058v1
2309.15053v1,eess.IV,Thalamic nuclei segmentation from T$_1$-weighted MRI: unifying and benchmarking state-of-the-art methods with young and old cohorts,2023-09-26 16:31:53+00:00,"The thalamus and its constituent nuclei are critical for a broad range of
cognitive and sensorimotor processes, and implicated in many neurological and
neurodegenerative conditions. However, the functional involvement and
specificity of thalamic nuclei in human neuroimaging is underappreciated and
not well studied due, in part, to technical challenges of accurately
identifying and segmenting nuclei. This challenge is further exacerbated by a
lack of common nomenclature for comparing segmentation methods. Here, we use
data from healthy young (Human Connectome Project, 100 subjects) and older
healthy adults, plus those with minor cognitive impairment and Alzheimer$'$s
disease (Alzheimer$'$s Disease Neuroimaging Initiative, 540 subjects), to
benchmark four state of the art thalamic segmentation methods for T1 MRI
(FreeSurfer, HIPS-THOMAS, SCS-CNN, and T1-THOMAS) under a single segmentation
framework. Segmentations were compared using overlap and dissimilarity metrics
to the Morel stereotaxic atlas. We also quantified each method$'$s estimation
of thalamic nuclear degeneration across Alzheimer$'$s disease progression, and
how accurately early and late mild cognitive impairment, and Alzheimers disease
could be distinguished from healthy controls. We show that HIPS-THOMAS produced
the most effective segmentations of individual thalamic nuclei and was also
most accurate in discriminating healthy controls from those with mild cognitive
impairment and Alzheimer$'$s disease using individual nucleus volumes. This
work is the first to systematically compare the efficacy of anatomical thalamic
segmentation approaches under a unified nomenclature. We also provide
recommendations of which segmentation method to use for studying the functional
relevance of specific thalamic nuclei, based on their overlap and dissimilarity
with the Morel atlas.",http://arxiv.org/pdf/2309.15053v1
2309.15052v1,hep-ph,Singlet-doublet Dirac fermion dark matter from Peccei-Quinn symmetry,2023-09-26 16:30:16+00:00,"Weakly Interacting Massive Particles (WIMPs) and axions are arguably the most
compelling dark matter (DM) candidates in the literature. Here, we consider a
model where the PQ symmetry solves the strong CP problem, generates radiatively
Dirac neutrino masses, and gives origin to multicomponent dark sector.
Specifically, scotogenic Dirac neutrino masses arise at one-loop level. The
lightest fermionic mediator acts as the second DM candidate due to a residual
$Z_2$ symmetry resulting from the PQ symmetry breaking. The WIMP DM component
resembles the well-known singlet-doublet fermion DM. While the lower WIMP dark
mass region is usually excluded, our model reopens that portion of the
parameter space (for DM masses below $\lesssim 100$ GeV). Therefore, we perform
a phenomenological analysis that addresses the constraints from direct searches
of DM, neutrino oscillation data, and charged lepton flavor violating (LFV)
processes. The model can be tested in future facilities where DM annihilation
into SM particles is searched for by neutrino telescopes.",http://arxiv.org/pdf/2309.15052v1
2309.15047v1,math.FA,Horocyclic harmonic Bergman spaces on homogeneous trees,2023-09-26 16:25:23+00:00,"The main focus of this contribution is on the harmonic Bergman spaces
$\mathcal{B}_{\alpha}^{p}$ on the $q$-homogeneous tree $\mathfrak{X}_q$ endowed
with a family of measures $\sigma_\alpha$ that are constant on the horocycles
tangent to a fixed boundary point and turn out to be doubling with respect to
the corresponding horocyclic Gromov distance. A central role is played by the
reproducing kernel Hilbert space $\mathcal{B}_{\alpha}^{2}$ for which we find a
natural orthonormal basis and formulae for the kernel. We also consider the
atomic Hardy space and the bounded mean oscillation space. Appealing to an
adaptation of Calder\'on-Zygmund theory and to standard boundedness results for
integral operators on $L^p_\alpha$ spaces with H\""ormander-type kernels, we
determine the boundedness properties of the Bergman projection.",http://arxiv.org/pdf/2309.15047v1
2309.15044v1,q-fin.CP,The ATM implied skew in the ADO-Heston model,2023-09-26 16:21:16+00:00,"In this paper similar to [P. Carr, A. Itkin, 2019] we construct another
Markovian approximation of the rough Heston-like volatility model - the
ADO-Heston model. The characteristic function (CF) of the model is derived
under both risk-neutral and real measures which is an unsteady
three-dimensional PDE with some coefficients being functions of the time $t$
and the Hurst exponent $H$. To replicate known behavior of the market implied
skew we proceed with a wise choice of the market price of risk, and then find a
closed form expression for the CF of the log-price and the ATM implied skew.
Based on the provided example, we claim that the ADO-Heston model (which is a
pure diffusion model but with a stochastic mean-reversion speed of the variance
process, or a Markovian approximation of the rough Heston model) is able
(approximately) to reproduce the known behavior of the vanilla implied skew at
small $T$. We conclude that the behavior of our implied volatility skew curve
${\cal S}(T) \propto a(H) T^{b\cdot (H-1/2)}, \, b = const$, is not exactly
same as in rough volatility models since $b \ne 1$, but seems to be close
enough for all practical values of $T$. Thus, the proposed Markovian model is
able to replicate some properties of the corresponding rough volatility model.
Similar analysis is provided for the forward starting options where we found
that the ATM implied skew for the forward starting options can blow-up for any
$s > t$ when $T \to s$. This result, however, contradicts to the observation of
[E. Alos, D.G. Lorite, 2021] that Markovian approximation is not able to catch
this behavior, so remains the question on which one is closer to reality.",http://arxiv.org/pdf/2309.15044v1
2309.15042v1,math.CV,Some results of topological genericity,2023-09-26 16:18:14+00:00,"We show topological genericity for the set of functions in the space X, where
X denotes the intersection of the Hardy spaces H^p with p<1, on the open unit
disc such that the sequence of Taylor coefficients of the function and of all
derivatives of the function are unbounded. Results of similar nature are valid
when the space X is replaced by H^p(0 < p < 1) and by localized versions of
such spaces. Looking at the smaller space A(D) \subseteq H^{\infty} we show
topological genericity for the set of functions in A(D) and of all derivatives
such that the sequence of Taylor coefficients of the function are outside of
(\el)^1. We also show topological genericity for the set of functions in the
space Y, where Y denotes the intersection of the harmonic Hardy spaces h^p with
p<1, whose harmonic conjugate does not belong in any h^q (q > 0)",http://arxiv.org/pdf/2309.15042v1
2309.15038v1,cs.LG,HPCR: Holistic Proxy-based Contrastive Replay for Online Continual Learning,2023-09-26 16:12:57+00:00,"Online continual learning (OCL) aims to continuously learn new data from a
single pass over the online data stream. It generally suffers from the
catastrophic forgetting issue. Existing replay-based methods effectively
alleviate this issue by replaying part of old data in a proxy-based or
contrastive-based replay manner. In this paper, we conduct a comprehensive
analysis of these two replay manners and find they can be complementary.
Inspired by this finding, we propose a novel replay-based method called
proxy-based contrastive replay (PCR), which replaces anchor-to-sample pairs
with anchor-to-proxy pairs in the contrastive-based loss to alleviate the
phenomenon of forgetting. Based on PCR, we further develop a more advanced
method named holistic proxy-based contrastive replay (HPCR), which consists of
three components. The contrastive component conditionally incorporates
anchor-to-sample pairs to PCR, learning more fine-grained semantic information
with a large training batch. The second is a temperature component that
decouples the temperature coefficient into two parts based on their impacts on
the gradient and sets different values for them to learn more novel knowledge.
The third is a distillation component that constrains the learning process to
keep more historical knowledge. Experiments on four datasets consistently
demonstrate the superiority of HPCR over various state-of-the-art methods.",http://arxiv.org/pdf/2309.15038v1
2309.15035v1,math.AC,On the Reduced Gröbner Bases of Blockwise Determinantal Ideals,2023-09-26 16:03:36+00:00,"Blockwise determinantal ideals are those generated by the union of all the
minors of specified sizes in certain blocks of a generic matrix, and they are
the natural generalization of many existing determinantal ideals like the
Schubert and ladder ones. In this paper we establish several criteria to verify
whether the Gr\""obner bases of blockwise determinantal ideals with respect to
(anti-)diagonal term orders are minimal or reduced. In particular, for Schubert
determinantal ideals, while all the elusive minors form the reduced Gr\""obner
bases when the defining permutations are vexillary, in the non-vexillary case
we derive an explicit formula for computing the reduced Gr\""obner basis from
elusive minors which avoids all algebraic operations. The fundamental
properties of being normal and strong for W-characteristic sets and
characteristic pairs, which are heavily connected to the reduced Gr\""obner
bases, of Schubert determinantal ideals are also proven.",http://arxiv.org/pdf/2309.15035v1
2309.15034v1,quant-ph,Measurement-induced phase transition in a single-body tight-binding model,2023-09-26 16:03:09+00:00,"We study the statistical properties of a single free quantum particle
evolving coherently on a discrete lattice in $d$ spatial dimensions where every
lattice site is additionally subject to continuous measurement of the
occupation number. Using perturbative renormalization group (RG) analysis, we
show that the systems undergoes a Measurement-induced Phase Transition (MiPT)
for $d>2$ from a $\textit{delocalized}$ to a $\textit{localized}$ phase as the
measurement strength $\gamma$ is increased beyond a critical value
$\gamma_{c}$. In the language of surface growth, the delocalized phase
corresponds to a $\textit{rough}$ phase while the localized phase corresponds
to a $\textit{smooth}$ phase. We support our analytical computations with
numerical analysis which are in qualitative and quantitative agreement with the
theory.",http://arxiv.org/pdf/2309.15034v1
2309.15030v1,cs.IT,Quadratic Detection in Noncoherent Massive SIMO Systems over Correlated Channels,2023-09-26 16:00:05+00:00,"With the goal of enabling ultrareliable and low-latency wireless
communications for industrial internet of things (IIoT), this paper studies the
use of energy-based modulations in noncoherent massive single input multiple
output (SIMO) systems. We consider a one-shot communication over a channel with
correlated Rayleigh fading and colored Gaussian noise. We first provide a
theoretical analysis on the limitations of non-negative pulse-amplitude
modulation (PAM) in systems of this kind, based on maximum likelihood
detection. The existence of a fundamental error floor at high signal-to-noise
ratio (SNR) regimes is proved for constellations with more than two energy
levels, when no (statistical) channel state information is available at the
transmitter. In the main body of the paper, we present a design framework for
quadratic detectors that generalizes the widely-used energy detector, to better
exploit the statistical knowledge of the channel. This allows us to design
receivers optimized according to information-theoretic criteria that exhibit
lower error rates at moderate and high SNR. We subsequently derive an analytic
approximation for the error probability of a general class of quadratic
detectors in the large array regime. Finally, we introduce an improved
reception scheme based on the combination of quadratic detectors and assess its
capabilities numerically.",http://arxiv.org/pdf/2309.15030v1
2309.15026v1,cs.CC,Instance complexity of Boolean functions,2023-09-26 15:56:14+00:00,"In the area of query complexity of Boolean functions, the most widely studied
cost measure of an algorithm is the worst-case number of queries made by it on
an input. Motivated by the most natural cost measure studied in online
algorithms, the competitive ratio, we consider a different cost measure for
query algorithms for Boolean functions that captures the ratio of the cost of
the algorithm and the cost of an optimal algorithm that knows the input in
advance. The cost of an algorithm is its largest cost over all inputs.
Grossman, Komargodski and Naor [ITCS'20] introduced this measure for Boolean
functions, and dubbed it instance complexity. Grossman et al. showed, among
other results, that monotone Boolean functions with instance complexity 1 are
precisely those that depend on one or two variables.
  We complement the above-mentioned result of Grossman et al. by completely
characterizing the instance complexity of symmetric Boolean functions. As a
corollary we conclude that the only symmetric Boolean functions with instance
complexity 1 are the Parity function and its complement. We also study the
instance complexity of some graph properties like Connectivity and k-clique
containment.
  In all the Boolean functions we study above, and those studied by Grossman et
al., the instance complexity turns out to be the ratio of query complexity to
minimum certificate complexity. It is a natural question to ask if this is the
correct bound for all Boolean functions. We show a negative answer in a very
strong sense, by analyzing the instance complexity of the Greater-Than and
Odd-Max-Bit functions. We show that the above-mentioned ratio is linear in the
input size for both of these functions, while we exhibit algorithms for which
the instance complexity is a constant.",http://arxiv.org/pdf/2309.15026v1
2309.15024v1,cs.SD,Synthia's Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio,2023-09-26 15:46:06+00:00,"Despite significant advancements in deep learning for vision and natural
language, unsupervised domain adaptation in audio remains relatively
unexplored. We, in part, attribute this to the lack of an appropriate benchmark
dataset. To address this gap, we present Synthia's melody, a novel audio data
generation framework capable of simulating an infinite variety of 4-second
melodies with user-specified confounding structures characterised by musical
keys, timbre, and loudness. Unlike existing datasets collected under
observational settings, Synthia's melody is free of unobserved biases, ensuring
the reproducibility and comparability of experiments. To showcase its utility,
we generate two types of distribution shifts-domain shift and sample selection
bias-and evaluate the performance of acoustic deep learning models under these
shifts. Our evaluations reveal that Synthia's melody provides a robust testbed
for examining the susceptibility of these models to varying levels of
distribution shift.",http://arxiv.org/pdf/2309.15024v1
2309.15020v1,gr-qc,GWSpace: a multi-mission science data simulator for space-based gravitational wave detection,2023-09-26 15:40:53+00:00,"Space-based gravitational wave detectors such as TianQin, LISA, and TaiJi
have the potential to outperform themselves through joint observation. To
achieve this, it is desirable to practice joint data analysis in advance on
simulated data that encodes the intrinsic correlation among the signals found
in different detectors that operate simultaneously. In this paper, we introduce
\texttt{GWSpace}, a package that can simulate the joint detection data from
TianQin, LISA, and TaiJi. The software is not a groundbreaking work that starts
from scratch. Rather, we use as many open-source resources as possible,
tailoring them to the needs of simulating the multi-mission science data and
putting everything into a ready-to-go and easy-to-use package. We shall
describe the main components, the construction, and a few examples of
application of the package. A common coordinate system, namely the Solar System
Barycenter (SSB) coordinate system, is utilized to calculate spacecraft orbits
for all three missions. The paper also provides a brief derivation of the
detection process and outlines the general waveform of sources detectable by
these detectors.",http://arxiv.org/pdf/2309.15020v1
2309.15016v1,cs.CL,Question-Answering Approach to Evaluate Legal Summaries,2023-09-26 15:36:29+00:00,"Traditional evaluation metrics like ROUGE compare lexical overlap between the
reference and generated summaries without taking argumentative structure into
account, which is important for legal summaries. In this paper, we propose a
novel legal summarization evaluation framework that utilizes GPT-4 to generate
a set of question-answer pairs that cover main points and information in the
reference summary. GPT-4 is then used to generate answers based on the
generated summary for the questions from the reference summary. Finally, GPT-4
grades the answers from the reference summary and the generated summary. We
examined the correlation between GPT-4 grading with human grading. The results
suggest that this question-answering approach with GPT-4 can be a useful tool
for gauging the quality of the summary.",http://arxiv.org/pdf/2309.15016v1
2309.15013v1,cs.CL,Updated Corpora and Benchmarks for Long-Form Speech Recognition,2023-09-26 15:32:09+00:00,"The vast majority of ASR research uses corpora in which both the training and
test data have been pre-segmented into utterances. In most real-word ASR
use-cases, however, test audio is not segmented, leading to a mismatch between
inference-time conditions and models trained on segmented utterances. In this
paper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and
VoxPopuli-en - with updated transcription and alignments to enable their use
for long-form ASR research. We use these reconstituted corpora to study the
train-test mismatch problem for transducers and attention-based
encoder-decoders (AEDs), confirming that AEDs are more susceptible to this
issue. Finally, we benchmark a simple long-form training for these models,
showing its efficacy for model robustness under this domain shift.",http://arxiv.org/pdf/2309.15013v1
2309.15012v1,physics.flu-dyn,Synchrotron X-ray phase-contrast imaging of ultrasonic drop atomization,2023-09-26 15:29:07+00:00,"Ultrasonic atomization is employed to generate size-controllable droplets for
a variety of applications. Here, we minimize the number of parameters dictating
the process by studying the atomization of a single drop pending from an
ultrasonic horn. Spatiotemporally resolved X-ray phase-contrast imaging
measurements show that the number-median sizes of the ejected droplets can be
predicted by the linear Navier-Stokes equations, signifying that the size
distribution is controlled by the fluid properties and the driving frequency.
Experiments with larger pendant water drops indicate that the fluid-structure
interaction plays a pivotal role in determining the ejection onset of the
pendant drop. The atomization of viscoelastic drops is dictated by extended
ligament formation, entrainment of air, and ejection of drop-encapsulated
bubbles. Existing scaling laws are used to explain the required higher input
amplitudes for the complete atomization of viscoelastic drops as compared to
inviscid drops. Finally, we elucidate the differences between capillary
wave-based and cavitation-based atomization and show that inducing cavitation
and strong bubble oscillations quickens the onset of daughter drop ejection but
impedes their size control.",http://arxiv.org/pdf/2309.15012v1
2309.14993v1,astro-ph.CO,Induced gravitational waves and baryon asymmetry fluctuations from primordial black hole formation,2023-09-26 15:02:21+00:00,"We consider black hole formation due to the gravitational collapse produced
by large density fluctuations during an epoch of reheating with a stiff
equation of state and calculate the induced gravitational wave spectrum. By
considering the existing bounds on the total energy density of gravitational
waves today, we find constraints on the parameter space of this scenario. We
also calculate the lepton asymmetry generated by metric perturbations via the
chiral gravitational anomaly present in the Standard Model and find that, once
the electroweak sphaleron processes have taken place, the large spectrum of
scalar perturbations responsible for black hole formation induces a peak in the
baryon asymmetry fluctuations on small scales.",http://arxiv.org/pdf/2309.14993v1
2309.14992v1,cs.SE,Exploring ChatGPT Approach to Bidirectional Traceability Problem between Design Models and Code,2023-09-26 15:01:48+00:00,"This study explores the capabilities of Large Language Models (LLMs),
particularly OpenAI's ChatGPT, in addressing the challenges associated with
software modeling, explicitly focusing on the bidirectional traceability
problem between design models and code. The objective of this study is to
demonstrate the proficiency of ChatGPT in understanding and integrating
specific requirements into design models and code and its potential to offer
solutions to the bidirectional traceability problem through a case study. The
findings indicate that ChatGPT is capable of generating design models and code
from natural language requirements, thereby bridging the gap between these
requirements and software modeling. Despite its limitations in suggesting a
specific method to resolve the problem using ChatGPT itself, it exhibited the
capacity to provide corrections to be consistent between design models and
code. As a result, the study concludes that achieving bidirectional
traceability between design models and code is feasible using ChatGPT.",http://arxiv.org/pdf/2309.14992v1
2309.14989v1,cs.LG,Tempo Adaption in Non-stationary Reinforcement Learning,2023-09-26 15:01:21+00:00,"We first raise and tackle ``time synchronization'' issue between the agent
and the environment in non-stationary reinforcement learning (RL), a crucial
factor hindering its real-world applications. In reality, environmental changes
occur over wall-clock time ($\mathfrak{t}$) rather than episode progress ($k$),
where wall-clock time signifies the actual elapsed time within the fixed
duration $\mathfrak{t} \in [0, T]$. In existing works, at episode $k$, the
agent rollouts a trajectory and trains a policy before transitioning to episode
$k+1$. In the context of the time-desynchronized environment, however, the
agent at time $\mathfrak{t}_k$ allocates $\Delta \mathfrak{t}$ for trajectory
generation and training, subsequently moves to the next episode at
$\mathfrak{t}_{k+1}=\mathfrak{t}_{k}+\Delta \mathfrak{t}$. Despite a fixed
total episode ($K$), the agent accumulates different trajectories influenced by
the choice of \textit{interaction times}
($\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfrak{t}_K$), significantly impacting
the sub-optimality gap of policy. We propose a Proactively Synchronizing Tempo
(ProST) framework that computes optimal $\{
\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfrak{t}_K \} (= \{ \mathfrak{t}
\}_{1:K})$. Our main contribution is that we show optimal $\{ \mathfrak{t}
\}_{1:K}$ trades-off between the policy training time (agent tempo) and how
fast the environment changes (environment tempo). Theoretically, this work
establishes an optimal $\{ \mathfrak{t} \}_{1:K}$ as a function of the degree
of the environment's non-stationarity while also achieving a sublinear dynamic
regret. Our experimental evaluation on various high dimensional non-stationary
environments shows that the ProST framework achieves a higher online return at
optimal $\{ \mathfrak{t} \}_{1:K}$ than the existing methods.",http://arxiv.org/pdf/2309.14989v1
2309.14985v1,cs.PL,Types and Semantics for Extensible Data Types (Extended Version),2023-09-26 14:57:02+00:00,"Developing and maintaining software commonly requires (1) adding new data
type constructors to existing applications, but also (2) adding new functions
that work on existing data. Most programming languages have native support for
defining data types and functions in a way that supports either (1) or (2), but
not both. This lack of native support makes it difficult to use and extend
libraries. A theoretically well-studied solution is to define data types and
functions using initial algebra semantics. While it is possible to encode this
solution in existing programming languages, such encodings add syntactic and
interpretive overhead, and commonly fail to take advantage of the map and fold
fusion laws of initial algebras which compilers could exploit to generate more
efficient code. A solution to these is to provide native support for initial
algebra semantics. In this paper, we develop such a solution and present a type
discipline and core calculus for a language with native support for initial
algebra semantics.",http://arxiv.org/pdf/2309.14985v1
2309.14984v1,cs.IR,The Role of Document Embedding in Research Paper Recommender Systems: To Breakdown or to Bolster Disciplinary Borders?,2023-09-26 14:56:56+00:00,"In the extensive recommender systems literature, novelty and diversity have
been identified as key properties of useful recommendations. However, these
properties have received limited attention in the specific sub-field of
research paper recommender systems. In this work, we argue for the importance
of offering novel and diverse research paper recommendations to scientists.
This approach aims to reduce siloed reading, break down filter bubbles, and
promote interdisciplinary research. We propose a novel framework for evaluating
the novelty and diversity of research paper recommendations that leverages
methods from network analysis and natural language processing. Using this
framework, we show that the choice of representational method within a larger
research paper recommendation system can have a measurable impact on the nature
of downstream recommendations, specifically on their novelty and diversity. We
introduce a novel paper embedding method, which we demonstrate offers more
innovative and diverse recommendations without sacrificing precision, compared
to other state-of-the-art baselines.",http://arxiv.org/pdf/2309.14984v1
2309.14980v1,quant-ph,Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks,2023-09-26 14:54:50+00:00,"Quantum neural networks (QNNs) have been a promising framework in pursuing
near-term quantum advantage in various fields, where many applications can be
viewed as learning a quantum state that encodes useful data. As a quantum
analog of probability distribution learning, quantum state learning is
theoretically and practically essential in quantum machine learning. In this
paper, we develop a no-go theorem for learning an unknown quantum state with
QNNs even starting from a high-fidelity initial state. We prove that when the
loss value is lower than a critical threshold, the probability of avoiding
local minima vanishes exponentially with the qubit count, while only grows
polynomially with the circuit depth. The curvature of local minima is
concentrated to the quantum Fisher information times a loss-dependent constant,
which characterizes the sensibility of the output state with respect to
parameters in QNNs. These results hold for any circuit structures,
initialization strategies, and work for both fixed ansatzes and adaptive
methods. Extensive numerical simulations are performed to validate our
theoretical results. Our findings place generic limits on good initial guesses
and adaptive methods for improving the learnability and scalability of QNNs,
and deepen the understanding of prior information's role in QNNs.",http://arxiv.org/pdf/2309.14980v1
2309.14978v1,physics.flu-dyn,Salinity-Dependent Interfacial Phenomena Towards Hydrovoltaic Device Optimization,2023-09-26 14:54:11+00:00,"Evaporation-driven fluid flow in porous or nanostructured materials has
recently opened a new paradigm for renewable energy generation. Despite recent
progress, major fundamental questions remain regarding the interfacial
phenomena governing these so-called hydrovoltaic (HV) devices. Together with
the lack of modelling tools, this limits the performance and application range
of this emerging technology. By leveraging ordered arrays of Silicon
nanopillars (NP) and developing a quantitative multiphysics model to study
their HV response across a wide parameter space, this work reveals the complex
interplay of surface-charge, liquid properties, and geometrical parameters,
including previously unexplored electrokinetic interactions. Notably, we find
that ion-concentration-dependent surface charge, together with ion mobility,
dictates multiple local maxima in open circuit voltage, with optimal conditions
deviating from conventional low-concentration expectations. Additionally,
assessing the HV response up to molar concentrations, we provide unique
evidence of ion adsorption and charge inversion for a number of monovalent
cations. This effect interestingly enables the operation of HV devices even at
such high concentrations. Finally, we highlight that, beyond electrokinetic
parameters, geometrical asymmetries in the device structure generate an
electrostatic potential that augments HV performance. Overall, our work, which
lies in between single nanochannel studies and macro-scale porous system
characterization, demonstrates that evaporation-driven HV devices can operate
across a wide range of salinities, with optimal operating conditions being
dictated by distinct interfacial phenomena. Thus it offers crucial insight and
a design tool for enhancing the performance of evaporation-driven HV devices
and enables their broader applicability across the salinity scale of natural
and processed waters.",http://arxiv.org/pdf/2309.14978v1
2309.14977v1,math.FA,Weak equals strong L2 regularity for partial tangential traces on Lipschitz domains,2023-09-26 14:53:01+00:00,"We investigate the boundary trace operators that naturally correspond to
$\mathrm{H}(\operatorname{curl},\Omega)$, namely the tangential and twisted
tangential trace, where $\Omega \subseteq \mathbb{R}^{3}$. In particular we
regard partial tangential traces, i.e., we look only on a subset $\Gamma$ of
the boundary $\partial\Omega$. We assume both $\Omega$ and $\Gamma$ to be
strongly Lipschitz. We define the space of all
$\mathrm{H}(\operatorname{curl},\Omega)$ fields that possess a $\mathrm{L}^{2}$
tangential trace in a weak sense and show that the set of all smooth fields is
dense in that space, which is a generalization of \cite{BeBeCoDa97}. This is
especially important for Maxwell's equation with mixed boundary condition as we
answer the open problem by Weiss and Staffans in \cite[Sec.~5]{WeSt13} for
strongly Lipschitz pairs.",http://arxiv.org/pdf/2309.14977v1
2309.14971v1,cs.NI,Minimizing Energy Consumption for 5G NR Beam Management for RedCap Devices,2023-09-26 14:44:08+00:00,"In 5G New Radio (NR), beam management entails periodic and continuous
transmission and reception of control signals in the form of synchronization
signal blocks (SSBs), used to perform initial access and/or channel estimation.
However, this procedure demands continuous energy consumption, which is
particularly challenging to handle for low-cost, low-complexity, and
battery-constrained devices, such as RedCap devices to support mid-market
Internet of Things (IoT) use cases. In this context, this work aims at reducing
the energy consumption during beam management for RedCap devices, while
ensuring that the desired Quality of Service (QoS) requirements are met. To do
so, we formalize an optimization problem in an Indoor Factory (InF) scenario to
select the best beam management parameters, including the beam update
periodicity and the beamwidth, to minimize energy consumption based on users'
distribution and their speed. The analysis yields the regions of feasibility,
i.e., the upper limit(s) on the beam management parameters for RedCap devices,
that we use to provide design guidelines accordingly.",http://arxiv.org/pdf/2309.14971v1
2309.14967v1,cs.CV,A novel approach for holographic 3D content generation without depth map,2023-09-26 14:37:31+00:00,"In preparation for observing holographic 3D content, acquiring a set of RGB
color and depth map images per scene is necessary to generate
computer-generated holograms (CGHs) when using the fast Fourier transform (FFT)
algorithm. However, in real-world situations, these paired formats of RGB color
and depth map images are not always fully available. We propose a deep
learning-based method to synthesize the volumetric digital holograms using only
the given RGB image, so that we can overcome environments where RGB color and
depth map images are partially provided. The proposed method uses only the
input of RGB image to estimate its depth map and then generate its CGH
sequentially. Through experiments, we demonstrate that the volumetric hologram
generated through our proposed model is more accurate than that of competitive
models, under the situation that only RGB color data can be provided.",http://arxiv.org/pdf/2309.14967v1
2309.14965v1,astro-ph.CO,A comprehensive forecast for cosmological parameter estimation using joint observations of gravitational-wave standard sirens and short $γ$-ray bursts,2023-09-26 14:35:03+00:00,"In the third-generation (3G) gravitational-wave (GW) detector era, the
multi-messenger GW observation for binary neutron star (BNS) merger events can
exert great impacts on exploring the cosmic expansion history. In this work, we
comprehensively explore the potential of 3G GW standard siren observations in
cosmological parameter estimations by considering the 3G GW detectors and the
future short $\gamma$-ray burst (GRB) detector THESEUS-like telescope joint
observations. Based on the 10-year observation of different detection
strategies, we predict that the numbers of detectable GW-GRB events are 277-685
with the redshifts $z<4$ and the inclination angles $\iota<17^{\circ}$. For the
cosmological analysis, we consider five typical dark energy models, i.e., the
$\Lambda$CDM, $w$CDM, $w_0w_a$CDM models, and interacting dark energy (IDE)
models (I$\Lambda$CDM and I$w$CDM). We find that GW can tightly constrain the
Hubble constant with precisions of $0.09\%$-$0.37\%$, but perform not well in
constraining other cosmological parameters. Fortunately, GW could effectively
break the cosmological parameter degeneracies generated by the mainstream EM
observations, CMB+BAO+SN (CBS). When combining the mock GW data with the CBS
data, CBS+GW can tightly constrain the equation of state parameter of dark
energy $w$ with a precision of $1.36\%$, close to the standard of precision
cosmology. Meanwhile, the addition of GW to CBS could improve constraints on
cosmological parameters by $35.3\%$-$92.0\%$. In conclusion, GW standard siren
observations from 3G GW detectors could play a crucial role in helping solve
the Hubble tension and probe the fundamental nature of dark energy.",http://arxiv.org/pdf/2309.14965v1
2309.14952v1,cond-mat.supr-con,On the interpretation of flux trapping experiments in hydrides,2023-09-26 14:09:53+00:00,"In Ref. [1], Minkov et al. reported measurements of the magnetic moment that
remains after a magnetic field is turned on and then turned off for hydride
materials under high pressure in a diamond anvil cell. In Refs. [2,3], Minkov
et al. reported magnetization measurements on the same samples as a function of
applied magnetic field. Here we argue that the latter indicate that the signal
measured in the former does not provide evidence for superconductivity in these
samples. Instead, the measured signal likely originates in ferromagnetism of
either the sample or the background.",http://arxiv.org/pdf/2309.14952v1
2309.14948v1,stat.AP,Functional zoning of biodiversity profiles,2023-09-26 14:04:10+00:00,"Spatial mapping of biodiversity is crucial to investigate spatial variations
in natural communities. Several indices have been proposed in the literature to
represent biodiversity as a single statistic. However, these indices only
provide information on individual dimensions of biodiversity, thus failing to
grasp its complexity comprehensively. Consequently, relying solely on these
single indices can lead to misleading conclusions about the actual state of
biodiversity. In this work, we focus on biodiversity profiles, which provide a
more flexible framework to express biodiversity through non-negative and convex
curves, which can be analyzed by means of functional data analysis. By treating
the whole curves as single entities, we propose to achieve a functional zoning
of the region of interest by means of a penalized model-based clustering
procedure. This provides a spatial clustering of the biodiversity profiles,
which is useful for policy-makers both for conserving and managing natural
resources and revealing patterns of interest. Our approach is discussed through
the analysis of Harvard Forest Data, which provides information on the spatial
distribution of woody stems within a plot of the Harvard Forest.",http://arxiv.org/pdf/2309.14948v1
2309.14945v1,cs.RO,Integration of Large Language Models within Cognitive Architectures for Autonomous Robots,2023-09-26 14:00:25+00:00,"The usage of Large Language Models (LLMs) has increased recently, not only
due to the significant improvements in their accuracy but also because of the
use of the quantization that allows running these models without intense
hardware requirements. As a result, the LLMs have proliferated. It implies the
creation of a great variety of LLMs with different capabilities. This way, this
paper proposes the integration of LLMs in cognitive architectures for
autonomous robots. Specifically, we present the design, development and
deployment of the llama\_ros tool that allows the easy use and integration of
LLMs in ROS 2-based environments, afterward integrated with the
state-of-the-art cognitive architecture MERLIN2 for updating a PDDL-based
planner system. This proposal is evaluated quantitatively and qualitatively,
measuring the impact of incorporating the LLMs in the cognitive architecture.",http://arxiv.org/pdf/2309.14945v1
2309.14938v1,cs.IR,Modeling Multi-aspect Preferences and Intents for Multi-behavioral Sequential Recommendation,2023-09-26 13:48:38+00:00,"Multi-behavioral sequential recommendation has recently attracted increasing
attention. However, existing methods suffer from two major limitations.
Firstly, user preferences and intents can be described in fine-grained detail
from multiple perspectives; yet, these methods fail to capture their
multi-aspect nature. Secondly, user behaviors may contain noises, and most
existing methods could not effectively deal with noises. In this paper, we
present an attentive recurrent model with multiple projections to capture
Multi-Aspect preferences and INTents (MAINT in short). To extract multi-aspect
preferences from target behaviors, we propose a multi-aspect projection
mechanism for generating multiple preference representations from multiple
aspects. To extract multi-aspect intents from multi-typed behaviors, we propose
a behavior-enhanced LSTM and a multi-aspect refinement attention mechanism. The
attention mechanism can filter out noises and generate multiple intent
representations from different aspects. To adaptively fuse user preferences and
intents, we propose a multi-aspect gated fusion mechanism. Extensive
experiments conducted on real-world datasets have demonstrated the
effectiveness of our model.",http://arxiv.org/pdf/2309.14938v1
2309.14935v1,physics.atom-ph,Circular dichroism in angular distribution of electron-hydrogen scattering in a two-color bicircular laser field,2023-09-26 13:47:04+00:00,"We study the origin of dichroic effects in elastic scattering of high energy
electrons by hydrogen atoms in the presence of a two-color bicircular laser
field of commensurate frequencies, in the domain of moderate intensities below
10 TW/cm2 . We use a semiperturbative approach in which the interaction of the
hydrogen atom with the laser field is treated in second-order perturbation
theory, while the interaction of the projectile electron with the laser field
is described by Gordon- Volkov wave functions. An analytical formula of
circular dichroism in the angular distribution of scattered electrons is
derived in the weak-field domain for a two-color laser field that is a
combination of the fundamental and its third harmonic. A comparison between the
two-photon differential cross sections for two-color co- and counterrotating
circularly polarized laser fields is made and the effect of the intensity ratio
of the monochromatic field components on the circular dichroism is
investigated. The dichroic effect in the angular distribution of scattered
electrons for two-photon absorption is analyzed as a function of the scattering
and azimuthal angles. We show that the two-color bicircular laser field can
induce a strong circular dichroism in the angular distribution of scattered
electrons at small scattering angles where the atomic dressing effects are
important, as well at larger scattering angles. At small scattering angles we
demonstrate that the dichroic effect for two-photon transitions can be
predicted under the following conditions: the scattering process is treated in
fist-order Born approximation and the dressing of the atomic states by the
laser field is carried out at least in first-order time-dependent perturbation
theory.",http://arxiv.org/pdf/2309.14935v1
2309.14930v1,astro-ph.CO,Large numbers in the Universe and the origin of the Cosmic magnetic field,2023-09-26 13:36:07+00:00,"The origin of the seeds of cosmic magnetic fields is still a mystery. Using
the required amplitude of seed magnetic fields before any astrophysical dynamo
process takes place, and the properties of a primeval Milky Way like galaxy, we
estimate that the required charge imbalance that would produce it is consistent
with $1$ every $\sim10^{39}$. This imbalance coincides with the ratio of
coulomb to gravitational potentials between protons and electrons, which would
allow either particle with such an imbalance to remain bound to the galaxy and
produce the seed. We show that this charge imbalance could be produced by
Poisson noise in the number of positive and negative charges left over by
Inflation, and that it could be longlived after decoupling. Furthermore, we
show that the same principle applies to the maximum charge that a stellar
remnant black hole can extract from the surrounding plasma in primeval galaxies
after the deaths of the first massive stars, and that their dynamics would also
allow them to produce the magnetic field seed. This letter describes these
remarkable coincidences that point to simple, compelling solutions for this
problem.",http://arxiv.org/pdf/2309.14930v1
2309.14928v1,cs.CV,Noise-Tolerant Unsupervised Adapter for Vision-Language Models,2023-09-26 13:35:31+00:00,"Recent advances in large-scale vision-language models have achieved very
impressive performance in various zero-shot image classification tasks. While
prior studies have demonstrated significant improvements by introducing
few-shot labelled target samples, they still require labelling of target
samples, which greatly degrades their scalability while handling various visual
recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that
allows learning superior target models with few-shot unlabelled target samples.
NtUA works as a key-value cache that formulates visual features and predicted
pseudo-labels of the few-shot unlabelled target samples as key-value pairs. It
consists of two complementary designs. The first is adaptive cache formation
that combats pseudo-label noises by weighting the key-value pairs according to
their prediction confidence. The second is pseudo-label rectification, which
corrects both pair values (i.e., pseudo-labels) and cache weights by leveraging
knowledge distillation from large-scale vision language models. Extensive
experiments show that NtUA achieves superior performance consistently across
multiple widely adopted benchmarks.",http://arxiv.org/pdf/2309.14928v1
2309.14923v1,eess.SP,ML-based PBCH symbol detection and equalization for 5G Non-Terrestrial Networks,2023-09-26 13:32:18+00:00,"This paper delves into the application of Machine Learning (ML) techniques in
the realm of 5G Non-Terrestrial Networks (5G-NTN), particularly focusing on
symbol detection and equalization for the Physical Broadcast Channel (PBCH). As
5G-NTN gains prominence within the 3GPP ecosystem, ML offers significant
potential to enhance wireless communication performance. To investigate these
possibilities, we present ML-based models trained with both synthetic and real
data from a real 5G over-the-satellite testbed. Our analysis includes examining
the performance of these models under various Signal-to-Noise Ratio (SNR)
scenarios and evaluating their effectiveness in symbol enhancement and channel
equalization tasks. The results highlight the ML performance in controlled
settings and their adaptability to real-world challenges, shedding light on the
potential benefits of the application of ML in 5G-NTN.",http://arxiv.org/pdf/2309.14923v1
2309.14922v1,eess.AS,Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference,2023-09-26 13:30:58+00:00,"Attention-based encoder-decoder models with autoregressive (AR) decoding have
proven to be the dominant approach for automatic speech recognition (ASR) due
to their superior accuracy. However, they often suffer from slow inference.
This is primarily attributed to the incremental calculation of the decoder.
This work proposes a partially AR framework, which employs segment-level
vectorized beam search for improving the inference speed of an ASR model based
on the hybrid connectionist temporal classification (CTC) attention-based
architecture. It first generates an initial hypothesis using greedy CTC
decoding, identifying low-confidence tokens based on their output
probabilities. We then utilize the decoder to perform segment-level vectorized
beam search on these tokens, re-predicting in parallel with minimal decoder
calculations. Experimental results show that our method is 12 to 13 times
faster in inference on the LibriSpeech corpus over AR decoding whilst
preserving high accuracy.",http://arxiv.org/pdf/2309.14922v1
2309.14918v1,cs.CY,Ethical Challenges in Gamified Education Research and Development: An Umbrella Review and Potential Directions,2023-09-26 13:23:50+00:00,"Gamification is a technological, economic, cultural, and societal development
toward promoting a more game-like reality. As this emergent phenomenon has been
gradually consolidated into our daily lives, especially in educational
settings, many scholars and practitioners face a major challenge ahead: how to
understand and mitigate the unethical impacts of gamification when researching
and developing such educational technologies? Thus, this study explores ethical
challenges in gamified educational applications and proposes potential
solutions to address them based on an umbrella review. After analysing
secondary studies, this study details and proposes recommendations on
addressing some ethical challenges in gamified education, such as power
dynamics and paternalism, lack of voluntarity and confidentiality, cognitive
manipulation, and social comparison. Research and development decision-making
processes affected by such challenges are also elaborated, and potential
actions to mitigate their effects in gamification planning, conducting and
communication are further introduced. Thus, this chapter provides an
understanding of ethical challenges posed by the literature in gamified
education and a set of guidelines for future research and development.",http://arxiv.org/pdf/2309.14918v1
2309.14913v1,cond-mat.dis-nn,Robustness of the Random Language Model,2023-09-26 13:14:35+00:00,"The Random Language Model (De Giuli 2019) is an ensemble of stochastic
context-free grammars, quantifying the syntax of human and computer languages.
The model suggests a simple picture of first language learning as a type of
annealing in the vast space of potential languages. In its simplest
formulation, it implies a single continuous transition to grammatical syntax,
at which the symmetry among potential words and categories is spontaneously
broken. Here this picture is scrutinized by considering its robustness against
explicit symmetry breaking, an inevitable component of learning in the real
world. It is shown that the scenario is robust to such symmetry breaking.
Comparison with human data on the clustering coefficient of syntax networks
suggests that the observed transition is equivalent to that normally
experienced by children at age 24 months.",http://arxiv.org/pdf/2309.14913v1
2309.14903v1,cs.SE,DAnTE: a taxonomy for the automation degree of software engineering tasks,2023-09-26 13:04:58+00:00,"Software engineering researchers and practitioners have pursued manners to
reduce the amount of time and effort required to develop code and increase
productivity since the emergence of the discipline. Generative language models
are just another step in this journey, but it will probably not be the last
one. In this chapter, we propose DAnTE, a Degree of Automation Taxonomy for
software Engineering, describing several levels of automation based on the
idiosyncrasies of the field. Based on the taxonomy, we evaluated several tools
used in the past and in the present for software engineering practices. Then,
we give particular attention to AI-based tools, including generative language
models, discussing how they are located within the proposed taxonomy, and
reasoning about possible limitations they currently have. Based on this
analysis, we discuss what novel tools could emerge in the middle and long term.",http://arxiv.org/pdf/2309.14903v1
2309.14900v1,cs.CV,Pre-training-free Image Manipulation Localization through Non-Mutually Exclusive Contrastive Learning,2023-09-26 12:58:44+00:00,"Deep Image Manipulation Localization (IML) models suffer from training data
insufficiency and thus heavily rely on pre-training. We argue that contrastive
learning is more suitable to tackle the data insufficiency problem for IML.
Crafting mutually exclusive positives and negatives is the prerequisite for
contrastive learning. However, when adopting contrastive learning in IML, we
encounter three categories of image patches: tampered, authentic, and contour
patches. Tampered and authentic patches are naturally mutually exclusive, but
contour patches containing both tampered and authentic pixels are non-mutually
exclusive to them. Simply abnegating these contour patches results in a drastic
performance loss since contour patches are decisive to the learning outcomes.
Hence, we propose the Non-mutually exclusive Contrastive Learning (NCL)
framework to rescue conventional contrastive learning from the above dilemma.
In NCL, to cope with the non-mutually exclusivity, we first establish a pivot
structure with dual branches to constantly switch the role of contour patches
between positives and negatives while training. Then, we devise a
pivot-consistent loss to avoid spatial corruption caused by the role-switching
process. In this manner, NCL both inherits the self-supervised merits to
address the data insufficiency and retains a high manipulation localization
accuracy. Extensive experiments verify that our NCL achieves state-of-the-art
performance on all five benchmarks without any pre-training and is more robust
on unseen real-life samples. The code is available at:
https://github.com/Knightzjz/NCL-IML.",http://arxiv.org/pdf/2309.14900v1
2309.14897v1,cs.CV,"FDLS: A Deep Learning Approach to Production Quality, Controllable, and Retargetable Facial Performances",2023-09-26 12:54:58+00:00,"Visual effects commonly requires both the creation of realistic synthetic
humans as well as retargeting actors' performances to humanoid characters such
as aliens and monsters. Achieving the expressive performances demanded in
entertainment requires manipulating complex models with hundreds of parameters.
Full creative control requires the freedom to make edits at any stage of the
production, which prohibits the use of a fully automatic ``black box'' solution
with uninterpretable parameters. On the other hand, producing realistic
animation with these sophisticated models is difficult and laborious. This
paper describes FDLS (Facial Deep Learning Solver), which is Weta Digital's
solution to these challenges. FDLS adopts a coarse-to-fine and
human-in-the-loop strategy, allowing a solved performance to be verified and
edited at several stages in the solving process. To train FDLS, we first
transform the raw motion-captured data into robust graph features. Secondly,
based on the observation that the artists typically finalize the jaw pass
animation before proceeding to finer detail, we solve for the jaw motion first
and predict fine expressions with region-based networks conditioned on the jaw
position. Finally, artists can optionally invoke a non-linear finetuning
process on top of the FDLS solution to follow the motion-captured virtual
markers as closely as possible. FDLS supports editing if needed to improve the
results of the deep learning solution and it can handle small daily changes in
the actor's face shape. FDLS permits reliable and production-quality
performance solving with minimal training and little or no manual effort in
many cases, while also allowing the solve to be guided and edited in unusual
and difficult cases. The system has been under development for several years
and has been used in major movies.",http://arxiv.org/pdf/2309.14897v1
2309.14894v1,cs.RO,Verifiable Learned Behaviors via Motion Primitive Composition: Applications to Scooping of Granular Media,2023-09-26 12:51:03+00:00,"A robotic behavior model that can reliably generate behaviors from natural
language inputs in real time would substantially expedite the adoption of
industrial robots due to enhanced system flexibility. To facilitate these
efforts, we construct a framework in which learned behaviors, created by a
natural language abstractor, are verifiable by construction. Leveraging recent
advancements in motion primitives and probabilistic verification, we construct
a natural-language behavior abstractor that generates behaviors by synthesizing
a directed graph over the provided motion primitives. If these component motion
primitives are constructed according to the criteria we specify, the resulting
behaviors are probabilistically verifiable. We demonstrate this verifiable
behavior generation capacity in both simulation on an exploration task and on
hardware with a robot scooping granular media.",http://arxiv.org/pdf/2309.14894v1
2309.14888v1,cs.CV,Nearest Neighbor Guidance for Out-of-Distribution Detection,2023-09-26 12:40:35+00:00,"Detecting out-of-distribution (OOD) samples are crucial for machine learning
models deployed in open-world environments. Classifier-based scores are a
standard approach for OOD detection due to their fine-grained detection
capability. However, these scores often suffer from overconfidence issues,
misclassifying OOD samples distant from the in-distribution region. To address
this challenge, we propose a method called Nearest Neighbor Guidance (NNGuide)
that guides the classifier-based score to respect the boundary geometry of the
data manifold. NNGuide reduces the overconfidence of OOD samples while
preserving the fine-grained capability of the classifier-based score. We
conduct extensive experiments on ImageNet OOD detection benchmarks under
diverse settings, including a scenario where the ID data undergoes natural
distribution shift. Our results demonstrate that NNGuide provides a significant
performance improvement on the base detection scores, achieving
state-of-the-art results on both AUROC, FPR95, and AUPR metrics. The code is
given at \url{https://github.com/roomo7time/nnguide}.",http://arxiv.org/pdf/2309.14888v1
2309.14884v1,cs.SE,To Do or Not to Do: Semantics and Patterns for Do Activities in UML PSSM State Machines,2023-09-26 12:30:51+00:00,"State machines are used ubiquitously in engineering software-intensive
systems. UML State Machines extend simple finite state machines with powerful
constructs. Among the many extensions, there is one seemingly simple and
innocent language construct that fundamentally changes state machines' reactive
model of computation: doActivity behaviors. DoActivity behaviors describe
behavior that is executed independently from the state machine once entered in
a given state, typically modeling complex computation or communication as
background tasks. However, the UML specification or textbooks are vague about
how the doActivity behavior construct should be appropriately used. This lack
of guidance is a severe issue as, when improperly used, doActivities can cause
concurrent, non-deterministic bugs that are especially challenging to find and
could ruin a seemingly correct software design. The Precise Semantics of UML
State Machines (PSSM) specification introduced detailed operational semantics
for state machines. To the best of our knowledge, there is no rigorous review
yet of doActivity's semantics as specified in PSSM. We analyzed the semantics
by collecting evidence from cross-checking the text of the specification, its
semantic model and executable test cases, and the simulators supporting PSSM.
We synthesized insights about subtle details and emergent behaviors relevant to
tool developers and advanced modelers. We reported inconsistencies and missing
clarifications in more than 20 issues to the standardization committee. Based
on these insights, we studied 11 patterns for doActivities detailing the
consequences of using a doActivity in a given situation and discussing
countermeasures or alternative design choices. We hope that our analysis of the
semantics and the patterns help vendors develop conformant simulators or
verification tools and engineers design better state machine models.",http://arxiv.org/pdf/2309.14884v1
2309.14876v1,cs.CY,APPRAISE: a framework for managing AI compliance,2023-09-26 12:20:07+00:00,"As AI systems increasingly impact society, the EU AI Act (AIA) is the first
serious attempt to contain its less desired effects. Among others the act
proposes audit as a mechanism and compliance products as tools for
organizations to demonstrate compliance. In this paper, a framework for
managing AI compliance, APPRAISE, is proposed. The framework is built upon the
rationale that driving a balance between generating shareholder value through
innovation in AI systems and managing compliance through organizational
processes will eventually result in value that is responsible. By adhering to
AIA compliance products, the framework operationalizes and hence safeguards
compliance. Furthermore, a two-phase experiment with a limited scope is
presented. The experiment aims to measure the extent to which companies
coordinate technical elements of AI systems to ultimately comply with the AIA.
In the first phase a survey is conducted and in the second phase the survey
results are validated with a couple of respondents to generate additional
in-depth insights and root causes.",http://arxiv.org/pdf/2309.14876v1
2309.14875v1,eess.SP,Enhanced Channel Estimation in mm-Wave MIMO Systems Leveraging Integrated Communication and Sensing,2023-09-26 12:15:55+00:00,"This paper tackles the challenge of wideband MIMO channel estimation within
indoor millimeter-wave scenarios. Our proposed approach exploits the integrated
sensing and communication paradigm, where sensing information aids in channel
estimation. The key innovation consists of employing both spatial and temporal
sensing modes to significantly reduce the number of required training pilots.
Moreover, our algorithm addresses and corrects potential mismatches between
sensing and communication modes, which can arise from differing sensing and
communication propagation paths. Extensive simulations demonstrate that the
proposed method requires 4x less pilots compared to the current
state-of-the-art, marking a substantial advancement in channel estimation
efficiency.",http://arxiv.org/pdf/2309.14875v1
2309.14868v1,cs.CV,Cross-Dataset-Robust Method for Blind Real-World Image Quality Assessment,2023-09-26 11:57:12+00:00,"Although many effective models and real-world datasets have been presented
for blind image quality assessment (BIQA), recent BIQA models usually tend to
fit specific training set. Hence, it is still difficult to accurately and
robustly measure the visual quality of an arbitrary real-world image. In this
paper, a robust BIQA method, is designed based on three aspects, i.e., robust
training strategy, large-scale real-world dataset, and powerful backbone.
First, many individual models based on popular and state-of-the-art (SOTA)
Swin-Transformer (SwinT) are trained on different real-world BIQA datasets
respectively. Then, these biased SwinT-based models are jointly used to
generate pseudo-labels, which adopts the probability of relative quality of two
random images instead of fixed quality score. A large-scale real-world image
dataset with 1,000,000 image pairs and pseudo-labels is then proposed for
training the final cross-dataset-robust model. Experimental results on
cross-dataset tests show that the performance of the proposed method is even
better than some SOTA methods that are directly trained on these datasets, thus
verifying the robustness and generalization of our method.",http://arxiv.org/pdf/2309.14868v1
2309.14860v1,cs.RO,A Wearable Robotic Hand for Hand-over-Hand Imitation Learning,2023-09-26 11:37:20+00:00,"Dexterous manipulation through imitation learning has gained significant
attention in robotics research. The collection of high-quality expert data
holds paramount importance when using imitation learning. The existing
approaches for acquiring expert data commonly involve utilizing a data glove to
capture hand motion information. However, this method suffers from limitations
as the collected information cannot be directly mapped to the robotic hand due
to discrepancies in their degrees of freedom or structures. Furthermore,it
fails to accurately capture force feedback information between the hand and
objects during the demonstration process. To overcome these challenges, this
paper presents a novel solution in the form of a wearable dexterous hand,
namely Hand-over-hand Imitation learning wearable RObotic Hand (HIRO
Hand),which integrates expert data collection and enables the implementation of
dexterous operations. This HIRO Hand empowers the operator to utilize their own
tactile feedback to determine appropriate force, position, and actions,
resulting in more accurate imitation of the expert's actions. We develop both
non-learning and visual behavior cloning based controllers allowing HIRO Hand
successfully achieves grasping and in-hand manipulation ability.",http://arxiv.org/pdf/2309.14860v1
2309.14854v1,math.DG,Rolling Stiefel manifolds equipped with $α$-metrics,2023-09-26 11:32:17+00:00,"We discuss the rolling, without slip and without twist, of Stiefel manifolds
equipped with $\alpha$-metrics, from an intrinsic and an extrinsic point of
view. We, however, start with a more general perspective, namely by
investigating intrinsic rolling of normal naturally reductive homogeneous
spaces. This gives evidence why a seemingly straightforward generalization of
intrinsic rolling of symmetric spaces to normal naturally reductive homogeneous
spaces is not possible, in general. For a given control curve, we derive a
system of explicit time-variant ODEs whose solutions describe the desired
rolling. These findings are applied to obtain the intrinsic rolling of Stiefel
manifolds, which is then extended to an extrinsic one. Moreover, explicit
solutions of the kinematic equations are obtained provided that the development
curve is the projection of a not necessarily horizontal one-parameter subgroup.
In addition, our results are put into perspective with examples of rolling
Stiefel manifolds known from the literature.",http://arxiv.org/pdf/2309.14854v1
2309.14845v1,cs.RO,Graph Neural Network Based Method for Path Planning Problem,2023-09-26 11:20:57+00:00,"Sampling-based path planning is a widely used method in robotics,
particularly in high-dimensional state space. Among the whole process of the
path planning, collision detection is the most time-consuming operation. In
this paper, we propose a learning-based path planning method that aims to
reduce the number of collision detection. We develop an efficient neural
network model based on Graph Neural Networks (GNN) and use the environment map
as input. The model outputs weights for each neighbor based on the input and
current vertex information, which are used to guide the planner in avoiding
obstacles. We evaluate the proposed method's efficiency through simulated
random worlds and real-world experiments, respectively. The results demonstrate
that the proposed method significantly reduces the number of collision
detection and improves the path planning speed in high-dimensional
environments.",http://arxiv.org/pdf/2309.14845v1
2309.14838v1,cs.SD,Emphasized Non-Target Speaker Knowledge in Knowledge Distillation for Automatic Speaker Verification,2023-09-26 11:09:12+00:00,"Knowledge distillation (KD) is used to enhance automatic speaker verification
performance by ensuring consistency between large teacher networks and
lightweight student networks at the embedding level or label level. However,
the conventional label-level KD overlooks the significant knowledge from
non-target speakers, particularly their classification probabilities, which can
be crucial for automatic speaker verification. In this paper, we first
demonstrate that leveraging a larger number of training non-target speakers
improves the performance of automatic speaker verification models. Inspired by
this finding about the importance of non-target speakers' knowledge, we
modified the conventional label-level KD by disentangling and emphasizing the
classification probabilities of non-target speakers during knowledge
distillation. The proposed method is applied to three different student model
architectures and achieves an average of 13.67% improvement in EER on the
VoxCeleb dataset compared to embedding-level and conventional label-level KD
methods.",http://arxiv.org/pdf/2309.14838v1
2309.14836v1,cond-mat.stat-mech,On a class of solvable stationary non equilibrium states for mass exchange models,2023-09-26 11:05:08+00:00,"We consider a family of models having an arbitrary positive amount of mass on
each site and randomly exchanging an arbitrary amount of mass with nearest
neighbor sites. We restrict to the case of diffusive models. We identify a
class of reversible models for which the product invariant measure is known and
the gradient condition is satisfied so that we can explicitly compute the
transport coefficients associated to the diffusive hydrodynamic rescaling.
Based on the Macroscopic Fluctuation Theory \cite{mft} we have that the large
deviations rate functional for a stationary non equilibrium state can be
computed solving a Hamilton-Jacobi equation depending only on the transport
coefficients and the details of the boundary sources. Thus, we are able to
identify a class of models having transport coefficients for which the
Hamilton-Jacobi equation can indeed be solved. We give a complete
characterization in the case of generalized zero range models and discuss
several other cases. For the generalized zero range models we identify a class
of discrete models that, modulo trivial extensions, coincides with the class
discussed in \cite{FG} and a class of continuous dynamics that coincides with
the class in \cite{FFG}. Along the discussion we obtain a complete
characterization of reversible misanthrope processes solving the discrete
equations in \cite{CC}.",http://arxiv.org/pdf/2309.14836v1
2309.14835v1,math.OC,A Partially Feasible Distributed SQO Method for Two-block General Linearly Constrained Smooth Optimization,2023-09-26 11:03:24+00:00,"This paper discusses a class of two-block smooth large-scale optimization
problems with both linear equality and linear inequality constraints, which
have a wide range of applications, such as economic power dispatch, data
mining, signal processing, etc.Our goal is to develop a novel partially
feasible distributed (PFD) sequential quadratic optimization (SQO) method
(PFD-SQO method) for this kind of problems. The design of the method is based
on the ideas of SQO method and augmented Lagrangian Jacobian splitting scheme
as well as feasible direction method,which decomposes the quadratic
optimization (QO) subproblem into two small-scale QOs that can be solved
independently and parallelly. A novel disturbance contraction term that can be
suitably adjusted is introduced into the inequality constraints so that the
feasible step size along the search direction can be increased to 1. The new
iteration points are generated by the Armijo line search and the partially
augmented Lagrangian function that only contains equality constraints as the
merit function. The iteration points always satisfy all the inequality
constraints of the problem. The theoretical properties, such as global
convergence, iterative complexity, superlinear and quadratic rates of
convergence of the proposed PFD-SQO method are analyzed under appropriate
assumptions, respectively. Finally, the numerical effectiveness of the method
is tested on a class of academic examples and an economic power dispatch
problem, which shows that the proposed method is quite promising.",http://arxiv.org/pdf/2309.14835v1
2309.14834v1,cs.LO,Leveraging Datapath Propagation in IC3 for Hardware Model Checking,2023-09-26 11:02:22+00:00,"IC3 is a famous bit-level framework for safety verification. By incorporating
datapath abstraction, a notable enhancement in the efficiency of hardware
verification can be achieved. However, datapath abstraction entails a coarse
level of abstraction where all datapath operations are approximated as
uninterpreted functions. This level of abstraction, albeit useful, can lead to
an increased computational burden during the verification process as it
necessitates extensive exploration of redundant abstract state space.
  In this paper, we introduce a novel approach called datapath propagation. Our
method involves leveraging concrete constant values to iteratively compute the
outcomes of relevant datapath operations and their associated uninterpreted
functions. Meanwhile, we generate potentially useful datapath propagation
lemmas in abstract state space and tighten the datapath abstraction. With this
technique, the abstract state space can be reduced, and the verification
efficiency is significantly improved. We implemented the proposed approach and
conducted extensive experiments. The results show promising improvements of our
approach compared to the state-of-the-art verifiers.",http://arxiv.org/pdf/2309.14834v1
2309.14828v1,nucl-th,The regime of applicability of Israel-Stewart hydrodynamics,2023-09-26 10:52:04+00:00,"Using analytical tools from linear response theory, we systematically assess
the accuracy of several microscopic derivations of Israel-Stewart hydrodynamics
near local equilibrium. This allows us to ""rank"" the different approaches in
decreasing order of accuracy as follows: IReD, DNMR, second-order gradient
expansion, and 14-moment approximation. We find that IReD theory is far
superior to Navier-Stokes, being very accurate both in the asymptotic regime
(i.e. for slow processes) and in the transient regime (i.e. on timescales
comparable to the relaxation time). Also, the high accuracy of DNMR is
confirmed, but neglecting second-order terms in the Knudsen number, which would
render the equations parabolic, introduces serious systematic errors. Finally,
second-order gradient expansion (a.k.a. non-resummed BRSSS) is shown to be more
inaccurate than Navier-Stokes in the transient regime. Overall, this analysis
clearly shows that Israel-Stewart hydrodynamics is falsifiable, and the
relaxation time is observable, thereby ending the debate on the viability of
transient hydrodynamics as a well-defined physical theory distinguished from
Navier-Stokes.",http://arxiv.org/pdf/2309.14828v1
2309.14823v1,cs.CL,Segmentation-Free Streaming Machine Translation,2023-09-26 10:43:52+00:00,"Streaming Machine Translation (MT) is the task of translating an unbounded
input text stream in real-time. The traditional cascade approach, which
combines an Automatic Speech Recognition (ASR) and an MT system, relies on an
intermediate segmentation step which splits the transcription stream into
sentence-like units. However, the incorporation of a hard segmentation
constrains the MT system and is a source of errors. This paper proposes a
Segmentation-Free framework that enables the model to translate an unsegmented
source stream by delaying the segmentation decision until the translation has
been generated. Extensive experiments show how the proposed Segmentation-Free
framework has better quality-latency trade-off than competing approaches that
use an independent segmentation model. Software, data and models will be
released upon paper acceptance.",http://arxiv.org/pdf/2309.14823v1
2309.14819v1,cs.CV,Discrepancy Matters: Learning from Inconsistent Decoder Features for Consistent Semi-supervised Medical Image Segmentation,2023-09-26 10:33:20+00:00,"Semi-supervised learning (SSL) has been proven beneficial for mitigating the
issue of limited labeled data especially on the task of volumetric medical
image segmentation. Unlike previous SSL methods which focus on exploring highly
confident pseudo-labels or developing consistency regularization schemes, our
empirical findings suggest that inconsistent decoder features emerge naturally
when two decoders strive to generate consistent predictions. Based on the
observation, we first analyze the treasure of discrepancy in learning towards
consistency, under both pseudo-labeling and consistency regularization
settings, and subsequently propose a novel SSL method called LeFeD, which
learns the feature-level discrepancy obtained from two decoders, by feeding the
discrepancy as a feedback signal to the encoder. The core design of LeFeD is to
enlarge the difference by training differentiated decoders, and then learn from
the inconsistent information iteratively. We evaluate LeFeD against eight
state-of-the-art (SOTA) methods on three public datasets. Experiments show
LeFeD surpasses competitors without any bells and whistles such as uncertainty
estimation and strong constraints, as well as setting a new state-of-the-art
for semi-supervised medical image segmentation. Code is available at
\textcolor{cyan}{https://github.com/maxwell0027/LeFeD}",http://arxiv.org/pdf/2309.14819v1
2309.14814v1,physics.ins-det,First measurements with monolithic active pixel test structures produced in a 65 nm CMOS process,2023-09-26 10:27:46+00:00,"The Inner Tracking System (ITS) of the ALICE experiment at CERN will undergo
an upgrade during the LHC long shutdown 3, in which the three innermost
tracking layers will be replaced. This upgrade, named the Inner Tracking System
3 (ITS3), employs stitched wafer-scale Monolithic Active Pixel Sensors
fabricated in a 65 nm CMOS process. The sensors are 260 mm in length and
thinned to less than 50 um then bent to form truly half-cylindrical
half-barrels. The feasibility of this process for the ITS3 was explored with
the first test production run (MLR1) in 2021, whose goal was to evaluate the
charged particle detection efficiency and the sensor performance under
non-ionising and ionising radiation up to the expected levels for ALICE ITS3 of
$10^{13}$ $1$ MeV n$_{\mathrm{eq}}$ cm$^{-2}$ (NIEL) and 10 kGy (TID). Three
sensor flavours were produced to investigate this process: Analog Pixel Test
Structure (APTS), Circuit Exploratoire 65 (CE65) and Digital Pixel Test
Structure (DPTS).
  This contribution gives an overview of the MLR1 submission and test results,
describing the different sensor flavours and presenting the results of the
performance measurements done with particle beams for various chip variants and
irradiation levels.",http://arxiv.org/pdf/2309.14814v1
2309.14811v1,cond-mat.mtrl-sci,Identifying and abating copper foil impurities to optimize graphene growth,2023-09-26 10:17:01+00:00,"Copper foil impurities are hampering scalable production of high-quality
graphene by chemical vapor deposition (CVD). Here, we conduct a thorough study
on the origin of these unavoidable contaminations at the surface of copper
after the CVD process. We identify two distinct origins for the impurities. The
first type is intrinsic impurities, originating from the manufacturing process
of the copper foils, already present at the surface before any high-temperature
treatment, or buried into the bulk of copper foils. The buried impurities
diffuse towards the copper surface during high-temperature treatment and
precipitate. The second source is external: silica contamination arising from
the quartz tube that also precipitate on copper. The problem of the extrinsic
silica contamination is readily solved upon using an adequate confinement the
copper foil samples. The intrinsic impurities are much more difficult to remove
since they appear spread in the whole foil. Nevertheless, electropolishing
proves particularly efficient in drastically reducing the issue.",http://arxiv.org/pdf/2309.14811v1
2309.14809v1,cs.CV,ENIGMA-51: Towards a Fine-Grained Understanding of Human-Object Interactions in Industrial Scenarios,2023-09-26 10:14:44+00:00,"ENIGMA-51 is a new egocentric dataset acquired in a real industrial domain by
19 subjects who followed instructions to complete the repair of electrical
boards using industrial tools (e.g., electric screwdriver) and electronic
instruments (e.g., oscilloscope). The 51 sequences are densely annotated with a
rich set of labels that enable the systematic study of human-object
interactions in the industrial domain. We provide benchmarks on four tasks
related to human-object interactions: 1) untrimmed action detection, 2)
egocentric human-object interaction detection, 3) short-term object interaction
anticipation and 4) natural language understanding of intents and entities.
Baseline results show that the ENIGMA-51 dataset poses a challenging benchmark
to study human-object interactions in industrial scenarios. We publicly release
the dataset at: https://iplab.dmi.unict.it/ENIGMA-51/.",http://arxiv.org/pdf/2309.14809v1
2309.14800v1,cs.CV,3D Density-Gradient based Edge Detection on Neural Radiance Fields (NeRFs) for Geometric Reconstruction,2023-09-26 09:56:27+00:00,"Generating geometric 3D reconstructions from Neural Radiance Fields (NeRFs)
is of great interest. However, accurate and complete reconstructions based on
the density values are challenging. The network output depends on input data,
NeRF network configuration and hyperparameter. As a result, the direct usage of
density values, e.g. via filtering with global density thresholds, usually
requires empirical investigations. Under the assumption that the density
increases from non-object to object area, the utilization of density gradients
from relative values is evident. As the density represents a position-dependent
parameter it can be handled anisotropically, therefore processing of the
voxelized 3D density field is justified. In this regard, we address geometric
3D reconstructions based on density gradients, whereas the gradients result
from 3D edge detection filters of the first and second derivatives, namely
Sobel, Canny and Laplacian of Gaussian. The gradients rely on relative
neighboring density values in all directions, thus are independent from
absolute magnitudes. Consequently, gradient filters are able to extract edges
along a wide density range, almost independent from assumptions and empirical
investigations. Our approach demonstrates the capability to achieve geometric
3D reconstructions with high geometric accuracy on object surfaces and
remarkable object completeness. Notably, Canny filter effectively eliminates
gaps, delivers a uniform point density, and strikes a favorable balance between
correctness and completeness across the scenes.",http://arxiv.org/pdf/2309.14800v1
2309.14799v1,cond-mat.mtrl-sci,Phonons from Density-Functional Perturbation Theory using the All-Electron Full-Potential Linearized Augmented Plane-Wave Method FLEUR,2023-09-26 09:55:41+00:00,"Phonons are quantized vibrations of a crystal lattice that play a crucial
role in understanding many properties of solids. Density functional theory
(DFT) provides a state-of-the-art computational approach to lattice vibrations
from first-principles. We present a successful software implementation for
calculating phonons in the harmonic approximation, employing density-functional
perturbation theory (DFPT) within the framework of the full-potential
linearized augmented plane-wave (FLAPW) method as implemented in the electronic
structure package FLEUR. The implementation, which involves the Sternheimer
equation for the linear response of the wave function, charge density, and
potential with respect to infinitesimal atomic displacements, as well as the
setup of the dynamical matrix, is presented and the specifics due to the
muffin-tin sphere centered LAPW basis-set and the all-electron nature are
discussed. As a test, we calculate the phonon dispersion of several solids
including an insulator, a semiconductor as well as several metals. The latter
are comprised of magnetic, simple, and transition metals. The results are
validated on the basis of phonon dispersions calculated using the finite
displacement approach in conjunction with the FLEUR code and the phonopy
package, as well as by some experimental results. An excellent agreement is
obtained.",http://arxiv.org/pdf/2309.14799v1
2309.14795v1,physics.optics,Assessing the alignment accuracy of state-of-the-art deterministic fabrication methods for single quantum dot devices,2023-09-26 09:42:56+00:00,"The realization of efficient quantum light sources relies on the integration
of self-assembled quantum dots (QDs) into photonic nanostructures with high
spatial positioning accuracy. In this work, we present a comprehensive
investigation of the QD position accuracy, obtained using two marker-based QD
positioning techniques, photoluminescence (PL) and cathodoluminescence (CL)
imaging, as well as using a marker-free in-situ electron beam lithography
(in-situ EBL) technique. We employ four PL imaging configurations with three
different image processing approaches and compare them with CL imaging. We
fabricate circular mesa structures based on the obtained QD coordinates from
both PL and CL image processing to evaluate the final positioning accuracy.
This yields final position offset of the QD relative to the mesa center of
$\mu_x$ = (-40$\pm$58) nm and $\mu_y$ = (-39$\pm$85) nm with PL imaging and
$\mu_x$ = (-39$\pm$30) nm and $\mu_y$ = (25$\pm$77) nm with CL imaging, which
are comparable to the offset $\mu_x$ = (20$\pm$40) nm and $\mu_y$ =
(-14$\pm$39) nm obtained using the in-situ EBL method. We discuss the possible
causes of the observed offsets, which are significantly larger than the QD
localization uncertainty obtained from simply imaging the QD light emission
from an unstructured wafer. Our study highlights the influences of the image
processing technique and the subsequent fabrication process on the final
positioning accuracy for a QD placed inside a photonic nanostructure.",http://arxiv.org/pdf/2309.14795v1
2309.14788v1,cs.DS,Small-Space Algorithms for the Online Language Distance Problem for Palindromes and Squares,2023-09-26 09:36:24+00:00,"We study the online variant of the language distance problem for two
classical formal languages, the language of palindromes and the language of
squares, and for the two most fundamental distances, the Hamming distance and
the edit (Levenshtein) distance. In this problem, defined for a fixed formal
language $L$, we are given a string $T$ of length $n$, and the task is to
compute the minimal distance to $L$ from every prefix of $T$. We focus on the
low-distance regime, where one must compute only the distances smaller than a
given threshold $k$. In this work, our contribution is twofold:
  - First, we show streaming algorithms, which access the input string $T$ only
through a single left-to-right scan. Both for palindromes and squares, our
algorithms use $O(k \cdot\mathrm{poly}~\log n)$ space and time per character in
the Hamming-distance case and $O(k^2 \cdot\mathrm{poly}~\log n)$ space and time
per character in the edit-distance case. These algorithms are randomised by
necessity, and they err with probability inverse-polynomial in $n$.
  - Second, we show deterministic read-only online algorithms, which are also
provided with read-only random access to the already processed characters of
$T$. Both for palindromes and squares, our algorithms use $O(k
\cdot\mathrm{poly}~\log n)$ space and time per character in the
Hamming-distance case and $O(k^4 \cdot\mathrm{poly}~\log n)$ space and
amortised time per character in the edit-distance case.",http://arxiv.org/pdf/2309.14788v1
2309.14784v1,q-fin.MF,Approximation Rates for Deep Calibration of (Rough) Stochastic Volatility Models,2023-09-26 09:32:38+00:00,"We derive quantitative error bounds for deep neural networks (DNNs)
approximating option prices on a $d$-dimensional risky asset as functions of
the underlying model parameters, payoff parameters and initial conditions. We
cover a general class of stochastic volatility models of Markovian nature as
well as the rough Bergomi model. In particular, under suitable assumptions we
show that option prices can be learned by DNNs up to an arbitrary small error
$\varepsilon \in (0,1/2)$ while the network size grows only sub-polynomially in
the asset vector dimension $d$ and the reciprocal $\varepsilon^{-1}$ of the
accuracy. Hence, the approximation does not suffer from the curse of
dimensionality. As quantitative approximation results for DNNs applicable in
our setting are formulated for functions on compact domains, we first consider
the case of the asset price restricted to a compact set, then we extend these
results to the general case by using convergence arguments for the option
prices.",http://arxiv.org/pdf/2309.14784v1
2309.14781v1,cs.CV,Frugal Satellite Image Change Detection with Deep-Net Inversion,2023-09-26 09:25:53+00:00,"Change detection in satellite imagery seeks to find occurrences of targeted
changes in a given scene taken at different instants. This task has several
applications ranging from land-cover mapping, to anthropogenic activity
monitory as well as climate change and natural hazard damage assessment.
However, change detection is highly challenging due to the acquisition
conditions and also to the subjectivity of changes. In this paper, we devise a
novel algorithm for change detection based on active learning. The proposed
method is based on a question and answer model that probes an oracle (user)
about the relevance of changes only on a small set of critical images (referred
to as virtual exemplars), and according to oracle's responses updates deep
neural network (DNN) classifiers. The main contribution resides in a novel
adversarial model that allows learning the most representative, diverse and
uncertain virtual exemplars (as inverted preimages of the trained DNNs) that
challenge (the most) the trained DNNs, and this leads to a better re-estimate
of these networks in the subsequent iterations of active learning. Experiments
show the out-performance of our proposed deep-net inversion against the related
work.",http://arxiv.org/pdf/2309.14781v1
2309.14778v1,eess.SP,Multi-static Parameter Estimation in the Near/Far Field Beam Space for Integrated Sensing and Communication Applications,2023-09-26 09:20:51+00:00,"This work proposes a maximum likelihood (ML)-based parameter estimation
framework for a millimeter wave (mmWave) integrated sensing and communication
(ISAC) system in a multi-static configuration using energy-efficient hybrid
digital-analog arrays. Due to the typically large arrays deployed in the higher
frequency bands to mitigate isotropic path loss, such arrays may operate in the
near-field regime. The proposed parameter estimation in this work consists of a
two-stage estimation process, where the first stage is based on far-field
assumptions, and is used to obtain a first estimate of the target parameters.
In cases where the target is determined to be in the near-field of the arrays,
a second estimation based on near-field assumptions is carried out to obtain
more accurate estimates. In particular, we select beamfocusing array weights
designed to achieve a constant gain over an extended spatial region and
re-estimate the target parameters at the receivers. We evaluate the
effectiveness of the proposed framework in numerous scenarios through numerical
simulations and demonstrate the impact of the custom-designed flat-gain
beamfocusing codewords in increasing the communication performance of the
system.",http://arxiv.org/pdf/2309.14778v1
2309.14777v1,stat.ME,Exact Likelihoods for N-mixture models with Time-to-Detection Data,2023-09-26 09:20:20+00:00,"This paper is concerned with the formulation of $N$-mixture models for
estimating the abundance and probability of detection of a species from binary
response, count and time-to-detection data. A modelling framework, which
encompasses time-to-first-detection within the context of
detection/non-detection and time-to-each-detection and time-to-first-detection
within the context of count data, is introduced. Two observation processes
which depend on whether or not double counting is assumed to occur are also
considered. The main focus of the paper is on the derivation of explicit forms
for the likelihoods associated with each of the proposed models. Closed-form
expressions for the likelihoods associated with time-to-detection data are new
and are developed from the theory of order statistics. A key finding of the
study is that, based on the assumption of no double counting, the likelihoods
associated with times-to-detection together with count data are the product of
the likelihood for the counts alone and a term which depends on the detection
probability parameter. This result demonstrates that, in this case, recording
times-to-detection could well improve precision in estimation over recording
counts alone. In contrast, for the double counting protocol with exponential
arrival times, no information was found to be gained by recording
times-to-detection in addition to the count data. An R package and an
accompanying vignette are also introduced in order to complement the algebraic
results and to demonstrate the use of the models in practice.",http://arxiv.org/pdf/2309.14777v1
2309.14774v1,cs.LG,BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning,2023-09-26 09:16:44+00:00,"This study aims to explore efficient tuning methods for the screenshot
captioning task. Recently, image captioning has seen significant advancements,
but research in captioning tasks for mobile screens remains relatively scarce.
Current datasets and use cases describing user behaviors within product
screenshots are notably limited. Consequently, we sought to fine-tune
pre-existing models for the screenshot captioning task. However, fine-tuning
large pre-trained models can be resource-intensive, requiring considerable
time, computational power, and storage due to the vast number of parameters in
image captioning models. To tackle this challenge, this study proposes a
combination of adapter methods, which necessitates tuning only the additional
modules on the model. These methods are originally designed for vision or
language tasks, and our intention is to apply them to address similar
challenges in screenshot captioning. By freezing the parameters of the image
caption models and training only the weights associated with the methods,
performance comparable to fine-tuning the entire model can be achieved, while
significantly reducing the number of parameters. This study represents the
first comprehensive investigation into the effectiveness of combining adapters
within the context of the screenshot captioning task. Through our experiments
and analyses, this study aims to provide valuable insights into the application
of adapters in vision-language models and contribute to the development of
efficient tuning techniques for the screenshot captioning task. Our study is
available at https://github.com/RainYuGG/BLIP-Adapter",http://arxiv.org/pdf/2309.14774v1
2309.14770v1,cs.CL,KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation,2023-09-26 09:03:25+00:00,"Knowledge graph completion is a task that revolves around filling in missing
triples based on the information available in a knowledge graph. Among the
current studies, text-based methods complete the task by utilizing textual
descriptions of triples. However, this modeling approach may encounter
limitations, particularly when the description fails to accurately and
adequately express the intended meaning. To overcome these challenges, we
propose the augmentation of data through two additional mechanisms. Firstly, we
employ ChatGPT as an external knowledge base to generate coherent descriptions
to bridge the semantic gap between the queries and answers. Secondly, we
leverage inverse relations to create a symmetric graph, thereby creating extra
labeling and providing supplementary information for link prediction. This
approach offers additional insights into the relationships between entities.
Through these efforts, we have observed significant improvements in knowledge
graph completion, as these mechanisms enhance the richness and diversity of the
available data, leading to more accurate results.",http://arxiv.org/pdf/2309.14770v1
2309.14763v1,cs.CL,ConPET: Continual Parameter-Efficient Tuning for Large Language Models,2023-09-26 08:52:04+00:00,"Continual learning necessitates the continual adaptation of models to newly
emerging tasks while minimizing the catastrophic forgetting of old ones. This
is extremely challenging for large language models (LLMs) with vanilla
full-parameter tuning due to high computation costs, memory consumption, and
forgetting issue. Inspired by the success of parameter-efficient tuning (PET),
we propose Continual Parameter-Efficient Tuning (ConPET), a generalizable
paradigm for continual task adaptation of LLMs with task-number-independent
training complexity. ConPET includes two versions with different application
scenarios. First, Static ConPET can adapt former continual learning methods
originally designed for relatively smaller models to LLMs through PET and a
dynamic replay strategy, which largely reduces the tuning costs and alleviates
the over-fitting and forgetting issue. Furthermore, to maintain scalability,
Dynamic ConPET adopts separate PET modules for different tasks and a PET module
selector for dynamic optimal selection. In our extensive experiments, the
adaptation of Static ConPET helps multiple former methods reduce the scale of
tunable parameters by over 3,000 times and surpass the PET-only baseline by at
least 5 points on five smaller benchmarks, while Dynamic ConPET gains its
advantage on the largest dataset. The codes and datasets are available at
https://github.com/Raincleared-Song/ConPET.",http://arxiv.org/pdf/2309.14763v1
2309.14761v1,eess.AS,Optimization Techniques for a Physical Model of Human Vocalisation,2023-09-26 08:45:41+00:00,"We present a non-supervised approach to optimize and evaluate the synthesis
of non-speech audio effects from a speech production model. We use the Pink
Trombone synthesizer as a case study of a simplified production model of the
vocal tract to target non-speech human audio signals --yawnings. We selected
and optimized the control parameters of the synthesizer to minimize the
difference between real and generated audio. We validated the most common
optimization techniques reported in the literature and a specifically designed
neural network. We evaluated several popular quality metrics as error
functions. These include both objective quality metrics and
subjective-equivalent metrics. We compared the results in terms of total error
and computational demand. Results show that genetic and swarm optimizers
outperform least squares algorithms at the cost of executing slower and that
specific combinations of optimizers and audio representations offer
significantly different results. The proposed methodology could be used in
benchmarking other physical models and audio types.",http://arxiv.org/pdf/2309.14761v1
2309.14759v1,cs.GR,Diffusion-based Holistic Texture Rectification and Synthesis,2023-09-26 08:44:46+00:00,"We present a novel framework for rectifying occlusions and distortions in
degraded texture samples from natural images. Traditional texture synthesis
approaches focus on generating textures from pristine samples, which
necessitate meticulous preparation by humans and are often unattainable in most
natural images. These challenges stem from the frequent occlusions and
distortions of texture samples in natural images due to obstructions and
variations in object surface geometry. To address these issues, we propose a
framework that synthesizes holistic textures from degraded samples in natural
images, extending the applicability of exemplar-based texture synthesis
techniques. Our framework utilizes a conditional Latent Diffusion Model (LDM)
with a novel occlusion-aware latent transformer. This latent transformer not
only effectively encodes texture features from partially-observed samples
necessary for the generation process of the LDM, but also explicitly captures
long-range dependencies in samples with large occlusions. To train our model,
we introduce a method for generating synthetic data by applying geometric
transformations and free-form mask generation to clean textures. Experimental
results demonstrate that our framework significantly outperforms existing
methods both quantitatively and quantitatively. Furthermore, we conduct
comprehensive ablation studies to validate the different components of our
proposed framework. Results are corroborated by a perceptual user study which
highlights the efficiency of our proposed approach.",http://arxiv.org/pdf/2309.14759v1
2309.14758v1,eess.AS,Exploring RWKV for Memory Efficient and Low Latency Streaming ASR,2023-09-26 08:41:24+00:00,"Recently, self-attention-based transformers and conformers have been
introduced as alternatives to RNNs for ASR acoustic modeling. Nevertheless, the
full-sequence attention mechanism is non-streamable and computationally
expensive, thus requiring modifications, such as chunking and caching, for
efficient streaming ASR. In this paper, we propose to apply RWKV, a variant of
linear attention transformer, to streaming ASR. RWKV combines the superior
performance of transformers and the inference efficiency of RNNs, which is
well-suited for streaming ASR scenarios where the budget for latency and memory
is restricted. Experiments on varying scales (100h - 10000h) demonstrate that
RWKV-Transducer and RWKV-Boundary-Aware-Transducer achieve comparable to or
even better accuracy compared with chunk conformer transducer, with minimal
latency and inference memory cost.",http://arxiv.org/pdf/2309.14758v1
2309.14756v1,cs.CV,On quantifying and improving realism of images generated with diffusion,2023-09-26 08:32:55+00:00,"Recent advances in diffusion models have led to a quantum leap in the quality
of generative visual content. However, quantification of realism of the content
is still challenging. Existing evaluation metrics, such as Inception Score and
Fr\'echet inception distance, fall short on benchmarking diffusion models due
to the versatility of the generated images. Moreover, they are not designed to
quantify realism of an individual image. This restricts their application in
forensic image analysis, which is becoming increasingly important in the
emerging era of generative models. To address that, we first propose a metric,
called Image Realism Score (IRS), computed from five statistical measures of a
given image. This non-learning based metric not only efficiently quantifies
realism of the generated images, it is readily usable as a measure to classify
a given image as real or fake. We experimentally establish the model- and
data-agnostic nature of the proposed IRS by successfully detecting fake images
generated by Stable Diffusion Model (SDM), Dalle2, Midjourney and BigGAN.
  We further leverage this attribute of our metric to minimize an IRS-augmented
generative loss of SDM, and demonstrate a convenient yet considerable quality
improvement of the SDM-generated content with our modification. Our efforts
have also led to Gen-100 dataset, which provides 1,000 samples for 100 classes
generated by four high-quality models. We will release the dataset and code.",http://arxiv.org/pdf/2309.14756v1
2309.14752v1,physics.flu-dyn,Bridging Polymeric Turbulence at different Reynolds numbers: From Multiscaling to Multifractality,2023-09-26 08:26:48+00:00,"The addition of polymers modifies a flow in a non-trivial way that depends on
fluid inertia (given by the Reynolds number Re) and polymer elasticity
(quantified by the Deborah number De). Using direct numerical simulations, we
show that polymeric flows exhibit a Re and De dependent multiscaling energy
spectrum. The different scaling regimes are tied to various dominant
contributions -- fluid, polymer, and dissipation -- to the total energy flux
across the scales. At small scales, energy is dissipated away by both polymers
and the fluid. Fluid energy dissipation, in particular, is shown to be more
intermittent in the presence of polymers, especially at small Re. The more
intermittent, singular nature of energy dissipation is revealed clearly by the
multifractal spectrum.",http://arxiv.org/pdf/2309.14752v1
2309.14744v1,cs.CV,ADU-Depth: Attention-based Distillation with Uncertainty Modeling for Depth Estimation,2023-09-26 08:12:37+00:00,"Monocular depth estimation is challenging due to its inherent ambiguity and
ill-posed nature, yet it is quite important to many applications. While recent
works achieve limited accuracy by designing increasingly complicated networks
to extract features with limited spatial geometric cues from a single RGB
image, we intend to introduce spatial cues by training a teacher network that
leverages left-right image pairs as inputs and transferring the learned 3D
geometry-aware knowledge to the monocular student network. Specifically, we
present a novel knowledge distillation framework, named ADU-Depth, with the
goal of leveraging the well-trained teacher network to guide the learning of
the student network, thus boosting the precise depth estimation with the help
of extra spatial scene information. To enable domain adaptation and ensure
effective and smooth knowledge transfer from teacher to student, we apply both
attention-adapted feature distillation and focal-depth-adapted response
distillation in the training stage. In addition, we explicitly model the
uncertainty of depth estimation to guide distillation in both feature space and
result space to better produce 3D-aware knowledge from monocular observations
and thus enhance the learning for hard-to-predict image regions. Our extensive
experiments on the real depth estimation datasets KITTI and DrivingStereo
demonstrate the effectiveness of the proposed method, which ranked 1st on the
challenging KITTI online benchmark.",http://arxiv.org/pdf/2309.14744v1
2309.14741v1,eess.AS,Rethinking Session Variability: Leveraging Session Embeddings for Session Robustness in Speaker Verification,2023-09-26 08:09:30+00:00,"In the field of speaker verification, session or channel variability poses a
significant challenge. While many contemporary methods aim to disentangle
session information from speaker embeddings, we introduce a novel approach
using an additional embedding to represent the session information. This is
achieved by training an auxiliary network appended to the speaker embedding
extractor which remains fixed in this training process. This results in two
similarity scores: one for the speakers information and one for the session
information. The latter score acts as a compensator for the former that might
be skewed due to session variations. Our extensive experiments demonstrate that
session information can be effectively compensated without retraining of the
embedding extractor.",http://arxiv.org/pdf/2309.14741v1
2309.14738v1,math.PR,The maximal displacement of radially symmetric branching random walk in $\mathbb{R}^d$,2023-09-26 08:07:03+00:00,"We consider discrete-time branching random walks with a radially symmetric
distribution. Independently of each other individuals generate offspring whose
relative locations are given by a copy of a radially symmetric point process
$\mathcal{L}$. The number of particles at time $t$ form a supercritical
Galton-Watson process. We investigate the maximal distance to the origin of
such branching random walks. Conditioned on survival, we show that, under some
assumptions on $\mathcal{L}$, it grows in the same way as for branching
Brownian motion or a broad class of one-dimensional branching random walks: the
first term is linear in time and the second logarithmic. The constants in front
of these terms are explicit and depend only on the mean measure of
$\mathcal{L}$ and dimension. Our main tool in the proof is a ballot theorem
with moving barrier which may be of independent interest.",http://arxiv.org/pdf/2309.14738v1
2309.14725v1,astro-ph.IM,Citizen Science Time Domain Astronomy with Astro-COLIBRI,2023-09-26 07:35:58+00:00,"Astro-COLIBRI is an innovative tool designed for professional astronomers to
facilitate the study of transient astronomical events. Transient events - such
as supernovae, gamma-ray bursts and stellar mergers - are fleeting cataclysmic
phenomena that can offer profound insights into the most violent processes in
the universe. Revealing their secrets requires rapid and precise observations:
Astro-COLIBRI alerts its users of new transient discoveries from observatories
all over the world in real-time. The platform also provides observers the
details they need to make follow-up observations.
  Some of the transient phenomena available through Astro-COLIBRI are
accessible by amateur astronomers and citizen scientists. A subset of the
features dedicated to this growing group of users are highlighted here. They
include the possibility of receiving only alerts on very bright events, the
possibility of defining custom observer locations, as well as the calculation
of optimized observation plans for searches for optical counterparts to
gravitational wave events.",http://arxiv.org/pdf/2309.14725v1
2309.14723v1,quant-ph,Geometricities of driven transport in presence of reservoir squeezing,2023-09-26 07:33:37+00:00,"In a bare site coupled to two reservoirs, we explore the statistics of boson
exchange in the presence of two simultaneous processes: squeezing the two
reservoirs and driving the two reservoirs. The squeezing parameters compete
with the geometric phaselike effect or geometricity to alter the nature of the
steadystate flux and noise. The even (odd) geometric cumulants and the total
minimum entropy are found to be symmetric (antisymmetric) with respect to
exchanging the left and right squeezing parameters. Upon increasing the
strength of the squeezing parameters, loss of geometricity is observed. Under
maximum squeezing, one can recover a standard steadystate fluctuation theorem
even in the presence of phase different driving protocol. A recently proposed
modified geometric thermodynamic uncertainty principle is found to be robust.",http://arxiv.org/pdf/2309.14723v1
2309.14717v1,cs.LG,QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models,2023-09-26 07:22:23+00:00,"Recently years have witnessed a rapid development of large language models
(LLMs). Despite the strong ability in many language-understanding tasks, the
heavy computational burden largely restricts the application of LLMs especially
when one needs to deploy them onto edge devices. In this paper, we propose a
quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies
in the imbalanced degrees of freedom of quantization and adaptation, and the
solution is to use group-wise operators which increase the degree of freedom of
quantization meanwhile decreasing that of adaptation. QA-LoRA is easily
implemented with a few lines of code, and it equips the original LoRA with
two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized
(e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the
LLM and auxiliary weights are naturally integrated into a quantized model
without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model
families and validate its effectiveness in different fine-tuning datasets and
downstream scenarios. Code will be made available at
https://github.com/yuhuixu1993/qa-lora.",http://arxiv.org/pdf/2309.14717v1
2309.14716v1,astro-ph.HE,Expansion and ongoing cosmic ray acceleration in HESS J1731-347,2023-09-26 07:17:25+00:00,"Diffusive shock acceleration in supernova remnants (SNRs) is considered one
of the prime mechanisms for Galactic Cosmic Ray (GCR) acceleration. It is still
unclear, however, whether SNRs can contribute to GCR spectrum up to the
``knee'' (1\,PeV) band as acceleration to such energies requires an efficient
magnetic field amplification process around the shocks. The presence of such a
process is challenging to test observationally. Here we report on the detection
of fast variability of the X-ray synchrotron emission from the forward shock in
the supernova remnant HESS J1731-347, which implies the presence of a strong
($\sim$0.2\,mG) field exceeding background values and thus of effective field
amplification. We also report a direct measurement of the high forward shock
expansion velocity of 4000-5500\,km/s confirming that the SNR is expanding in a
tenuous wind bubble blown by the SNR progenitor, is significantly younger
(2.4-9\,kyr) than previously assumed by some authors, and only recently started
interaction with the dense material outside the bubble. We finally conclude
that there is strong evidence for ongoing hadronic CR acceleration in this SNR.",http://arxiv.org/pdf/2309.14716v1
2309.14712v1,physics.comp-ph,Efficient Tracking of Dispersion Surfaces for Printed Structures using the Method of Moments,2023-09-26 07:09:15+00:00,"The dispersion surfaces of printed periodic structures in layered media are
efficiently computed using a full-wave method based on the periodic Method of
Moments (MoM). The geometry of the dispersion surface is estimated after
mapping the determinant of the periodic MoM impedance matrix over a range of
frequencies and impressed phase shifts. For lossless periodic structures in the
long-wavelength regime, such as lossless metasurfaces, a tracking algorithm is
proposed to represent the dispersion surface as a superposition of
parameterized iso-frequency curves. The mapping process of the determinant is
accelerated using a specialized interpolation technique with respect to the
frequency and impressed phase shifts. The algorithm combines a fast evaluation
of the rapidly varying part of the periodic impedance matrix and the
interpolation of the computationally intensive but slowly varying remainder.
The mapping is further accelerated through the use of Macro basis functions
(MBFs). The method has been first tested on lossless metasurface-type
structures and validated using the commercial software CST. The specialized
technique enables a drastic reduction of the number of periodic impedance
matrices that needs to be explicitly computed. In the two examples considered,
only 12 matrices are required to cover any phase shift and a frequency band
larger than one octave. An important advantage of the proposed method is that
it does not entail any approximation, so that it can be used for lossy
structure and leaky waves, as demonstrated through two additional examples.",http://arxiv.org/pdf/2309.14712v1
2309.14705v1,cond-mat.stat-mech,Dynamic fluctuations of current and mass in nonequilibrium mass transport processes,2023-09-26 06:58:02+00:00,"We study steady-state dynamic fluctuations of current and mass, as well as
the corresponding power spectra, in conserved-mass transport processes on a
ring of $L$ sites; these processes violate detailed balance, have nontrivial
spatial structures, and their steady states are not described by the
Boltzmann-Gibbs distribution. We exactly calculate, for all times $T$, the
fluctuations $\langle \mathcal{Q}_i^2(T) \rangle$ and $\langle
\mathcal{Q}_{sub}^2(l, T) \rangle$ of the cumulative currents upto time $T$
across $i$th bond and across a subsystem of size $l$ (summed over bonds in the
subsystem), respectively; we also calculate the (two-point) dynamic correlation
function for subsystem mass. In particular, we show that, for large $L \gg 1$,
the bond-current fluctuation grows linearly for $T \sim {\cal O}(1)$,
subdiffusively for $T \ll L^2$ and then again linearly for $T \gg L^2$. The
scaled subsystem current fluctuation $\lim_{l \rightarrow \infty, T \rightarrow
\infty} \langle \mathcal{Q}^2_{sub}(l, T) \rangle/2lT$ converges to the
density-dependent particle mobility $\chi$ when the large subsystem size limit
is taken first, followed by the large time limit. Remarkably, the scaled
current fluctuation $D \langle \mathcal{Q}_i^2(T)\rangle/2 \chi L \equiv {\cal
W}(y)$ as a function of scaled time $y=DT/L^2$ is expressed in terms of a
universal scaling function ${\cal W}(y)$, where $D$ is the bulk-diffusion
coefficient. Similarly, the power spectra for current and mass time series are
characterized by the respective universal scaling functions, which are
calculated exactly. We provide a microscopic derivation of equilibrium-like
Green-Kubo and Einstein relations, that connect the steady-state current
fluctuations to the response to an external force and to mass fluctuation,
respectively.",http://arxiv.org/pdf/2309.14705v1
2309.14704v1,cs.CV,Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer,2023-09-26 06:56:01+00:00,"Viewport prediction is a crucial aspect of tile-based 360 video streaming
system. However, existing trajectory based methods lack of robustness, also
oversimplify the process of information construction and fusion between
different modality inputs, leading to the error accumulation problem. In this
paper, we propose a tile classification based viewport prediction method with
Multi-modal Fusion Transformer, namely MFTR. Specifically, MFTR utilizes
transformer-based networks to extract the long-range dependencies within each
modality, then mine intra- and inter-modality relations to capture the combined
impact of user historical inputs and video contents on future viewport
selection. In addition, MFTR categorizes future tiles into two categories: user
interested or not, and selects future viewport as the region that contains most
user interested tiles. Comparing with predicting head trajectories, choosing
future viewport based on tile's binary classification results exhibits better
robustness and interpretability. To evaluate our proposed MFTR, we conduct
extensive experiments on two widely used PVS-HM and Xu-Gaze dataset. MFTR shows
superior performance over state-of-the-art methods in terms of average
prediction accuracy and overlap ratio, also presents competitive computation
efficiency.",http://arxiv.org/pdf/2309.14704v1
2309.14701v1,physics.optics,Light Field Ghost Imaging,2023-09-26 06:40:10+00:00,"Techniques based on classical and quantum correlations in light beams, such
as ghost imaging, allow us to overcome many limitations of conventional imaging
and sensing protocols. Despite their advantages, applications of such
techniques are often limited in practical scenarios where the position and the
longitudinal extension of the target object are unknown. In this work, we
propose and experimentally demonstrate a novel imaging technique, named Light
Field Ghost Imaging, that exploits light correlations and light field imaging
principles to enable going beyond the limitations of ghost imaging in a wide
range of applications. Notably, our technique removes the requirement to have
prior knowledge of the object distance allowing the possibility of refocusing
in post-processing, as well as performing 3D imaging while retaining all the
benefits of ghost imaging protocols.",http://arxiv.org/pdf/2309.14701v1
2309.14692v1,astro-ph.GA,Selection of Compton-thick AGN from a hard photometric sample using XMM-Newton observations,2023-09-26 06:09:06+00:00,"We present a selection technique to detect Compton-thick (CT) active galactic
nuclei (AGNs) in the 3XMM/SDSS-DR7 cross-correlation. A subsample of 3481 X-ray
sources that are detected in the hard band (2-8 keV) and have photometric
redshifts constitute our parent sample. We first applied an automated
spectral-fitting procedure to select highly absorbed sources (N_H > 10^23
cm^-2). We found 184 highly absorbed candidates. Then, we performed the
Bayesian Monte Carlo Markov chains (MCMCs) selection technique to find CT AGNs.
We also tested the MCMC selection technique by applying Monte Carlo
simulations. We found that the method is accurate at 90 percent independently
of the nature of the underlying source. Our sample contains 52 bona fide CT
AGNs. The CT AGNs were selected to have a range > 0.75 of probability of being
CT when either fitting with the two models Torus and MYTorus. About 75 percent
of CT AGNs in our sample had probabilities > 90 percent. From the spectral
analysis, we significantly found an anticorrelation between the equivalent
width of the neutral Fe K_{\alpha} line and the X-ray luminosity at 2-10 keV,
the so-called X-ray Baldwin effect.",http://arxiv.org/pdf/2309.14692v1
2309.14686v1,astro-ph.GA,Clump-scale Gas Infall in High-mass Star Formation: a Multi-transition View with JCMT HCN (4--3) Mapping,2023-09-26 05:45:19+00:00,"Gas infall motions play a crucial role in high-mass star formation and are
characterized by observable signatures in the form of blue-shifted asymmetric
spectral line profiles (""blue profiles""). However, the connection between blue
profiles and infall motions is unclear due to complex gas motions at parsec
scales. In this study, we present the results of an HCN (4-3) mapping survey
conducted with the JCMT, towards 38 massive clumps exhibiting blue profiles in
HCO+ (3-2). We extract 34 HCN cores from the 38 observed fields. The
core-averaged spectra show various line profiles, indicating that blue-profile
HCO+ (3-2) does not guarantee the same in HCN (4-3). Through non-LTE radiation
transfer calculations, we attribute the low detection rate of high-$J$ blue
profiles to a combination of insufficient HCN (4-3) opacity and intricate gas
motion across different density layers. The comparison between the MALT90 and
BGPS line surveys highlights the importance of appropriate tracers, high
spectral resolution, and column density thresholds when searching for blue
profiles. We select 11 reliable infall candidates and adopt the Hill5 model to
fit the infall velocity of 0.2-1.9 km/s, corresponding to 5% to 74% of
free-fall velocity. Assuming a spherically collapsing model, we estimate the
median and mean mass infall rates to be 4.5E-3 and 7.6E-3 Msun/year,
respectively. The consistency of the mass infall rates among different
transitions suggests a steady accretion process from the clump gas envelope to
the inner region.",http://arxiv.org/pdf/2309.14686v1
2309.14685v1,cs.RO,DriveSceneGen: Generating Diverse and Realistic Driving Scenarios from Scratch,2023-09-26 05:40:43+00:00,"Realistic and diverse traffic scenarios in large quantities are crucial for
the development and validation of autonomous driving systems. However, owing to
numerous difficulties in the data collection process and the reliance on
intensive annotations, real-world datasets lack sufficient quantity and
diversity to support the increasing demand for data. This work introduces
DriveSceneGen, a data-driven driving scenario generation method that learns
from the real-world driving dataset and generates entire dynamic driving
scenarios from scratch. DriveSceneGen is able to generate novel driving
scenarios that align with real-world data distributions with high fidelity and
diversity. Experimental results on 5k generated scenarios highlight the
generation quality, diversity, and scalability compared to real-world datasets.
To the best of our knowledge, DriveSceneGen is the first method that generates
novel driving scenarios involving both static map elements and dynamic traffic
participants from scratch.",http://arxiv.org/pdf/2309.14685v1
2309.14684v1,astro-ph.GA,"The ALMA Survey of Star Formation and Evolution in Massive Protoclusters with Blue Profiles (ASSEMBLE): Core Growth, Cluster Contraction, and Primordial Mass Segregation",2023-09-26 05:38:04+00:00,"The ALMA Survey of Star Formation and Evolution in Massive Protoclusters with
Blue Profiles (ASSEMBLE) aims to investigate the process of mass assembly and
its connection to high-mass star formation theories in protoclusters in a
dynamic view. We observed 11 massive (Mclump>1000 Msun), luminous (Lbol>10,000
Lsun), and blue-profile (infall signature) clumps by ALMA with resolution of
2200-5500 au at 350 GHz (870 um) in continuum and line emission. 248 dense
cores were identified, including 106 cores showing protostellar signatures and
142 prestellar core candidates. Compared to early-stage infrared dark clouds
(IRDCs) by ASHES, the core mass and surface density within the ASSEMBLE clumps
exhibited significant increment, suggesting concurrent core accretion during
the evolution of the clumps. The maximum mass of prestellar cores was found to
be 2 times larger than that in IRDCs, indicating evolved protoclusters have the
potential to harbor massive prestellar cores. The mass relation between clumps
and their most massive core (MMCs) is observed in ASSEMBLE but not in IRDCs,
which is suggested to be regulated by multiscale mass accretion. The mass
correlation between the core clusters and their MMCs has a steeper slope
compared to that observed in stellar clusters, which can be due to
fragmentation of the MMC and stellar multiplicity. We observe a decrease in
core separation and an increase in central concentration as protoclusters
evolve. We confirm primordial mass segregation in the ASSEMBLE protoclusters,
possibly resulting from gravitational concentration and/or gas accretion.",http://arxiv.org/pdf/2309.14684v1
2309.14683v1,cs.CV,A Simple Text to Video Model via Transformer,2023-09-26 05:26:30+00:00,"We present a general and simple text to video model based on Transformer.
Since both text and video are sequential data, we encode both texts and images
into the same hidden space, which are further fed into Transformer to capture
the temporal consistency and then decoder to generate either text or images.
Considering the image signal may become weak in the long sequence, we introduce
the U-Net to reconstruct image from its noised version. Specifically, we
increase the noise level to the original image in the long sequence, then use
the $down$ module from U-Net to encode noised images, which are further input
to transformer to predict next clear images. We also add a constraint to
promote motion between any generated image pair in the video. We use GPT2 and
test our approach on UCF101 dataset and show it can generate promising videos.",http://arxiv.org/pdf/2309.14683v1
2309.14675v1,cs.LG,FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler,2023-09-26 05:03:13+00:00,"Cross-silo federated learning offers a promising solution to collaboratively
train robust and generalized AI models without compromising the privacy of
local datasets, e.g., healthcare, financial, as well as scientific projects
that lack a centralized data facility. Nonetheless, because of the disparity of
computing resources among different clients (i.e., device heterogeneity),
synchronous federated learning algorithms suffer from degraded efficiency when
waiting for straggler clients. Similarly, asynchronous federated learning
algorithms experience degradation in the convergence rate and final model
accuracy on non-identically and independently distributed (non-IID)
heterogeneous datasets due to stale local models and client drift. To address
these limitations in cross-silo federated learning with heterogeneous clients
and data, we propose FedCompass, an innovative semi-asynchronous federated
learning algorithm with a computing power aware scheduler on the server side,
which adaptively assigns varying amounts of training tasks to different clients
using the knowledge of the computing power of individual clients. FedCompass
ensures that multiple locally trained models from clients are received almost
simultaneously as a group for aggregation, effectively reducing the staleness
of local models. At the same time, the overall training process remains
asynchronous, eliminating prolonged waiting periods from straggler clients.
Using diverse non-IID heterogeneous distributed datasets, we demonstrate that
FedCompass achieves faster convergence and higher accuracy than other
asynchronous algorithms while remaining more efficient than synchronous
algorithms when performing federated learning on heterogeneous clients.",http://arxiv.org/pdf/2309.14675v1
2309.14668v1,physics.optics,Depolarized Holography with Polarization-multiplexing Metasurface,2023-09-26 04:47:04+00:00,"The evolution of computer-generated holography (CGH) algorithms has prompted
significant improvements in the performances of holographic displays.
Nonetheless, they start to encounter a limited degree of freedom in CGH
optimization and physical constraints stemming from the coherent nature of
holograms. To surpass the physical limitations, we consider polarization as a
new degree of freedom by utilizing a novel optical platform called metasurface.
Polarization-multiplexing metasurfaces enable incoherent-like behavior in
holographic displays due to the mutual incoherence of orthogonal polarization
states. We leverage this unique characteristic of a metasurface by integrating
it into a holographic display and exploiting polarization diversity to bring an
additional degree of freedom for CGH algorithms. To minimize the speckle noise
while maximizing the image quality, we devise a fully differentiable
optimization pipeline by taking into account the metasurface proxy model,
thereby jointly optimizing spatial light modulator phase patterns and geometric
parameters of metasurface nanostructures. We evaluate the metasurface-enabled
depolarized holography through simulations and experiments, demonstrating its
ability to reduce speckle noise and enhance image quality.",http://arxiv.org/pdf/2309.14668v1
2309.14667v1,hep-ex,Investigating the $ΔI = 1/2$ rule and CP violation through the measurement of decay asymmetry parameters in $Ξ^-$ decays,2023-09-26 04:46:22+00:00,"Using $(10087\pm44)\times 10^{6}$ $J/\psi$ events collected with the BESIII
detector, numerous $\Xi^-$ and $\Lambda$ decay asymmetry parameters are
simultaneously determined from the process $J/\psi \to \Xi^- \bar{\Xi}^+ \to
\Lambda(p\pi^-) \pi^- \bar{\Lambda}(\bar{n} \pi^0) \pi^+$ and its
charge-conjugate channel. The precisions of $\alpha_0$ for $\Lambda \to n\pi^0$
and $\bar{\alpha}_0$ for $\bar{\Lambda} \to \bar{n}\pi^0$ compared to world
averages are improved by factors of 4 and 1.7, respectively. The ratio of decay
asymmetry parameters of $\Lambda \to n\pi^0$ to that of $\Lambda \to p\pi^-$,
$\langle \alpha_0 \rangle/ \langle \alpha_{\Lambda -} \rangle $, is determined
to be $ 0.873 \pm 0.012^{+0.011}_{-0.010}$, where the first and the second
uncertainties are statistical and systematic, respectively. The ratio is
smaller than unity, which is predicted by the $\Delta I = 1/2$ rule, with a
statistical significance of more than $5\sigma$. We test for CP violation in
$\Xi^- \to \Lambda \pi^-$ and in $\Lambda \to n \pi^{0}$ with the best
precision to date.",http://arxiv.org/pdf/2309.14667v1
2309.14661v1,cond-mat.soft,Micro-Macro Modeling of Polymeric Fluids and Shear-Induced Microscopic Behaviors,2023-09-26 04:34:55+00:00,"This article delves into the micro-macro modeling of polymeric fluids,
considering various microscopic potential energies, including the classical
Hookean potential, as well as newly proposed modified Morse and Elastic-plastic
potentials. These proposed potentials encompass microscopic-scale bond-breaking
processes. The development of a thermodynamically consistent micro-macro model
is revisited, employing the energy variational method. To validate the model's
predictions, we conduct numerical simulations utilizing a deterministic
particle-FEM method. Our numerical findings shed light on the distinct
behaviors exhibited by polymer chains at the micro-scale in comparison to the
macro-scale velocity and induced shear stresses of fluids under shear flow.
Notably, we observe that polymer elongation, rotation, and bond breaking
contribute to the zero polymer-induced stress in the micro-macro model when
employing Morse and Elastic-plastic potentials. Furthermore, at high shear
rates, polymer rotation is found to induce shear-thinning behavior in the model
employing the classical Hookean potential.",http://arxiv.org/pdf/2309.14661v1
2309.14658v1,stat.CO,Improvements on Scalable Stochastic Bayesian Inference Methods for Multivariate Hawkes Process,2023-09-26 04:28:58+00:00,"Multivariate Hawkes Processes (MHPs) are a class of point processes that can
account for complex temporal dynamics among event sequences. In this work, we
study the accuracy and computational efficiency of three classes of algorithms
which, while widely used in the context of Bayesian inference, have rarely been
applied in the context of MHPs: stochastic gradient expectation-maximization,
stochastic gradient variational inference and stochastic gradient Langevin
Monte Carlo. An important contribution of this paper is a novel approximation
to the likelihood function that allows us to retain the computational
advantages associated with conjugate settings while reducing approximation
errors associated with the boundary effects. The comparisons are based on
various simulated scenarios as well as an application to the study the risk
dynamics in the Standard & Poor's 500 intraday index prices among its 11
sectors.",http://arxiv.org/pdf/2309.14658v1
2309.14657v1,cs.RO,Field Testing of a Stochastic Planner for ASV Navigation Using Satellite Images,2023-09-26 04:27:41+00:00,"We introduce a multi-sensor navigation system for autonomous surface vessels
(ASV) intended for water-quality monitoring in freshwater lakes. Our mission
planner uses satellite imagery as a prior map, formulating offline a
mission-level policy for global navigation of the ASV and enabling autonomous
online execution via local perception and local planning modules. A significant
challenge is posed by the inconsistencies in traversability estimation between
satellite images and real lakes, due to environmental effects such as wind,
aquatic vegetation, shallow waters, and fluctuating water levels. Hence, we
specifically modelled these traversability uncertainties as stochastic edges in
a graph and optimized for a mission-level policy that minimizes the expected
total travel distance. To execute the policy, we propose a modern local planner
architecture that processes sensor inputs and plans paths to execute the
high-level policy under uncertain traversability conditions. Our system was
tested on three km-scale missions on a Northern Ontario lake, demonstrating
that our GPS-, vision-, and sonar-enabled ASV system can effectively execute
the mission-level policy and disambiguate the traversability of stochastic
edges. Finally, we provide insights gained from practical field experience and
offer several future directions to enhance the overall reliability of ASV
navigation systems.",http://arxiv.org/pdf/2309.14657v1
2309.14656v1,math.AG,Auto-Arcs of Complete Intersection Varieties,2023-09-26 04:23:51+00:00,"We systematically study the so-called auto-arc spaces. Auto-arc spaces were
originally introduced by Schoutens in 2012 and later generalized and studied by
the author in his PhD Thesis and subsequent work. In that aforementioned work,
only results concerning trivial deformations were explicitly considered because
even in that case auto-arc spaces being a subset of generalized jet schemes are
difficult to understand. The major advance in this work is obtained by
considering auto arc spaces of complete intersections. It is shown that over
$k[t]/(t^{n+1})$, these spaces can be viewed as global flat deformations over
$\mathbb{A}_k^n$ of the classical jet scheme of order $n$. We propose the
project in general of investigating the flat locus of this naturally induced
morphism as a type of relativized version of previous results by Mustata on jet
schemes a local complete intersections. We also introduce the study of
so-called strong/weak deformations of curves in this context, and we show that
a motivic volume can be defined in this case.",http://arxiv.org/pdf/2309.14656v1
2309.14651v1,cond-mat.mtrl-sci,CO Adsorption on the Surface of MgO from Periodic Coupled-Cluster Theory with Local Natural Orbitals: Adding to the Consensus,2023-09-26 04:05:17+00:00,"Accurate determination of the adsorption energy of CO on the MgO (001)
surface has been a challenge for both computations and experiments over the
past three decades. A recent computational study by Shi and co-workers
(10.26434/chemrxiv-2023-h4czl) reported good agreement within $11$ meV ($1$
kJ/mol) between two popular theoretical methods: coupled-cluster with singles,
doubles, and perturbative triples [CCSD(T)] and diffusion Monte Carlo. In this
short note, we report results on the same problem from periodic Gaussian-based
MP2, CCSD, and CCSD(T), with the latter two performed using a recently
developed extension of the local natural orbital (LNO) approximation to
problems with periodic boundary conditions. Our final periodic LNO-CCSD(T)
adsorption energy ($-198 \pm 11$ meV) is in quantitative agreement with the
embedded cluster-based LNO-CCSD(T) result ($-199 \pm 11$ meV) by Shi and
co-workers. The computational cost of our periodic LNO-CCSD(T) calculations is
comparable to that of the embedded cluster-based LNO-CCSD(T) and is 10 times
less expensive than the plane-wave-based periodic canonical CCSD(T) or 50 times
less expensive than the DMC calculations reported by Shi and co-workers. Our
findings highlight the accuracy and computational efficiency of the periodic
LNO-based approach for the simulation of surface chemistry with correlated
wavefunction methods.",http://arxiv.org/pdf/2309.14651v1
2309.14647v1,cs.NI,State-Compute Replication: Parallelizing High-Speed Stateful Packet Processing,2023-09-26 03:55:46+00:00,"With the slowdown of Moore's law, CPU-oriented packet processing in software
will be significantly outpaced by emerging line speeds of network interface
cards (NICs). Single-core packet-processing throughput has saturated.
  We consider the problem of high-speed packet processing with multiple CPU
cores. The key challenge is state--memory that multiple packets must read and
update. The prevailing method to scale throughput with multiple cores involves
state sharding, processing all packets that update the same state, i.e., flow,
at the same core. However, given the heavy-tailed nature of realistic flow size
distributions, this method will be untenable in the near future, since total
throughput is severely limited by single core performance.
  This paper introduces state-compute replication, a principle to scale the
throughput of a single stateful flow across multiple cores using replication.
Our design leverages a packet history sequencer running on a NIC or
top-of-the-rack switch to enable multiple cores to update state without
explicit synchronization. Our experiments with realistic data center and
wide-area Internet traces shows that state-compute replication can scale total
packet-processing throughput linearly with cores, deterministically and
independent of flow size distributions, across a range of realistic
packet-processing programs.",http://arxiv.org/pdf/2309.14647v1
2309.14638v1,cond-mat.mtrl-sci,Deep Charge: A Deep Learning Model of Electron Density from One-Shot Density Functional Theory Calculation,2023-09-26 03:36:13+00:00,"Electron charge density is a fundamental physical quantity, determining
various properties of matter. In this study, we have proposed a deep-learning
model for accurate charge density prediction. Our model naturally preserves
physical symmetries and can be effectively trained from one-shot density
functional theory calculation toward high accuracy. It captures detailed atomic
environment information, ensuring accurate predictions of charge density across
bulk, surface, molecules, and amorphous structures. This implementation
exhibits excellent scalability and provides efficient analyses of material
properties in large-scale condensed matter systems.",http://arxiv.org/pdf/2309.14638v1
2309.14633v1,physics.soc-ph,How crowd accidents are reported in the media: Lexical and sentiment analyses,2023-09-26 03:26:44+00:00,"The portrayal of crowd accidents by the media can influence public
understanding and emotional response, shaping societal perceptions and
potentially impacting safety measures and preparedness strategies. This paper
critically examines the portrayal of crowd accidents in news coverage by
analyzing the texts of 372 media reports of crowd accidents spanning 26 diverse
news sources from 1900 to 2019. We investigate how media representations of
crowd accidents vary across time and geographical origins. Our methodology
combines lexical analysis to unveil prevailing terminologies and sentiment
analysis to discern the emotional tenor of the reports. Contrary to anticipated
results, the findings reveal the prevalence of the term ""stampede"" over ""panic""
in media descriptions of crowd accidents. Notably, divergent patterns are
observable when comparing Western versus South Asian media (notably India and
Pakistan), unveiling a cross-cultural dimension. Moreover, the analysis detects
a gradual transition from ""crowd stampede"" to ""crowd crush"" in media and
Wikipedia narratives in recent years, suggesting evolving lexical
sensitivities. Sentiment analysis uncovers a consistent association with
fear-related language, indicative of media's propensity towards sensationalism.
This fear-infused narrative has intensified over time. The study underscores
the potential impact of language and sentiment in shaping public perspectives
on crowd accidents, revealing a pressing need for responsible and balanced
reporting that moves beyond sensationalism and promotes a nuanced
understanding. This will be crucial for increasing public awareness and
preparedness against such accidents.",http://arxiv.org/pdf/2309.14633v1
2309.14629v1,cs.CE,Decarbonization of aviation via hydrogen propulsion: technology performance targets and energy system impacts,2023-09-26 02:48:56+00:00,"The aviation sector is challenging to decarbonize since aircraft require high
power and energy per unit of weight. Liquid hydrogen is an interesting solution
due to its high gravimetric energy density, minimal warming impact, and
low-carbon production potential. We quantify the performance targets for fuel
cell systems and on-board storage to enable hydrogen-powered regional aviation.
We then explore the energy infrastructure impacts of meeting this additional H2
demand in the European context under deep decarbonization scenarios. We find
that minimal payload reduction would be needed for powering regional aviation
up to 1000 nmi if fuel cell system specific power of 2 kW/kg and tank
gravimetric index of 50% can be achieved. The energy systems analysis
highlights the importance of utilizing multiple technology options: such as
nuclear expansion and natural gas reforming with CCS for hydrogen production.
Levelized cost of liquid hydrogen as low as 3.5 Euros/kg demonstrates pathways
for Europe to achieve cost-competitive production.",http://arxiv.org/pdf/2309.14629v1
2309.14625v1,math.FA,Riesz bases of exponentials for multi-tiling measures,2023-09-26 02:26:15+00:00,"Let $G$ be a closed subgroup of ${\mathbb R}^d$ and let $\nu$ be a Borel
probability measure admitting a Riesz basis of exponentials with frequency sets
in the dual group $G^{\perp}$. We form a multi-tiling measure $\mu =
\mu_1+...+\mu_N$ where $\mu_i$ is translationally equivalent to $\nu$ and
different $\mu_i$ and $\mu_j$ have essentially disjoint support. We obtain some
necessary and sufficient conditions for $\mu$ to admit a Riesz basis of
exponentials . As an application, the square boundary, after a rotation, is a
union of two fundamental domains of $G = {\mathbb Z}\times {\mathbb R}$ and can
be regarded as a multi-tiling measure. We show that, unfortunately, the square
boundary does not admit a Riesz basis of exponentials of the form as a union of
translate of discrete subgroups ${\mathbb Z}\times \{0\}$. This rules out a
natural candidate of potential Riesz basis for the square boundary.",http://arxiv.org/pdf/2309.14625v1
2309.14623v1,cs.CV,Text-to-Image Generation for Abstract Concepts,2023-09-26 02:22:39+00:00,"Recent years have witnessed the substantial progress of large-scale models
across various domains, such as natural language processing and computer
vision, facilitating the expression of concrete concepts. Unlike concrete
concepts that are usually directly associated with physical objects, expressing
abstract concepts through natural language requires considerable effort, which
results from their intricate semantics and connotations. An alternative
approach is to leverage images to convey rich visual information as a
supplement. Nevertheless, existing Text-to-Image (T2I) models are primarily
trained on concrete physical objects and tend to fail to visualize abstract
concepts. Inspired by the three-layer artwork theory that identifies critical
factors, intent, object and form during artistic creation, we propose a
framework of Text-to-Image generation for Abstract Concepts (TIAC). The
abstract concept is clarified into a clear intent with a detailed definition to
avoid ambiguity. LLMs then transform it into semantic-related physical objects,
and the concept-dependent form is retrieved from an LLM-extracted form pattern
set. Information from these three aspects will be integrated to generate
prompts for T2I models via LLM. Evaluation results from human assessments and
our newly designed metric concept score demonstrate the effectiveness of our
framework in creating images that can sufficiently express abstract concepts.",http://arxiv.org/pdf/2309.14623v1
2309.14621v1,stat.ME,Confidence Intervals for the F1 Score: A Comparison of Four Methods,2023-09-26 02:20:13+00:00,"In Natural Language Processing (NLP), binary classification algorithms are
often evaluated using the F1 score. Because the sample F1 score is an estimate
of the population F1 score, it is not sufficient to report the sample F1 score
without an indication of how accurate it is. Confidence intervals are an
indication of how accurate the sample F1 score is. However, most studies either
do not report them or report them using methods that demonstrate poor
statistical properties. In the present study, I review current analytical
methods (i.e., Clopper-Pearson method and Wald method) to construct confidence
intervals for the population F1 score, propose two new analytical methods
(i.e., Wilson direct method and Wilson indirect method) to do so, and compare
these methods based on their coverage probabilities and interval lengths, as
well as whether these methods suffer from overshoot and degeneracy. Theoretical
results demonstrate that both proposed methods do not suffer from overshoot and
degeneracy. Experimental results suggest that both proposed methods perform
better, as compared to current methods, in terms of coverage probabilities and
interval lengths. I illustrate both current and proposed methods on two
suggestion mining tasks. I discuss the practical implications of these results,
and suggest areas for future research.",http://arxiv.org/pdf/2309.14621v1
2309.14613v1,cs.ET,Design of a Superconducting Multiflux Non-Destructive Readout Memory Unit,2023-09-26 01:48:52+00:00,"Due to low power consumption and high-speed performance, superconductor
circuit technology has emerged as an attractive and compelling post-CMOS
technology candidate. However, the design of dense memory circuits presents a
significant challenge, especially for tasks that demand substantial memory
resources. While superconductor memory cells offer impressive speed, their
limited density is the primary yet-to-be-solved challenge. This study tackles
this challenge head-on by introducing a novel design for a Non-Destructive
Readout (NDRO) memory unit with single or multi-fluxon storage capabilities
within the same circuit architecture. Notably, single storage demonstrates a
critical margin exceeding 20\%, and multi-fluxon storage demonstrates 64\%,
ensuring reliable and robust operation even in the face of process variations.
These memory units exhibit high clock frequencies of 10GHz. The proposed
circuits offer compelling characteristics, including rapid data propagation and
minimal data refreshment requirements, while effectively addressing the density
concerns associated with superconductor memory, doubling the memory capacity
while maintaining the high throughput speed.",http://arxiv.org/pdf/2309.14613v1
2309.14612v1,stat.ML,Reparameterized Variational Rejection Sampling,2023-09-26 01:46:53+00:00,"Traditional approaches to variational inference rely on parametric families
of variational distributions, with the choice of family playing a critical role
in determining the accuracy of the resulting posterior approximation. Simple
mean-field families often lead to poor approximations, while rich families of
distributions like normalizing flows can be difficult to optimize and usually
do not incorporate the known structure of the target distribution due to their
black-box nature. To expand the space of flexible variational families, we
revisit Variational Rejection Sampling (VRS) [Grover et al., 2018], which
combines a parametric proposal distribution with rejection sampling to define a
rich non-parametric family of distributions that explicitly utilizes the known
target distribution. By introducing a low-variance reparameterized gradient
estimator for the parameters of the proposal distribution, we make VRS an
attractive inference strategy for models with continuous latent variables. We
argue theoretically and demonstrate empirically that the resulting
method--Reparameterized Variational Rejection Sampling (RVRS)--offers an
attractive trade-off between computational cost and inference fidelity. In
experiments we show that our method performs well in practice and that it is
well-suited for black-box inference, especially for models with local latent
variables.",http://arxiv.org/pdf/2309.14612v1
2309.14603v1,astro-ph.SR,MgAl burning chain in Omega Centauri,2023-09-26 01:16:44+00:00,"In this study, we report the results of Fe, Mg, Al, and Si abundances
analysis for a sample of 439 stars in Omega Centauri, using high-resolution
spectra obtained with the VLT/FLAMES multi-object spectrograph. Our analysis
reveals the presence of four distinct Fe populations, with the main peak
occurring at low metallicity, consistent with previous literature findings. We
observe a discrete and pronounced Mg-Al anti-correlation, which exhibits
variations in shape and extension as a function of metallicity. Specifically,
this anti-correlation is present in stars with metallicities lower than
approximately -1.3 dex, while it becomes less evident or absent for higher
[Fe/H] values. Additionally, we detect (anti-) correlations between Mg and Si,
and between Al and Si, whose extensions also vary with metallicity, similar to
the Mg-Al anti-correlation. These results suggest that the MgAl cycle plays a
crucial role in the formation of multiple populations in Omega Centauri, with
the presence of all (anti-) correlations at metallicities lower than -1.3 dex
providing evidence for the burning of Mg at very high temperatures (> 10^8 K),
at least in the metal-poor regime. Furthermore, we observe a clear trend of
stars with [Al/Fe] > +0.5 dex as a function of metallicity, confirming for the
first time the existence of the two channels of Al production and destruction.
This evidence can help to provide further constraints on the potential nature
of the polluters responsible for the observed chemical anomalies in this
stellar system. Finally, we find that the two most metal-poor populations
identified in our sample are compatible with null or very small metallicity
dispersion and we discuss how this result fit into a scenario where Omega
Centauri is the remnant of a disrupted nucleated dwarf galaxy.",http://arxiv.org/pdf/2309.14603v1
2309.14601v1,cs.LG,Neuro-Visualizer: An Auto-encoder-based Loss Landscape Visualization Method,2023-09-26 01:10:16+00:00,"In recent years, there has been a growing interest in visualizing the loss
landscape of neural networks. Linear landscape visualization methods, such as
principal component analysis, have become widely used as they intuitively help
researchers study neural networks and their training process. However, these
linear methods suffer from limitations and drawbacks due to their lack of
flexibility and low fidelity at representing the high dimensional landscape. In
this paper, we present a novel auto-encoder-based non-linear landscape
visualization method called Neuro-Visualizer that addresses these shortcoming
and provides useful insights about neural network loss landscapes. To
demonstrate its potential, we run experiments on a variety of problems in two
separate applications of knowledge-guided machine learning (KGML). Our findings
show that Neuro-Visualizer outperforms other linear and non-linear baselines
and helps corroborate, and sometime challenge, claims proposed by machine
learning community. All code and data used in the experiments of this paper are
available at an anonymous link
https://anonymous.4open.science/r/NeuroVisualizer-FDD6",http://arxiv.org/pdf/2309.14601v1
2309.14600v1,cs.CV,Progressive Text-to-3D Generation for Automatic 3D Prototyping,2023-09-26 01:08:35+00:00,"Text-to-3D generation is to craft a 3D object according to a natural language
description. This can significantly reduce the workload for manually designing
3D models and provide a more natural way of interaction for users. However,
this problem remains challenging in recovering the fine-grained details
effectively and optimizing a large-size 3D output efficiently. Inspired by the
success of progressive learning, we propose a Multi-Scale Triplane Network
(MTN) and a new progressive learning strategy. As the name implies, the
Multi-Scale Triplane Network consists of four triplanes transitioning from low
to high resolution. The low-resolution triplane could serve as an initial shape
for the high-resolution ones, easing the optimization difficulty. To further
enable the fine-grained details, we also introduce the progressive learning
strategy, which explicitly demands the network to shift its focus of attention
from simple coarse-grained patterns to difficult fine-grained patterns. Our
experiment verifies that the proposed method performs favorably against
existing methods. For even the most challenging descriptions, where most
existing methods struggle to produce a viable shape, our proposed method
consistently delivers. We aspire for our work to pave the way for automatic 3D
prototyping via natural language descriptions.",http://arxiv.org/pdf/2309.14600v1
2309.14598v1,q-bio.PE,Decoding trust: A reinforcement learning perspective,2023-09-26 01:06:29+00:00,"Behavioral experiments on the trust game have shown that trust and
trustworthiness are universal among human beings, contradicting the prediction
by assuming \emph{Homo economicus} in orthodox Economics. This means some
mechanism must be at work that favors their emergence. Most previous
explanations however need to resort to some factors based upon imitative
learning, a simple version of social learning. Here, we turn to the paradigm of
reinforcement learning, where individuals update their strategies by evaluating
the long-term return through accumulated experience. Specifically, we
investigate the trust game with the Q-learning algorithm, where each
participant is associated with two evolving Q-tables that guide one's decision
making as trustor and trustee respectively. In the pairwise scenario, we reveal
that high levels of trust and trustworthiness emerge when individuals
appreciate both their historical experience and returns in the future.
Mechanistically, the evolution of the Q-tables shows a crossover that resembles
human's psychological changes. We also provide the phase diagram for the game
parameters, where the boundary analysis is conducted. These findings are robust
when the scenario is extended to a latticed population. Our results thus
provide a natural explanation for the emergence of trust and trustworthiness
without external factors involved. More importantly, the proposed paradigm
shows the potential in deciphering many puzzles in human behaviors.",http://arxiv.org/pdf/2309.14598v1
2309.14591v1,eess.IV,Applications of Sequential Learning for Medical Image Classification,2023-09-26 00:46:25+00:00,"Purpose: The aim of this work is to develop a neural network training
framework for continual training of small amounts of medical imaging data and
create heuristics to assess training in the absence of a hold-out validation or
test set.
  Materials and Methods: We formulated a retrospective sequential learning
approach that would train and consistently update a model on mini-batches of
medical images over time. We address problems that impede sequential learning
such as overfitting, catastrophic forgetting, and concept drift through PyTorch
convolutional neural networks (CNN) and publicly available Medical MNIST and
NIH Chest X-Ray imaging datasets. We begin by comparing two methods for a
sequentially trained CNN with and without base pre-training. We then transition
to two methods of unique training and validation data recruitment to estimate
full information extraction without overfitting. Lastly, we consider an example
of real-life data that shows how our approach would see mainstream research
implementation.
  Results: For the first experiment, both approaches successfully reach a ~95%
accuracy threshold, although the short pre-training step enables sequential
accuracy to plateau in fewer steps. The second experiment comparing two methods
showed better performance with the second method which crosses the ~90%
accuracy threshold much sooner. The final experiment showed a slight advantage
with a pre-training step that allows the CNN to cross ~60% threshold much
sooner than without pre-training.
  Conclusion: We have displayed sequential learning as a serviceable
multi-classification technique statistically comparable to traditional CNNs
that can acquire data in small increments feasible for clinically realistic
scenarios.",http://arxiv.org/pdf/2309.14591v1
2309.14578v1,physics.chem-ph,Solvent Effects on Extractant Conformational Energetics in Liquid-Liquid Extraction: A Simulation Study of Molecular Solvents and Ionic Liquids,2023-09-25 23:36:08+00:00,"Extractant design in liquid-liquid extraction (LLE) is a research frontier of
metal ion separations that typically focuses on the direct extractant-metal
interactions. However, a more detailed understanding of energetic drivers of
separations beyond primary metal coordination is often lacking, including the
role of solvent in the extractant phase. In this work, we propose a new
mechanism for enhancing metal-complexant energetics with nanostructured
solvents. Using molecular dynamics simulations with umbrella sampling, we find
that the organic solvent can reshape the energetics of the extractant's
intramolecular conformational landscape. We calculate free energy profiles of
different conformations of a representative bidentate extractant,
n-octyl(phenyl)-N,N-diisobutyl carbamoyl methyl phosphinoxide (CMPO), in four
different solvents: dodecane, tributyl phosphate (TBP), and dry and wet ionic
liquid (IL) 1-ethyl-3-methylimidazolium bis(trifluoromethylsulfonyl)imide
([EMIM][Tf_2N]). By promoting reorganization of the extractant molecule into
its binding conformation, our findings reveal how particular solvents can
ameliorate this unfavorable step of the metal separation process. In
particular, the charge alternating nanodomains formed in ILs substantially
reduce the free energy penalty associated with extractant reorganization.
Importantly, using alchemical free energy calculations, we find that this
stabilization persists even when we explicitly include the extracted cation.
These findings provide insight into the energic drivers of metal ion
separations and potentially suggest a new approach to designing effective
separations using a molecular-level understanding of solvent effects.",http://arxiv.org/pdf/2309.14578v1
2309.14574v1,cond-mat.supr-con,Absence of nematic instability in the kagome metal CsV$_3$Sb$_5$,2023-09-25 23:15:13+00:00,"Ever since the discovery of the charge density wave (CDW) transition in the
kagome metal CsV$_3$Sb$_5$, the nature of its symmetry breaking is under
intense debate. While evidence suggests that the rotational symmetry is already
broken at the CDW transition temperature ($T_{\rm CDW}$), an additional
electronic nematic instability well below $T_{\rm CDW}$ was reported based on
the diverging elastoresistivity coefficient in the anisotropic channel
($m_{E_{2g}}$). Verifying the existence of a nematic transition below $T_{\rm
CDW}$ is not only critical for establishing the correct description of the CDW
order parameter, but also important for understanding the low-temperature
superconductivity. Here, we report elastoresistivity measurements of
CsV$_3$Sb$_5$ using three different techniques probing both isotropic and
anisotropic symmetry channels. Contrary to previous reports, we find the
anisotropic elastoresistivity coefficient $m_{E_{2g}}$ is
temperature-independent except for a step jump at $T_{\rm CDW}$. The absence of
nematic fluctuations is further substantiated by measurements of the
elastocaloric effect, which show no enhancement associated with nematic
susceptibility. On the other hand, the symmetric elastoresistivity coefficient
$m_{A_{1g}}$ increases below $T_{\rm CDW}$, reaching a peak value of 90 at $T^*
= 20$ K. Our results strongly indicate that the phase transition at $T^*$ is
not nematic in nature and the previously reported diverging elastoresistivity
is due to the contamination from the $A_{1g}$ channel.",http://arxiv.org/pdf/2309.14574v1
2309.14570v1,astro-ph.SR,MEMPSEP I : Forecasting the Probability of Solar Energetic Particle Event Occurrence using a Multivariate Ensemble of Convolutional Neural Networks,2023-09-25 22:44:41+00:00,"The Sun continuously affects the interplanetary environment through a host of
interconnected and dynamic physical processes. Solar flares, Coronal Mass
Ejections (CMEs), and Solar Energetic Particles (SEPs) are among the key
drivers of space weather in the near-Earth environment and beyond. While some
CMEs and flares are associated with intense SEPs, some show little to no SEP
association. To date, robust long-term (hours-days) forecasting of SEP
occurrence and associated properties (e.g., onset, peak intensities) does not
effectively exist and the search for such development continues. Through an
Operations-2-Research support, we developed a self-contained model that
utilizes a comprehensive dataset and provides a probabilistic forecast for SEP
event occurrence and its properties. The model is named Multivariate Ensemble
of Models for Probabilistic Forecast of Solar Energetic Particles (MEMPSEP).
MEMPSEP workhorse is an ensemble of Convolutional Neural Networks that ingests
a comprehensive dataset (MEMPSEP III - (Moreland et al., 2023)) of full-disc
magnetogram-sequences and in-situ data from different sources to forecast the
occurrence (MEMPSEP I - this work) and properties (MEMPSEP II - Dayeh et al.
(2023)) of a SEP event. This work focuses on estimating true SEP occurrence
probabilities achieving a 2.5% improvement in reliability and a Brier score of
0.14. The outcome provides flexibility for the end-users to determine their own
acceptable level of risk, rather than imposing a detection threshold that
optimizes an arbitrary binary classification metric. Furthermore, the
model-ensemble, trained to utilize the large class-imbalance between events and
non-events, provides a clear measure of uncertainty in our forecast",http://arxiv.org/pdf/2309.14570v1
2309.14569v1,physics.med-ph,Towards a Novel Ultrasound System Based on Low-Frequency Feature Extraction From a Fully-Printed Flexible Transducer,2023-09-25 22:42:27+00:00,"Ultrasound is a key technology in healthcare, and it is being explored for
non-invasive, wearable, continuous monitoring of vital signs. However, its
widespread adoption in this scenario is still hindered by the size, complexity,
and power consumption of current devices. Moreover, such an application demands
adaptability to human anatomy, which is hard to achieve with current transducer
technology. This paper presents a novel ultrasound system prototype based on a
fully printed, lead-free, and flexible polymer ultrasound transducer, whose
bending radius promises good adaptability to the human anatomy. Our application
scenario focuses on continuous blood flow monitoring. We implemented a hardware
envelope filter to efficiently transpose high-frequency ultrasound signals to a
lower-frequency spectrum. This reduces computational and power demands with
little to no degradation in the task proposed for this work. We validated our
method on a setup that mimics human blood flow by using a flow phantom and a
peristaltic pump simulating 3 different heartbeat rhythms: 60, 90 and 120 beats
per minute. Our ultrasound setup reconstructs peristaltic pump frequencies with
errors of less than 0.05 Hz (3 bpm) from the set pump frequency, both for the
raw echo and the enveloped echo. The analog pre-processing showed a promising
reduction of signal bandwidth of more than 6x: pulse-echo signals of
transducers excited at 12.5 MHz were reduced to about 2 MHz. Thus, allowing
consumer MCUs to acquire and elaborate signals within mW-power range in an
inexpensive fashion.",http://arxiv.org/pdf/2309.14569v1
2309.14568v1,cs.CL,Introducing DictaLM -- A Large Generative Language Model for Modern Hebrew,2023-09-25 22:42:09+00:00,"We present DictaLM, a large-scale language model tailored for Modern Hebrew.
Boasting 7B parameters, this model is predominantly trained on Hebrew-centric
data. As a commitment to promoting research and development in the Hebrew
language, we release both the foundation model and the instruct-tuned model
under a Creative Commons license. Concurrently, we introduce DictaLM-Rab,
another foundation model geared towards Rabbinic/Historical Hebrew. These
foundation models serve as ideal starting points for fine-tuning various
Hebrew-specific tasks, such as instruction, Q&A, sentiment analysis, and more.
This release represents a preliminary step, offering an initial Hebrew LLM
model for the Hebrew NLP community to experiment with.",http://arxiv.org/pdf/2309.14568v1
2309.14567v1,astro-ph.SR,The First Y Dwarf Data From JWST Show That Dynamic and Diabatic Processes Regulate Cold Brown Dwarf Atmospheres,2023-09-25 22:40:11+00:00,"The James Webb Space Telescope (JWST) is now observing Y dwarfs, the coldest
known brown dwarfs, with effective temperatures T_eff <= 475 K. The first
published observations provide important information: not only is the
atmospheric chemistry out of equilibrium, as previously known, but the
pressure-temperature profile is not in the standard adiabatic form. The rapid
rotation of these Jupiter-size, isolated, brown dwarfs dominates the
atmospheric dynamics, and thermal and compositional changes disrupt convection.
These processes produce a colder lower atmosphere, and a warmer upper
atmosphere, compared to a standard adiabatic profile. Leggett et al. (2021)
presented empirical models where the pressure-temperature profile was adjusted
so that synthetic spectra reproduced the 1 <= lambda um <= 20 spectral energy
distributions of brown dwarfs with 260 <= T_eff K <= 540. We show that spectra
generated by these models fit the first JWST Y dwarf spectrum better than
standard-adiabat models. Unexpectedly, there is no 4.3 um PH_3 feature in the
JWST spectrum and atmospheres without phosphorus better reproduce the 4 um flux
peak. Our analysis of new JWST photometry indicates that the recently
discovered faint secondary of the WISE J033605.05-014350AB system
(Calissendorff et al. 2023) has T_eff = 295 K, making it the first dwarf in the
significant luminosity gap between the 260 K WISE J085510.83-071442.5, and all
other known Y dwarfs. The adiabat-adjusted disequilibrium-chemistry models are
recommended for analyses of all brown dwarfs cooler than 600 K, and a grid is
publicly available. Photometric color transformations are provided in an
Appendix.",http://arxiv.org/pdf/2309.14567v1
2309.14562v1,cond-mat.stat-mech,Yang-Lee Zeros of Certain Antiferromagnetic Models,2023-09-25 22:22:54+00:00,"We revisit the somewhat less studied problem of Yang-Lee zeros of the Ising
antiferromagnet. For this purpose, we study two models, the nearest-neighbor
model on a square lattice, and the more tractable mean-field model
corresponding to infinite-ranged coupling between all sites. In the
high-temperature limit, we show that the logarithm of the Yang-Lee zeros can be
written as a series in half odd integer powers of the inverse temperature, $k$,
with the leading term $\sim k^{1/2}$. This result is true in any dimension and
for arbitrary lattices. We also show that the coefficients of the expansion
satisfy simple identities (akin to sum rules) for the nearest-neighbor case.
These new identities are verified numerically by computing the exact partition
function for a 2D square lattice of size $16\times16$. For the mean-field
model, we write down the partition function (termed the mean-field polynomials)
for the ferromagnetic (FM) and antiferromagnetic (AFM) cases, and derive from
them the mean-field equations. We analytically show that at high temperatures
the zeros of the AFM mean-field polynomial scale as $\sim k^{1/2}$ as well.
Using a simple numerical method, we find the roots lie on certain curves (the
root curves), in the thermodynamic limit for the mean-field polynomials for the
AFM case as well as for the FM one. Our results show a new root curve, that was
not found earlier. Our results also clearly illustrate the phase transition
expected for the FM and AFM cases, in the language of Yang-Lee zeros. Moreover,
for the AFM case, we observe that the root curves separate two distinct phases
of zero and non-zero complex staggered magnetization, and thus depict a complex
phase boundary.",http://arxiv.org/pdf/2309.14562v1
2309.14553v1,physics.flu-dyn,Inverse non-linear problem of the long wave run-up on coast,2023-09-25 21:52:41+00:00,"The study of the process of catastrophic tsunami-type waves on the coast
makes it possible to determine the destructive force of waves on the coast. In
hydrodynamics, the one-dimensional theory of the run-up of non-linear waves on
a flat slope has gained great popularity, within which rigorous analytical
results have been obtained in the class of non-breaking waves. In general, the
result depends on the characteristics of the wave approaching (or generated on)
the slope, which is usually not known in the measurements. Here we describe a
rigorous method for recovering the initial displacement in a source localised
in an inclined power-shaped channel from the characteristics of a moving
shoreline. The method uses the generalised Carrier-Greenspan transformation,
which allows one-dimensional non-linear shallow-water equations to be reduced
to linear ones. The solution is found in terms of Erd\'elyi-Kober integral
operator. Numerical verification of our results is presented for the cases of a
parabolic bay and an infinite plane beach.",http://arxiv.org/pdf/2309.14553v1
2309.14550v1,eess.IV,MEMO: Dataset and Methods for Robust Multimodal Retinal Image Registration with Large or Small Vessel Density Differences,2023-09-25 21:47:41+00:00,"The measurement of retinal blood flow (RBF) in capillaries can provide a
powerful biomarker for the early diagnosis and treatment of ocular diseases.
However, no single modality can determine capillary flowrates with high
precision. Combining erythrocyte-mediated angiography (EMA) with optical
coherence tomography angiography (OCTA) has the potential to achieve this goal,
as EMA can measure the absolute 2D RBF of retinal microvasculature and OCTA can
provide the 3D structural images of capillaries. However, multimodal retinal
image registration between these two modalities remains largely unexplored. To
fill this gap, we establish MEMO, the first public multimodal EMA and OCTA
retinal image dataset. A unique challenge in multimodal retinal image
registration between these modalities is the relatively large difference in
vessel density (VD). To address this challenge, we propose a segmentation-based
deep-learning framework (VDD-Reg) and a new evaluation metric (MSD), which
provide robust results despite differences in vessel density. VDD-Reg consists
of a vessel segmentation module and a registration module. To train the vessel
segmentation module, we further designed a two-stage semi-supervised learning
framework (LVD-Seg) combining supervised and unsupervised losses. We
demonstrate that VDD-Reg outperforms baseline methods quantitatively and
qualitatively for cases of both small VD differences (using the CF-FA dataset)
and large VD differences (using our MEMO dataset). Moreover, VDD-Reg requires
as few as three annotated vessel segmentation masks to maintain its accuracy,
demonstrating its feasibility.",http://arxiv.org/pdf/2309.14550v1
2309.15117v1,cs.CV,Generating Visual Scenes from Touch,2023-09-26 17:59:52+00:00,"An emerging line of work has sought to generate plausible imagery from touch.
Existing approaches, however, tackle only narrow aspects of the visuo-tactile
synthesis problem, and lag significantly behind the quality of cross-modal
synthesis methods in other domains. We draw on recent advances in latent
diffusion to create a model for synthesizing images from tactile signals (and
vice versa) and apply it to a number of visuo-tactile synthesis tasks. Using
this model, we significantly outperform prior work on the tactile-driven
stylization problem, i.e., manipulating an image to match a touch signal, and
we are the first to successfully generate images from touch without additional
sources of information about the scene. We also successfully use our model to
address two novel synthesis problems: generating images that do not contain the
touch sensor or the hand holding it, and estimating an image's shading from its
reflectance and touch.",http://arxiv.org/pdf/2309.15117v1
2309.15115v1,math.ST,Planted Random Number Partitioning Problem,2023-09-26 17:59:29+00:00,"We consider the random number partitioning problem (\texttt{NPP}): given a
list $X\sim \mathcal{N}(0,I_n)$ of numbers, find a partition
$\sigma\in\{-1,1\}^n$ with a small objective value
$H(\sigma)=\frac{1}{\sqrt{n}}\left|\langle \sigma,X\rangle\right|$. The
\texttt{NPP} is widely studied in computer science; it is also closely related
to the design of randomized controlled trials. In this paper, we propose a
planted version of the \texttt{NPP}: fix a $\sigma^*$ and generate $X\sim
\mathcal{N}(0,I_n)$ conditional on $H(\sigma^*)\le 3^{-n}$. The \texttt{NPP}
and its planted counterpart are statistically distinguishable as the smallest
objective value under the former is $\Theta(\sqrt{n}2^{-n})$ w.h.p.
  Our first focus is on the values of $H(\sigma)$. We show that, perhaps
surprisingly, planting does not induce partitions with an objective value
substantially smaller than $2^{-n}$: $\min_{\sigma \ne \pm \sigma^*}H(\sigma) =
\widetilde{\Theta}(2^{-n})$ w.h.p. Furthermore, we completely characterize the
smallest $H(\sigma)$ achieved at any fixed distance from $\sigma^*$. Our second
focus is on the algorithmic problem of efficiently finding a partition
$\sigma$, not necessarily equal to $\pm\sigma^*$, with a small $H(\sigma)$. We
show that planted \texttt{NPP} exhibits an intricate geometrical property known
as the multi Overlap Gap Property ($m$-OGP) for values $2^{-\Theta(n)}$. We
then leverage the $m$-OGP to show that stable algorithms satisfying a certain
anti-concentration property fail to find a $\sigma$ with
$H(\sigma)=2^{-\Theta(n)}$.
  Our results are the first instance of the $m$-OGP being established and
leveraged to rule out stable algorithms for a planted model. More importantly,
they show that the $m$-OGP framework can also apply to planted models, if the
algorithmic goal is to return a solution with a small objective value.",http://arxiv.org/pdf/2309.15115v1
2309.15109v1,cs.CV,DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation,2023-09-26 17:56:21+00:00,"3D perception based on the representations learned from multi-camera
bird's-eye-view (BEV) is trending as cameras are cost-effective for mass
production in autonomous driving industry. However, there exists a distinct
performance gap between multi-camera BEV and LiDAR based 3D object detection.
One key reason is that LiDAR captures accurate depth and other geometry
measurements, while it is notoriously challenging to infer such 3D information
from merely image input. In this work, we propose to boost the representation
learning of a multi-camera BEV based student detector by training it to imitate
the features of a well-trained LiDAR based teacher detector. We propose
effective balancing strategy to enforce the student to focus on learning the
crucial features from the teacher, and generalize knowledge transfer to
multi-scale layers with temporal fusion. We conduct extensive evaluations on
multiple representative models of multi-camera BEV. Experiments reveal that our
approach renders significant improvement over the student models, leading to
the state-of-the-art performance on the popular benchmark nuScenes.",http://arxiv.org/pdf/2309.15109v1
2309.15104v1,cs.GT,"Efficient, traceable, and numerical error-free implementation of the MMS voting rule",2023-09-26 17:52:41+00:00,"I propose an alternative algorithm to compute the MMS voting rule. Instead of
using linear programming, in this new algorithm the maximin support value of a
committee is computed using a sequence of maximum flow problems.",http://arxiv.org/pdf/2309.15104v1
2309.15097v1,cs.CV,Case Study: Ensemble Decision-Based Annotation of Unconstrained Real Estate Images,2023-09-26 17:43:58+00:00,"We describe a proof-of-concept for annotating real estate images using simple
iterative rule-based semi-supervised learning. In this study, we have gained
important insights into the content characteristics and uniqueness of
individual image classes as well as essential requirements for a practical
implementation.",http://arxiv.org/pdf/2309.15097v1
2309.15092v1,astro-ph.CO,Formation of the hydrogen line 21-cm in Dark Ages and Cosmic Dawn: dependences on cosmology and first light,2023-09-26 17:38:15+00:00,"We analyze the formation of the redshifted hyperfine structure line 21-cm of
hydrogen atom in the Dark Ages, Cosmic Dawn, and Reionization epochs. The
evolution of the global differential brightness temperature in this line was
computed to study its dependence on the values of cosmological parameters and
physical conditions in the intergalactic medium. Variations of the depth of the
Dark Ages absorption line at $z\sim80$ with variations of the cosmological
parameters $\Omega_b$, $\Omega_{cdm}$, $\Omega_{\Lambda}$, $\Omega_K$ and $H_0$
are studied. The standard model with post-Planck parameters predicts a value of
the differential brightness temperature in the center of the absorption line
$\sim$30-50 mK. The profile of this line can be quite another in the
non-standard cosmological models, which include the annihilating or decaying
dark matter, a primordial stochastic magnetic field, etc. It can be shallower
or be an emission bump instead of an absorption trough. It is also shown that
the position and depth of the Cosmic Dawn absorption line formed at 10<z<30,
due to the Wouthuysen-Field effect, is mainly defined by the spectral energy
distribution of the first sources of light. If reionization occurs at
$z_{ri}=7\pm1$, then the differential brightness temperature in the center of
this line is $\sim$80 mK. During the reionization, the emission with an
amplitude of $\sim$20 mK is possible. It is also shown that the temperature,
density, and degree of ionization of the baryonic component are decisive in
calculating the intensity of the 21-cm absorption/emission line from these
epochs.",http://arxiv.org/pdf/2309.15092v1
2309.15090v1,q-bio.NC,Single Biological Neurons as Temporally Precise Spatio-Temporal Pattern Recognizers,2023-09-26 17:32:08+00:00,"This PhD thesis is focused on the central idea that single neurons in the
brain should be regarded as temporally precise and highly complex
spatio-temporal pattern recognizers. This is opposed to the prevalent view of
biological neurons as simple and mainly spatial pattern recognizers by most
neuroscientists today. In this thesis, I will attempt to demonstrate that this
is an important distinction, predominantly because the above-mentioned
computational properties of single neurons have far-reaching implications with
respect to the various brain circuits that neurons compose, and on how
information is encoded by neuronal activity in the brain. Namely, that these
particular ""low-level"" details at the single neuron level have substantial
system-wide ramifications. In the introduction we will highlight the main
components that comprise a neural microcircuit that can perform useful
computations and illustrate the inter-dependence of these components from a
system perspective. In chapter 1 we discuss the great complexity of the
spatio-temporal input-output relationship of cortical neurons that are the
result of morphological structure and biophysical properties of the neuron. In
chapter 2 we demonstrate that single neurons can generate temporally precise
output patterns in response to specific spatio-temporal input patterns with a
very simple biologically plausible learning rule. In chapter 3, we use the
differentiable deep network analog of a realistic cortical neuron as a tool to
approximate the gradient of the output of the neuron with respect to its input
and use this capability in an attempt to teach the neuron to perform nonlinear
XOR operation. In chapter 4 we expand chapter 3 to describe extension of our
ideas to neuronal networks composed of many realistic biological spiking
neurons that represent either small microcircuits or entire brain regions.",http://arxiv.org/pdf/2309.15090v1
2309.15086v1,cs.CV,Video-adverb retrieval with compositional adverb-action embeddings,2023-09-26 17:31:02+00:00,"Retrieving adverbs that describe an action in a video poses a crucial step
towards fine-grained video understanding. We propose a framework for
video-to-adverb retrieval (and vice versa) that aligns video embeddings with
their matching compositional adverb-action text embedding in a joint embedding
space. The compositional adverb-action text embedding is learned using a
residual gating mechanism, along with a novel training objective consisting of
triplet losses and a regression target. Our method achieves state-of-the-art
performance on five recent benchmarks for video-adverb retrieval. Furthermore,
we introduce dataset splits to benchmark video-adverb retrieval for unseen
adverb-action compositions on subsets of the MSR-VTT Adverbs and ActivityNet
Adverbs datasets. Our proposed framework outperforms all prior works for the
generalisation task of retrieving adverbs from videos for unseen adverb-action
compositions. Code and dataset splits are available at
https://hummelth.github.io/ReGaDa/.",http://arxiv.org/pdf/2309.15086v1
2309.15082v1,cs.CV,RPEFlow: Multimodal Fusion of RGB-PointCloud-Event for Joint Optical Flow and Scene Flow Estimation,2023-09-26 17:23:55+00:00,"Recently, the RGB images and point clouds fusion methods have been proposed
to jointly estimate 2D optical flow and 3D scene flow. However, as both
conventional RGB cameras and LiDAR sensors adopt a frame-based data acquisition
mechanism, their performance is limited by the fixed low sampling rates,
especially in highly-dynamic scenes. By contrast, the event camera can
asynchronously capture the intensity changes with a very high temporal
resolution, providing complementary dynamic information of the observed scenes.
In this paper, we incorporate RGB images, Point clouds and Events for joint
optical flow and scene flow estimation with our proposed multi-stage multimodal
fusion model, RPEFlow. First, we present an attention fusion module with a
cross-attention mechanism to implicitly explore the internal cross-modal
correlation for 2D and 3D branches, respectively. Second, we introduce a mutual
information regularization term to explicitly model the complementary
information of three modalities for effective multimodal feature learning. We
also contribute a new synthetic dataset to advocate further research.
Experiments on both synthetic and real datasets show that our model outperforms
the existing state-of-the-art by a wide margin. Code and dataset is available
at https://npucvr.github.io/RPEFlow.",http://arxiv.org/pdf/2309.15082v1
2309.15071v1,astro-ph.CO,Sensitivity Analysis of Simulation-Based Inference for Galaxy Clustering,2023-09-26 17:08:24+00:00,"Simulation-based inference (SBI) is a promising approach to leverage high
fidelity cosmological simulations and extract information from the
non-Gaussian, non-linear scales that cannot be modeled analytically. However,
scaling SBI to the next generation of cosmological surveys faces the
computational challenge of requiring a large number of accurate simulations
over a wide range of cosmologies, while simultaneously encompassing large
cosmological volumes at high resolution. This challenge can potentially be
mitigated by balancing the accuracy and computational cost for different
components of the the forward model while ensuring robust inference. To guide
our steps in this, we perform a sensitivity analysis of SBI for galaxy
clustering on various components of the cosmological simulations: gravity
model, halo-finder and the galaxy-halo distribution models (halo-occupation
distribution, HOD). We infer the $\sigma_8$ and $\Omega_m$ using galaxy power
spectrum multipoles and the bispectrum monopole assuming a galaxy number
density expected from the luminous red galaxies observed using the Dark Energy
Spectroscopy Instrument (DESI). We find that SBI is insensitive to changing
gravity model between $N$-body simulations and particle mesh (PM) simulations.
However, changing the halo-finder from friends-of-friends (FoF) to Rockstar can
lead to biased estimate of $\sigma_8$ based on the bispectrum. For galaxy
models, training SBI on more complex HOD leads to consistent inference for less
complex HOD models, but SBI trained on simpler HOD models fails when applied to
analyze data from a more complex HOD model. Based on our results, we discuss
the outlook on cosmological simulations with a focus on applying SBI approaches
to future galaxy surveys.",http://arxiv.org/pdf/2309.15071v1
2309.15059v1,hep-th,An Elliptic One-Loop Amplitude in Anti-de-Sitter Space,2023-09-26 16:39:04+00:00,"We present full analytic results for the four-point one-loop amplitude of a
conformally coupled scalar in four-dimensional Anti-de-Sitter space dual to a
primary operator with scaling dimension 1. The computation is based on an
intriguing recent discovery, connecting Witten diagrams and flat-space Feynman
integrals, which led to an expression of the amplitude of interest as a pure
combination of single-valued multiple polylogarithms and an integral which
cannot be reduced to multiple polylogarithms. We explicitly evaluate that
integral in terms of elliptic multiple polylogarithms, finding that it is not
manifestly single-valued unlike the polylogarithmic contributions to the
amplitude. Further we compute the symbol of the integral and observe similar
structures as for (elliptic) flat-space amplitudes. The result presented here
adds to the relatively short list of explicitly known position space
curved-space amplitudes beyond tree level, and constitutes the first
curved-space amplitude evaluated in terms of elliptic multiple polylogarithms.",http://arxiv.org/pdf/2309.15059v1
2309.15041v1,physics.app-ph,Estimation of fatigue strength of TiN coatings using cyclic micro-impact testing,2023-09-26 16:16:59+00:00,"This study delves into the behaviour of a thin TiN coating on a tool steel
substrate material under dynamic and cyclic impacts through a comprehensive
approach combining experimental testing and computational modelling. In dynamic
impact tests, a pendulumbased setup investigates material responses under
varying acceleration loads, revealing a distinctive ""ringing effect"" as the
indenter bounces off the specimen's surface, with all plastic deformation
concentrated during the initial impact. The study also quantifies dynamic
hardness values, highlighting load-dependent behaviour and assessing the
coating system's energy dissipation capabilities. In cyclic impact tests,
materials experience permanent plastic deformation with each cycle, ultimately
leading to coating failure. Chemical analysis identifies an interlayer between
the coating and substrate, while cross-sectional analysis reveals the extent of
coating damage due to cycling and load. A three-dimensional map is constructed,
connecting acceleration load, sensed depth, and cycles to coating failure, and
an empirical equation characterizes the relationship between depth and cycles
before failure. The computational model scrutinizes traction component
distribution during loading and unloading, with a focus on normal and shear
tractions. The findings suggest the potential significance of normal traction
in interface fatigue failure. Overall, offering implications for understanding
and mitigating fatigue-related failures across various applications.",http://arxiv.org/pdf/2309.15041v1
2309.15033v1,cond-mat.stat-mech,Unveiling the Phase Diagram and Reaction Paths of the Active Model B with the Deep Minimum Action Method,2023-09-26 16:02:35+00:00,"Nonequilibrium phase transitions are notably difficult to analyze because
their mechanisms depend on the system's dynamics in a complex way due to the
lack of time-reversal symmetry. To complicate matters, the system's
steady-state distribution is unknown in general. Here, the phase diagram of the
active Model B is computed with a deep neural network implementation of the
geometric minimum action method (gMAM). This approach unveils the
unconventional reaction paths and nucleation mechanism by which the system
switches between the homogeneous and inhomogeneous phases in the binodal
region. Our main findings are: (i) the mean time to escape the phase-separated
state is (exponentially) extensive in the system size $L$, but it increases
non-monotonically with $L$; (ii) the mean time to escape the homogeneous state
is always finite, in line with the recent work of Cates and Nardini~[Phys. Rev.
Lett. 130, 098203]; (iii) at fixed $L$, the active term increases the stability
of the homogeneous phase, eventually destroying the phase separation in the
binodal for large but finite systems. Our results are particularly relevant for
active matter systems in which the number of constituents hardly goes beyond
$10^7$ and where finite-size effects matter.",http://arxiv.org/pdf/2309.15033v1
2309.15031v1,cs.CV,Nuclear Morphometry using a Deep Learning-based Algorithm has Prognostic Relevance for Canine Cutaneous Mast Cell Tumors,2023-09-26 16:01:15+00:00,"Variation in nuclear size and shape is an important criterion of malignancy
for many tumor types; however, categorical estimates by pathologists have poor
reproducibility. Measurements of nuclear characteristics (morphometry) can
improve reproducibility, but manual methods are time consuming. In this study,
we evaluated fully automated morphometry using a deep learning-based algorithm
in 96 canine cutaneous mast cell tumors with information on patient survival.
Algorithmic morphometry was compared with karyomegaly estimates by 11
pathologists, manual nuclear morphometry of 12 cells by 9 pathologists, and the
mitotic count as a benchmark. The prognostic value of automated morphometry was
high with an area under the ROC curve regarding the tumor-specific survival of
0.943 (95% CI: 0.889 - 0.996) for the standard deviation (SD) of nuclear area,
which was higher than manual morphometry of all pathologists combined (0.868,
95% CI: 0.737 - 0.991) and the mitotic count (0.885, 95% CI: 0.765 - 1.00). At
the proposed thresholds, the hazard ratio for algorithmic morphometry (SD of
nuclear area $\geq 9.0 \mu m^2$) was 18.3 (95% CI: 5.0 - 67.1), for manual
morphometry (SD of nuclear area $\geq 10.9 \mu m^2$) 9.0 (95% CI: 6.0 - 13.4),
for karyomegaly estimates 7.6 (95% CI: 5.7 - 10.1), and for the mitotic count
30.5 (95% CI: 7.8 - 118.0). Inter-rater reproducibility for karyomegaly
estimates was fair ($\kappa$ = 0.226) with highly variable
sensitivity/specificity values for the individual pathologists. Reproducibility
for manual morphometry (SD of nuclear area) was good (ICC = 0.654). This study
supports the use of algorithmic morphometry as a prognostic test to overcome
the limitations of estimates and manual measurements.",http://arxiv.org/pdf/2309.15031v1
2309.15019v1,cs.CV,IFT: Image Fusion Transformer for Ghost-free High Dynamic Range Imaging,2023-09-26 15:38:52+00:00,"Multi-frame high dynamic range (HDR) imaging aims to reconstruct ghost-free
images with photo-realistic details from content-complementary but spatially
misaligned low dynamic range (LDR) images. Existing HDR algorithms are prone to
producing ghosting artifacts as their methods fail to capture long-range
dependencies between LDR frames with large motion in dynamic scenes. To address
this issue, we propose a novel image fusion transformer, referred to as IFT,
which presents a fast global patch searching (FGPS) module followed by a
self-cross fusion module (SCF) for ghost-free HDR imaging. The FGPS searches
the patches from supporting frames that have the closest dependency to each
patch of the reference frame for long-range dependency modeling, while the SCF
conducts intra-frame and inter-frame feature fusion on the patches obtained by
the FGPS with linear complexity to input resolution. By matching similar
patches between frames, objects with large motion ranges in dynamic scenes can
be aligned, which can effectively alleviate the generation of artifacts. In
addition, the proposed FGPS and SCF can be integrated into various deep HDR
methods as efficient plug-in modules. Extensive experiments on multiple
benchmarks show that our method achieves state-of-the-art performance both
quantitatively and qualitatively.",http://arxiv.org/pdf/2309.15019v1
2309.15014v1,cond-mat.mes-hall,Efficient adaptive Bayesian estimation of a slowly fluctuating Overhauser field gradient,2023-09-26 15:34:33+00:00,"Slow fluctuations of Overhauser fields are an important source for
decoherence in spin qubits hosted in III-V semiconductor quantum dots. Focusing
on the effect of the field gradient on double-dot singlet-triplet qubits, we
present two adaptive Bayesian schemes to estimate the magnitude of the gradient
by a series of free induction decay experiments. We concentrate on reducing the
computational overhead, with a real-time implementation of the schemes in mind.
We show how it is possible to achieve a significant improvement of estimation
accuracy compared to more traditional estimation methods. We include an
analysis of the effects of dephasing and the drift of the gradient itself.",http://arxiv.org/pdf/2309.15014v1
2309.15005v1,math.AP,Sharp conditions for exponential and non-exponential uniform stabilization of the time dependent damped wave equation,2023-09-26 15:24:06+00:00,"It is classical that uniform stabilization of solutions to the damped wave
equation is equivalent to the geometric control condition The author previously
showed that, when the damping depends on time, a generalization of the
geometric control condition implies uniform stabilization at an exponential
rate. In this paper, it is shown that this generalization of the geometric
control condition is necessary for uniform stabilization at an exponential
rate. Furthermore, when the damping does not satisfy this generalization, and
has some additional structure, upper and lower bounds on non-exponential
uniform stabilization are computed. The qualitative behavior of these upper and
lower bounds coincide.",http://arxiv.org/pdf/2309.15005v1
2309.15003v1,math.NA,Convergence Analysis of Nonlinear Kaczmarz Method for Systems of Nonlinear Equations with Component-wise Convex Mapping,2023-09-26 15:18:42+00:00,"Motivated by a class of nonlinear imaging inverse problems, for instance,
multispectral computed tomography (MSCT), this paper studies the convergence
theory of the nonlinear Kaczmarz method (NKM) for solving systems of nonlinear
equations with component-wise convex mapping, namely, the function
corresponding to each equation being convex. Although the tangential cone
condition (TCC) is often used to prove the convergence of NKM, it may be
impossible or difficult to verify/satisfy this condition for such kind of
nonlinear systems. We propose a novel condition named relative gradient
discrepancy condition (RGDC), and make use of it to prove the convergence and
even the convergence rate of NKM with several general index selection
strategies, where these strategies include the cyclic strategy and maximum
residual strategy. Particularly, we investigate the application of NKM for
solving nonlinear systems in MSCT image reconstruction. We prove that the
nonlinear mapping of interest fulfills the proposed RGDC rather than the
component-wise local TCC, and provide the global convergence of NKM based on
the previously obtained results. Numerical experiments further illustrate the
numerical convergence of NKM for MSCT image reconstruction.",http://arxiv.org/pdf/2309.15003v1
2309.14999v1,cs.CV,Object-Centric Open-Vocabulary Image-Retrieval with Aggregated Features,2023-09-26 15:13:09+00:00,"The task of open-vocabulary object-centric image retrieval involves the
retrieval of images containing a specified object of interest, delineated by an
open-set text query. As working on large image datasets becomes standard,
solving this task efficiently has gained significant practical importance.
Applications include targeted performance analysis of retrieved images using
ad-hoc queries and hard example mining during training. Recent advancements in
contrastive-based open vocabulary systems have yielded remarkable
breakthroughs, facilitating large-scale open vocabulary image retrieval.
However, these approaches use a single global embedding per image, thereby
constraining the system's ability to retrieve images containing relatively
small object instances. Alternatively, incorporating local embeddings from
detection pipelines faces scalability challenges, making it unsuitable for
retrieval from large databases.
  In this work, we present a simple yet effective approach to object-centric
open-vocabulary image retrieval. Our approach aggregates dense embeddings
extracted from CLIP into a compact representation, essentially combining the
scalability of image retrieval pipelines with the object identification
capabilities of dense detection methods. We show the effectiveness of our
scheme to the task by achieving significantly better results than global
feature approaches on three datasets, increasing accuracy by up to 15 mAP
points. We further integrate our scheme into a large scale retrieval framework
and demonstrate our method's advantages in terms of scalability and
interpretability.",http://arxiv.org/pdf/2309.14999v1
2309.14998v1,cs.CV,An Ensemble Model for Distorted Images in Real Scenarios,2023-09-26 15:12:55+00:00,"Image acquisition conditions and environments can significantly affect
high-level tasks in computer vision, and the performance of most computer
vision algorithms will be limited when trained on distortion-free datasets.
Even with updates in hardware such as sensors and deep learning methods, it
will still not work in the face of variable conditions in real-world
applications. In this paper, we apply the object detector YOLOv7 to detect
distorted images from the dataset CDCOCO. Through carefully designed
optimizations including data enhancement, detection box ensemble, denoiser
ensemble, super-resolution models, and transfer learning, our model achieves
excellent performance on the CDCOCO test set. Our denoising detection model can
denoise and repair distorted images, making the model useful in a variety of
real-world scenarios and environments.",http://arxiv.org/pdf/2309.14998v1
2309.14997v1,cs.CV,IAIFNet: An Illumination-Aware Infrared and Visible Image Fusion Network,2023-09-26 15:12:29+00:00,"Infrared and visible image fusion (IVIF) is used to generate fusion images
with comprehensive features of both images, which is beneficial for downstream
vision tasks. However, current methods rarely consider the illumination
condition in low-light environments, and the targets in the fused images are
often not prominent. To address the above issues, we propose an
Illumination-Aware Infrared and Visible Image Fusion Network, named as IAIFNet.
In our framework, an illumination enhancement network first estimates the
incident illumination maps of input images. Afterwards, with the help of
proposed adaptive differential fusion module (ADFM) and salient target aware
module (STAM), an image fusion network effectively integrates the salient
features of the illumination-enhanced infrared and visible images into a fusion
image of high visual quality. Extensive experimental results verify that our
method outperforms five state-of-the-art methods of fusing infrared and visible
images.",http://arxiv.org/pdf/2309.14997v1
2309.14996v1,cs.DC,Implementation-Oblivious Transparent Checkpoint-Restart for MPI,2023-09-26 15:11:33+00:00,"This work presents experience with traditional use cases of checkpointing on
a novel platform. A single codebase (MANA) transparently checkpoints production
workloads for major available MPI implementations: ""develop once, run
everywhere"". The new platform enables application developers to compile their
application against any of the available standards-compliant MPI
implementations, and test each MPI implementation according to performance or
other features.",http://arxiv.org/pdf/2309.14996v1
2309.14991v1,cs.CV,Robust Sequential DeepFake Detection,2023-09-26 15:01:43+00:00,"Since photorealistic faces can be readily generated by facial manipulation
technologies nowadays, potential malicious abuse of these technologies has
drawn great concerns. Numerous deepfake detection methods are thus proposed.
However, existing methods only focus on detecting one-step facial manipulation.
As the emergence of easy-accessible facial editing applications, people can
easily manipulate facial components using multi-step operations in a sequential
manner. This new threat requires us to detect a sequence of facial
manipulations, which is vital for both detecting deepfake media and recovering
original faces afterwards. Motivated by this observation, we emphasize the need
and propose a novel research problem called Detecting Sequential DeepFake
Manipulation (Seq-DeepFake). Unlike the existing deepfake detection task only
demanding a binary label prediction, detecting Seq-DeepFake manipulation
requires correctly predicting a sequential vector of facial manipulation
operations. To support a large-scale investigation, we construct the first
Seq-DeepFake dataset, where face images are manipulated sequentially with
corresponding annotations of sequential facial manipulation vectors. Based on
this new dataset, we cast detecting Seq-DeepFake manipulation as a specific
image-to-sequence task and propose a concise yet effective Seq-DeepFake
Transformer (SeqFakeFormer). To better reflect real-world deepfake data
distributions, we further apply various perturbations on the original
Seq-DeepFake dataset and construct the more challenging Sequential DeepFake
dataset with perturbations (Seq-DeepFake-P). To exploit deeper correlation
between images and sequences when facing Seq-DeepFake-P, a dedicated
Seq-DeepFake Transformer with Image-Sequence Reasoning (SeqFakeFormer++) is
devised, which builds stronger correspondence between image-sequence pairs for
more robust Seq-DeepFake detection.",http://arxiv.org/pdf/2309.14991v1
2309.14976v1,cs.CV,MoCaE: Mixture of Calibrated Experts Significantly Improves Object Detection,2023-09-26 14:52:51+00:00,"We propose an extremely simple and highly effective approach to faithfully
combine different object detectors to obtain a Mixture of Experts (MoE) that
has a superior accuracy to the individual experts in the mixture. We find that
naively combining these experts in a similar way to the well-known Deep
Ensembles (DEs), does not result in an effective MoE. We identify the
incompatibility between the confidence score distribution of different
detectors to be the primary reason for such failure cases. Therefore, to
construct the MoE, our proposal is to first calibrate each individual detector
against a target calibration function. Then, filter and refine all the
predictions from different detectors in the mixture. We term this approach as
MoCaE and demonstrate its effectiveness through extensive experiments on object
detection, instance segmentation and rotated object detection tasks.
Specifically, MoCaE improves (i) three strong object detectors on COCO test-dev
by $2.4$ $\mathrm{AP}$ by reaching $59.0$ $\mathrm{AP}$; (ii) instance
segmentation methods on the challenging long-tailed LVIS dataset by $2.3$
$\mathrm{AP}$; and (iii) all existing rotated object detectors by reaching
$82.62$ $\mathrm{AP_{50}}$ on DOTA dataset, establishing a new state-of-the-art
(SOTA). Code will be made public.",http://arxiv.org/pdf/2309.14976v1
2309.14973v1,cond-mat.dis-nn,Linking Network and Neuron-level Correlations by Renormalized Field Theory,2023-09-26 14:46:44+00:00,"It is frequently hypothesized that cortical networks operate close to a
critical point. Advantages of criticality include rich dynamics well-suited for
computation and critical slowing down, which may offer a mechanism for dynamic
memory. However, mean-field approximations, while versatile and popular,
inherently neglect the fluctuations responsible for such critical dynamics.
Thus, a renormalized theory is necessary. We consider the
Sompolinsky-Crisanti-Sommers model which displays a well studied chaotic as
well as a magnetic transition. Based on the analogue of a quantum effective
action, we derive self-consistency equations for the first two renormalized
Greens functions. Their self-consistent solution reveals a coupling between the
population level activity and single neuron heterogeneity. The quantitative
theory explains the population autocorrelation function, the single-unit
autocorrelation function with its multiple temporal scales, and cross
correlations.",http://arxiv.org/pdf/2309.14973v1
2309.14962v1,cs.CV,GridFormer: Towards Accurate Table Structure Recognition via Grid Prediction,2023-09-26 14:29:45+00:00,"All tables can be represented as grids. Based on this observation, we propose
GridFormer, a novel approach for interpreting unconstrained table structures by
predicting the vertex and edge of a grid. First, we propose a flexible table
representation in the form of an MXN grid. In this representation, the vertexes
and edges of the grid store the localization and adjacency information of the
table. Then, we introduce a DETR-style table structure recognizer to
efficiently predict this multi-objective information of the grid in a single
shot. Specifically, given a set of learned row and column queries, the
recognizer directly outputs the vertexes and edges information of the
corresponding rows and columns. Extensive experiments on five challenging
benchmarks which include wired, wireless, multi-merge-cell, oriented, and
distorted tables demonstrate the competitive performance of our model over
other methods.",http://arxiv.org/pdf/2309.14962v1
2309.14958v1,math.SP,Minimum trace norm of real symmetric and Hermitian matrices with zero diagonal,2023-09-26 14:26:46+00:00,"In this paper, we obtain tight lower bounds for the trace norm $\Vert \cdot
\Vert_1$ of some matrices with diagonal zero, in terms of the entry-wise
$l_1$-norm (denoted by $\Vert \cdot \Vert_{(1)}$). It is shown that on the
space of nonzero real symmetric matrices $A$ of order $n$ with diagonal zero,
the minimum value of the quantity $\frac{\Vert A\Vert_1}{\Vert A\Vert_{(1)}}$
is equal to $\frac{2}{n}$. The answer of the similar problem in the space of
Hermitian matrices, is also obtained to be equal to $\tan(\frac{\pi}{2n})$. The
equivalent ""dual"" form of these results, give some upper bounds for the
distance to the nearest diagonal matrix for a given symmetric or Hermitian
matrix, when the distance is computed in the spectral norm.",http://arxiv.org/pdf/2309.14958v1
2309.14949v1,cs.LG,Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with Balanced Normalization,2023-09-26 14:06:26+00:00,"Test-Time Adaptation aims to adapt source domain model to testing data at
inference stage with success demonstrated in adapting to unseen corruptions.
However, these attempts may fail under more challenging real-world scenarios.
Existing works mainly consider real-world test-time adaptation under non-i.i.d.
data stream and continual domain shift. In this work, we first complement the
existing real-world TTA protocol with a globally class imbalanced testing set.
We demonstrate that combining all settings together poses new challenges to
existing methods. We argue the failure of state-of-the-art methods is first
caused by indiscriminately adapting normalization layers to imbalanced testing
data. To remedy this shortcoming, we propose a balanced batchnorm layer to swap
out the regular batchnorm at inference stage. The new batchnorm layer is
capable of adapting without biasing towards majority classes. We are further
inspired by the success of self-training~(ST) in learning from unlabeled data
and adapt ST for test-time adaptation. However, ST alone is prone to over
adaption which is responsible for the poor performance under continual domain
shift. Hence, we propose to improve self-training under continual domain shift
by regularizing model updates with an anchored loss. The final TTA model,
termed as TRIBE, is built upon a tri-net architecture with balanced batchnorm
layers. We evaluate TRIBE on four datasets representing real-world TTA
settings. TRIBE consistently achieves the state-of-the-art performance across
multiple evaluation protocols. The code is available at
\url{https://github.com/Gorilla-Lab-SCUT/TRIBE}.",http://arxiv.org/pdf/2309.14949v1
2309.14941v1,eess.SY,Learning Generative Models for Climbing Aircraft from Radar Data,2023-09-26 13:53:53+00:00,"Accurate trajectory prediction (TP) for climbing aircraft is hampered by the
presence of epistemic uncertainties concerning aircraft operation, which can
lead to significant misspecification between predicted and observed
trajectories. This paper proposes a generative model for climbing aircraft in
which the standard Base of Aircraft Data (BADA) model is enriched by a
functional correction to the thrust that is learned from data. The method
offers three features: predictions of the arrival time with 66.3% less error
when compared to BADA; generated trajectories that are realistic when compared
to test data; and a means of computing confidence bounds for minimal
computational cost.",http://arxiv.org/pdf/2309.14941v1
2309.14936v1,cs.LG,Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives,2023-09-26 13:48:04+00:00,"Machine learning (ML) methods offer a wide range of configurable
hyperparameters that have a significant influence on their performance. While
accuracy is a commonly used performance objective, in many settings, it is not
sufficient. Optimizing the ML models with respect to multiple objectives such
as accuracy, confidence, fairness, calibration, privacy, latency, and memory
consumption is becoming crucial. To that end, hyperparameter optimization, the
approach to systematically optimize the hyperparameters, which is already
challenging for a single objective, is even more challenging for multiple
objectives. In addition, the differences in objective scales, the failures, and
the presence of outlier values in objectives make the problem even harder. We
propose a multi-objective Bayesian optimization (MoBO) algorithm that addresses
these problems through uniform objective normalization and randomized weights
in scalarization. We increase the efficiency of our approach by imposing
constraints on the objective to avoid exploring unnecessary configurations
(e.g., insufficient accuracy). Finally, we leverage an approach to parallelize
the MoBO which results in a 5x speed-up when using 16x more workers.",http://arxiv.org/pdf/2309.14936v1
2309.14934v1,cs.CV,FEC: Three Finetuning-free Methods to Enhance Consistency for Real Image Editing,2023-09-26 13:43:06+00:00,"Text-conditional image editing is a very useful task that has recently
emerged with immeasurable potential. Most current real image editing methods
first need to complete the reconstruction of the image, and then editing is
carried out by various methods based on the reconstruction. Most methods use
DDIM Inversion for reconstruction, however, DDIM Inversion often fails to
guarantee reconstruction performance, i.e., it fails to produce results that
preserve the original image content. To address the problem of reconstruction
failure, we propose FEC, which consists of three sampling methods, each
designed for different editing types and settings. Our three methods of FEC
achieve two important goals in image editing task: 1) ensuring successful
reconstruction, i.e., sampling to get a generated result that preserves the
texture and features of the original real image. 2) these sampling methods can
be paired with many editing methods and greatly improve the performance of
these editing methods to accomplish various editing tasks. In addition, none of
our sampling methods require fine-tuning of the diffusion model or
time-consuming training on large-scale datasets. Hence the cost of time as well
as the use of computer memory and computation can be significantly reduced.",http://arxiv.org/pdf/2309.14934v1
2309.14932v1,cs.CV,Addressing Data Misalignment in Image-LiDAR Fusion on Point Cloud Segmentation,2023-09-26 13:41:30+00:00,"With the advent of advanced multi-sensor fusion models, there has been a
notable enhancement in the performance of perception tasks within in terms of
autonomous driving. Despite these advancements, the challenges persist,
particularly in the fusion of data from cameras and LiDAR sensors. A critial
concern is the accurate alignment of data from these disparate sensors. Our
observations indicate that the projected positions of LiDAR points often
misalign on the corresponding image. Furthermore, fusion models appear to
struggle in accurately segmenting these misaligned points. In this paper, we
would like to address this problem carefully, with a specific focus on the
nuScenes dataset and the SOTA of fusion models 2DPASS, and providing the
possible solutions or potential improvements.",http://arxiv.org/pdf/2309.14932v1
2309.14925v1,gr-qc,Comparing One-loop Gravitational Bremsstrahlung Amplitudes to the Multipolar-Post-Minkowskian Waveform,2023-09-26 13:34:04+00:00,"We compare recent one-loop-level, scattering-amplitude-based, computations of
the classical part of the gravitational bremsstrahlung waveform to the
frequency-domain version of the corresponding
  Multipolar-Post-Minkowskian waveform result. The two waveforms are found to
agree at the Newtonian and first post-Newtonian levels, as well as at the
first-and-a-half post-Newtonian level, i.e. for the leading-order quadrupolar
tail. However, we find that there are significant differences at the
second-and-a-half post-Newtonian level, $O\left( \frac{G^2}{c^5} \right)$, i.e.
when reaching: (i) the first post-Newtonian correction to the linear quadrupole
tail; (ii) Newtonian-level linear tails of higher multipolarity (odd octupole
and even hexadecapole); (iii) radiation-reaction effects on the worldlines; and
(iv) various contributions of cubically nonlinear origin (notably linked to the
quadrupole$\times$ quadrupole$\times$ quadrupole coupling in the wavezone).
These differences are reflected at the sub-sub-sub-leading level in the soft
expansion, $ \sim \omega \ln \omega $, i.e. $O\left(\frac{1}{t^2} \right)$ in
the time domain. Our comparison also shows the importance of not neglecting the
formally $O(\hbar)$ momentum transfers but to consistently refer the waveform
to the averaged momenta $\bar p_a = \frac12 (p_a+p'_a)$. Finally, the first
three terms of the low-frequency expansion of the Multipolar-Post-Minkowskian
waveform are checked to agree with the corresponding classical soft graviton
theorem.",http://arxiv.org/pdf/2309.14925v1
2309.14916v1,cs.CV,PHRIT: Parametric Hand Representation with Implicit Template,2023-09-26 13:22:33+00:00,"We propose PHRIT, a novel approach for parametric hand mesh modeling with an
implicit template that combines the advantages of both parametric meshes and
implicit representations. Our method represents deformable hand shapes using
signed distance fields (SDFs) with part-based shape priors, utilizing a
deformation field to execute the deformation. The model offers efficient
high-fidelity hand reconstruction by deforming the canonical template at
infinite resolution. Additionally, it is fully differentiable and can be easily
used in hand modeling since it can be driven by the skeleton and shape latent
codes. We evaluate PHRIT on multiple downstream tasks, including
skeleton-driven hand reconstruction, shapes from point clouds, and single-view
3D reconstruction, demonstrating that our approach achieves realistic and
immersive hand modeling with state-of-the-art performance.",http://arxiv.org/pdf/2309.14916v1
2309.14912v1,gr-qc,Numerical evolutions of boson stars in Palatini $f(\mathcal{R})$ gravity,2023-09-26 13:14:26+00:00,"We investigate the time evolution of spherically symmetric boson stars in
Palatini $f(\mathcal{R})$ gravity through Numerical Relativity computations.
Employing a novel approach that establishes a correspondence between modified
gravity with scalar matter and General Relativity with modified scalar matter,
we are able to use the techniques of Numerical Relativity to simulate these
systems. Specifically, we focus on the quadratic theory
$f(\mathcal{R})=\mathcal{R}+\xi\mathcal{R}^2$ and compare the obtained
solutions with those in General Relativity, exploring both positive and
negative values of the coupling parameter $\xi$. Our findings reveal that boson
stars in Palatini $f(\mathcal{R})$ gravity exhibit both stable and unstable
evolutions. The latter give rise to three distinct scenarios: migration towards
a stable configuration, complete dispersion, and gravitational collapse leading
to the formation of a baby universe structure.",http://arxiv.org/pdf/2309.14912v1
2309.14910v1,physics.chem-ph,Universal Pairwise Interatomic van der Waals Potentials Based on Quantum Drude Oscillators,2023-09-26 13:10:57+00:00,"Repulsive short-range and attractive long-range van der Waals (vdW) forces
have an appreciable role in the behavior of extended molecular systems. When
using empirical force fields - the most popular computational methods applied
to such systems - vdW forces are typically described by Lennard-Jones-like
potentials, which unfortunately have a limited predictive power. Here, we
present a universal parameterization of a quantum-mechanical vdW potential,
which requires only two free-atom properties - the static dipole polarizability
$\alpha_1$ and the dipole-dipole $C_6$ dispersion coefficient. This is achieved
by deriving the functional form of the potential from the quantum Drude
oscillator (QDO) model, employing scaling laws for the equilibrium distance and
the binding energy as well as applying the microscopic law of corresponding
states. The vdW-QDO potential is shown to be accurate for vdW binding energy
curves, as demonstrated by comparing to ab initio binding curves of 21
noble-gas dimers. The functional form of the vdW-QDO potential has the correct
asymptotic behavior both at zero and infinite distances. In addition, it is
shown that the damped vdW-QDO potential can accurately describe vdW
interactions in dimers consisting of group II elements. Finally, we demonstrate
the applicability of the atom-in-molecule vdW-QDO model for predicting accurate
dispersion energies for molecular systems. The present work makes an important
step towards constructing universal vdW potentials, which could benefit
(bio)molecular computational studies.",http://arxiv.org/pdf/2309.14910v1
2309.14908v1,cs.CV,Face Cartoonisation For Various Poses Using StyleGAN,2023-09-26 13:10:25+00:00,"This paper presents an innovative approach to achieve face cartoonisation
while preserving the original identity and accommodating various poses. Unlike
previous methods in this field that relied on conditional-GANs, which posed
challenges related to dataset requirements and pose training, our approach
leverages the expressive latent space of StyleGAN. We achieve this by
introducing an encoder that captures both pose and identity information from
images and generates a corresponding embedding within the StyleGAN latent
space. By subsequently passing this embedding through a pre-trained generator,
we obtain the desired cartoonised output. While many other approaches based on
StyleGAN necessitate a dedicated and fine-tuned StyleGAN model, our method
stands out by utilizing an already-trained StyleGAN designed to produce
realistic facial images. We show by extensive experimentation how our encoder
adapts the StyleGAN output to better preserve identity when the objective is
cartoonisation.",http://arxiv.org/pdf/2309.14908v1
2309.14896v1,math.KT,Hermitian K-theory of Grassmannians,2023-09-26 12:51:57+00:00,"We compute the additive structure of the Hermitian $K$-theory spectrum of an
even-dimensional Grassmannian over a base field $k$ of characteristic zero in
terms of the Hermitian $K$-theory of $X$, using certain symmetries on Young
diagrams. The result is a direct sum of copies of the $K$-theory of the base
field and copies of the $GW$-theory of the base field, indexed by
\emph{asymmetric} and \emph{symmetric} Young diagrams, respectively.",http://arxiv.org/pdf/2309.14896v1
2309.14893v1,cs.RO,A Passive Variable Impedance Control Strategy with Viscoelastic Parameters Estimation of Soft Tissues for Safe Ultrasonography,2023-09-26 12:51:00+00:00,"In the context of telehealth, robotic approaches have proven a valuable
solution to in-person visits in remote areas, with decreased costs for patients
and infection risks. In particular, in ultrasonography, robots have the
potential to reproduce the skills required to acquire high-quality images while
reducing the sonographer's physical efforts. In this paper, we address the
control of the interaction of the probe with the patient's body, a critical
aspect of ensuring safe and effective ultrasonography. We introduce a novel
approach based on variable impedance control, allowing real-time optimisation
of a compliant controller parameters during ultrasound procedures. This
optimisation is formulated as a quadratic programming problem and incorporates
physical constraints derived from viscoelastic parameter estimations. Safety
and passivity constraints, including an energy tank, are also integrated to
minimise potential risks during human-robot interaction. The proposed method's
efficacy is demonstrated through experiments on a patient dummy torso,
highlighting its potential for achieving safe behaviour and accurate force
control during ultrasound procedures, even in cases of contact loss.",http://arxiv.org/pdf/2309.14893v1
2309.14889v1,cond-mat.mtrl-sci,"Electronic and optical properties of core-shell InAlN nanorods: a comparative study via LDA, LDA-1/2, mBJ and $G_0W_0$ methods",2023-09-26 12:41:00+00:00,"Currently, self-induced InAlN core-shell nanorods enjoy an advanced stage of
accumulation of experimental data from their growth and characterization as
well as a comprehensive understanding of their formation mechanism by the ab
initio modeling based on Synthetic Growth Concept. However, their electronic
and optical properties, on which most of their foreseen applications are
expected to depend, have not been investigated comprehensively. $G_0W_0$ is
currently regarded as a gold-standard methodology with quasi-particle
corrections to calculate electronic properties of materials in general. It is
also the starting point for higher-order methods that study excitonic effects,
such as those based on the Bethe-Salpeter equation. One major drawback of
$G_0W_0$, however, is its computational cost, much higher than
density-functional theory (DFT). Therefore, in many applications, it is highly
desirable to answer the question of how well approaches based on DFT, such as
e. g. LDA, LDA-1/2, and mBJ, can approximately reproduce $G_0W_0$ results with
respect to the electronic and optical properties. Thus, the purpose of the
present paper is to investigate how the DFT-based methodologies LDA, LDA-1/2,
and mBJ can be used as tools to approximate $G_0W_0$ in studies of the
electronic and optical properties of scaled down models of core-shell InAlN
nanorods. For these systems, we observed that band gaps, density of states,
dielectric functions, refractive indexes, absorption and reflectance
coefficients are reasonably well described by LDA-1/2 and mBJ when compared to
$G_0W_0$, however, at a much more favorable computational cost.",http://arxiv.org/pdf/2309.14889v1
2309.14886v1,hep-ph,Two-Loop Amplitude Reduction with HELAC,2023-09-26 12:38:49+00:00,"We discuss recent progress towards extending the Helac framework to the
calculation of two-loop amplitudes. A general algorithm for the automated
computation of two-loop integrands is described. The algorithm covers all the
steps of the computation, from the generation of loop topologies up to the
construction of recursion relations for two loop integrands. Finally, first
steps towards the formulation of a new approach for reducing two-loop
amplitudes to a basis of master integrals are discussed.",http://arxiv.org/pdf/2309.14886v1
2309.14883v1,cs.CV,Locality-preserving Directions for Interpreting the Latent Space of Satellite Image GANs,2023-09-26 12:29:36+00:00,"We present a locality-aware method for interpreting the latent space of
wavelet-based Generative Adversarial Networks (GANs), that can well capture the
large spatial and spectral variability that is characteristic to satellite
imagery. By focusing on preserving locality, the proposed method is able to
decompose the weight-space of pre-trained GANs and recover interpretable
directions that correspond to high-level semantic concepts (such as
urbanization, structure density, flora presence) - that can subsequently be
used for guided synthesis of satellite imagery. In contrast to typically used
approaches that focus on capturing the variability of the weight-space in a
reduced dimensionality space (i.e., based on Principal Component Analysis,
PCA), we show that preserving locality leads to vectors with different angles,
that are more robust to artifacts and can better preserve class information.
Via a set of quantitative and qualitative examples, we further show that the
proposed approach can outperform both baseline geometric augmentations, as well
as global, PCA-based approaches for data synthesis in the context of data
augmentation for satellite scene classification.",http://arxiv.org/pdf/2309.14883v1
2309.14872v1,cs.CV,ITEM3D: Illumination-Aware Directional Texture Editing for 3D Models,2023-09-26 12:01:13+00:00,"Texture editing is a crucial task in 3D modeling that allows users to
automatically manipulate the surface materials of 3D models. However, the
inherent complexity of 3D models and the ambiguous text description lead to the
challenge in this task. To address this challenge, we propose ITEM3D, an
illumination-aware model for automatic 3D object editing according to the text
prompts. Leveraging the diffusion models and the differentiable rendering,
ITEM3D takes the rendered images as the bridge of text and 3D representation,
and further optimizes the disentangled texture and environment map. Previous
methods adopt the absolute editing direction namely score distillation sampling
(SDS) as the optimization objective, which unfortunately results in the noisy
appearance and text inconsistency. To solve the problem caused by the ambiguous
text, we introduce a relative editing direction, an optimization objective
defined by the noise difference between the source and target texts, to release
the semantic ambiguity between the texts and images. Additionally, we gradually
adjust the direction during optimization to further address the unexpected
deviation in the texture domain. Qualitative and quantitative experiments show
that our ITEM3D outperforms the state-of-the-art methods on various 3D objects.
We also perform text-guided relighting to show explicit control over lighting.",http://arxiv.org/pdf/2309.14872v1
2309.14870v1,math.HO,Abstraction boundaries and spec driven development in pure mathematics,2023-09-26 11:59:32+00:00,"In this article we discuss how abstraction boundaries can help tame
complexity in mathematical research, with the help of an interactive theorem
prover. While many of the ideas we present here have been used implicitly by
mathematicians for some time, we argue that the use of an interactive theorem
prover introduces additional qualitative benefits in the implementation of
these ideas.",http://arxiv.org/pdf/2309.14870v1
2309.14865v1,cs.CV,Unsupervised Reconstruction of 3D Human Pose Interactions From 2D Poses Alone,2023-09-26 11:42:56+00:00,"Current unsupervised 2D-3D human pose estimation (HPE) methods do not work in
multi-person scenarios due to perspective ambiguity in monocular images.
Therefore, we present one of the first studies investigating the feasibility of
unsupervised multi-person 2D-3D HPE from just 2D poses alone, focusing on
reconstructing human interactions. To address the issue of perspective
ambiguity, we expand upon prior work by predicting the cameras' elevation angle
relative to the subjects' pelvis. This allows us to rotate the predicted poses
to be level with the ground plane, while obtaining an estimate for the vertical
offset in 3D between individuals. Our method involves independently lifting
each subject's 2D pose to 3D, before combining them in a shared 3D coordinate
system. The poses are then rotated and offset by the predicted elevation angle
before being scaled. This by itself enables us to retrieve an accurate 3D
reconstruction of their poses. We present our results on the CHI3D dataset,
introducing its use for unsupervised 2D-3D pose estimation with three new
quantitative metrics, and establishing a benchmark for future research.",http://arxiv.org/pdf/2309.14865v1
2309.14844v1,physics.chem-ph,Importance profiles. Visualization of basis set superposition errors,2023-09-26 11:20:31+00:00,"Recent developments in fully numerical methods promise interesting
opportunities for new, compact atomic orbital (AO) basis sets that maximize the
overlap to fully numerical reference wave functions, following the pioneering
work of Richardson and coworkers from the early 1960s. Motivated by this
technique, we suggest a way to visualize the importance of AO basis functions
in polyatomic calculations, employing fully numerical calculations at the
complete basis set (CBS) limit: the importance of a normalized AO basis
function $|\alpha\rangle$ centered on some nucleus can be visualized by
projecting $|\alpha\rangle$ on the set of numerically represented occupied
orbitals $|\psi_{i}\rangle$ as
$I(\alpha)=\sum_{i}\langle\alpha|\psi_{i}\rangle\langle\psi_{i}|\alpha\rangle$.
Choosing $\alpha$ to be a continuous parameter describing the orbital basis,
such as the exponent of a Gaussian-type orbital (GTO) or Slater-type orbital
(STO) basis function, one is then able to visualize the importance of various
functions on various centers in various molecules. The proposed visualization
$I(\alpha)$ has the important property $0\leq I(\alpha)\leq1$ which allows
unambiguous interpretation. We exemplify the method with importance profiles
computed for atoms from the first three rows in a set of chemically diverse
diatomic molecules. We find that the method offers a good way to visualize
basis set superposition errors: the non-orthonormality of AO basis functions on
different atomic centers is unambiguously revealed by the importance profiles
computed for the ghost atom in an atomic calculation performed in the numerical
basis set for a diatomic molecule.",http://arxiv.org/pdf/2309.14844v1
2309.14842v1,math.AG,Kappa classes on KSBA spaces,2023-09-26 11:15:37+00:00,"We define kappa classes on moduli spaces of KSBA stable varieties and pairs,
generalizing the Miller-Morita-Mumford classes on moduli of curves, and compute
them in some cases where the virtual fundamental class is known to exist,
including Burniat and Campedelli surfaces. For Campedelli surfaces, an
intermediate step is finding the Chow (same as cohomology) ring of the GIT
quotient $(\mathbb P^2)^7//SL(3)$.",http://arxiv.org/pdf/2309.14842v1
2309.14833v1,hep-ph,Rapidity distribution of pseudo-scalar Higgs boson to $\rm{\textbf{NNLO}_A+\overline{\textbf {NNLL}}}$,2023-09-26 11:02:05+00:00,"We present the differential predictions for the rapidity distribution of
pseudo-scalar Higgs boson through gluon fusion at the LHC. These results are
obtained taking into account the soft-virtual (SV) as well as the next-to-soft
virtual (NSV) resummation effects to next-to-next-to-leading-logarithmic
($\rm{\overline{NNLL}}$) accuracy and matching them to the approximate fixed
order next-to-next-to-leading-order ($\rm{NNLO_A}$) computation. We perform the
resummation in two dimensional Mellin space using our recent formalism
\cite{Ajjath:2020lwb} by limiting ourselves to the contributions only from
gluon-gluon ($gg$) initiated channels. The $\rm{NNLO_A}$ rapidity distribution
of pseudo-scalar Higgs is obtained by applying a ratio method on the NNLO
rapidity distribution of the scalar Higgs boson. We also present the first
analytical results of $\rm{N^3LO}$ rapidity distribution of pseudo-scalar Higgs
at SV+NSV accuracy. The phenomenological impacts of
$\rm{{NNLO}_A+\overline{{NNLL}}}$ predictions for 13 TeV LHC are studied. We
observe that, for $m_A$ =125(700) GeV, the SV+NSV resummation at $\rm{
\overline{NNLL}}$ level brings about 14.76\% (11.48\%) corrections to the
$\rm{NNLO}_A$ results at the central scale value of $\mu_R=\mu_F=m_A$. Further,
we find that the sensitivity to the renormalisation scale gets improved
substantially by the inclusion of NSV resummed predictions at $\rm
\overline{NNLL}$ accuracy.",http://arxiv.org/pdf/2309.14833v1
2309.14824v1,cs.CV,Generalization of pixel-wise phase estimation by CNN and improvement of phase-unwrapping by MRF optimization for one-shot 3D scan,2023-09-26 10:45:04+00:00,"Active stereo technique using single pattern projection, a.k.a. one-shot 3D
scan, have drawn a wide attention from industry, medical purposes, etc. One
severe drawback of one-shot 3D scan is sparse reconstruction. In addition,
since spatial pattern becomes complicated for the purpose of efficient
embedding, it is easily affected by noise, which results in unstable decoding.
To solve the problems, we propose a pixel-wise interpolation technique for
one-shot scan, which is applicable to any types of static pattern if the
pattern is regular and periodic. This is achieved by U-net which is pre-trained
by CG with efficient data augmentation algorithm. In the paper, to further
overcome the decoding instability, we propose a robust correspondence finding
algorithm based on Markov random field (MRF) optimization. We also propose a
shape refinement algorithm based on b-spline and Gaussian kernel interpolation
using explicitly detected laser curves. Experiments are conducted to show the
effectiveness of the proposed method using real data with strong noises and
textures.",http://arxiv.org/pdf/2309.14824v1
2309.14821v1,cs.DC,Expedited Data Transfers for Serverless Clouds,2023-09-26 10:39:59+00:00,"Serverless computing has emerged as a popular cloud deployment paradigm. In
serverless, the developers implement their application as a set of chained
functions that form a workflow in which functions invoke each other. The cloud
providers are responsible for automatically scaling the number of instances for
each function on demand and forwarding the requests in a workflow to the
appropriate function instance. Problematically, today's serverless clouds lack
efficient support for cross-function data transfers in a workflow, preventing
the efficient execution of data-intensive serverless applications. In
production clouds, functions transmit intermediate, i.e., ephemeral, data to
other functions either as part of invocation HTTP requests (i.e., inline) or
via third-party services, such as AWS S3 storage or AWS ElastiCache in-memory
cache. The former approach is restricted to small transfer sizes, while the
latter supports arbitrary transfers but suffers from performance and cost
overheads. This work introduces Expedited Data Transfers (XDT), an
API-preserving high-performance data communication method for serverless that
enables direct function-to-function transfers. With XDT, a trusted component of
the sender function buffers the payload in its memory and sends a secure
reference to the receiver, which is picked by the load balancer and autoscaler
based on the current load. Using the reference, the receiver instance pulls the
transmitted data directly from the sender's memory. XDT is natively compatible
with existing autoscaling infrastructure, preserves function invocation
semantics, is secure, and avoids the cost and performance overheads of using an
intermediate service for data transfers. We prototype our system in
vHive/Knative deployed on a cluster of AWS EC2 nodes, showing that XDT improves
latency, bandwidth, and cost over AWS S3 and ElasticCache.",http://arxiv.org/pdf/2309.14821v1
2309.14820v1,cs.CV,Three-dimensional Tracking of a Large Number of High Dynamic Objects from Multiple Views using Current Statistical Model,2023-09-26 10:36:59+00:00,"Three-dimensional tracking of multiple objects from multiple views has a wide
range of applications, especially in the study of bio-cluster behavior which
requires precise trajectories of research objects. However, there are
significant temporal-spatial association uncertainties when the objects are
similar to each other, frequently maneuver, and cluster in large numbers.
Aiming at such a multi-view multi-object 3D tracking scenario, a current
statistical model based Kalman particle filter (CSKPF) method is proposed
following the Bayesian tracking-while-reconstruction framework. The CSKPF
algorithm predicts the objects' states and estimates the objects' state
covariance by the current statistical model to importance particle sampling
efficiency, and suppresses the measurement noise by the Kalman filter. The
simulation experiments prove that the CSKPF method can improve the tracking
integrity, continuity, and precision compared with the existing constant
velocity based particle filter (CVPF) method. The real experiment on fruitfly
clusters also confirms the effectiveness of the CSKPF method.",http://arxiv.org/pdf/2309.14820v1
2309.14816v1,cs.LG,A Comparative Study of Population-Graph Construction Methods and Graph Neural Networks for Brain Age Regression,2023-09-26 10:30:45+00:00,"The difference between the chronological and biological brain age of a
subject can be an important biomarker for neurodegenerative diseases, thus
brain age estimation can be crucial in clinical settings. One way to
incorporate multimodal information into this estimation is through population
graphs, which combine various types of imaging data and capture the
associations among individuals within a population. In medical imaging,
population graphs have demonstrated promising results, mostly for
classification tasks. In most cases, the graph structure is pre-defined and
remains static during training. However, extracting population graphs is a
non-trivial task and can significantly impact the performance of Graph Neural
Networks (GNNs), which are sensitive to the graph structure. In this work, we
highlight the importance of a meaningful graph construction and experiment with
different population-graph construction methods and their effect on GNN
performance on brain age estimation. We use the homophily metric and graph
visualizations to gain valuable quantitative and qualitative insights on the
extracted graph structures. For the experimental evaluation, we leverage the UK
Biobank dataset, which offers many imaging and non-imaging phenotypes. Our
results indicate that architectures highly sensitive to the graph structure,
such as Graph Convolutional Network (GCN) and Graph Attention Network (GAT),
struggle with low homophily graphs, while other architectures, such as
GraphSage and Chebyshev, are more robust across different homophily ratios. We
conclude that static graph construction approaches are potentially insufficient
for the task of brain age estimation and make recommendations for alternative
research directions.",http://arxiv.org/pdf/2309.14816v1
2309.14806v1,cs.CV,3D printed realistic finger vein phantoms,2023-09-26 10:03:57+00:00,"Finger vein pattern recognition is an emerging biometric with a good
resistance to presentation attacks and low error rates. One problem is that it
is hard to obtain ground truth finger vein patterns from live fingers. In this
paper we propose an advanced method to create finger vein phantoms using 3D
printing where we mimic the optical properties of the various tissues inside
the fingers, like bone, veins and soft tissues using different printing
materials and parameters. We demonstrate that we are able to create finger
phantoms that result in realistic finger vein images and precisely known vein
patterns. These phantoms can be used to develop and evaluate finger vein
extraction and recognition methods. In addition, we show that the finger vein
phantoms can be used to spoof a finger vein recognition system. This paper is
based on the Master's thesis of Rasmus van der Grift.",http://arxiv.org/pdf/2309.14806v1
2309.14801v1,q-bio.PE,Increasing situational awareness through nowcasting of the reproduction number,2023-09-26 09:56:40+00:00,"The time varying reproduction number R is a critical variable for situational
awareness during infectious disease outbreaks, but delays between infection and
reporting hinder its accurate estimation in real time. We propose a nowcasting
method for improving the timeliness and accuracy of R estimates, based on
comparisons of successive versions of surveillance databases. The method was
validated against COVID-19 surveillance data collected in Italy over an
18-month period. Compared to traditional methods, the nowcasted reproduction
number reduced the estimation delay from 13 to 8 days, while maintaining a
better accuracy. Moreover, it allowed anticipating the detection of periods of
epidemic growth by between 6 and 23 days. The method offers a simple and
generally applicable tool to improve situational awareness during an epidemic
outbreak, allowing for informed public health response planning.",http://arxiv.org/pdf/2309.14801v1
2309.14789v1,physics.chem-ph,Prediction of Fluorescence Quantum Yields using the Extended Thawed Gaussian Approximation,2023-09-26 09:37:13+00:00,"Spontaneous emission and internal conversion rates are calculated within
harmonic approximations and compared to results obtained within the
semi-classical extended thawed Gaussian approximation. This is the first
application of the ETGA in the calculation of internal conversion and emission
rates for real molecular systems, namely formaldehyde, fluorobenzene, azulene
and a dicyano-squaraine dye. The viability of the models as black-box tools for
prediction of spontaneous emission and internal conversion rates is assessed.
All calculations were done using a consistent protocol in order to investigate
how different methods perform without previous experimental knowledge.
Contrasting the results with experimental data shows that there are further
improvements required before theoretical predictions of emission and internal
conversion rates can be used as reliable indicator for the photo-luminescence
properties of molecules. We find that the extended thawed Gaussian
approximation performs rather similar to the vertical harmonical model.
Including anharmonicities in the calculation of internal conversion rates has a
moderate effect on the quantitative results in the studied systems. The
electronic structure calculations were done using the B3LYP, PBE0,
$\omega$B97XD and CAM-B3LYP functionals. The choice of the functional does not
appear to be a major limiting factor for a black-box approach, when it comes to
the prediction of radiative and nonradiative rates for organic molecules. The
emission rates are fairly stable with respect to computational parameters, but
the internal conversion rate reveals itself to be highly dependent on the
choice of the spectral lineshape function, particularly the width of the
Lorentzian function, associated with homogeneous broadening.",http://arxiv.org/pdf/2309.14789v1
2309.14786v1,cs.CV,Treating Motion as Option with Output Selection for Unsupervised Video Object Segmentation,2023-09-26 09:34:13+00:00,"Unsupervised video object segmentation (VOS) is a task that aims to detect
the most salient object in a video without external guidance about the object.
To leverage the property that salient objects usually have distinctive
movements compared to the background, recent methods collaboratively use motion
cues extracted from optical flow maps with appearance cues extracted from RGB
images. However, as optical flow maps are usually very relevant to segmentation
masks, the network is easy to be learned overly dependent on the motion cues
during network training. As a result, such two-stream approaches are vulnerable
to confusing motion cues, making their prediction unstable. To relieve this
issue, we design a novel motion-as-option network by treating motion cues as
optional. During network training, RGB images are randomly provided to the
motion encoder instead of optical flow maps, to implicitly reduce motion
dependency of the network. As the learned motion encoder can deal with both RGB
images and optical flow maps, two different predictions can be generated
depending on which source information is used as motion input. In order to
fully exploit this property, we also propose an adaptive output selection
algorithm to adopt optimal prediction result at test time. Our proposed
approach affords state-of-the-art performance on all public benchmark datasets,
even maintaining real-time inference speed.",http://arxiv.org/pdf/2309.14786v1
2309.14782v1,cond-mat.quant-gas,Perturbative computation of thermal characteristics of the Stoner phase transition,2023-09-26 09:31:02+00:00,"We apply the thermal (imaginary time) perturbative expansion to the relevant
effective field theory to compute characteristics of the phase transition to
the ordered state which can occur at low temperatures in the gas of
(nonrelativistic) spin 1/2 fermions interacting through a short-range spin
independent repulsive binary interaction potential. We show how to obtain a
systematic expansion of the system's free energy depending on the densities
$n_+$ and $n_-$ of spin-up and spin-down fermions. In this paper we truncate
this expansion at the second order and determine, by numerically minimizing the
free energy, the equilibrium proportions of $n_+$ and $n_-$ (that is, the
system's polarization) as functions of the temperature, the system's overall
density $n = n_+ + n_-$ and the strength of the interaction.",http://arxiv.org/pdf/2309.14782v1
2309.14775v1,cs.LG,Markov Chain Mirror Descent On Data Federation,2023-09-26 09:18:55+00:00,"Stochastic optimization methods such as mirror descent have wide applications
due to low computational cost. Those methods have been well studied under
assumption of the independent and identical distribution, and usually achieve
sublinear rate of convergence. However, this assumption may be too strong and
unpractical in real application scenarios. Recent researches investigate
stochastic gradient descent when instances are sampled from a Markov chain.
Unfortunately, few results are known for stochastic mirror descent. In the
paper, we propose a new version of stochastic mirror descent termed by MarchOn
in the scenario of the federated learning. Given a distributed network, the
model iteratively travels from a node to one of its neighbours randomly.
Furthermore, we propose a new framework to analyze MarchOn, which yields best
rates of convergence for convex, strongly convex, and non-convex loss. Finally,
we conduct empirical studies to evaluate the convergence of MarchOn, and
validate theoretical results.",http://arxiv.org/pdf/2309.14775v1
2309.14768v1,cs.CV,Multi-Label Feature Selection Using Adaptive and Transformed Relevance,2023-09-26 09:01:38+00:00,"Multi-label learning has emerged as a crucial paradigm in data analysis,
addressing scenarios where instances are associated with multiple class labels
simultaneously. With the growing prevalence of multi-label data across diverse
applications, such as text and image classification, the significance of
multi-label feature selection has become increasingly evident. This paper
presents a novel information-theoretical filter-based multi-label feature
selection, called ATR, with a new heuristic function. Incorporating a
combinations of algorithm adaptation and problem transformation approaches, ATR
ranks features considering individual labels as well as abstract label space
discriminative powers. Our experimental studies encompass twelve benchmarks
spanning various domains, demonstrating the superiority of our approach over
ten state-of-the-art information-theoretical filter-based multi-label feature
selection methods across six evaluation metrics. Furthermore, our experiments
affirm the scalability of ATR for benchmarks characterized by extensive feature
and label spaces. The codes are available at https://github.com/Sadegh28/ATR",http://arxiv.org/pdf/2309.14768v1
2309.14764v1,cs.CV,InvKA: Gait Recognition via Invertible Koopman Autoencoder,2023-09-26 08:53:54+00:00,"Most current gait recognition methods suffer from poor interpretability and
high computational cost. To improve interpretability, we investigate gait
features in the embedding space based on Koopman operator theory. The
transition matrix in this space captures complex kinematic features of gait
cycles, namely the Koopman operator. The diagonal elements of the operator
matrix can represent the overall motion trend, providing a physically
meaningful descriptor. To reduce the computational cost of our algorithm, we
use a reversible autoencoder to reduce the model size and eliminate
convolutional layers to compress its depth, resulting in fewer floating-point
operations. Experimental results on multiple datasets show that our method
reduces computational cost to 1% compared to state-of-the-art methods while
achieving competitive recognition accuracy 98% on non-occlusion datasets.",http://arxiv.org/pdf/2309.14764v1
2309.14755v1,cs.CV,Image Denoising via Style Disentanglement,2023-09-26 08:29:33+00:00,"Image denoising is a fundamental task in low-level computer vision. While
recent deep learning-based image denoising methods have achieved impressive
performance, they are black-box models and the underlying denoising principle
remains unclear. In this paper, we propose a novel approach to image denoising
that offers both clear denoising mechanism and good performance. We view noise
as a type of image style and remove it by incorporating noise-free styles
derived from clean images. To achieve this, we design novel losses and network
modules to extract noisy styles from noisy images and noise-free styles from
clean images. The noise-free style induces low-response activations for noise
features and high-response activations for content features in the feature
space. This leads to the separation of clean contents from noise, effectively
denoising the image. Unlike disentanglement-based image editing tasks that edit
semantic-level attributes using styles, our main contribution lies in editing
pixel-level attributes through global noise-free styles. We conduct extensive
experiments on synthetic noise removal and real-world image denoising datasets
(SIDD and DND), demonstrating the effectiveness of our method in terms of both
PSNR and SSIM metrics. Moreover, we experimentally validate that our method
offers good interpretability.",http://arxiv.org/pdf/2309.14755v1
2309.14753v1,cs.CV,Advanced Volleyball Stats for All Levels: Automatic Setting Tactic Detection and Classification with a Single Camera,2023-09-26 08:29:02+00:00,"This paper presents PathFinder and PathFinderPlus, two novel end-to-end
computer vision frameworks designed specifically for advanced setting strategy
classification in volleyball matches from a single camera view. Our frameworks
combine setting ball trajectory recognition with a novel set trajectory
classifier to generate comprehensive and advanced statistical data. This
approach offers a fresh perspective for in-game analysis and surpasses the
current level of granularity in volleyball statistics. In comparison to
existing methods used in our baseline PathFinder framework, our proposed ball
trajectory detection methodology in PathFinderPlus exhibits superior
performance for classifying setting tactics under various game conditions. This
robustness is particularly advantageous in handling complex game situations and
accommodating different camera angles. Additionally, our study introduces an
innovative algorithm for automatic identification of the opposing team's
right-side (opposite) hitter's current row (front or back) during gameplay,
providing critical insights for tactical analysis. The successful demonstration
of our single-camera system's feasibility and benefits makes high-level
technical analysis accessible to volleyball enthusiasts of all skill levels and
resource availability. Furthermore, the computational efficiency of our system
allows for real-time deployment, enabling in-game strategy analysis and
on-the-spot gameplan adjustments.",http://arxiv.org/pdf/2309.14753v1
2309.14751v1,cs.CV,Text-image guided Diffusion Model for generating Deepfake celebrity interactions,2023-09-26 08:24:37+00:00,"Deepfake images are fast becoming a serious concern due to their realism.
Diffusion models have recently demonstrated highly realistic visual content
generation, which makes them an excellent potential tool for Deepfake
generation. To curb their exploitation for Deepfakes, it is imperative to first
explore the extent to which diffusion models can be used to generate realistic
content that is controllable with convenient prompts. This paper devises and
explores a novel method in that regard. Our technique alters the popular stable
diffusion model to generate a controllable high-quality Deepfake image with
text and image prompts. In addition, the original stable model lacks severely
in generating quality images that contain multiple persons. The modified
diffusion model is able to address this problem, it add input anchor image's
latent at the beginning of inferencing rather than Gaussian random latent as
input. Hence, we focus on generating forged content for celebrity interactions,
which may be used to spread rumors. We also apply Dreambooth to enhance the
realism of our fake images. Dreambooth trains the pairing of center words and
specific features to produce more refined and personalized output images. Our
results show that with the devised scheme, it is possible to create fake visual
content with alarming realism, such that the content can serve as believable
evidence of meetings between powerful political figures.",http://arxiv.org/pdf/2309.14751v1
2309.14745v1,cs.CV,SSPFusion: A Semantic Structure-Preserving Approach for Infrared and Visible Image Fusion,2023-09-26 08:13:32+00:00,"Most existing learning-based infrared and visible image fusion (IVIF) methods
exhibit massive redundant information in the fusion images, i.e., yielding
edge-blurring effect or unrecognizable for object detectors. To alleviate these
issues, we propose a semantic structure-preserving approach for IVIF, namely
SSPFusion. At first, we design a Structural Feature Extractor (SFE) to extract
the structural features of infrared and visible images. Then, we introduce a
multi-scale Structure-Preserving Fusion (SPF) module to fuse the structural
features of infrared and visible images, while maintaining the consistency of
semantic structures between the fusion and source images. Owing to these two
effective modules, our method is able to generate high-quality fusion images
from pairs of infrared and visible images, which can boost the performance of
downstream computer-vision tasks. Experimental results on three benchmarks
demonstrate that our method outperforms eight state-of-the-art image fusion
methods in terms of both qualitative and quantitative evaluations. The code for
our method, along with additional comparison results, will be made available
at: https://github.com/QiaoYang-CV/SSPFUSION.",http://arxiv.org/pdf/2309.14745v1
2309.14737v1,cs.RO,Volumetric Semantically Consistent 3D Panoptic Mapping,2023-09-26 08:03:10+00:00,"We introduce an online 2D-to-3D semantic instance mapping algorithm aimed at
generating comprehensive, accurate, and efficient semantic 3D maps suitable for
autonomous agents in unstructured environments. The proposed approach is based
on a Voxel-TSDF representation used in recent algorithms. It introduces novel
ways of integrating semantic prediction confidence during mapping, producing
semantic and instance-consistent 3D regions. Further improvements are achieved
by graph optimization-based semantic labeling and instance refinement. The
proposed method achieves accuracy superior to the state of the art on public
large-scale datasets, improving on a number of widely used metrics. We also
highlight a downfall in the evaluation of recent studies: using the ground
truth trajectory as input instead of a SLAM-estimated one substantially affects
the accuracy, creating a large gap between the reported results and the actual
performance on real-world data.",http://arxiv.org/pdf/2309.14737v1
2309.14732v1,math.CV,Schwarzian Norm Estimate for Functions in Generalized Robertson Class,2023-09-26 07:48:06+00:00,"Let $\mathcal{A}$ be the class of analytic functions $f$ in the unit disk
$\mathbb{D}=\{z\in\mathbb{C}:|z|<1\}$ with the normalized conditions $f(0)=0$,
$f'(0)=1$. For $-\pi/2<\alpha<\pi/2$ and $0\le \beta<1$, let
$\mathcal{S}_{\alpha}(\beta)$ be the subclass of $\mathcal{A}$ consisting of
functions $f$ that satisfy the relation $${\rm Re\,}
\left\{e^{i\alpha}\left(1+\frac{zf''(z)}{f'(z)}\right)\right\}>\beta\cos{\alpha}\quad\text{for}~z\in\mathbb{D}.$$
In the present study, we will compute the sharp estimate of the pre-Schwarzian
and Schwarzian norms for functions in $\mathcal{S}_{\alpha}(\beta)$.",http://arxiv.org/pdf/2309.14732v1
2309.14722v1,physics.flu-dyn,Physics-informed neural network to augment experimental data: an application to stratified flows,2023-09-26 07:29:42+00:00,"We develop a physics-informed neural network (PINN) to significantly augment
state-of-the-art experimental data and apply it to stratified flows. The PINN
is a fully-connected deep neural network fed with time-resolved,
three-component velocity fields and density fields measured simultaneously in
three dimensions at $Re = O(10^3)$ in a stratified inclined duct experiment.
The PINN enforces incompressibility, the governing equations for momentum and
buoyancy, and the boundary conditions by automatic differentiation. The
physics-constrained, augmented data are output at an increased spatio-temporal
resolution and demonstrate five key results: (i) the elimination of measurement
noise; (ii) the correction of distortion caused by the scanning measurement
technique; (iii) the identification of weak but dynamically important
three-dimensional vortices; (iv) the revision of turbulent energy budgets and
mixing efficiency; and (v) the prediction of the latent pressure field and its
role in the observed Holmboe wave dynamics. These results mark a significant
step forward in furthering the reach of experiments, especially in the context
of turbulence, where accurately computing three-dimensional gradients and
resolving small scales remain enduring challenges.",http://arxiv.org/pdf/2309.14722v1
2309.14715v1,cs.CV,Explaining Deep Face Algorithms through Visualization: A Survey,2023-09-26 07:16:39+00:00,"Although current deep models for face tasks surpass human performance on some
benchmarks, we do not understand how they work. Thus, we cannot predict how it
will react to novel inputs, resulting in catastrophic failures and unwanted
biases in the algorithms. Explainable AI helps bridge the gap, but currently,
there are very few visualization algorithms designed for faces. This work
undertakes a first-of-its-kind meta-analysis of explainability algorithms in
the face domain. We explore the nuances and caveats of adapting general-purpose
visualization algorithms to the face domain, illustrated by computing
visualizations on popular face models. We review existing face explainability
works and reveal valuable insights into the structure and hierarchy of face
networks. We also determine the design considerations for practical face
visualizations accessible to AI practitioners by conducting a user study on the
utility of various explainability algorithms.",http://arxiv.org/pdf/2309.14715v1
2309.14709v1,cs.CV,Bootstrap Diffusion Model Curve Estimation for High Resolution Low-Light Image Enhancement,2023-09-26 07:04:47+00:00,"Learning-based methods have attracted a lot of research attention and led to
significant improvements in low-light image enhancement. However, most of them
still suffer from two main problems: expensive computational cost in high
resolution images and unsatisfactory performance in simultaneous enhancement
and denoising. To address these problems, we propose BDCE, a bootstrap
diffusion model that exploits the learning of the distribution of the curve
parameters instead of the normal-light image itself. Specifically, we adopt the
curve estimation method to handle the high-resolution images, where the curve
parameters are estimated by our bootstrap diffusion model. In addition, a
denoise module is applied in each iteration of curve adjustment to denoise the
intermediate enhanced result of each iteration. We evaluate BDCE on commonly
used benchmark datasets, and extensive experiments show that it achieves
state-of-the-art qualitative and quantitative performance.",http://arxiv.org/pdf/2309.14709v1
2309.14706v1,hep-ph,Three-loop $b\to sγ$ vertex with current-current operators,2023-09-26 07:00:01+00:00,"We compute three-loop vertex corrections to $b\to s\gamma$ induced by
current-current operators. The results are presented as expansions in $m_c/m_b$
with numerical coefficients which allow to cover all relevant values for the
heavy quark masses in different renormalization schemes. Moreover we provide
for the first time analytic results for the next-to-leading order contribution.
Our results present an important building block to the next-to-next-to-leading
order interference contributions of the current-current operators $Q_1$ and
$Q_2$ with the electric dipole operator $Q_7$.",http://arxiv.org/pdf/2309.14706v1
2309.14703v1,quant-ph,In-situ characterization of qubit drive-phase distortions,2023-09-26 06:45:38+00:00,"Reducing errors in quantum gates is critical to the development of quantum
computers. To do so, any distortions in the control signals should be
identified, however, conventional tools are not always applicable when part of
the system is under high vacuum, cryogenic, or microscopic. Here, we
demonstrate a method to detect and compensate for amplitude-dependent phase
changes, using the qubit itself as a probe. The technique is implemented using
a microwave-driven trapped ion qubit, where correcting phase distortions leads
to a three-fold improvement in single-qubit gate error, to attain
state-of-the-art performance benchmarked at $1.6(4)\times 10^{-6}$ error per
Clifford gate.",http://arxiv.org/pdf/2309.14703v1
2309.14700v1,cs.CV,Structure Invariant Transformation for better Adversarial Transferability,2023-09-26 06:31:32+00:00,"Given the severe vulnerability of Deep Neural Networks (DNNs) against
adversarial examples, there is an urgent need for an effective adversarial
attack to identify the deficiencies of DNNs in security-sensitive applications.
As one of the prevalent black-box adversarial attacks, the existing
transfer-based attacks still cannot achieve comparable performance with the
white-box attacks. Among these, input transformation based attacks have shown
remarkable effectiveness in boosting transferability. In this work, we find
that the existing input transformation based attacks transform the input image
globally, resulting in limited diversity of the transformed images. We
postulate that the more diverse transformed images result in better
transferability. Thus, we investigate how to locally apply various
transformations onto the input image to improve such diversity while preserving
the structure of image. To this end, we propose a novel input transformation
based attack, called Structure Invariant Attack (SIA), which applies a random
image transformation onto each image block to craft a set of diverse images for
gradient calculation. Extensive experiments on the standard ImageNet dataset
demonstrate that SIA exhibits much better transferability than the existing
SOTA input transformation based attacks on CNN-based and transformer-based
models, showing its generality and superiority in boosting transferability.
Code is available at https://github.com/xiaosen-wang/SIT.",http://arxiv.org/pdf/2309.14700v1
2309.14698v1,math.FA,A Toeplitz-like operator with rational matrix symbol having poles on the unit circle: Invertibility and Riccati equations,2023-09-26 06:20:06+00:00,"This paper is a continuation of the work on unbounded Toeplitz-like operators
$T_\Om$ with rational matrix symbol $\Om$ initiated in Groenewald et. al
(Complex Anal. Oper. Theory 15, 1(2021)), where a Wiener-Hopf type
factorization of $\Om$ is obtained and used to determine when $T_\Om$ is
Fredholm and compute the Fredholm index in case $T_\Om$ is Fredholm. Due to the
high level of non-uniqueness and complicated form of the Wiener-Hopf type
factorization, it does not appear useful in determining when $T_\Om$ is
invertible. In the present paper we use state space methods to characterize
invertibility of $T_\Om$ in terms of the existence of a stabilizing solution of
an associated nonsymmetric discrete algebraic Riccati equation, which in turn
leads to a pseudo-canonical factorization of $\Om$ and concrete formulas of
$T_\Om^{-1}$.",http://arxiv.org/pdf/2309.14698v1
2309.14696v1,cs.DS,On Deterministically Approximating Total Variation Distance,2023-09-26 06:17:53+00:00,"Total variation distance (TV distance) is an important measure for the
difference between two distributions. Recently, there has been progress in
approximating the TV distance between product distributions: a deterministic
algorithm for a restricted class of product distributions (Bhattacharyya,
Gayen, Meel, Myrisiotis, Pavan and Vinodchandran 2023) and a randomized
algorithm for general product distributions (Feng, Guo, Jerrum and Wang 2023).
We give a deterministic fully polynomial-time approximation algorithm (FPTAS)
for the TV distance between product distributions. Given two product
distributions $\mathbb{P}$ and $\mathbb{Q}$ over $[q]^n$, our algorithm
approximates their TV distance with relative error $\varepsilon$ in time
$O\bigl( \frac{qn^2}{\varepsilon} \log q \log \frac{n}{\varepsilon
\Delta_{\text{TV}}(\mathbb{P},\mathbb{Q}) } \bigr)$.
  Our algorithm is built around two key concepts: 1) The likelihood ratio as a
distribution, which captures sufficient information to compute the TV distance.
2) We introduce a metric between likelihood ratio distributions, called the
minimum total variation distance. Our algorithm computes a sparsified
likelihood ratio distribution that is close to the original one w.r.t. the new
metric. The approximated TV distance can be computed from the sparsified
likelihood ratio.
  Our technique also implies deterministic FPTAS for the TV distance between
Markov chains.",http://arxiv.org/pdf/2309.14696v1
2309.14695v1,math.CA,"Strong Szegő Limit Theorems for Multi-Bordered, Framed, and Multi-Framed Toeplitz Determinants",2023-09-26 06:15:37+00:00,"This work provides the general framework for obtaining Strong Szeg\H{o} Limit
Theorems for multi-bordered, semi-framed, framed, and multi-framed Toeplitz
determinants, extending the results of \cite{BEGIL} beyond the (single)
bordered Toeplitz case. For the two-bordered and also the semi-framed Toeplitz
determinants, we compute the Strong Szeg\H{o} Limit Theorems associated with
certain classes of symbols, and for the $k$-bordered ($k \geq 3$), framed, and
multi-framed Toeplitz determinants we demonstrate the recursive fashion offered
by the Dodgson Condensation identities via which Strong Szeg\H{o} Limit
Theorems can be obtained. One instance of appearance of semi-framed Toeplitz
determinants is in calculations related to the entanglement entropy for
disjoint subsystems in the XX spin chain \cite{JK, BGIKMMV}. In addition, in
the unpublished works of Professor Karl Liechty and Professor Nicholas Witte,
such determinants have found relevance respectively in the study of ensembles
of nonintersecting paths and in the study of off-diagonal correlations of the
anisotropic square-lattice Ising model. Besides the intrinsic mathematical
interest in these structured determinants, the aforementioned applications have
further motivated the study of the present work.",http://arxiv.org/pdf/2309.14695v1
2309.14678v1,cond-mat.mtrl-sci,"Theory of defect-mediated ionic transport in Li$^{+}$, Na$^{+}$ and K$^{+}$ $β$ and $β^{\prime\prime}$ aluminas",2023-09-26 05:05:57+00:00,"Alkali metal $\beta$/$\beta^{\prime\prime}$ aluminas are among the fastest
ionic conductors, yet little is understood about the role of defects in the ion
transport mechanism. Here, we use density functional theory (DFT) to
investigate the crystal structures of $\beta$ and $\beta^{\prime\prime}$
phases, and vacancy and interstitial defects in these materials. We find that
charge transport is likely to be dominated by alkali metal interstitials in
$\beta$-aluminas and by vacancies in $\beta^{\prime\prime}$ aluminas. Lower
bounds for the activation energy for diffusion are found by determining the
minimum energy paths for defect migration. The resulting migration barriers are
lower than the experimental activation energies for conduction in Na $\beta$
and $\beta^{\prime\prime}$ aluminas, suggesting a latent potential for
optimization. The lowest activation energy of about 20 meV is predicted for
correlated vacancy migration in K $\beta^{\prime\prime}$ alumina.",http://arxiv.org/pdf/2309.14678v1
2309.14672v1,gr-qc,Tidal Forces in Kerr-AdS and Grey Galaxies,2023-09-26 04:59:05+00:00,"In a recent paper [arXiv:2305.08922], it has been proposed that the endpoint
of the Kerr-AdS superradiant instability is a Grey Galaxy. The conjectured
solutions are supposed to be made up of a black hole with critical angular
velocity in the centre of AdS, surrounded by a large flat disk of thermal bulk
gas that revolves around the black hole. In the analysis of the proposed
solutions so far, gravitational effects due to the black hole on the thermal
gas have been neglected. A way to estimate these effects is via computing tidal
forces. With this motivation, we study tidal forces on objects moving in the
Kerr-AdS spacetime. To do so, we construct a parallel-transported orthonormal
frame along an arbitrary timelike or null geodesic. We then specialise to the
class of fast rotating geodesics lying in the equatorial plane, and estimate
tidal forces on the gas in the Grey Galaxies, modelling it as a collection of
particles moving on timelike geodesics. We show that the tidal forces are small
(and remain small even in the large mass limit), thereby providing additional
support to the idea that the gas is weakly interacting with the black hole.",http://arxiv.org/pdf/2309.14672v1
2309.14670v1,cs.CV,DONNAv2 -- Lightweight Neural Architecture Search for Vision tasks,2023-09-26 04:48:50+00:00,"With the growing demand for vision applications and deployment across edge
devices, the development of hardware-friendly architectures that maintain
performance during device deployment becomes crucial. Neural architecture
search (NAS) techniques explore various approaches to discover efficient
architectures for diverse learning tasks in a computationally efficient manner.
In this paper, we present the next-generation neural architecture design for
computationally efficient neural architecture distillation - DONNAv2 .
Conventional NAS algorithms rely on a computationally extensive stage where an
accuracy predictor is learned to estimate model performance within search
space. This building of accuracy predictors helps them predict the performance
of models that are not being finetuned. Here, we have developed an elegant
approach to eliminate building the accuracy predictor and extend DONNA to a
computationally efficient setting. The loss metric of individual blocks forming
the network serves as the surrogate performance measure for the sampled models
in the NAS search stage. To validate the performance of DONNAv2 we have
performed extensive experiments involving a range of diverse vision tasks
including classification, object detection, image denoising, super-resolution,
and panoptic perception network (YOLOP). The hardware-in-the-loop experiments
were carried out using the Samsung Galaxy S10 mobile platform. Notably, DONNAv2
reduces the computational cost of DONNA by 10x for the larger datasets.
Furthermore, to improve the quality of NAS search space, DONNAv2 leverages a
block knowledge distillation filter to remove blocks with high inference costs.",http://arxiv.org/pdf/2309.14670v1
2309.14666v1,cs.CV,ZiCo-BC: A Bias Corrected Zero-Shot NAS for Vision Tasks,2023-09-26 04:44:40+00:00,"Zero-Shot Neural Architecture Search (NAS) approaches propose novel
training-free metrics called zero-shot proxies to substantially reduce the
search time compared to the traditional training-based NAS. Despite the success
on image classification, the effectiveness of zero-shot proxies is rarely
evaluated on complex vision tasks such as semantic segmentation and object
detection. Moreover, existing zero-shot proxies are shown to be biased towards
certain model characteristics which restricts their broad applicability. In
this paper, we empirically study the bias of state-of-the-art (SOTA) zero-shot
proxy ZiCo across multiple vision tasks and observe that ZiCo is biased towards
thinner and deeper networks, leading to sub-optimal architectures. To solve the
problem, we propose a novel bias correction on ZiCo, called ZiCo-BC. Our
extensive experiments across various vision tasks (image classification, object
detection and semantic segmentation) show that our approach can successfully
search for architectures with higher accuracy and significantly lower latency
on Samsung Galaxy S10 devices.",http://arxiv.org/pdf/2309.14666v1
2309.14662v1,cs.LG,Tranformer-based classification of user queries for medical consultancy with respect to expert specialisation,2023-09-26 04:36:12+00:00,"The need for skilled medical support is growing in the era of digital
healthcare. This research presents an innovative strategy, utilising the RuBERT
model, for categorising user inquiries in the field of medical consultation
with a focus on expert specialisation. By harnessing the capabilities of
transformers, we fine-tuned the pre-trained RuBERT model on a varied dataset,
which facilitates precise correspondence between queries and particular medical
specialisms. Using a comprehensive dataset, we have demonstrated our approach's
superior performance with an F1-score of over 92%, calculated through both
cross-validation and the traditional split of test and train datasets. Our
approach has shown excellent generalisation across medical domains such as
cardiology, neurology and dermatology. This methodology provides practical
benefits by directing users to appropriate specialists for prompt and targeted
medical advice. It also enhances healthcare system efficiency, reduces
practitioner burden, and improves patient care quality. In summary, our
suggested strategy facilitates the attainment of specific medical knowledge,
offering prompt and precise advice within the digital healthcare field.",http://arxiv.org/pdf/2309.14662v1
2309.14655v1,cs.RO,Probabilistic 3D Multi-Object Cooperative Tracking for Autonomous Driving via Differentiable Multi-Sensor Kalman Filter,2023-09-26 04:14:13+00:00,"Current state-of-the-art autonomous driving vehicles mainly rely on each
individual sensor system to perform perception tasks. Such a framework's
reliability could be limited by occlusion or sensor failure. To address this
issue, more recent research proposes using vehicle-to-vehicle (V2V)
communication to share perception information with others. However, most
relevant works focus only on cooperative detection and leave cooperative
tracking an underexplored research field. A few recent datasets, such as
V2V4Real, provide 3D multi-object cooperative tracking benchmarks. However,
their proposed methods mainly use cooperative detection results as input to a
standard single-sensor Kalman Filter-based tracking algorithm. In their
approach, the measurement uncertainty of different sensors from different
connected autonomous vehicles (CAVs) may not be properly estimated to utilize
the theoretical optimality property of Kalman Filter-based tracking algorithms.
In this paper, we propose a novel 3D multi-object cooperative tracking
algorithm for autonomous driving via a differentiable multi-sensor Kalman
Filter. Our algorithm learns to estimate measurement uncertainty for each
detection that can better utilize the theoretical property of Kalman
Filter-based tracking methods. The experiment results show that our algorithm
improves the tracking accuracy by 17% with only 0.037x communication costs
compared with the state-of-the-art method in V2V4Real.",http://arxiv.org/pdf/2309.14655v1
2309.14652v1,cs.GT,Pricing Personalized Preferences for Privacy Protection in Constant Function Market Makers,2023-09-26 04:12:50+00:00,"Constant function market makers (CFMMs) are a popular decentralized exchange
mechanism and have recently been the subject of much research, but major CFMMs
give traders no privacy. Prior work proposes randomly splitting and shuffling
trades to give some privacy to all users [Chitra et al. 2022], or adding noise
to the market state after each trade and charging a fixed `privacy fee' to all
traders [Frongillo and Waggoner 2018]. In contrast, we propose a noisy CFMM
mechanism where users specify personal privacy requirements and pay
personalized fees. We show that the noise added for privacy protection creates
additional arbitrage opportunities. We call a mechanism priceable if there
exists a privacy fee that always matches the additional arbitrage loss in
expectation. We show that a mechanism is priceable if and only if the noise
added is zero-mean in the asset amount. We also show that priceability and
setting the right fee are necessary for a mechanism to be truthful, and that
this fee is inversely proportional to the CFMM's liquidity.",http://arxiv.org/pdf/2309.14652v1
2309.14649v1,cs.CG,Pattern Formation for Fat Robots with Memory,2023-09-26 03:58:37+00:00,"Given a set of $n\geq 1$ autonomous, anonymous, indistinguishable, silent,
and possibly disoriented mobile unit disk (i.e., fat) robots operating
following Look-Compute-Move cycles in the Euclidean plane, we consider the
Pattern Formation problem: from arbitrary starting positions, the robots must
reposition themselves to form a given target pattern. This problem arises under
obstructed visibility, where a robot cannot see another robot if there is a
third robot on the straight line segment between the two robots. We assume that
a robot's movement cannot be interrupted by an adversary and that robots have a
small $O(1)$-sized memory that they can use to store information, but that
cannot be communicated to the other robots. To solve this problem, we present
an algorithm that works in three steps. First it establishes mutual visibility,
then it elects one robot to be the leader, and finally it forms the required
pattern. The whole algorithm runs in $O(n) + O(q \log n)$ rounds, where $q>0$
is related to leader election, which takes $O(q \log n)$ rounds with
probability at least $1-n^{-q}$. The algorithms are collision-free and do not
require the knowledge of the number of robots.",http://arxiv.org/pdf/2309.14649v1
2309.14641v1,cs.RO,Ambient-Aware LiDAR Odometry in Variable Terrains,2023-09-26 03:44:41+00:00,"The flexibility of Simultaneous Localization and Mapping (SLAM) algorithms in
various environments has consistently been a significant challenge. To address
the issue of LiDAR odometry drift in high-noise settings, integrating
clustering methods to filter out unstable features has become an effective
module of SLAM frameworks. However, reducing the amount of point cloud data can
lead to potential loss of information and possible degeneration. As a result,
this research proposes a LiDAR odometry that can dynamically assess the point
cloud's reliability. The algorithm aims to improve adaptability in diverse
settings by selecting important feature points with sensitivity to the level of
environmental degeneration. Firstly, a fast adaptive Euclidean clustering
algorithm based on range image is proposed, which, combined with depth
clustering, extracts the primary structural points of the environment defined
as ambient skeleton points. Then, the environmental degeneration level is
computed through the dense normal features of the skeleton points, and the
point cloud cleaning is dynamically adjusted accordingly. The algorithm is
validated on the KITTI benchmark and real environments, demonstrating higher
accuracy and robustness in different environments.",http://arxiv.org/pdf/2309.14641v1
2309.14630v1,econ.EM,Free Discontinuity Design: With an Application to the Economic Effects of Internet Shutdowns,2023-09-26 02:49:30+00:00,"Thresholds in treatment assignments can produce discontinuities in outcomes,
revealing causal insights. In many contexts, like geographic settings, these
thresholds are unknown and multivariate. We propose a non-parametric method to
estimate the resulting discontinuities by segmenting the regression surface
into smooth and discontinuous parts. This estimator uses a convex relaxation of
the Mumford-Shah functional, for which we establish identification and
convergence. Using our method, we estimate that an internet shutdown in India
resulted in a reduction of economic activity by over 50%, greatly surpassing
previous estimates and shedding new light on the true cost of such shutdowns
for digital economies globally.",http://arxiv.org/pdf/2309.14630v1
2309.14624v1,gr-qc,Gauge-Invariant Scalar-Induced Gravitational Waves from Physical Observables,2023-09-26 02:26:10+00:00,"This paper discusses the gauge issue touching the gravitational waves induced
at the second order by the scalar modes of cosmological perturbations. These
waves are known to depend on the gauge used for their calculation. In this
paper, we propose a simple method of obtaining physically meaningful
expressions for such scalar-induced gravitational waves at the leading order.
The method is centred on well-defined observables, such as the magnetic part of
the Weyl tensor, or the Cotton tensor of a slicing of spacetime, which vanish
in the background and do not depend linearly on the scalar perturbations.
Generalizing the Stewart-Walker lemma, it is shown that the gravitational waves
contributing to such observables at the second order are automatically
gauge-invariant, even when the observable itself does not vanish at the first
order. In each case, the scalar-induced gravitational waves are related to the
ones computed in the Newtonian gauge, first for a general background, and then
for the particular case of a spacetime dominated by either radiation or cold
matter.",http://arxiv.org/pdf/2309.14624v1
2309.14618v1,cs.GT,"Communication games, sequential equilibrium, and mediators",2023-09-26 02:18:32+00:00,"We consider $k$-resilient sequential equilibria, strategy profiles where no
player in a coalition of at most $k$ players believes that it can increase its
utility by deviating, regardless of its local state. We prove that all
$k$-resilient sequential equilibria that can be implemented with a trusted
mediator can also be implemented without the mediator in a synchronous system
of $n$ players if $n >3k$. In asynchronous systems, where there is no global
notion of time and messages may take arbitrarily long to get to their
recipient, we prove that a $k$-resilient sequential equilibrium with a mediator
can be implemented without the mediator if $n > 4k$. These results match the
lower bounds given by Abraham, Dolev, and Halpern (2008) and Geffner and
Halpern (2023) for implementing a Nash equilibrium without a mediator (which
are easily seen to apply to implementing a sequential equilibrium) and improve
the results of Gerardi, who showed that, in the case that $k=1$, a sequential
equilibrium can be implemented in synchronous systems if $n \ge 5$.",http://arxiv.org/pdf/2309.14618v1
2309.14616v1,cs.CV,NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space,2023-09-26 02:09:52+00:00,"Monocular 3D Semantic Scene Completion (SSC) has garnered significant
attention in recent years due to its potential to predict complex semantics and
geometry shapes from a single image, requiring no 3D inputs. In this paper, we
identify several critical issues in current state-of-the-art methods, including
the Feature Ambiguity of projected 2D features in the ray to the 3D space, the
Pose Ambiguity of the 3D convolution, and the Computation Imbalance in the 3D
convolution across different depth levels. To address these problems, we devise
a novel Normalized Device Coordinates scene completion network (NDC-Scene) that
directly extends the 2D feature map to a Normalized Device Coordinates (NDC)
space, rather than to the world space directly, through progressive restoration
of the dimension of depth with deconvolution operations. Experiment results
demonstrate that transferring the majority of computation from the target 3D
space to the proposed normalized device coordinates space benefits monocular
SSC tasks. Additionally, we design a Depth-Adaptive Dual Decoder to
simultaneously upsample and fuse the 2D and 3D feature maps, further improving
overall performance. Our extensive experiments confirm that the proposed method
consistently outperforms state-of-the-art methods on both outdoor SemanticKITTI
and indoor NYUv2 datasets. Our code are available at
https://github.com/Jiawei-Yao0812/NDCScene.",http://arxiv.org/pdf/2309.14616v1
2309.15096v1,cs.LG,Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs,2023-09-26 17:42:52+00:00,"Recently, theoretical analyses of deep neural networks have broadly focused
on two directions: 1) Providing insight into neural network training by SGD in
the limit of infinite hidden-layer width and infinitesimally small learning
rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2)
Globally optimizing the regularized training objective via cone-constrained
convex reformulations of ReLU networks. The latter research direction also
yielded an alternative formulation of the ReLU network, called a gated ReLU
network, that is globally optimizable via efficient unconstrained convex
programs. In this work, we interpret the convex program for this gated ReLU
network as a Multiple Kernel Learning (MKL) model with a weighted data masking
feature map and establish a connection to the NTK. Specifically, we show that
for a particular choice of mask weights that do not depend on the learning
targets, this kernel is equivalent to the NTK of the gated ReLU network on the
training data. A consequence of this lack of dependence on the targets is that
the NTK cannot perform better than the optimal MKL kernel on the training set.
By using iterative reweighting, we improve the weights induced by the NTK to
obtain the optimal MKL kernel which is equivalent to the solution of the exact
convex reformulation of the gated ReLU network. We also provide several
numerical simulations corroborating our theory. Additionally, we provide an
analysis of the prediction error of the resulting optimal kernel via
consistency results for the group lasso.",http://arxiv.org/pdf/2309.15096v1
2309.15079v1,cs.RO,Towards High Efficient Long-horizon Planning with Expert-guided Motion-encoding Tree Search,2023-09-26 17:19:40+00:00,"Autonomous driving holds promise for increased safety, optimized traffic
management, and a new level of convenience in transportation. While model-based
reinforcement learning approaches such as MuZero enables long-term planning,
the exponentially increase of the number of search nodes as the tree goes
deeper significantly effect the searching efficiency.To deal with this problem,
in this paper we proposed the expert-guided motion-encoding tree search (EMTS)
algorithm. EMTS extends the MuZero algorithm by representing possible motions
with a comprehensive motion primitives latent space and incorporating expert
policies toimprove the searching efficiency. The comprehensive motion
primitives latent space enables EMTS to sample arbitrary trajectories instead
of raw action to reduce the depth of the search tree. And the incorporation of
expert policies guided the search and training phases the EMTS algorithm to
enable early convergence. In the experiment section, the EMTS algorithm is
compared with other four algorithms in three challenging scenarios. The
experiment result verifies the effectiveness and the searching efficiency of
the proposed EMTS algorithm.",http://arxiv.org/pdf/2309.15079v1
2309.15075v1,stat.ML,On Excess Risk Convergence Rates of Neural Network Classifiers,2023-09-26 17:14:10+00:00,"The recent success of neural networks in pattern recognition and
classification problems suggests that neural networks possess qualities
distinct from other more classical classifiers such as SVMs or boosting
classifiers. This paper studies the performance of plug-in classifiers based on
neural networks in a binary classification setting as measured by their excess
risks. Compared to the typical settings imposed in the literature, we consider
a more general scenario that resembles actual practice in two respects: first,
the function class to be approximated includes the Barron functions as a proper
subset, and second, the neural network classifier constructed is the minimizer
of a surrogate loss instead of the $0$-$1$ loss so that gradient descent-based
numerical optimizations can be easily applied. While the class of functions we
consider is quite large that optimal rates cannot be faster than
$n^{-\frac{1}{3}}$, it is a regime in which dimension-free rates are possible
and approximation power of neural networks can be taken advantage of. In
particular, we analyze the estimation and approximation properties of neural
networks to obtain a dimension-free, uniform rate of convergence for the excess
risk. Finally, we show that the rate obtained is in fact minimax optimal up to
a logarithmic factor, and the minimax lower bound shows the effect of the
margin assumption in this regime.",http://arxiv.org/pdf/2309.15075v1
2309.15060v1,eess.SY,Learning-Based Latency-Constrained Fronthaul Compression Optimization in C-RAN,2023-09-26 16:40:47+00:00,"The evolution of wireless mobile networks towards cloudification, where Radio
Access Network (RAN) functions can be hosted at either a central or distributed
locations, offers many benefits like low cost deployment, higher capacity, and
improved hardware utilization. The flexibility in the functional deployment
comes at the cost of stringent fronthaul (FH) capacity and latency
requirements. One possible approach to deal with these rigorous constraints is
to use FH compression techniques. Due to varying FH load, FH compression needs
to be adaptive, i.e., apply more FH compression in high load to meet FH
capacity and latency requirements and less compression in medium and low load
to improve FH utilization and throughput performance. In this paper, a
model-free deep reinforcement learning (DRL) based FH compression (DRL-FC)
framework is proposed that dynamically controls FH compression through various
configuration parameters such as modulation order, precoder granularity, and
precoder weight quantization that affect both FH load and throughput
performance. Simulation results show that DRL-FC exhibits significantly higher
FH utilization (70.2% on average) and throughput than a reference scheme across
different FH load levels. At the same time, the proposed DRL-FC framework is
able to meet the predefined FH latency constraints under various FH loads.",http://arxiv.org/pdf/2309.15060v1
2309.15032v1,stat.ME,SOFARI: High-Dimensional Manifold-Based Inference,2023-09-26 16:01:54+00:00,"Multi-task learning is a widely used technique for harnessing information
from various tasks. Recently, the sparse orthogonal factor regression (SOFAR)
framework, based on the sparse singular value decomposition (SVD) within the
coefficient matrix, was introduced for interpretable multi-task learning,
enabling the discovery of meaningful latent feature-response association
networks across different layers. However, conducting precise inference on the
latent factor matrices has remained challenging due to orthogonality
constraints inherited from the sparse SVD constraint. In this paper, we suggest
a novel approach called high-dimensional manifold-based SOFAR inference
(SOFARI), drawing on the Neyman near-orthogonality inference while
incorporating the Stiefel manifold structure imposed by the SVD constraints. By
leveraging the underlying Stiefel manifold structure, SOFARI provides
bias-corrected estimators for both latent left factor vectors and singular
values, for which we show to enjoy the asymptotic mean-zero normal
distributions with estimable variances. We introduce two SOFARI variants to
handle strongly and weakly orthogonal latent factors, where the latter covers a
broader range of applications. We illustrate the effectiveness of SOFARI and
justify our theoretical results through simulation examples and a real data
application in economic forecasting.",http://arxiv.org/pdf/2309.15032v1
2309.15017v1,cs.SE,Studying the association between Gitcoin's issues and resolving outcomes,2023-09-26 15:36:55+00:00,"The development of open-source software (OSS) projects usually have been
driven through collaborations among contributors and strongly relies on
volunteering. Thus, allocating software practitioners (e.g., contributors) to a
particular task is non-trivial and draws attention away from the development.
Therefore, a number of bug bounty platforms have emerged to address this
problem through bounty rewards. Especially, Gitcoin, a new bounty platform,
introduces a bounty reward mechanism that allows individual issue owners
(backers) to define a reward value using cryptocurrencies rather than using
crowdfunding mechanisms. Although a number of studies have investigated the
phenomenon on bounty platforms, those rely on different bounty reward systems.
Our study thus investigates the association between the Gitcoin bounties and
their outcomes (i.e., success and non-success). We empirically study over 4,000
issues with Gitcoin bounties using statistical analysis and machine learning
techniques. We also conducted a comparative study with the Bountysource
platform to gain insights into the usage of both platforms. Our study
highlights the importance of factors such as the length of the project, issue
description, type of bounty issue, and the bounty value, which are found to be
highly correlated with the outcome of bounty issues. These findings can provide
useful guidance to practitioners.",http://arxiv.org/pdf/2309.15017v1
2309.15002v1,hep-lat,Scalar field Restricted Boltzmann Machine as an ultraviolet regulator,2023-09-26 15:17:43+00:00,"Restricted Boltzmann Machines (RBMs) are well-known tools used in Machine
Learning to learn probability distribution functions from data. We analyse RBMs
with scalar fields on the nodes from the perspective of lattice field theory.
Starting with the simplest case of Gaussian fields, we show that the RBM acts
as an ultraviolet regulator, with the cutoff determined by either the number of
hidden nodes or a model mass parameter. We verify these ideas in the scalar
field case, where the target distribution is known, and explore implications
for cases where it is not known using the MNIST data set. We also demonstrate
that infrared modes are learnt quickest.",http://arxiv.org/pdf/2309.15002v1
2309.14975v1,cs.RO,Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the Wild,2023-09-26 14:48:29+00:00,"While humans can use parts of their arms other than the hands for
manipulations like gathering and supporting, whether robots can effectively
learn and perform the same type of operations remains relatively unexplored. As
these manipulations require joint-level control to regulate the complete poses
of the robots, we develop AirExo, a low-cost, adaptable, and portable dual-arm
exoskeleton, for teleoperation and demonstration collection. As collecting
teleoperated data is expensive and time-consuming, we further leverage AirExo
to collect cheap in-the-wild demonstrations at scale. Under our in-the-wild
learning framework, we show that with only 3 minutes of the teleoperated
demonstrations, augmented by diverse and extensive in-the-wild data collected
by AirExo, robots can learn a policy that is comparable to or even better than
one learned from teleoperated demonstrations lasting over 20 minutes.
Experiments demonstrate that our approach enables the model to learn a more
general and robust policy across the various stages of the task, enhancing the
success rates in task completion even with the presence of disturbances.
Project website: https://airexo.github.io/",http://arxiv.org/pdf/2309.14975v1
2309.14957v1,eess.SY,Context-Aware Generative Models for Prediction of Aircraft Ground Tracks,2023-09-26 14:20:09+00:00,"Trajectory prediction (TP) plays an important role in supporting the
decision-making of Air Traffic Controllers (ATCOs). Traditional TP methods are
deterministic and physics-based, with parameters that are calibrated using
aircraft surveillance data harvested across the world. These models are,
therefore, agnostic to the intentions of the pilots and ATCOs, which can have a
significant effect on the observed trajectory, particularly in the lateral
plane. This work proposes a generative method for lateral TP, using
probabilistic machine learning to model the effect of the epistemic uncertainty
arising from the unknown effect of pilot behaviour and ATCO intentions. The
models are trained to be specific to a particular sector, allowing local
procedures such as coordinated entry and exit points to be modelled. A dataset
comprising a week's worth of aircraft surveillance data, passing through a busy
sector of the United Kingdom's upper airspace, was used to train and test the
models. Specifically, a piecewise linear model was used as a functional,
low-dimensional representation of the ground tracks, with its control points
determined by a generative model conditioned on partial context. It was found
that, of the investigated models, a Bayesian Neural Network using the Laplace
approximation was able to generate the most plausible trajectories in order to
emulate the flow of traffic through the sector.",http://arxiv.org/pdf/2309.14957v1
2309.14871v1,astro-ph.SR,Solar Jet Hunter: a citizen science initiative to identify coronal jets in EUV data sets,2023-09-26 11:59:58+00:00,"Context. Solar coronal jets seen in EUV are ubiquitous on the Sun, have been
found in and at the edges of active regions, at the boundaries of coronal
holes, and in the quiet Sun. Jets have various shapes, sizes, brightness,
velocities and duration in time, which complicates their detection by automated
algorithms. So far, solar jets reported in the Heliophysics Event Knowledgebase
(HEK) have been mostly reported by humans looking for them in the data, with
different levels of precision regarding their timing and positions. Aims. We
create a catalogue of solar jets observed in EUV at 304 {\AA} containing
precise and consistent information on the jet timing, position and extent.
Methods. We designed a citizen science project, ""Solar Jet Hunter"", on the
Zooniverse platform, to analyze EUV observations at 304 {\AA} from the Solar
Dynamic Observatory/Atmospheric Imaging Assembly (SDO/AIA). We created movie
strips for regions of the Sun in which jets have been reported in HEK and ask
the volunteers to 1) confirm the presence of at least one jet in the data and
2) report the timing, position and extent of the jet. Results. We report here
the design of the project and the results obtained after the analysis of data
from 2011 to 2016. 365 ""coronal jet"" events from HEK served as input for the
citizen science project, equivalent to more than 120,000 images distributed
into 9,689 ""movie strips"". Classification by the citizen scientists resulted
with only 21% of the data containing a jet, and 883 individual jets being
identified. Conclusions. We demonstrate how citizen science can enhance the
analysis of solar data with the example of Solar Jet Hunter. The catalogue of
jets thus created is publicly available and will enable statistical studies of
jets and related phenomena. This catalogue will also be used as a training set
for machines to learn to recognize jets in further data sets.",http://arxiv.org/pdf/2309.14871v1
2309.14861v1,cs.RO,Seafloor Classification based on an AUV Based Sub-bottom Acoustic Probe Data for Mn-crust survey,2023-09-26 11:37:34+00:00,"The possibility of automatically classifying high frequency sub-bottom
acoustic reflections collected from an Autonomous Underwater Robot is
investigated in this paper. In field surveys of Cobalt-rich Manganese Crusts
(Mn-crusts), existing methods relies on visual confirmation of seafloor from
images and thickness measurements using the sub-bottom probe. Using these
visual classification results as ground truth, an autoencoder is trained to
extract latent features from bundled acoustic reflections. A Support Vector
Machine classifier is then trained to classify the latent space to idetify
seafloor classes. Results from data collected from seafloor at 1500m deep
regions of Mn-crust showed an accuracy of about 70%.",http://arxiv.org/pdf/2309.14861v1
2309.14857v1,cs.LG,Cluster Exploration using Informative Manifold Projections,2023-09-26 11:35:25+00:00,"Dimensionality reduction (DR) is one of the key tools for the visual
exploration of high-dimensional data and uncovering its cluster structure in
two- or three-dimensional spaces. The vast majority of DR methods in the
literature do not take into account any prior knowledge a practitioner may have
regarding the dataset under consideration. We propose a novel method to
generate informative embeddings which not only factor out the structure
associated with different kinds of prior knowledge but also aim to reveal any
remaining underlying structure. To achieve this, we employ a linear combination
of two objectives: firstly, contrastive PCA that discounts the structure
associated with the prior information, and secondly, kurtosis projection
pursuit which ensures meaningful data separation in the obtained embeddings. We
formulate this task as a manifold optimization problem and validate it
empirically across a variety of datasets considering three distinct types of
prior knowledge. Lastly, we provide an automated framework to perform iterative
visual exploration of high-dimensional data.",http://arxiv.org/pdf/2309.14857v1
2309.14837v1,cs.RO,Realtime Motion Generation with Active Perception Using Attention Mechanism for Cooking Robot,2023-09-26 11:05:37+00:00,"To support humans in their daily lives, robots are required to autonomously
learn, adapt to objects and environments, and perform the appropriate actions.
We tackled on the task of cooking scrambled eggs using real ingredients, in
which the robot needs to perceive the states of the egg and adjust stirring
movement in real time, while the egg is heated and the state changes
continuously. In previous works, handling changing objects was found to be
challenging because sensory information includes dynamical, both important or
noisy information, and the modality which should be focused on changes every
time, making it difficult to realize both perception and motion generation in
real time. We propose a predictive recurrent neural network with an attention
mechanism that can weigh the sensor input, distinguishing how important and
reliable each modality is, that realize quick and efficient perception and
motion generation. The model is trained with learning from the demonstration,
and allows the robot to acquire human-like skills. We validated the proposed
technique using the robot, Dry-AIREC, and with our learning model, it could
perform cooking eggs with unknown ingredients. The robot could change the
method of stirring and direction depending on the status of the egg, as in the
beginning it stirs in the whole pot, then subsequently, after the egg started
being heated, it starts flipping and splitting motion targeting specific areas,
although we did not explicitly indicate them.",http://arxiv.org/pdf/2309.14837v1
2309.14830v1,astro-ph.GA,GALAXY CRUISE: Spiral and ring classifications for 700K bright galaxies at z=0.01-0.3,2023-09-26 10:53:42+00:00,"This paper presents a morphology classification catalog of spiral and ring
features of 687,859 magnitude-limited galaxies at $z$=0.01-0.3 (r<20 mag) based
on the Third Public Data Release of the Hyper Suprime-Cam Subaru Strategic
Program. We employ two deep learning classifiers to determine the spiral and
ring structures separately based on GALAXY CRUISE Data Release 1, which is
dedicated to Hyper Suprime-Cam data. The number of spiral and ring galaxies
contain 385,449 and 33,993 sources, respectively, which constitute $56\%$ and
$5\%$ of the sample. A notable result of this study is the construction of a
large sample of ring galaxies utilizing high-quality imaging data delivered by
the Subaru Hyper Suprime-Cam. However, the accurate identification of ring
galaxies remains difficult at a limited seeing resolution. Additionally, we
confirm that most spiral galaxies are located on the star-forming main
sequence, whereas ring galaxies preferentially reside in the green valley at
stellar mass of 1E10.5-11 solar mass. Furthermore, decreasing fractions of
spiral and ring galaxies are observed toward the centers of the galaxy
clusters. The obtained morphology catalog is publicly available on the GALAXY
CRUISE website.",http://arxiv.org/pdf/2309.14830v1
2309.14829v1,cs.RO,A Structured Prediction Approach for Robot Imitation Learning,2023-09-26 10:52:45+00:00,"We propose a structured prediction approach for robot imitation learning from
demonstrations. Among various tools for robot imitation learning, supervised
learning has been observed to have a prominent role. Structured prediction is a
form of supervised learning that enables learning models to operate on output
spaces with complex structures. Through the lens of structured prediction, we
show how robots can learn to imitate trajectories belonging to not only
Euclidean spaces but also Riemannian manifolds. Exploiting ideas from
information theory, we propose a class of loss functions based on the
f-divergence to measure the information loss between the demonstrated and
reproduced probabilistic trajectories. Different types of f-divergence will
result in different policies, which we call imitation modes. Furthermore, our
approach enables the incorporation of spatial and temporal trajectory
modulation, which is necessary for robots to be adaptive to the change in
working conditions. We benchmark our algorithm against state-of-the-art methods
in terms of trajectory reproduction and adaptation. The quantitative evaluation
shows that our approach outperforms other algorithms regarding both accuracy
and efficiency. We also report real-world experimental results on learning
manifold trajectories in a polishing task with a KUKA LWR robot arm,
illustrating the effectiveness of our algorithmic framework.",http://arxiv.org/pdf/2309.14829v1
2309.14822v1,math.DS,OS-net: Orbitally Stable Neural Networks,2023-09-26 10:40:04+00:00,"We introduce OS-net (Orbitally Stable neural NETworks), a new family of
neural network architectures specifically designed for periodic dynamical data.
OS-net is a special case of Neural Ordinary Differential Equations (NODEs) and
takes full advantage of the adjoint method based backpropagation method.
Utilizing ODE theory, we derive conditions on the network weights to ensure
stability of the resulting dynamics. We demonstrate the efficacy of our
approach by applying OS-net to discover the dynamics underlying the R\""{o}ssler
and Sprott's systems, two dynamical systems known for their period doubling
attractors and chaotic behavior.",http://arxiv.org/pdf/2309.14822v1
2309.14792v1,cs.RO,Less Is More: Robust Robot Learning via Partially Observable Multi-Agent Reinforcement Learning,2023-09-26 09:40:35+00:00,"In many multi-agent and high-dimensional robotic tasks, the controller can be
designed in either a centralized or decentralized way. Correspondingly, it is
possible to use either single-agent reinforcement learning (SARL) or
multi-agent reinforcement learning (MARL) methods to learn such controllers.
However, the relationship between these two paradigms remains under-studied in
the literature. This work explores research questions in terms of robustness
and performance of SARL and MARL approaches to the same task, in order to gain
insight into the most suitable methods. We start by analytically showing the
equivalence between these two paradigms under the full-state observation
assumption. Then, we identify a broad subclass of \textit{Dec-POMDP} tasks
where the agents are weakly or partially interacting. In these tasks, we show
that partial observations of each agent are sufficient for near-optimal
decision-making. Furthermore, we propose to exploit such partially observable
MARL to improve the robustness of robots when joint or agent failures occur.
Our experiments on both simulated multi-agent tasks and a real robot task with
a mobile manipulator validate the presented insights and the effectiveness of
the proposed robust robot learning method via partially observable MARL.",http://arxiv.org/pdf/2309.14792v1
2309.14720v1,cs.RO,Learning to Assist Different Wearers in Multitasks: Efficient and Individualized Human-In-the-Loop Adaption Framework for Exoskeleton Robots,2023-09-26 07:26:48+00:00,"One of the typical purposes of using lower-limb exoskeleton robots is to
provide assistance to the wearer by supporting their weight and augmenting
their physical capabilities according to a given task and human motion
intentions. The generalizability of robots across different wearers in multiple
tasks is important to ensure that the robot can provide correct and effective
assistance in actual implementation. However, most lower-limb exoskeleton
robots exhibit only limited generalizability. Therefore, this paper proposes a
human-in-the-loop learning and adaptation framework for exoskeleton robots to
improve their performance in various tasks and for different wearers. To suit
different wearers, an individualized walking trajectory is generated online
using dynamic movement primitives and Bayes optimization. To accommodate
various tasks, a task translator is constructed using a neural network to
generalize a trajectory to more complex scenarios. These generalization
techniques are integrated into a unified variable impedance model, which
regulates the exoskeleton to provide assistance while ensuring safety. In
addition, an anomaly detection network is developed to quantitatively evaluate
the wearer's comfort, which is considered in the trajectory learning procedure
and contributes to the relaxation of conflicts in impedance control. The
proposed framework is easy to implement, because it requires proprioceptive
sensors only to perform and deploy data-efficient learning schemes. This makes
the exoskeleton practical for deployment in complex scenarios, accommodating
different walking patterns, habits, tasks, and conflicts. Experiments and
comparative studies on a lower-limb exoskeleton robot are performed to
demonstrate the effectiveness of the proposed framework.",http://arxiv.org/pdf/2309.14720v1
2309.14710v1,astro-ph.GA,GALAXY CRUISE: Deep Insights into Interacting Galaxies in the Local Universe,2023-09-26 07:05:29+00:00,"We present the first results from GALAXY CRUISE, a community (or citizen)
science project based on data from the Hyper Suprime-Cam Subaru Strategic
Program (HSC-SSP). The current paradigm of galaxy evolution suggests that
galaxies grow hierarchically via mergers, but our observational understanding
of the role of mergers is still limited. The data from HSC-SSP are ideally
suited to improve our understanding with improved identifications of
interacting galaxies thanks to the superb depth and image quality of HSC-SSP.
We have launched a community science project, GALAXY CRUISE, in 2019 and
collected over 2 million independent classifications of 20,686 galaxies at z <
0.2. We first characterize the accuracy of the participants' classifications
and demonstrate that it surpasses previous studies based on shallower imaging
data. We then investigate various aspects of interacting galaxies in detail. We
show that there is a clear sign of enhanced activities of super massive black
holes and star formation in interacting galaxies compared to those in isolated
galaxies. The enhancement seems particularly strong for galaxies undergoing
violent merger. We also show that the mass growth rate inferred from our
results is roughly consistent with the observed evolution of the stellar mass
function. The 2nd season of GALAXY CRUISE is currently under way and we
conclude with future prospects. We make the morphological classification
catalog used in this paper publicly available at the GALAXY CRUISE website,
which will be particularly useful for machine-learning applications.",http://arxiv.org/pdf/2309.14710v1
2309.14648v1,math.OC,Learning the Uncertainty Sets for Control Dynamics via Set Membership: A Non-Asymptotic Analysis,2023-09-26 03:58:06+00:00,"Set-membership estimation is commonly used in adaptive/learning-based control
algorithms that require robustness over the model uncertainty sets, e.g.,
online robustly stabilizing control and robust adaptive model predictive
control. Despite having broad applications, non-asymptotic estimation error
bounds in the stochastic setting are limited. This paper provides such a
non-asymptotic bound on the diameter of the uncertainty sets generated by set
membership estimation on linear dynamical systems under bounded, i.i.d.
disturbances. Further, this result is applied to robust adaptive model
predictive control with uncertainty sets updated by set membership. We
numerically demonstrate the performance of the robust adaptive controller,
which rapidly approaches the performance of the offline optimal model
predictive controller, in comparison with the control design based on least
square estimation's confidence regions.",http://arxiv.org/pdf/2309.14648v1
2309.14645v1,eess.SY,A nonparametric learning framework for nonlinear robust output regulation,2023-09-26 03:54:43+00:00,"This paper proposes a nonparametric learning solution framework for a generic
internal model design of nonlinear robust output regulation. The global robust
output regulation problem for a class of nonlinear systems with output feedback
subject to a nonlinear exosystem can be tackled by constructing a linear
generic internal model, provided that a continuous nonlinear mapping exists. An
explicit continuous nonlinear mapping was constructed recently in [1] under the
assumption that the steady-state generator is linear in the exogenous signal.
We further relax such an assumption to a relaxed assumption that the
steady-state generator is polynomial in the exogenous signal. A nonparametric
learning framework is proposed to solve a linear time-varying equation to make
the nonlinear continuous mapping always exist. With the help of the proposed
framework, the nonlinear robust output regulation problem can be converted into
a robust non-adaptive stabilization problem for the augmented system with
integral Input-to-State Stable (iISS) inverse dynamics. Moreover, a dynamic
gain approach can adaptively raise the gain to a sufficiently large constant to
achieve stabilization without requiring any a priori knowledge of the
uncertainties appearing in the dynamics of the exosystem and the system. We
further apply the nonparametric learning framework to globally reconstruct and
estimate multiple sinusoidal signals with unknown frequencies without using
adaptive techniques. An explicit nonlinear mapping can directly provide the
estimated parameters, which will exponentially converge to the unknown
frequencies. As a result, a feedforward control design is proposed to solve the
output regulation using our nonparametric learning framework.",http://arxiv.org/pdf/2309.14645v1
2309.14615v1,cs.LG,Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents,2023-09-26 02:07:26+00:00,"In recent years, deep reinforcement learning (Deep RL) has been successfully
implemented as a smart agent in many systems such as complex games,
self-driving cars, and chat-bots. One of the interesting use cases of Deep RL
is its application as an automated stock trading agent. In general, any
automated trading agent is prone to manipulations by adversaries in the trading
environment. Thus studying their robustness is vital for their success in
practice. However, typical mechanism to study RL robustness, which is based on
white-box gradient-based adversarial sample generation techniques (like FGSM),
is obsolete for this use case, since the models are protected behind secure
international exchange APIs, such as NASDAQ. In this research, we demonstrate
that a ""gray-box"" approach for attacking a Deep RL-based trading agent is
possible by trading in the same stock market, with no extra access to the
trading agent. In our proposed approach, an adversary agent uses a hybrid Deep
Neural Network as its policy consisting of Convolutional layers and
fully-connected layers. On average, over three simulated trading market
configurations, the adversary policy proposed in this research is able to
reduce the reward values by 214.17%, which results in reducing the potential
profits of the baseline by 139.4%, ensemble method by 93.7%, and an automated
trading software developed by our industrial partner by 85.5%, while consuming
significantly less budget than the victims (427.77%, 187.16%, and 66.97%,
respectively).",http://arxiv.org/pdf/2309.14615v1
2309.14611v1,cs.CV,Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline,2023-09-26 01:42:26+00:00,"Tracking using bio-inspired event cameras has drawn more and more attention
in recent years. Existing works either utilize aligned RGB and event data for
accurate tracking or directly learn an event-based tracker. The first category
needs more cost for inference and the second one may be easily influenced by
noisy events or sparse spatial resolution. In this paper, we propose a novel
hierarchical knowledge distillation framework that can fully utilize
multi-modal / multi-view information during training to facilitate knowledge
transfer, enabling us to achieve high-speed and low-latency visual tracking
during testing by using only event signals. Specifically, a teacher
Transformer-based multi-modal tracking framework is first trained by feeding
the RGB frame and event stream simultaneously. Then, we design a new
hierarchical knowledge distillation strategy which includes pairwise
similarity, feature representation, and response maps-based knowledge
distillation to guide the learning of the student Transformer network.
Moreover, since existing event-based tracking datasets are all low-resolution
($346 \times 260$), we propose the first large-scale high-resolution ($1280
\times 720$) dataset named EventVOT. It contains 1141 videos and covers a wide
range of categories such as pedestrians, vehicles, UAVs, ping pongs, etc.
Extensive experiments on both low-resolution (FE240hz, VisEvent, COESOT), and
our newly proposed high-resolution EventVOT dataset fully validated the
effectiveness of our proposed method. The dataset, evaluation toolkit, and
source code are available on
\url{https://github.com/Event-AHU/EventVOT_Benchmark}",http://arxiv.org/pdf/2309.14611v1
2309.14597v1,cs.LG,Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control,2023-09-26 01:03:54+00:00,"Deep reinforcement learning agents for continuous control are known to
exhibit significant instability in their performance over time. In this work,
we provide a fresh perspective on these behaviors by studying the return
landscape: the mapping between a policy and a return. We find that popular
algorithms traverse noisy neighborhoods of this landscape, in which a single
update to the policy parameters leads to a wide range of returns. By taking a
distributional view of these returns, we map the landscape, characterizing
failure-prone regions of policy space and revealing a hidden dimension of
policy quality. We show that the landscape exhibits surprising structure by
finding simple paths in parameter space which improve the stability of a
policy. To conclude, we develop a distribution-aware procedure which finds such
paths, navigating away from noisy neighborhoods in order to improve the
robustness of a policy. Taken together, our results provide new insight into
the optimization, evaluation, and design of agents.",http://arxiv.org/pdf/2309.14597v1
2309.14596v1,math.ST,Model averaging: A shrinkage perspective,2023-09-26 01:00:36+00:00,"Model averaging (MA), a technique for combining estimators from a set of
candidate models, has attracted increasing attention in machine learning and
statistics. In the existing literature, there is an implicit understanding that
MA can be viewed as a form of shrinkage estimation that draws the response
vector towards the subspaces spanned by the candidate models. This paper
explores this perspective by establishing connections between MA and shrinkage
in a linear regression setting with multiple nested models. We first
demonstrate that the optimal MA estimator is the best linear estimator with
monotone non-increasing weights in a Gaussian sequence model. The Mallows MA,
which estimates weights by minimizing the Mallows' $C_p$, is a variation of the
positive-part Stein estimator. Motivated by these connections, we develop a
novel MA procedure based on a blockwise Stein estimation. Our resulting
Stein-type MA estimator is asymptotically optimal across a broad parameter
space when the variance is known. Numerical results support our theoretical
findings. The connections established in this paper may open up new avenues for
investigating MA from different perspectives. A discussion on some topics for
future research concludes the paper.",http://arxiv.org/pdf/2309.14596v1
2309.14595v1,cs.RO,Neural Informed RRT* with Point-based Network Guidance for Optimal Sampling-based Path Planning,2023-09-26 01:00:02+00:00,"Sampling-based planning algorithms like Rapidly-exploring Random Tree (RRT)
are versatile in solving path planning problems. RRT* offers asymptotical
optimality but requires growing the tree uniformly over the free space, which
leaves room for efficiency improvement. To accelerate convergence, informed
approaches sample states in an ellipsoidal subset of the search space
determined by current path cost during iteration. Learning-based alternatives
model the topology of the search space and infer the states close to the
optimal path to guide planning. We combine the strengths from both sides and
propose Neural Informed RRT* with Point-based Network Guidance. We introduce
Point-based Network to infer the guidance states, and integrate the network
into Informed RRT* for guidance state refinement. We use Neural Connect to
build connectivity of the guidance state set and further boost performance in
challenging planning problems. Our method surpasses previous works in path
planning benchmarks while preserving probabilistic completeness and
asymptotical optimality. We demonstrate the deployment of our method on mobile
robot navigation in the real world.",http://arxiv.org/pdf/2309.14595v1
2309.14594v1,cs.RO,Learning Vision-Based Bipedal Locomotion for Challenging Terrain,2023-09-26 00:59:59+00:00,"Reinforcement learning (RL) for bipedal locomotion has recently demonstrated
robust gaits over moderate terrains using only proprioceptive sensing. However,
such blind controllers will fail in environments where robots must anticipate
and adapt to local terrain, which requires visual perception. In this paper, we
propose a fully-learned system that allows bipedal robots to react to local
terrain while maintaining commanded travel speed and direction. Our approach
first trains a controller in simulation using a heightmap expressed in the
robot's local frame. Next, data is collected in simulation to train a heightmap
predictor, whose input is the history of depth images and robot states. We
demonstrate that with appropriate domain randomization, this approach allows
for successful sim-to-real transfer with no explicit pose estimation and no
fine-tuning using real-world data. To the best of our knowledge, this is the
first example of sim-to-real learning for vision-based bipedal locomotion over
challenging terrains.",http://arxiv.org/pdf/2309.14594v1
2309.14585v1,cs.CV,DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space,2023-09-26 00:15:13+00:00,"This work investigates efficient score-based black-box adversarial attacks
with a high Attack Success Rate (ASR) and good generalizability. We design a
novel attack method based on a Disentangled Feature space, called DifAttack,
which differs significantly from the existing ones operating over the entire
feature space. Specifically, DifAttack firstly disentangles an image's latent
feature into an adversarial feature and a visual feature, where the former
dominates the adversarial capability of an image, while the latter largely
determines its visual appearance. We train an autoencoder for the
disentanglement by using pairs of clean images and their Adversarial Examples
(AEs) generated from available surrogate models via white-box attack methods.
Eventually, DifAttack iteratively optimizes the adversarial feature according
to the query feedback from the victim model until a successful AE is generated,
while keeping the visual feature unaltered. In addition, due to the avoidance
of using surrogate models' gradient information when optimizing AEs for
black-box models, our proposed DifAttack inherently possesses better attack
capability in the open-set scenario, where the training dataset of the victim
model is unknown. Extensive experimental results demonstrate that our method
achieves significant improvements in ASR and query efficiency simultaneously,
especially in the targeted attack and open-set scenarios. The code will be
available at https://github.com/csjunjun/DifAttack.git soon.",http://arxiv.org/pdf/2309.14585v1
2309.14563v1,stat.ML,Towards a statistical theory of data selection under weak supervision,2023-09-25 22:23:27+00:00,"Given a sample of size $N$, it is often useful to select a subsample of
smaller size $n<N$ to be used for statistical estimation or learning. Such a
data selection step is useful to reduce the requirements of data labeling and
the computational complexity of learning. We assume to be given $N$ unlabeled
samples $\{{\boldsymbol x}_i\}_{i\le N}$, and to be given access to a
`surrogate model' that can predict labels $y_i$ better than random guessing.
Our goal is to select a subset of the samples, to be denoted by $\{{\boldsymbol
x}_i\}_{i\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we
use them to train a model via regularized empirical risk minimization.
  By using a mixture of numerical experiments on real and synthetic data, and
mathematical derivations under low- and high- dimensional asymptotics, we show
that: $(i)$~Data selection can be very effective, in particular beating
training on the full sample in some cases; $(ii)$~Certain popular choices in
data selection methods (e.g. unbiased reweighted subsampling, or influence
function-based subsampling) can be substantially suboptimal.",http://arxiv.org/pdf/2309.14563v1
2309.14557v1,cs.LG,Disruption Detection for a Cognitive Digital Supply Chain Twin Using Hybrid Deep Learning,2023-09-25 22:03:09+00:00,"Purpose: Recent disruptive events, such as COVID-19 and Russia-Ukraine
conflict, had a significant impact of global supply chains. Digital supply
chain twins have been proposed in order to provide decision makers with an
effective and efficient tool to mitigate disruption impact. Methods: This paper
introduces a hybrid deep learning approach for disruption detection within a
cognitive digital supply chain twin framework to enhance supply chain
resilience. The proposed disruption detection module utilises a deep
autoencoder neural network combined with a one-class support vector machine
algorithm. In addition, long-short term memory neural network models are
developed to identify the disrupted echelon and predict time-to-recovery from
the disruption effect. Results: The obtained information from the proposed
approach will help decision-makers and supply chain practitioners make
appropriate decisions aiming at minimizing negative impact of disruptive events
based on real-time disruption detection data. The results demonstrate the
trade-off between disruption detection model sensitivity, encountered delay in
disruption detection, and false alarms. This approach has seldom been used in
recent literature addressing this issue.",http://arxiv.org/pdf/2309.14557v1
2309.14541v1,stat.ML,Cluster-based Method for Eavesdropping Identification and Localization in Optical Links,2023-09-25 21:35:44+00:00,"We propose a cluster-based method to detect and locate eavesdropping events
in optical line systems characterized by small power losses. Our findings
indicate that detecting such subtle losses from eavesdropping can be
accomplished solely through optical performance monitoring (OPM) data collected
at the receiver. On the other hand, the localization of such events can be
effectively achieved by leveraging in-line OPM data.",http://arxiv.org/pdf/2309.14541v1
2309.14534v1,cs.HC,"""Teach AI How to Code"": Using Large Language Models as Teachable Agents for Programming Education",2023-09-25 21:20:04+00:00,"This work investigates large language models (LLMs) as teachable agents for
learning by teaching (LBT). LBT with teachable agents helps learners identify
their knowledge gaps and discover new knowledge. However, teachable agents
require expensive programming of subject-specific knowledge. While LLMs as
teachable agents can reduce the cost, LLMs' over-competence as tutees
discourages learners from teaching. We propose a prompting pipeline that
restrains LLMs' competence and makes them initiate ""why"" and ""how"" questions
for effective knowledge-building. We combined these techniques into TeachYou,
an LBT environment for algorithm learning, and AlgoBo, an LLM-based tutee
chatbot that can simulate misconceptions and unawareness prescribed in its
knowledge state. Our technical evaluation confirmed that our prompting pipeline
can effectively configure AlgoBo's problem-solving performance. Through a
between-subject study with 40 algorithm novices, we also observed that AlgoBo's
questions led to knowledge-dense conversations (effect size=0.73). Lastly, we
discuss design implications, cost-efficiency, and personalization of LLM-based
teachable agents.",http://arxiv.org/pdf/2309.14534v1
2309.14531v1,cs.CV,Pixel-Grounded Prototypical Part Networks,2023-09-25 21:09:49+00:00,"Prototypical part neural networks (ProtoPartNNs), namely PROTOPNET and its
derivatives, are an intrinsically interpretable approach to machine learning.
Their prototype learning scheme enables intuitive explanations of the form,
this (prototype) looks like that (testing image patch). But, does this actually
look like that? In this work, we delve into why object part localization and
associated heat maps in past work are misleading. Rather than localizing to
object parts, existing ProtoPartNNs localize to the entire image, contrary to
generated explanatory visualizations. We argue that detraction from these
underlying issues is due to the alluring nature of visualizations and an
over-reliance on intuition. To alleviate these issues, we devise new receptive
field-based architectural constraints for meaningful localization and a
principled pixel space mapping for ProtoPartNNs. To improve interpretability,
we propose additional architectural improvements, including a simplified
classification head. We also make additional corrections to PROTOPNET and its
derivatives, such as the use of a validation set, rather than a test set, to
evaluate generalization during training. Our approach, PIXPNET (Pixel-grounded
Prototypical part Network), is the only ProtoPartNN that truly learns and
localizes to prototypical object parts. We demonstrate that PIXPNET achieves
quantifiably improved interpretability without sacrificing accuracy.",http://arxiv.org/pdf/2309.14531v1
2309.14525v1,cs.CV,Aligning Large Multimodal Models with Factually Augmented RLHF,2023-09-25 20:59:33+00:00,"Large Multimodal Models (LMM) are built across modalities and the
misalignment between two modalities can result in ""hallucination"", generating
textual outputs that are not grounded by the multimodal information in context.
To address the multimodal misalignment issue, we adapt the Reinforcement
Learning from Human Feedback (RLHF) from the text domain to the task of
vision-language alignment, where human annotators are asked to compare two
responses and pinpoint the more hallucinated one, and the vision-language model
is trained to maximize the simulated human rewards. We propose a new alignment
algorithm called Factually Augmented RLHF that augments the reward model with
additional factual information such as image captions and ground-truth
multi-choice options, which alleviates the reward hacking phenomenon in RLHF
and further improves the performance. We also enhance the GPT-4-generated
training data (for vision instruction tuning) with previously available
human-written image-text pairs to improve the general capabilities of our
model. To evaluate the proposed approach in real-world scenarios, we develop a
new evaluation benchmark MMHAL-BENCH with a special focus on penalizing
hallucinations. As the first LMM trained with RLHF, our approach achieves
remarkable improvement on the LLaVA-Bench dataset with the 94% performance
level of the text-only GPT-4 (while previous best methods can only achieve the
87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We
opensource our code, model, data at https://llava-rlhf.github.io.",http://arxiv.org/pdf/2309.14525v1
2309.14518v1,cs.LG,Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels,2023-09-25 20:24:36+00:00,"Time series classification is essential in many fields, such as medicine,
finance, environmental science, and manufacturing, enabling tasks like disease
diagnosis, anomaly detection, and stock price prediction. Machine learning
models like Recurrent Neural Networks and InceptionTime, while successful in
numerous applications, can face scalability limitations due to intensive
training requirements. To address this, random convolutional kernel models such
as Rocket and its derivatives have emerged, simplifying training and achieving
state-of-the-art performance by utilizing a large number of randomly generated
features from time series data. However, due to their random nature, most of
the generated features are redundant or non-informative, adding unnecessary
computational load and compromising generalization. Here, we introduce
Sequential Feature Detachment (SFD) as a method to identify and prune these
non-essential features. SFD uses model coefficients to estimate feature
importance and, unlike previous algorithms, can handle large feature sets
without the need for complex hyperparameter tuning. Testing on the UCR archive
demonstrates that SFD can produce models with $10\%$ of the original features
while improving $0.2\%$ the accuracy on the test set. We also present an
end-to-end procedure for determining an optimal balance between the number of
features and model accuracy, called Detach-ROCKET. When applied to the largest
binary UCR dataset, Detach-ROCKET is capable of reduce model size by $98.9\%$
and increases test accuracy by $0.6\%$.",http://arxiv.org/pdf/2309.14518v1
2309.14512v1,cs.IT,Byzantine-Resilient Federated PCA and Low Rank Matrix Recovery,2023-09-25 20:21:11+00:00,"In this work we consider the problem of estimating the principal subspace
(span of the top r singular vectors) of a symmetric matrix in a federated
setting, when each node has access to estimates of this matrix. We study how to
make this problem Byzantine resilient. We introduce a novel provably
Byzantine-resilient, communication-efficient, and private algorithm, called
Subspace-Median, to solve it. We also study the most natural solution for this
problem, a geometric median based modification of the federated power method,
and explain why it is not useful. We consider two special cases of the
resilient subspace estimation meta-problem - federated principal components
analysis (PCA) and the spectral initialization step of horizontally federated
low rank column-wise sensing (LRCCS) in this work. For both these problems we
show how Subspace Median provides a resilient solution that is also
communication-efficient. Median of Means extensions are developed for both
problems. Extensive simulation experiments are used to corroborate our
theoretical guarantees. Our second contribution is a complete AltGDmin based
algorithm for Byzantine-resilient horizontally federated LRCCS and guarantees
for it. We do this by developing a geometric median of means estimator for
aggregating the partial gradients computed at each node, and using Subspace
Median for initialization.",http://arxiv.org/pdf/2309.14512v1
2309.14509v1,cs.LG,DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models,2023-09-25 20:15:57+00:00,"Computation in a typical Transformer-based large language model (LLM) can be
characterized by batch size, hidden dimension, number of layers, and sequence
length. Until now, system works for accelerating LLM training have focused on
the first three dimensions: data parallelism for batch size, tensor parallelism
for hidden size and pipeline parallelism for model depth or layers. These
widely studied forms of parallelism are not targeted or optimized for long
sequence Transformer models. Given practical application needs for long
sequence LLM, renewed attentions are being drawn to sequence parallelism.
However, existing works in sequence parallelism are constrained by
memory-communication inefficiency, limiting their scalability to long sequence
large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable
and effective methodology for enabling highly efficient and scalable LLM
training with extremely long sequence length. DeepSpeed-Ulysses at its core
partitions input data along the sequence dimension and employs an efficient
all-to-all collective communication for attention computation. Theoretical
communication analysis shows that whereas other methods incur communication
overhead as sequence length increases, DeepSpeed-Ulysses maintains constant
communication volume when sequence length and compute devices are increased
proportionally. Furthermore, experimental evaluations show that
DeepSpeed-Ulysses trains 2.5X faster with 4X longer sequence length than the
existing method SOTA baseline.",http://arxiv.org/pdf/2309.14509v1
2309.14508v1,cs.RO,HEROES: Unreal Engine-based Human and Emergency Robot Operation Education System,2023-09-25 20:14:38+00:00,"Training and preparing first responders and humanitarian robots for Mass
Casualty Incidents (MCIs) often poses a challenge owing to the lack of
realistic and easily accessible test facilities. While such facilities can
offer realistic scenarios post an MCI that can serve training and educational
purposes for first responders and humanitarian robots, they are often hard to
access owing to logistical constraints. To overcome this challenge, we present
HEROES- a versatile Unreal Engine simulator for designing novel training
simulations for humans and emergency robots for such urban search and rescue
operations. The proposed HEROES simulator is capable of generating synthetic
datasets for machine learning pipelines that are used for training robot
navigation. This work addresses the necessity for a comprehensive training
platform in the robotics community, ensuring pragmatic and efficient
preparation for real-world emergency scenarios. The strengths of our simulator
lie in its adaptability, scalability, and ability to facilitate collaboration
between robot developers and first responders, fostering synergy in developing
effective strategies for search and rescue operations in MCIs. We conducted a
preliminary user study with an 81% positive response supporting the ability of
HEROES to generate sufficiently varied environments, and a 78% positive
response affirming the usefulness of the simulation environment of HEROES.",http://arxiv.org/pdf/2309.14508v1
2309.14502v1,cs.LG,Uncertainty Aware Deep Learning for Particle Accelerators,2023-09-25 20:01:57+00:00,"Standard deep learning models for classification and regression applications
are ideal for capturing complex system dynamics. However, their predictions can
be arbitrarily inaccurate when the input samples are not similar to the
training data. Implementation of distance aware uncertainty estimation can be
used to detect these scenarios and provide a level of confidence associated
with their predictions. In this paper, we present results from using Deep
Gaussian Process Approximation (DGPA) methods for errant beam prediction at
Spallation Neutron Source (SNS) accelerator (classification) and we provide an
uncertainty aware surrogate model for the Fermi National Accelerator Lab (FNAL)
Booster Accelerator Complex (regression).",http://arxiv.org/pdf/2309.14502v1
2309.14500v1,cs.CV,Assessment of IBM and NASA's geospatial foundation model in flood inundation mapping,2023-09-25 19:50:47+00:00,"Vision foundation models are a new frontier in GeoAI research because of
their potential to enable powerful image analysis by learning and extracting
important image features from vast amounts of geospatial data. This paper
evaluates the performance of the first-of-its-kind geospatial foundation model,
IBM-NASA's Prithvi, to support a crucial geospatial analysis task: flood
inundation mapping. This model is compared with popular convolutional neural
network and vision transformer-based architectures in terms of mapping accuracy
for flooded areas. A benchmark dataset, Sen1Floods11, is used in the
experiments, and the models' predictability, generalizability, and
transferability are evaluated based on both a test dataset and a dataset that
is completely unseen by the model. Results show the impressive transferability
of the Prithvi model, highlighting its performance advantages in segmenting
flooded areas in previously unseen regions. The findings also suggest areas for
improvement for the Prithvi model in terms of adopting multi-scale
representation learning, developing more end-to-end pipelines for high-level
image analysis tasks, and offering more flexibility in terms of input data
bands.",http://arxiv.org/pdf/2309.14500v1
2309.14495v1,cs.LG,Classifying token frequencies using angular Minkowski $p$-distance,2023-09-25 19:45:11+00:00,"Angular Minkowski $p$-distance is a dissimilarity measure that is obtained by
replacing Euclidean distance in the definition of cosine dissimilarity with
other Minkowski $p$-distances. Cosine dissimilarity is frequently used with
datasets containing token frequencies, and angular Minkowski $p$-distance may
potentially be an even better choice for certain tasks. In a case study based
on the 20-newsgroups dataset, we evaluate clasification performance for
classical weighted nearest neighbours, as well as fuzzy rough nearest
neighbours. In addition, we analyse the relationship between the hyperparameter
$p$, the dimensionality $m$ of the dataset, the number of neighbours $k$, the
choice of weights and the choice of classifier. We conclude that it is possible
to obtain substantially higher classification performance with angular
Minkowski $p$-distance with suitable values for $p$ than with classical cosine
dissimilarity.",http://arxiv.org/pdf/2309.14495v1
2309.14492v1,eess.IV,AiAReSeg: Catheter Detection and Segmentation in Interventional Ultrasound using Transformers,2023-09-25 19:34:12+00:00,"To date, endovascular surgeries are performed using the golden standard of
Fluoroscopy, which uses ionising radiation to visualise catheters and
vasculature. Prolonged Fluoroscopic exposure is harmful for the patient and the
clinician, and may lead to severe post-operative sequlae such as the
development of cancer. Meanwhile, the use of interventional Ultrasound has
gained popularity, due to its well-known benefits of small spatial footprint,
fast data acquisition, and higher tissue contrast images. However, ultrasound
images are hard to interpret, and it is difficult to localise vessels,
catheters, and guidewires within them. This work proposes a solution using an
adaptation of a state-of-the-art machine learning transformer architecture to
detect and segment catheters in axial interventional Ultrasound image
sequences. The network architecture was inspired by the Attention in Attention
mechanism, temporal tracking networks, and introduced a novel 3D segmentation
head that performs 3D deconvolution across time. In order to facilitate
training of such deep learning networks, we introduce a new data synthesis
pipeline that used physics-based catheter insertion simulations, along with a
convolutional ray-casting ultrasound simulator to produce synthetic ultrasound
images of endovascular interventions. The proposed method is validated on a
hold-out validation dataset, thus demonstrated robustness to ultrasound noise
and a wide range of scanning angles. It was also tested on data collected from
silicon-based aorta phantoms, thus demonstrated its potential for translation
from sim-to-real. This work represents a significant step towards safer and
more efficient endovascular surgery using interventional ultrasound.",http://arxiv.org/pdf/2309.14492v1
2309.14485v1,cs.LG,Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond,2023-09-25 19:30:44+00:00,"Joint intent detection and slot filling, which is also termed as joint NLU
(Natural Language Understanding) is invaluable for smart voice assistants.
Recent advancements in this area have been heavily focusing on improving
accuracy using various techniques. Explainability is undoubtedly an important
aspect for deep learning-based models including joint NLU models. Without
explainability, their decisions are opaque to the outside world and hence, have
tendency to lack user trust. Therefore to bridge this gap, we transform the
full joint NLU model to be `inherently' explainable at granular levels without
compromising on accuracy. Further, as we enable the full joint NLU model
explainable, we show that our extension can be successfully used in other
general classification tasks. We demonstrate this using sentiment analysis and
named entity recognition.",http://arxiv.org/pdf/2309.14485v1
2309.14483v1,astro-ph.SR,Unveiling the Potential of Deep Learning Models for Solar Flare Prediction in Near-Limb Regions,2023-09-25 19:30:02+00:00,"This study aims to evaluate the performance of deep learning models in
predicting $\geq$M-class solar flares with a prediction window of 24 hours,
using hourly sampled full-disk line-of-sight (LoS) magnetogram images,
particularly focusing on the often overlooked flare events corresponding to the
near-limb regions (beyond $\pm$70$^{\circ}$ of the solar disk). We trained
three well-known deep learning architectures--AlexNet, VGG16, and ResNet34
using transfer learning and compared and evaluated the overall performance of
our models using true skill statistics (TSS) and Heidke skill score (HSS) and
computed recall scores to understand the prediction sensitivity in central and
near-limb regions for both X- and M-class flares. The following points
summarize the key findings of our study: (1) The highest overall performance
was observed with the AlexNet-based model, which achieved an average
TSS$\sim$0.53 and HSS$\sim$0.37; (2) Further, a spatial analysis of recall
scores disclosed that for the near-limb events, the VGG16- and ResNet34-based
models exhibited superior prediction sensitivity. The best results, however,
were seen with the ResNet34-based model for the near-limb flares, where the
average recall was approximately 0.59 (the recall for X- and M-class was 0.81
and 0.56 respectively) and (3) Our research findings demonstrate that our
models are capable of discerning complex spatial patterns from full-disk
magnetograms and exhibit skill in predicting solar flares, even in the vicinity
of near-limb regions. This ability holds substantial importance for operational
flare forecasting systems.",http://arxiv.org/pdf/2309.14483v1
2309.14482v1,cs.LG,LogGPT: Log Anomaly Detection via GPT,2023-09-25 19:29:50+00:00,"Detecting system anomalies based on log data is important for ensuring the
security and reliability of computer systems. Recently, deep learning models
have been widely used for log anomaly detection. The core idea is to model the
log sequences as natural language and adopt deep sequential models, such as
LSTM or Transformer, to encode the normal patterns in log sequences via
language modeling. However, there is a gap between language modeling and
anomaly detection as the objective of training a sequential model via a
language modeling loss is not directly related to anomaly detection. To fill up
the gap, we propose LogGPT, a novel framework that employs GPT for log anomaly
detection. LogGPT is first trained to predict the next log entry based on the
preceding sequence. To further enhance the performance of LogGPT, we propose a
novel reinforcement learning strategy to finetune the model specifically for
the log anomaly detection task. The experimental results on three datasets show
that LogGPT significantly outperforms existing state-of-the-art approaches.",http://arxiv.org/pdf/2309.14482v1
2309.14468v1,cs.CV,FARSEC: A Reproducible Framework for Automatic Real-Time Vehicle Speed Estimation Using Traffic Cameras,2023-09-25 19:02:40+00:00,"Estimating the speed of vehicles using traffic cameras is a crucial task for
traffic surveillance and management, enabling more optimal traffic flow,
improved road safety, and lower environmental impact. Transportation-dependent
systems, such as for navigation and logistics, have great potential to benefit
from reliable speed estimation. While there is prior research in this area
reporting competitive accuracy levels, their solutions lack reproducibility and
robustness across different datasets. To address this, we provide a novel
framework for automatic real-time vehicle speed calculation, which copes with
more diverse data from publicly available traffic cameras to achieve greater
robustness. Our model employs novel techniques to estimate the length of road
segments via depth map prediction. Additionally, our framework is capable of
handling realistic conditions such as camera movements and different video
stream inputs automatically. We compare our model to three well-known models in
the field using their benchmark datasets. While our model does not set a new
state of the art regarding prediction performance, the results are competitive
on realistic CCTV videos. At the same time, our end-to-end pipeline offers more
consistent results, an easier implementation, and better compatibility. Its
modular structure facilitates reproducibility and future improvements.",http://arxiv.org/pdf/2309.14468v1
2309.14466v1,astro-ph.IM,"The MAPS Adaptive Secondary Mirror: First Light, Laboratory Work, and Achievements",2023-09-25 18:57:34+00:00,"The MMT Adaptive Optics exoPlanet Characterization System (MAPS) is a
comprehensive update to the first generation MMT adaptive optics system
(MMTAO), designed to produce a facility class suite of instruments whose
purpose is to image nearby exoplanets. The system's adaptive secondary mirror
(ASM), although comprised in part of legacy components from the MMTAO ASM,
represents a major leap forward in engineering, structure and function. The
subject of this paper is the design, operation, achievements and technical
issues of the MAPS adaptive secondary mirror. We discuss laboratory preparation
for on-sky engineering runs, the results of those runs and the issues we
discovered, what we learned about those issues in a follow-up period of
laboratory work, and the steps we are taking to mitigate them.",http://arxiv.org/pdf/2309.14466v1
2309.14462v1,eess.AS,"On the Impact of Quantization and Pruning of Self-Supervised Speech Models for Downstream Speech Recognition Tasks ""In-the-Wild''",2023-09-25 18:54:16+00:00,"Recent advances with self-supervised learning have allowed speech recognition
systems to achieve state-of-the-art (SOTA) word error rates (WER) while
requiring only a fraction of the labeled training data needed by its
predecessors. Notwithstanding, while such models achieve SOTA performance in
matched train/test conditions, their performance degrades substantially when
tested in unseen conditions. To overcome this problem, strategies such as data
augmentation and/or domain shift training have been explored. Available models,
however, are still too large to be considered for edge speech applications on
resource-constrained devices, thus model compression tools are needed. In this
paper, we explore the effects that train/test mismatch conditions have on
speech recognition accuracy based on compressed self-supervised speech models.
In particular, we report on the effects that parameter quantization and model
pruning have on speech recognition accuracy based on the so-called robust
wav2vec 2.0 model under noisy, reverberant, and noise-plus-reverberation
conditions.",http://arxiv.org/pdf/2309.14462v1
2309.14458v1,cs.HC,More than Model Documentation: Uncovering Teachers' Bespoke Information Needs for Informed Classroom Integration of ChatGPT,2023-09-25 18:43:07+00:00,"ChatGPT has entered classrooms, but not via the typical route of other
educational technology, which includes comprehensive training, documentation,
and vetting. Consequently, teachers are urgently tasked to assess its
capabilities to determine potential effects on student learning and instruct
their use of ChatGPT. However, it is unclear what support teachers have and
need and whether existing documentation, such as model cards, provides adequate
direction for educators in this new paradigm. By interviewing 22 middle- and
high-school teachers, we connect the discourse on AI transparency and
documentation with educational technology integration, highlighting the
critical information needs of teachers. Our findings reveal that teachers
confront significant information gaps, lacking clarity on exploring ChatGPT's
capabilities for bespoke learning tasks and ensuring its fit with the needs of
diverse learners. As a solution, we propose a framework for interactive model
documentation that empowers teachers to navigate the interplay between
pedagogical and technical knowledge.",http://arxiv.org/pdf/2309.14458v1
2309.14455v1,eess.SP,Skilog: A Smart Sensor System for Performance Analysis and Biofeedback in Ski Jumping,2023-09-25 18:27:29+00:00,"In ski jumping, low repetition rates of jumps limit the effectiveness of
training. Thus, increasing learning rate within every single jump is key to
success. A critical element of athlete training is motor learning, which has
been shown to be accelerated by feedback methods. In particular, a fine-grained
control of the center of gravity in the in-run is essential. This is because
the actual takeoff occurs within a blink of an eye ($\sim$300ms), thus any
unbalanced body posture during the in-run will affect flight. This paper
presents a smart, compact, and energy-efficient wireless sensor system for
real-time performance analysis and biofeedback during ski jumping. The system
operates by gauging foot pressures at three distinct points on the insoles of
the ski boot at 100Hz. Foot pressure data can either be directly sent to
coaches to improve their feedback, or fed into a ML model to give athletes
instantaneous in-action feedback using a vibration motor in the ski boot. In
the biofeedback scenario, foot pressures act as input variables for an
optimized XGBoost model. We achieve a high predictive accuracy of 92.7% for
center of mass predictions (dorsal shift, neutral stand, ventral shift).
Subsequently, we parallelized and fine-tuned our XGBoost model for a RISC-V
based low power parallel processor (GAP9), based on the PULP architecture. We
demonstrate real-time detection and feedback (0.0109ms/inference) using our
on-chip deployment. The proposed smart system is unobtrusive with a slim form
factor (13mm baseboard, 3.2mm antenna) and a lightweight build (26g). Power
consumption analysis reveals that the system's energy-efficient design enables
sustained operation over multiple days (up to 300 hours) without requiring
recharge.",http://arxiv.org/pdf/2309.14455v1
2309.14450v1,cond-mat.mtrl-sci,Learning dislocation dynamics mobility laws from large-scale MD simulations,2023-09-25 18:16:45+00:00,"The computational method of discrete dislocation dynamics (DDD), used as a
coarse-grained model of true atomistic dynamics of lattice dislocations, has
become of powerful tool to study metal plasticity arising from the collective
behavior of dislocations. As a mesoscale approach, motion of dislocations in
the DDD model is prescribed via the mobility law; a function which specifies
how dislocation lines should respond to the driving force. However, the
development of traditional hand-crafted mobility laws can be a cumbersome task
and may involve detrimental simplifications. Here we introduce a
machine-learning (ML) framework to streamline the development of data-driven
mobility laws which are modeled as graph neural networks (GNN) trained on
large-scale Molecular Dynamics (MD) simulations of crystal plasticity. We
illustrate our approach on BCC tungsten and demonstrate that our GNN mobility
implemented in large-scale DDD simulations accurately reproduces the
challenging tension/compression asymmetry observed in ground-truth MD
simulations while correctly predicting the flow stress at lower straining rate
conditions unseen during training, thereby demonstrating the ability of our
method to learn relevant dislocation physics. Our DDD+ML approach opens new
promising avenues to improve fidelity of the DDD model and to incorporate more
complex dislocation motion behaviors in an automated way, providing a faithful
proxy for dislocation dynamics several orders of magnitude faster than
ground-truth MD simulations.",http://arxiv.org/pdf/2309.14450v1
2309.14449v1,astro-ph.GA,Explaining the Chemical Inventory of Orion KL through Machine Learning,2023-09-25 18:14:02+00:00,"The interplay of the chemistry and physics that exists within astrochemically
relevant sources can only be fully appreciated if we can gain a holistic
understanding of their chemical inventories. Previous work by Lee et al. (2021)
demonstrated the capabilities of simple regression models to reproduce the
abundances of the chemical inventory of the Taurus Molecular Cloud 1 (TMC-1),
as well as provide abundance predictions for new candidate molecules. It
remains to be seen, however, to what degree TMC-1 is a ``unicorn'' in
astrochemistry, where the simplicity of its chemistry and physics readily
facilitates characterization with simple machine learning models. Here we
present an extension in chemical complexity to a heavily studied high-mass star
forming region: the Orion Kleinmann-Low (Orion KL) nebula. Unlike TMC-1, Orion
KL is composed of several structurally distinct environments that differ
chemically and kinematically, wherein the column densities of molecules between
these components can have non-linear correlations that cause the unexpected
appearance or even lack of likely species in various environments. This
proof-of-concept study used similar regression models sampled by Lee et al.
(2021) to accurately reproduce the column densities from the XCLASS fitting
program presented in Crockett et al. (2014).",http://arxiv.org/pdf/2309.14449v1
2309.14417v1,hep-ph,Binary Discrimination Through Next-to-Leading Order,2023-09-25 18:00:01+00:00,"Binary discrimination between well-defined signal and background datasets is
a problem of fundamental importance in particle physics. With detailed event
simulation and the advent of extensive deep learning tools, identification of
the likelihood ratio has typically been reserved as a computational problem.
However, this approach can obscure overtraining or excessive sensitivity to
tuned features of the simulation that may not be well-defined theoretically.
Here, we present the first analysis of binary discrimination for signal and
background distributions for which their likelihood ratio is infrared and
collinear safe, and can therefore be calculated order-by-order in perturbation
theory. We present explicit, general formulas for receiver operator
characteristic curves and the area under it through next-to-leading order. As a
demonstration of this formalism, we apply it to discrimination of
highly-boosted Higgs decays from gluon splitting to bottom quarks. Effects at
next-to-leading order are first sensitive to the flow of color in the jet and
significantly modify discrimination performance at leading-order. In the limit
of infinite boost, these events can be perfectly discriminated because only the
gluon will radiate at finite angles from the bottom quarks, and we find that
large effects persist at energies accessible at the Large Hadron Collider.
Next-to-leading order is therefore required to qualitatively understand results
using machine-learning methods.",http://arxiv.org/pdf/2309.14417v1
2309.14419v1,quant-ph,On the expressivity of embedding quantum kernels,2023-09-25 18:00:01+00:00,"One of the most natural connections between quantum and classical machine
learning has been established in the context of kernel methods. Kernel methods
rely on kernels, which are inner products of feature vectors living in large
feature spaces. Quantum kernels are typically evaluated by explicitly
constructing quantum feature states and then taking their inner product, here
called embedding quantum kernels. Since classical kernels are usually evaluated
without using the feature vectors explicitly, we wonder how expressive
embedding quantum kernels are. In this work, we raise the fundamental question:
can all quantum kernels be expressed as the inner product of quantum feature
states? Our first result is positive: Invoking computational universality, we
find that for any kernel function there always exists a corresponding quantum
feature map and an embedding quantum kernel. The more operational reading of
the question is concerned with efficient constructions, however. In a second
part, we formalize the question of universality of efficient embedding quantum
kernels. For shift-invariant kernels, we use the technique of random Fourier
features to show that they are universal within the broad class of all kernels
which allow a variant of efficient Fourier sampling. We then extend this result
to a new class of so-called composition kernels, which we show also contains
projected quantum kernels introduced in recent works. After proving the
universality of embedding quantum kernels for both shift-invariant and
composition kernels, we identify the directions towards new, more exotic, and
unexplored quantum kernel families, for which it still remains open whether
they correspond to efficient embedding quantum kernels.",http://arxiv.org/pdf/2309.14419v1
2309.14406v1,quant-ph,Provable advantages of kernel-based quantum learners and quantum preprocessing based on Grover's algorithm,2023-09-25 18:00:00+00:00,"There is an ongoing effort to find quantum speedups for learning problems.
Recently, [Y. Liu et al., Nat. Phys. $\textbf{17}$, 1013--1017 (2021)] have
proven an exponential speedup for quantum support vector machines by leveraging
the speedup of Shor's algorithm. We expand upon this result and identify a
speedup utilizing Grover's algorithm in the kernel of a support vector machine.
To show the practicality of the kernel structure we apply it to a problem
related to pattern matching, providing a practical yet provable advantage.
Moreover, we show that combining quantum computation in a preprocessing step
with classical methods for classification further improves classifier
performance.",http://arxiv.org/pdf/2309.14406v1
