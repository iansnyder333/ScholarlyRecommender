Id,Category,Title,Published,Abstract,URL,Author
2308.16361v1,cs.AI,Large Language Models as Data Preprocessors,2023-08-30 23:28:43+00:00,"Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's
LLaMA variants, have marked a significant advancement in artificial
intelligence. Trained on vast amounts of text data, LLMs are capable of
understanding and generating human-like text across a diverse range of topics.
This study expands on the applications of LLMs, exploring their potential in
data preprocessing, a critical stage in data mining and analytics applications.
We delve into the applicability of state-of-the-art LLMs such as GPT-3.5,
GPT-4, and Vicuna-13B for error detection, data imputation, schema matching,
and entity matching tasks. Alongside showcasing the inherent capabilities of
LLMs, we highlight their limitations, particularly in terms of computational
expense and inefficiency. We propose an LLM-based framework for data
preprocessing, which integrates cutting-edge prompt engineering techniques,
coupled with traditional methods like contextualization and feature selection,
to improve the performance and efficiency of these models. The effectiveness of
LLMs in data preprocessing is evaluated through an experimental study spanning
12 datasets. GPT-4 emerged as a standout, achieving 100\% accuracy or F1 score
on 4 datasets, suggesting LLMs' immense potential in these tasks. Despite
certain limitations, our study underscores the promise of LLMs in this domain
and anticipates future developments to overcome current hurdles.",http://arxiv.org/pdf/2308.16361v1,"[arxiv.Result.Author('Haochen Zhang'), arxiv.Result.Author('Yuyang Dong'), arxiv.Result.Author('Chuan Xiao'), arxiv.Result.Author('Masafumi Oyamada')]"
2308.15930v1,cs.CL,LLaSM: Large Language and Speech Model,2023-08-30 10:12:39+00:00,"Multi-modal large language models have garnered significant interest
recently. Though, most of the works focus on vision-language multi-modal models
providing strong capabilities in following vision-and-language instructions.
However, we claim that speech is also an important modality through which
humans interact with the world. Hence, it is crucial for a general-purpose
assistant to be able to follow multi-modal speech-and-language instructions. In
this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an
end-to-end trained large multi-modal speech-language model with cross-modal
conversational abilities, capable of following speech-and-language
instructions. Our early experiments show that LLaSM demonstrates a more
convenient and natural way for humans to interact with artificial intelligence.
Specifically, we also release a large Speech Instruction Following dataset
LLaSM-Audio-Instructions. Code and demo are available at
https://github.com/LinkSoul-AI/LLaSM and
https://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions
dataset is available at
https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.",http://arxiv.org/pdf/2308.15930v1,"[arxiv.Result.Author('Yu Shu'), arxiv.Result.Author('Siwei Dong'), arxiv.Result.Author('Guangyao Chen'), arxiv.Result.Author('Wenhao Huang'), arxiv.Result.Author('Ruihua Zhang'), arxiv.Result.Author('Daochen Shi'), arxiv.Result.Author('Qiqi Xiang'), arxiv.Result.Author('Yemin Shi')]"
2308.16557v1,cs.SE,Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing,2023-08-31 08:48:31+00:00,"One of the critical phases in software development is software testing.
Testing helps with identifying potential bugs and reducing maintenance costs.
The goal of automated test generation tools is to ease the development of tests
by suggesting efficient bug-revealing tests. Recently, researchers have
leveraged Large Language Models (LLMs) of code to generate unit tests. While
the code coverage of generated tests was usually assessed, the literature has
acknowledged that the coverage is weakly correlated with the efficiency of
tests in bug detection. To improve over this limitation, in this paper, we
introduce MuTAP for improving the effectiveness of test cases generated by LLMs
in terms of revealing bugs by leveraging mutation testing. Our goal is achieved
by augmenting prompts with surviving mutants, as those mutants highlight the
limitations of test cases in detecting bugs. MuTAP is capable of generating
effective test cases in the absence of natural language descriptions of the
Program Under Test (PUTs). We employ different LLMs within MuTAP and evaluate
their performance on different benchmarks. Our results show that our proposed
method is able to detect up to 28% more faulty human-written code snippets.
Among these, 17% remained undetected by both the current state-of-the-art fully
automated test generation tool (i.e., Pynguin) and zero-shot/few-shot learning
approaches on LLMs. Furthermore, MuTAP achieves a Mutation Score (MS) of 93.57%
on synthetic buggy code, outperforming all other approaches in our evaluation.
Our findings suggest that although LLMs can serve as a useful tool to generate
test cases, they require specific post-processing steps to enhance the
effectiveness of the generated test cases which may suffer from syntactic or
functional errors and may be ineffective in detecting certain types of bugs and
testing corner cases PUTs.",http://arxiv.org/pdf/2308.16557v1,"[arxiv.Result.Author('Arghavan Moradi Dakhel'), arxiv.Result.Author('Amin Nikanjam'), arxiv.Result.Author('Vahid Majdinasab'), arxiv.Result.Author('Foutse Khomh'), arxiv.Result.Author('Michel C. Desmarais')]"
2308.15962v2,cs.RO,WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model,2023-08-30 11:35:21+00:00,"Enabling robots to understand language instructions and react accordingly to
visual perception has been a long-standing goal in the robotics research
community. Achieving this goal requires cutting-edge advances in natural
language processing, computer vision, and robotics engineering. Thus, this
paper mainly investigates the potential of integrating the most recent Large
Language Models (LLMs) and existing visual grounding and robotic grasping
system to enhance the effectiveness of the human-robot interaction. We
introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language
model) as an example of this integration. The system utilizes the LLM of
ChatGPT to summarize the preference object of the users as a target instruction
via the multi-round interactive dialogue. The target instruction is then
forwarded to a visual grounding system for object pose and size estimation,
following which the robot grasps the object accordingly. We deploy this
LLM-empowered system on the physical robot to provide a more user-friendly
interface for the instruction-guided grasping task. The further experimental
results on various real-world scenarios demonstrated the feasibility and
efficacy of our proposed framework. See the project website at:
https://star-uu-wang.github.io/WALL-E/",http://arxiv.org/pdf/2308.15962v2,"[arxiv.Result.Author('Tianyu Wang'), arxiv.Result.Author('Yifan Li'), arxiv.Result.Author('Haitao Lin'), arxiv.Result.Author('Xiangyang Xue'), arxiv.Result.Author('Yanwei Fu')]"
2308.15812v1,cs.LG,Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models,2023-08-30 07:35:32+00:00,"Aligning large language models (LLMs) with human values and intents
critically involves the use of human or AI feedback. While dense feedback
annotations are expensive to acquire and integrate, sparse feedback presents a
structural design choice between ratings (e.g., score Response A on a scale of
1-7) and rankings (e.g., is Response A better than Response B?). In this work,
we analyze the effect of this design choice for the alignment and evaluation of
LLMs. We uncover an inconsistency problem wherein the preferences inferred from
ratings and rankings significantly disagree 60% for both human and AI
annotators. Our subsequent analysis identifies various facets of annotator
biases that explain this phenomena, such as human annotators would rate denser
responses higher while preferring accuracy during pairwise judgments. To our
surprise, we also observe that the choice of feedback protocol also has a
significant effect on the evaluation of aligned LLMs. In particular, we find
that LLMs that leverage rankings data for alignment (say model X) are preferred
over those that leverage ratings data (say model Y), with a rank-based
evaluation protocol (is X/Y's response better than reference response?) but not
with a rating-based evaluation protocol (score Rank X/Y's response on a scale
of 1-7). Our findings thus shed light on critical gaps in methods for
evaluating the real-world utility of language models and their strong
dependence on the feedback protocol used for alignment. Our code and data are
available at https://github.com/Hritikbansal/sparse_feedback.",http://arxiv.org/pdf/2308.15812v1,"[arxiv.Result.Author('Hritik Bansal'), arxiv.Result.Author('John Dang'), arxiv.Result.Author('Aditya Grover')]"
